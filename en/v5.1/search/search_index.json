{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"download/","text":"Siddhi 5.1.0 Download Siddhi Select the appropriate Siddhi distribution for your use case. Siddhi Distribution Daily Build Siddhi Tooling & Siddhi Runner 5.1.0-alpha Based on Siddhi Core 5.1.3 Siddhi Tooling md5 SHA-1 asc source code Siddhi Runner md5 SHA-1 asc source code 5.1.0-m2 Based on Siddhi Core 5.1.2 Siddhi Tooling md5 SHA-1 asc source code Siddhi Runner md5 SHA-1 asc source code 5.1.0-m1 Based on Siddhi Core 5.1.0 Siddhi Tooling md5 SHA-1 asc source code Siddhi Runner md5 SHA-1 asc source code Refer the user guide to use Siddhi as a Local Microservice . Siddhi Docker 5.1.0-alpha Based on Siddhi distribution 5.1.0-alpha and Siddhi Core 5.1.3 . Siddhi Tooling Siddhi Runner - Alpine Siddhi Runner - Ubuntu 5.1.0-m2 Based on Siddhi distribution 5.1.0-m2 and Siddhi Core 5.1.2 . Siddhi Tooling Siddhi Runner - Alpine Siddhi Runner - Ubuntu 5.1.0-m1 Based on Siddhi distribution 5.1.0-m1 and Siddhi Core 5.1.0 . Siddhi Tooling Siddhi Runner - Alpine Siddhi Runner - Ubuntu Refer the user guide to use Siddhi as a Docker Microservice . Siddhi Kubernetes 5.1.0-alpha Based on Siddhi distribution 5.1.0-alpha and Siddhi Core 5.1.3 . Siddhi CRD Siddhi Operator Refer the user guide to use Siddhi as Kubernetes Microservice . 5.1.0-m2 Based on Siddhi distribution 5.1.0-m2 and Siddhi Core 5.1.2 . Siddhi CRD Siddhi Operator Refer the user guide to use Siddhi as Kubernetes Microservice . 5.1.0-m1 Based on Siddhi distribution 5.1.0-m1 and Siddhi Core 5.1.0 . Siddhi CRD Siddhi Operator Refer the user guide to use Siddhi as Kubernetes Microservice . Siddhi Libs 5.1.x Siddhi Core Siddhi Query API Siddhi Query Compiler Siddhi Annotation Refer the user guide to use Siddhi as a Java library . For other Siddhi Versions refer the Download Archives .","title":"Download"},{"location":"download/#siddhi-510-download-siddhi","text":"Select the appropriate Siddhi distribution for your use case.","title":"Siddhi 5.1.0 Download Siddhi"},{"location":"download/#siddhi-distribution","text":"","title":"Siddhi Distribution"},{"location":"download/#daily-build","text":"Siddhi Tooling & Siddhi Runner","title":"Daily Build"},{"location":"download/#510-alpha","text":"Based on Siddhi Core 5.1.3 Siddhi Tooling md5 SHA-1 asc source code Siddhi Runner md5 SHA-1 asc source code","title":"5.1.0-alpha"},{"location":"download/#510-m2","text":"Based on Siddhi Core 5.1.2 Siddhi Tooling md5 SHA-1 asc source code Siddhi Runner md5 SHA-1 asc source code","title":"5.1.0-m2"},{"location":"download/#510-m1","text":"Based on Siddhi Core 5.1.0 Siddhi Tooling md5 SHA-1 asc source code Siddhi Runner md5 SHA-1 asc source code Refer the user guide to use Siddhi as a Local Microservice .","title":"5.1.0-m1"},{"location":"download/#siddhi-docker","text":"","title":"Siddhi Docker"},{"location":"download/#510-alpha_1","text":"Based on Siddhi distribution 5.1.0-alpha and Siddhi Core 5.1.3 . Siddhi Tooling Siddhi Runner - Alpine Siddhi Runner - Ubuntu","title":"5.1.0-alpha"},{"location":"download/#510-m2_1","text":"Based on Siddhi distribution 5.1.0-m2 and Siddhi Core 5.1.2 . Siddhi Tooling Siddhi Runner - Alpine Siddhi Runner - Ubuntu","title":"5.1.0-m2"},{"location":"download/#510-m1_1","text":"Based on Siddhi distribution 5.1.0-m1 and Siddhi Core 5.1.0 . Siddhi Tooling Siddhi Runner - Alpine Siddhi Runner - Ubuntu Refer the user guide to use Siddhi as a Docker Microservice .","title":"5.1.0-m1"},{"location":"download/#siddhi-kubernetes","text":"","title":"Siddhi Kubernetes"},{"location":"download/#510-alpha_2","text":"Based on Siddhi distribution 5.1.0-alpha and Siddhi Core 5.1.3 . Siddhi CRD Siddhi Operator Refer the user guide to use Siddhi as Kubernetes Microservice .","title":"5.1.0-alpha"},{"location":"download/#510-m2_2","text":"Based on Siddhi distribution 5.1.0-m2 and Siddhi Core 5.1.2 . Siddhi CRD Siddhi Operator Refer the user guide to use Siddhi as Kubernetes Microservice .","title":"5.1.0-m2"},{"location":"download/#510-m1_2","text":"Based on Siddhi distribution 5.1.0-m1 and Siddhi Core 5.1.0 . Siddhi CRD Siddhi Operator Refer the user guide to use Siddhi as Kubernetes Microservice .","title":"5.1.0-m1"},{"location":"download/#siddhi-libs","text":"","title":"Siddhi Libs"},{"location":"download/#51x","text":"Siddhi Core Siddhi Query API Siddhi Query Compiler Siddhi Annotation Refer the user guide to use Siddhi as a Java library . For other Siddhi Versions refer the Download Archives .","title":"5.1.x"},{"location":"introduction/","text":"Siddhi Deployment Guide This section provides information on developing and running Siddhi. Siddhi Application A self contained stream processing logic can be written as a Siddhi Application and put together in a single file with .siddhi extension. The stream processing constructs, such as streams and queries, defined within a Siddhi App is not visible even to the other Siddhi Apps running in the same JVM. It is recommended to have different business usecase in separate Siddhi Applications, where it allow users to selectively deploy the applications based on business needs. It is also recommended to move the repeated steam processing logic that exist in multiple Siddhi Applications, such as message retrieval and preprocessing, to a common Siddhi Application, whereby reducing code duplication and improving maintainability. In this case, to pass the events from one Siddhi App to another, configure them using a common topic using In-Memory Sink and In-Memory Source . For writing Siddhi Application using Streaming SQL refer Siddhi Query Guide Execution Environments Siddhi can run in multiple environments as follows. As a Java Library As a Local Microservice As a Docker Microservice As a Kubernetes Microservice As a Python Library (WIP) System Requirements For all execution modes following are the general system requirements. Memory - 128 MB (minimum), 500 MB (recommended), higher memory might be needed based on in-memory data stored for processing Cores - 2 cores (recommended), use lower number of cores after testing Siddhi Apps for performance JDK - 8 or 11 To build Siddhi from the Source distribution, it is necessary that you have JDK version 8 or 11 and Maven 3.0.4 or later","title":"Siddhi Deployment Guide"},{"location":"introduction/#siddhi-deployment-guide","text":"This section provides information on developing and running Siddhi.","title":"Siddhi Deployment Guide"},{"location":"introduction/#siddhi-application","text":"A self contained stream processing logic can be written as a Siddhi Application and put together in a single file with .siddhi extension. The stream processing constructs, such as streams and queries, defined within a Siddhi App is not visible even to the other Siddhi Apps running in the same JVM. It is recommended to have different business usecase in separate Siddhi Applications, where it allow users to selectively deploy the applications based on business needs. It is also recommended to move the repeated steam processing logic that exist in multiple Siddhi Applications, such as message retrieval and preprocessing, to a common Siddhi Application, whereby reducing code duplication and improving maintainability. In this case, to pass the events from one Siddhi App to another, configure them using a common topic using In-Memory Sink and In-Memory Source . For writing Siddhi Application using Streaming SQL refer Siddhi Query Guide","title":"Siddhi Application"},{"location":"introduction/#execution-environments","text":"Siddhi can run in multiple environments as follows. As a Java Library As a Local Microservice As a Docker Microservice As a Kubernetes Microservice As a Python Library (WIP)","title":"Execution Environments"},{"location":"introduction/#system-requirements","text":"For all execution modes following are the general system requirements. Memory - 128 MB (minimum), 500 MB (recommended), higher memory might be needed based on in-memory data stored for processing Cores - 2 cores (recommended), use lower number of cores after testing Siddhi Apps for performance JDK - 8 or 11 To build Siddhi from the Source distribution, it is necessary that you have JDK version 8 or 11 and Maven 3.0.4 or later","title":"System Requirements"},{"location":"release-notes/","text":"Release Notes Siddhi Core Libraries Releases Siddhi Core 5.1.2 Highlights Improvements done for use cases such as throttling, continuous testing integration and error handling. Features Improvements Introduce RESET processing mode to preserve memory optimization. (#1444) Add support YAML Config Manager for easy setting of system properties in SiddhiManager through a YAML file (#1446) Support to create a Sandbox SiddhiAppRuntime for testing purposes (#1451) Improve convert function to provide message cause for Throwable objects (#1463) Support a way to retrieve the sink options and type at sink mapper. (#1473) Support error handling (log/wait/fault-stream) when event sinks publish data asynchronously. (#1473) Bug Fixes Fixes to TimeBatchWindow to process events in a streaming manner, when it's enabled to send current events in streaming mode. This makes sure all having conditions are matched against the output, whereby allowing users to effectively implement throttling use cases with alert suppression. (#1441) Complete Changes Please find the complete changes here Please find more details about the release here Siddhi Core 5.1.2 Highlights There is an improvement done for Template Builder by removing Java Message Format dependency since it is causing some inconsistencies with performing custom mapping for float, double and long values. Due to this fix, there might be some differences (corrected proper output) in the output that you get for custom output mapping with Text, XML, JSON, and CSV. ( #1431 ) There is a behavioral change introduced with the improvements done with ( #1421 ). When counting patterns are used such as e1=StockStream 2:8 and when they are referred without indexes such as e1.price it collects the price values from all the events in the counting pattern e1 and produces it as a list. Since the list is not native to Siddhi the attribute will have the object as its type. In older Siddhi version, it will output the last matching event\u2019s attribute value. Features Improvements SiddhiManager permits user-defined data to be propagated throughout the stack ( #1406 ) API to check whether the Siddhi App is stateful or not ( #1413 ) Support outputting the events collected in counting-pattern as a list ( #1421 ) Support API docs having multiline code segments ( #1430 ) Improve TemplateBuilder remove Java MessageFormat dependency ( #1431 ) Support pattern \u2018every\u2019 clause containing multiple state elements with within condition ( #1435 ) Bug Fixes Siddhi Error Handlers not getting engaged ( #1419 ) Incremental persistence to work on Windows Environment ( 9c37b0d8fc8ce271551d4106bb20231334846f59 ) Complete Changes Please find the complete changes here Please find more details about the release here Siddhi Core 5.1.1 Features Improvements Siddhi store join query optimizations ( #1382 ) Bug Fixes Log Rolling when aggregation query runs when Database is down ( #1380 ) Fix to avoid API changes introduced for Siddhi store implementation in Siddhi 5.1.0 ( #1388 ) Counting pattern issue with \u2018every\u2019 ( #1392 ) Complete Changes Please find the complete changes here Please find more details about the release here Siddhi Core 5.1.0 Features Improvements Minor improvements related to error messages used for the no param case when paramOverload annotation is in place. ( #1375 ) Complete Changes Please find the complete changes here Please find more details about the release here Siddhi Distribution Releases Siddhi Distribution 5.1.0-Alpha Highlights Refer the Siddhi 5.1.3 release note to get to know about the latest feature improvements and bug fixes done for Siddhi engine. Features Improvements Add support for offset in siddhi parser (#291) Add overload param support for source view editor (#310) Improve design view to show the connection between *-call-request and *-call-response IOs. (#310) Feature to support downloading docker and Kubernetes artifacts from Tooling UI (#349) Bug Fixes Fix for snakeyaml dependency issue. (#310) Complete Changes Please find the complete changes here Please find more details about the release here Please find the details of the corresponding docker release here Siddhi Distribution 5.1.0-M2 Highlights Refer the Siddhi 5.1.2 release note to get to know about the latest feature improvements and bug fixes done for Siddhi engine. Features Improvements Improve deployment.yaml configuration for better user experience ( #262 , #269 , #276 ) Siddhi Parser API to validate Siddhi Apps ( #273 ) Bug Fixes Fix design view toggle button position ( #243 ) Complete Changes Please find the complete changes here Please find more details about the release here Please find the details of the corresponding docker release here Siddhi Distribution 5.1.0-M1 New Features Siddhi Test Framework: Provides the capability to write integration tests using Docker containers ( #1327 ) Complete Changes v0.1.0...v5.1.0-m1 Please find more details about the release here Please find the details of the corresponding docker release here Siddhi K8s Operator Releases Siddhi Operator 0.2.0-alpha Highlights Changed Change YAML naming convention of the messaging system and persistent volume claim. Change clusterId - streamingClusterId Change persistentVolume - persistentVolumeClaim 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 messagingSystem : type : nats config : bootstrapServers : - nats://nats-siddhi:4222 streamingClusterId : stan-siddhi persistentVolumeClaim : accessModes : - ReadWriteOnce resources : requests : storage : 1Gi storageClassName : standard volumeMode : Filesystem Bug Fixes Getting segmentation fault error when creating PVC automatically https://github.com/siddhi-io/siddhi-operator/issues/86 !!! info \"Please find more details about the release here \" Siddhi Operator 0.2.0-m2 Highlights Changed Change YAML naming convention to the Camel case. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 messagingSystem : type : nats config : bootstrapServers : - nats://nats-siddhi:4222 clusterId : stan-siddhi persistentVolume : accessModes : - ReadWriteOnce resources : requests : storage : 1Gi storageClassName : standard volumeMode : Filesystem Use a dynamic Siddhi Parser for each Siddhi Custom Resource object, embedded within the Siddhi Runner distribution in-order to share the classpaths . (https://github.com/siddhi-io/siddhi-operator/pull/71) Features Improvements Enable version controlling for SiddhiProcesses.(https://github.com/siddhi-io/siddhi-operator/pull/57, https://github.com/siddhi-io/siddhi-operator/pull/66) NGINX ingress 0.22.0+ support. Enabling readiness and liveness probes with the Siddhi runner. (https://github.com/siddhi-io/siddhi-operator/pull/46) Bug Fixes Fix Operator startup failing when NATS Operator is unavailable. https://github.com/siddhi-io/siddhi-operator/issues/50 Fix Siddhi Process not getting updated when the Config map used to pass the Siddhi application in Siddhi custom resource object is updated. https://github.com/siddhi-io/siddhi-operator/issues/42 Please find more details about the release here Siddhi Operator 0.2.0-m1 Highlights Changed Aggregate previous apps and query specs to a single spec called apps . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 apps : - configMap : app - script : |- @App:name( MonitorApp ) @App:description( Description of the plan ) @sink(type= log , prefix= LOGGER ) @source( type= http , receiver.url= http://0.0.0.0:8080/example , basic.auth.enabled= false , @map(type= json ) ) define stream DevicePowerStream (type string, deviceID string, power int); @sink(type= log , prefix= LOGGER ) define stream MonitorDevicesPowerStream(sumPower long); @info(name= monitored-filter ) from DevicePowerStream#window.time(100 min) select sum(power) as sumPower insert all events into MonitorDevicesPowerStream; Replace previous pod spec with the container spec. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 container : env : - name : RECEIVER_URL value : http://0.0.0.0:8080/example - name : BASIC_AUTH_ENABLED value : false - name : NATS_URL value : nats://siddhi-nats:4222 - name : NATS_DEST value : siddhi - name : NATS_CLUSTER_ID value : siddhi-stan image : buddhiwathsala/siddhi-runner:0.1.1 The imagePullSecret under pod spec which was in previous releases moved to the top level in the YAML. (i.e Directly under the spec of CRD ) Removed Remove previous tls spec. Now you can configure ingress TLS secret using the siddhi-operator-config config map. Features Improvements Added the messagingSystem spec to the CRD. 1 2 3 4 5 6 messagingSystem : type : nats config : bootstrap.servers : - nats://siddhi-nats:4222 cluster.id : siddhi-stan Added persistentVolume spec to the CRD. 1 2 3 4 5 6 7 8 persistentVolume : access.modes : - ReadWriteOnce resources : requests : storage : 1Gi storageClassName : standard volume.mode : Filesystem Bug Fixes Find all the fixes and functionality changes from this issue https://github.com/siddhi-io/siddhi-operator/issues/33 Please find more details about the release here","title":"Release Notes"},{"location":"release-notes/#release-notes","text":"","title":"Release Notes"},{"location":"release-notes/#siddhi-core-libraries-releases","text":"","title":"Siddhi Core Libraries Releases"},{"location":"release-notes/#siddhi-core-512","text":"","title":"Siddhi Core 5.1.2"},{"location":"release-notes/#highlights","text":"Improvements done for use cases such as throttling, continuous testing integration and error handling.","title":"Highlights"},{"location":"release-notes/#features-improvements","text":"Introduce RESET processing mode to preserve memory optimization. (#1444) Add support YAML Config Manager for easy setting of system properties in SiddhiManager through a YAML file (#1446) Support to create a Sandbox SiddhiAppRuntime for testing purposes (#1451) Improve convert function to provide message cause for Throwable objects (#1463) Support a way to retrieve the sink options and type at sink mapper. (#1473) Support error handling (log/wait/fault-stream) when event sinks publish data asynchronously. (#1473)","title":"Features &amp; Improvements"},{"location":"release-notes/#bug-fixes","text":"Fixes to TimeBatchWindow to process events in a streaming manner, when it's enabled to send current events in streaming mode. This makes sure all having conditions are matched against the output, whereby allowing users to effectively implement throttling use cases with alert suppression. (#1441)","title":"Bug Fixes"},{"location":"release-notes/#complete-changes","text":"Please find the complete changes here Please find more details about the release here","title":"Complete Changes"},{"location":"release-notes/#siddhi-core-512_1","text":"","title":"Siddhi Core 5.1.2"},{"location":"release-notes/#highlights_1","text":"There is an improvement done for Template Builder by removing Java Message Format dependency since it is causing some inconsistencies with performing custom mapping for float, double and long values. Due to this fix, there might be some differences (corrected proper output) in the output that you get for custom output mapping with Text, XML, JSON, and CSV. ( #1431 ) There is a behavioral change introduced with the improvements done with ( #1421 ). When counting patterns are used such as e1=StockStream 2:8 and when they are referred without indexes such as e1.price it collects the price values from all the events in the counting pattern e1 and produces it as a list. Since the list is not native to Siddhi the attribute will have the object as its type. In older Siddhi version, it will output the last matching event\u2019s attribute value.","title":"Highlights"},{"location":"release-notes/#features-improvements_1","text":"SiddhiManager permits user-defined data to be propagated throughout the stack ( #1406 ) API to check whether the Siddhi App is stateful or not ( #1413 ) Support outputting the events collected in counting-pattern as a list ( #1421 ) Support API docs having multiline code segments ( #1430 ) Improve TemplateBuilder remove Java MessageFormat dependency ( #1431 ) Support pattern \u2018every\u2019 clause containing multiple state elements with within condition ( #1435 )","title":"Features &amp; Improvements"},{"location":"release-notes/#bug-fixes_1","text":"Siddhi Error Handlers not getting engaged ( #1419 ) Incremental persistence to work on Windows Environment ( 9c37b0d8fc8ce271551d4106bb20231334846f59 )","title":"Bug Fixes"},{"location":"release-notes/#complete-changes_1","text":"Please find the complete changes here Please find more details about the release here","title":"Complete Changes"},{"location":"release-notes/#siddhi-core-511","text":"","title":"Siddhi Core 5.1.1"},{"location":"release-notes/#features-improvements_2","text":"Siddhi store join query optimizations ( #1382 )","title":"Features &amp; Improvements"},{"location":"release-notes/#bug-fixes_2","text":"Log Rolling when aggregation query runs when Database is down ( #1380 ) Fix to avoid API changes introduced for Siddhi store implementation in Siddhi 5.1.0 ( #1388 ) Counting pattern issue with \u2018every\u2019 ( #1392 )","title":"Bug Fixes"},{"location":"release-notes/#complete-changes_2","text":"Please find the complete changes here Please find more details about the release here","title":"Complete Changes"},{"location":"release-notes/#siddhi-core-510","text":"","title":"Siddhi Core 5.1.0"},{"location":"release-notes/#features-improvements_3","text":"Minor improvements related to error messages used for the no param case when paramOverload annotation is in place. ( #1375 )","title":"Features &amp; Improvements"},{"location":"release-notes/#complete-changes_3","text":"Please find the complete changes here Please find more details about the release here","title":"Complete Changes"},{"location":"release-notes/#siddhi-distribution-releases","text":"","title":"Siddhi Distribution Releases"},{"location":"release-notes/#siddhi-distribution-510-alpha","text":"","title":"Siddhi Distribution 5.1.0-Alpha"},{"location":"release-notes/#highlights_2","text":"Refer the Siddhi 5.1.3 release note to get to know about the latest feature improvements and bug fixes done for Siddhi engine.","title":"Highlights"},{"location":"release-notes/#features-improvements_4","text":"Add support for offset in siddhi parser (#291) Add overload param support for source view editor (#310) Improve design view to show the connection between *-call-request and *-call-response IOs. (#310) Feature to support downloading docker and Kubernetes artifacts from Tooling UI (#349)","title":"Features &amp; Improvements"},{"location":"release-notes/#bug-fixes_3","text":"Fix for snakeyaml dependency issue. (#310)","title":"Bug Fixes"},{"location":"release-notes/#complete-changes_4","text":"Please find the complete changes here Please find more details about the release here Please find the details of the corresponding docker release here","title":"Complete Changes"},{"location":"release-notes/#siddhi-distribution-510-m2","text":"","title":"Siddhi Distribution 5.1.0-M2"},{"location":"release-notes/#highlights_3","text":"Refer the Siddhi 5.1.2 release note to get to know about the latest feature improvements and bug fixes done for Siddhi engine.","title":"Highlights"},{"location":"release-notes/#features-improvements_5","text":"Improve deployment.yaml configuration for better user experience ( #262 , #269 , #276 ) Siddhi Parser API to validate Siddhi Apps ( #273 )","title":"Features &amp; Improvements"},{"location":"release-notes/#bug-fixes_4","text":"Fix design view toggle button position ( #243 )","title":"Bug Fixes"},{"location":"release-notes/#complete-changes_5","text":"Please find the complete changes here Please find more details about the release here Please find the details of the corresponding docker release here","title":"Complete Changes"},{"location":"release-notes/#siddhi-distribution-510-m1","text":"","title":"Siddhi Distribution 5.1.0-M1"},{"location":"release-notes/#new-features","text":"Siddhi Test Framework: Provides the capability to write integration tests using Docker containers ( #1327 )","title":"New Features"},{"location":"release-notes/#complete-changes_6","text":"v0.1.0...v5.1.0-m1 Please find more details about the release here Please find the details of the corresponding docker release here","title":"Complete Changes"},{"location":"release-notes/#siddhi-k8s-operator-releases","text":"","title":"Siddhi K8s Operator Releases"},{"location":"release-notes/#siddhi-operator-020-alpha","text":"","title":"Siddhi Operator 0.2.0-alpha"},{"location":"release-notes/#highlights_4","text":"","title":"Highlights"},{"location":"release-notes/#changed","text":"Change YAML naming convention of the messaging system and persistent volume claim. Change clusterId - streamingClusterId Change persistentVolume - persistentVolumeClaim 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 messagingSystem : type : nats config : bootstrapServers : - nats://nats-siddhi:4222 streamingClusterId : stan-siddhi persistentVolumeClaim : accessModes : - ReadWriteOnce resources : requests : storage : 1Gi storageClassName : standard volumeMode : Filesystem","title":"Changed"},{"location":"release-notes/#bug-fixes_5","text":"Getting segmentation fault error when creating PVC automatically https://github.com/siddhi-io/siddhi-operator/issues/86 !!! info \"Please find more details about the release here \"","title":"Bug Fixes"},{"location":"release-notes/#siddhi-operator-020-m2","text":"","title":"Siddhi Operator 0.2.0-m2"},{"location":"release-notes/#highlights_5","text":"","title":"Highlights"},{"location":"release-notes/#changed_1","text":"Change YAML naming convention to the Camel case. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 messagingSystem : type : nats config : bootstrapServers : - nats://nats-siddhi:4222 clusterId : stan-siddhi persistentVolume : accessModes : - ReadWriteOnce resources : requests : storage : 1Gi storageClassName : standard volumeMode : Filesystem Use a dynamic Siddhi Parser for each Siddhi Custom Resource object, embedded within the Siddhi Runner distribution in-order to share the classpaths . (https://github.com/siddhi-io/siddhi-operator/pull/71)","title":"Changed"},{"location":"release-notes/#features-improvements_6","text":"Enable version controlling for SiddhiProcesses.(https://github.com/siddhi-io/siddhi-operator/pull/57, https://github.com/siddhi-io/siddhi-operator/pull/66) NGINX ingress 0.22.0+ support. Enabling readiness and liveness probes with the Siddhi runner. (https://github.com/siddhi-io/siddhi-operator/pull/46)","title":"Features &amp; Improvements"},{"location":"release-notes/#bug-fixes_6","text":"Fix Operator startup failing when NATS Operator is unavailable. https://github.com/siddhi-io/siddhi-operator/issues/50 Fix Siddhi Process not getting updated when the Config map used to pass the Siddhi application in Siddhi custom resource object is updated. https://github.com/siddhi-io/siddhi-operator/issues/42 Please find more details about the release here","title":"Bug Fixes"},{"location":"release-notes/#siddhi-operator-020-m1","text":"","title":"Siddhi Operator 0.2.0-m1"},{"location":"release-notes/#highlights_6","text":"","title":"Highlights"},{"location":"release-notes/#changed_2","text":"Aggregate previous apps and query specs to a single spec called apps . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 apps : - configMap : app - script : |- @App:name( MonitorApp ) @App:description( Description of the plan ) @sink(type= log , prefix= LOGGER ) @source( type= http , receiver.url= http://0.0.0.0:8080/example , basic.auth.enabled= false , @map(type= json ) ) define stream DevicePowerStream (type string, deviceID string, power int); @sink(type= log , prefix= LOGGER ) define stream MonitorDevicesPowerStream(sumPower long); @info(name= monitored-filter ) from DevicePowerStream#window.time(100 min) select sum(power) as sumPower insert all events into MonitorDevicesPowerStream; Replace previous pod spec with the container spec. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 container : env : - name : RECEIVER_URL value : http://0.0.0.0:8080/example - name : BASIC_AUTH_ENABLED value : false - name : NATS_URL value : nats://siddhi-nats:4222 - name : NATS_DEST value : siddhi - name : NATS_CLUSTER_ID value : siddhi-stan image : buddhiwathsala/siddhi-runner:0.1.1 The imagePullSecret under pod spec which was in previous releases moved to the top level in the YAML. (i.e Directly under the spec of CRD )","title":"Changed"},{"location":"release-notes/#removed","text":"Remove previous tls spec. Now you can configure ingress TLS secret using the siddhi-operator-config config map.","title":"Removed"},{"location":"release-notes/#features-improvements_7","text":"Added the messagingSystem spec to the CRD. 1 2 3 4 5 6 messagingSystem : type : nats config : bootstrap.servers : - nats://siddhi-nats:4222 cluster.id : siddhi-stan Added persistentVolume spec to the CRD. 1 2 3 4 5 6 7 8 persistentVolume : access.modes : - ReadWriteOnce resources : requests : storage : 1Gi storageClassName : standard volume.mode : Filesystem","title":"Features &amp; Improvements"},{"location":"release-notes/#bug-fixes_7","text":"Find all the fixes and functionality changes from this issue https://github.com/siddhi-io/siddhi-operator/issues/33 Please find more details about the release here","title":"Bug Fixes"},{"location":"development/","text":"Siddhi 5.1 Development Guide Obtaining and Building Project Source code Find the project source code here and the instruction to building the project repos here . Getting Involved in Project Development Siddhi design-related discussions are carried out in the Siddhi-Dev Google Group , you can subscribe to it to get notifications on the discussions and please feel free to get involved by contributing and sharing your thoughts and ideas. You can also propose changes or improvements by starting a thread in the Siddhi-Dev Google Group, and also by reporting issues in the Siddhi GitHub repository with the label type/improvement or type/new-feature . Project Architecture Find out about the architecture of Siddhi for the Siddhi Architecture documentation.","title":"Introduction"},{"location":"development/#siddhi-51-development-guide","text":"","title":"Siddhi 5.1 Development Guide"},{"location":"development/#obtaining-and-building-project-source-code","text":"Find the project source code here and the instruction to building the project repos here .","title":"Obtaining and Building Project Source code"},{"location":"development/#getting-involved-in-project-development","text":"Siddhi design-related discussions are carried out in the Siddhi-Dev Google Group , you can subscribe to it to get notifications on the discussions and please feel free to get involved by contributing and sharing your thoughts and ideas. You can also propose changes or improvements by starting a thread in the Siddhi-Dev Google Group, and also by reporting issues in the Siddhi GitHub repository with the label type/improvement or type/new-feature .","title":"Getting Involved in Project Development"},{"location":"development/#project-architecture","text":"Find out about the architecture of Siddhi for the Siddhi Architecture documentation.","title":"Project Architecture"},{"location":"development/architecture/","text":"Siddhi 5.1 Architecture Siddhi is an open source, cloud-native, stream processing and complex event processing engine. It can be utilized in any of the following ways: Run as a server on its own Run as a micro service on bare metal, VM, Docker and natively in Kubernetes Embedded into any Java or Python based application Run on an Android application Siddhi provides streaming data integration and data analytical operators. It connects multiple disparate live data sources, orchestrates data flows, calculates analytics, and also detects complex event patterns. This allows developers to build applications that collect data, perform data transformation and analytics, and publish the results to data sinks in real time. This section illustrates the architecture of the Siddhi Engine and guides you through its key functionality. We hope this article helps developers to understand Siddhi and its codebase better, and also help them to contribute and improve Siddhi. Main Design Decisions Event-by-event processing of real-time streaming data to achieve low latency. Ease of use with Streaming SQL providing an intuitive way to express stream processing logic and complex event processing constructs such as Patterns. Achieve high performance by processing events in-memory and using data stores for long term data storage. Optimize performance by enforcing a strict event stream schema and by pre-compiling the queries. Optimize memory consumption by having only the absolutely necessary information in-memory and dropping the rest as soon as possible. Supporting multiple extension points to accommodate a diverse set of functionality such as supporting multiple sources, sinks, functions, aggregation operations, windows, etc. High-Level Architecture At a high level, Siddhi consumes events from various events sources, processes them according to the defined Siddhi application, and produces results to the subscribed event sinks. Siddhi can store and consume events from in-memory tables or from external data stores such as RDBMS , MongoDB , Hazelcast in-memory grid, etc. (i.e., when configured to do so). Siddhi also allows applications and users to query Siddhi via its Store Query API to interactively retrieve data from in-memory and other stores. Main Modules in Siddhi Engine Siddhi Engine comprises four main modules, they are: Siddhi Query API : This allows users to define the execution logic of the Siddhi application as queries and definitions using POJOs (Plain Old Java Objects). Internally, Siddhi uses these objects to identify the logic that it is expected to perform. Siddhi Query Compiler : This allows users to define the Siddhi application using the Siddhi Streaming SQL, and it compiles the Streaming SQL script to Siddhi Query API POJOs so that Siddhi can execute them. Siddhi Core : This builds the execution runtime based on the defined Siddhi Application POJOs and processes the incoming events as and when they arrive. Siddhi Annotation : This is a helper module that allows all extensions to be annotated so that they can be picked by Siddhi Core for processing. This also helps Siddhi to generate the extension documentation. Siddhi Component Architecture The following diagram illustrates the main components of Siddhi and how they work together. Here the Siddhi Core module maintains the execution logic. It also interacts with the external environment and systems for consuming, processing and publishing events. It uses the following components to achieve its tasks: SiddhiManager : This is a key component of Siddhi Core that manages Siddhi Application Runtimes and facilitates their functionality via Siddhi Context with periodic state persistence, statistics reporting and extension loading. It is recommended to use one Siddhi Manager for a single JVM. SiddhiAppRuntime : Siddhi Application Runtime can be generated for each Siddhi Application through the Siddhi Manager. Siddhi Application Runtimes provide an isolated execution environment for each defined Siddhi Application. These Siddhi Application Runtimes can have their own lifecycle and they execute based on the logic defined in their Siddhi Application. SiddhiContext : This is a shared object across all the Siddhi Application Runtimes within the same Siddhi manager. It contains references to the persistence store for periodic persistence, statistics manager to report performance statistics of Siddhi Application Runtimes, and extension holders for loading Siddhi extensions. Siddhi Application Creation Execution logic of the Siddhi Engine is composed as a Siddhi Application, and this is usually passed as a string to SiddhiManager to create the SiddhiAppRuntime for execution. When a Siddhi Application is passed to the SiddhiManager.createSiddhiAppRuntime() , it is processed internally with the SiddhiCompiler . Here, the SiddhiApp String is compiled to SiddhiApp object model by the SiddhiQLBaseVisitorImpl class. This validates the syntax of the given Siddhi Application. The model is then passed to the SiddhiAppParser to create the SiddhiAppRuntime . During this phase, the semantics of the Siddhi Application is validated and the execution logic of the Siddhi Application is optimized. Siddhi App Execution Flow Following diagram depicts the execution flow within a Siddhi App Runtime. The path taken by events within Siddhi Engine is indicated in blue. The components that are involved in handling the events are the following: StreamJunction This routes events of a particular stream to various components within the Siddhi App Runtime. A stream junction is generated for each defined or inferred Stream in the Siddhi Application. A stream junction by default uses the incoming event's thread and passes all the events to its subscribed components as soon as they arrive, but this behaviour can be altered by configuring @Async annotation to buffer the events at the and stream junction and to use another one or more threads to collect the events from the buffer and process the subsequent executions. InputHandler Input handler is used to push Event and Event[] objects into stream junctions from defined event sources, and from Java/Python programmes. StreamCallback This receives Event[] s from stream junction and passes them to event sinks to publish to external endpoints, and/or passes them to subscribed Java/Python programmes for further processing. Queries Partitions These components process events by filtering, transforming, aggregating, joining, pattern matching, etc. They consume events from one or more stream junctions, process them and publish the processed events into a set of stream junctions based on the defined queries or partitions. Source Sources consume events from external sources in various data formats, convert them into Siddhi events using SourceMapper s and pass them to corresponding stream junction via their associated input handlers. A source is generated for each @Source annotation defined above a stream definition. SourceMapper A source mapper is a sub-component of source, and it needs to be configured for each source in order to convert the incoming event into Siddhi event. The source mapper type can be configured using the @Map annotation within the @Source annotation. When the @Map annotation is not defined, Siddhi uses the PassThroughSourceMapper , where it assumes that the incoming message is already in the Siddhi Event format (i.e Event or Event[] ), and therefore makes no changes to the incoming event format. Sink Sinks consumes events from its associated stream junction, convert them to various data formats via SinkMapper and publish them to external endpoints as defined in the @Sink annotation. A sink is generated for each @Sink annotation defined above a stream definition. SinkMapper A sink mapper is a sub-component of sink. and its need to be configured for each sink in order to map the Siddhi events to the specified data format so that they can be published via the sink. The sink mapper type can be configured using the @Map annotation within the @Sink annotation. When the @Map annotation is not defined, Siddhi uses PassThroughSinkMapper , where it passes the Siddhi Event (i.e Event or Event[] ) without any formatting to the Sink. Table Tables are used to store events. When tables are defined by default, Siddhi uses the InMemoryTable implementation to store events in-memory. When @Store annotation is used on top of the table definition, it loads the associated external data store connector based on the defined store type. Most table implementations are extended from either AbstractRecordTable or AbstractQueryableRecordTable abstract classes the former provides the functionality to query external data store based on a given filtering condition, and the latter queries external data store by providing projection, limits, and ordering parameters in addition to data filter condition. Window Windows store events as and when they arrive and automatically expire/clean them based on the given window constraint. Multiple types of windows are can be implemented by extending the WindowProcessor abstract class. IncrementalAggregation Long running time series aggregates defined via the aggregation definition is calculated in an incremental manner using the Incremental Aggregation Processor for the defined time periods. Incremental aggregation functions can be implemented by extending IncrementalAttributeAggregator . By default, incremental aggregations aggregate all the values in-memory, but when it is associated with a store by adding @store annotation it uses in-memory to aggregate partial results and uses data stores to persist those increments. When requested for aggregate results it retrieves data from data stores and (if needed from) in-memory, computes combined aggregate results and provides as the output. Trigger A trigger triggers events at a given interval as given in the trigger definition. The triggered events are pushed to a stream junction having the same name as the trigger. QueryCallback A query callback taps into the events that are emitted by a particular query. It notifies the event occurrence timestamp and classifies the output events into currentEvents , and expiredEvents . Siddhi Query Execution Siddhi QueryRuntimes can be categorized into three main types: SingleInputStream : Queries that consist of query types such as filters and windows. JoinInputStream : Queries that consist of joins. StateInputStream : Queries that consist of patterns and sequences. The following section explains the internals of each query type. SingleInputStream Query Runtime (Filter Windows) A single input stream query runtime is generated for filter and window queries. They consume events from a stream junction or a window and convert the incoming events according to the expected output stream format at the ProcessStreamReceiver by dropping all the unrelated incoming stream attributes. Then the converted events are passed through a few Processors such as FilterProcessor , StreamProcessor , StreamFunctionProcessor , WindowProcessor , and QuerySelector . Here, the StreamProcessor , StreamFunctionProcessor , and WindowProcessor can be extended with various stream processing capabilities. The last processor of the chain of processors must always be a QuerySelector and it can't appear anywhere else. When the query runtime consumes events from a stream, its processor chain can maximum contain one WindowProcessor , and when query runtime consumes events from a window, its chain of processors cannot contain any WindowProcessor . The FilterProcessor is implemented using expressions that return a boolean value. ExpressionExecutor is used to process conditions, mathematical operations, unary operations, constant values, variables, and functions. Expressions have a tree structure, and they are processed based using the Depth First search algorithm. To achieve high performance, Siddhi currently depends on the user to formulate the least successful case in the leftmost side of the condition, thereby increasing the chance of early false detection. The condition expression price = 100 and ( Symbol == 'IBM' or Symbol == 'MSFT' ) is represented as shown below. These expressions also support the execution of user-defined functions (UDFs), and they can be implemented by extending the FunctionExecutor class. After getting processed by all the processors, events reach the QuerySelector for transformation. At the QuerySelector , events are transformed based on the select clause of the query. The select clause produces one AttributeProcessor for each output stream attribute, and these AttributeProcessor s contain expressions defining data transformation including constant values, variables, user-defined functions, etc. They can also contain AttributeAggregatorExecutor s to process aggregation operations such as sum , count , etc. If there is a Group By clause defined, then the GroupByKeyGenerator is used to identify the composite group-by key, and then for each key, an AttributeAggregatorExecutor state is generated to maintain per group-by key aggregations. When each time AttributeProcessor is executed the AttributeAggregatorExecutor calculates per group-by aggregation results and output the values. When AttributeAggregatorExecutor group-by states become obsolete, they are destroyed and automatically cleaned. After an event is transformed to the output format through the above process, it is evaluated against the having condition executor if a having clause is provided. The succeeding events are then ordered, and limited based on order by , limit and offset clauses before they pushed to the OutputRateLimiter . At OutputRateLimiter , the event output is controlled before sending the events to the stream junction or to the query callback. When the output clause is not defined, the PassThroughOutputRateLimiter is used by passing all the events without any rate limiting. Temporal Processing with Windows The temporal event processing aspect is achieved via Window and AttributeAggregators To achieve temporal processing, Siddhi uses the following four type of events: Current Events : Events that are newly arriving to the query from streams. Expired Events : Events that have expired from a window. Timer Events : Events that inform the query about an update of execution time. These events are usually generated by schedulers. Reset Events : Events that resets the Siddhi query states. In Siddhi, when an event comes into a WindowProcessor , it creates an appropriate expired event corresponding to the incoming current event with the expiring timestamp, and stores that event in the window. At the same time, WindowProcessor also forwards the current event to the next processor for further processing. It uses a scheduler or some other counting approach to determine when to emit the events that are stored in in-memory. When the expired events meet the condition for expiry based on the window contains, it emits the expired events to the next processor. At times like in window.timeBatch() there can be cases that need emitting all the events in-memory at once and the output does not need individual expired events values, in this cases the window emits a single reset event instead of sending one expired event for each event it has stored, so that it can reset the states in one go. For the QuerySelector aggregations to work correctly the window must emit a corresponding expired event for each current event it has emitted or it must send a reset event . In the QuerySelector , the arrived current events increase the aggregation values, expired events decrease the values, and reset events reset the aggregation calculation to produce correct query output. For example, the sliding TimeWindow ( window.time() ) creates a corresponding expired event for each current event that arrives, adds the expired event s to the window, adds an entry to the scheduler to notify when that event need to be expired, and finally sends the current event to the next processor for subsequent processing. The scheduler notifies the window by sending a timer event , and when the window receives an indication that the expected expiry time has come for the oldest event in the window via a timer event or by other means, it removes the expired event from the window and passes that to the next processor. JoinInputStream Query Runtime (Join) Join input stream query runtime is generated for join queries. This can consume events from two stream junctions and perform a join operation as depicted above. It can also perform a join by consuming events from one stream junction and join against itself, or it can also join against a table, window or an aggregation. When a join is performed with a table, window or aggregation, the WindowProcessor in the above image is replaced with the corresponding table, window or aggregation and no basic processors are used on their side. The joining operation is triggered by the events that arrive from the stream junction. Here, when an event from one stream reaches the pre JoinProcessor , it matches against all the available events of the other stream's WindowProcessor . When a match is found, those matched events are sent to the QuerySelector as current events , and at the same time, the original event is added to the WindowProcessor where it remains until it expires. Similarly, when an event expires from the WindowProcessor , it matches against all the available events of the other stream's WindowProcessor , and when a match is found, those matched events are sent to the QuerySelector as expired events . Note Despite the optimizations, a join query is quite expensive when it comes to performance. This is because the WindowProcessor is locked during the matching process to avoid race conditions and to achieve accuracy while joining. Therefore, when possible avoid matching large (time or length) windows in high volume streams. StateInputStream Query Runtime (Pattern Sequence) The state input stream query runtime is generated for pattern and sequence queries. This consumes events from one or more stream junctions via ProcessStreamReceiver s and checks whether the events match each pattern or sequence condition by processing the set of basic processors associated with each ProcessStreamReceiver . The PreStateProcessor s usually contains lists of state events that are already matched by previous conditions, and if its the first condition then it will have an empty state event in its list. When ProcessStreamReceiver consumes an event, it passes the event to the PreStateProcessor which updates the list of state events it has with the incoming event and executes the condition by passing the events to the basic processors. The state events that match the conditions reach the PostStateProcessor which will then stores the events to the state event list of the following PreStateProcessor . If it is the final condition's PostStateProcessor , then it will pass the state event to the QuerySelector to generate and emit the output. Siddhi Partition Execution A partition is a wrapper around one or more Siddhi queries and inner streams that connect them. A partition is implemented in Siddhi as a PartitionRuntime which contains multiple QueryRuntime s and inner stream junctions. Each partitioned stream entering the partition goes through a designated PartitionStreamReceiver . The PartitionExecutor of PartitionStreamReceiver evaluates the incoming events to identify their associated partition-key using either RangePartitionExecutor or ValuePartitionExecutor . The identified partition-key is then set as thread local variable and the event is passed to the QueryRuntime s of processing. The QueryRuntime s process events by maintaining separate states for each partition-key such that producing separate output per partition. When a partition query consumes a non-partitioned global stream, the QueryRuntime s are executed for each available partition-key in the system such that allowing all partitions to receive the same event. When the partitions are obsolete PartitionRuntime deletes all the partition states from its QueryRuntime s. Siddhi Aggregation Siddhi supports long duration time series aggregations via its aggregation definition. AggregationRuntime implements this by the use of streaming lambda architecture , where it processes part of the data in-memory and gets part of the data from data stores. AggregationRuntime creates an in-memory table or external store for each time granularity (i.e seconds, minutes, days, etc) it has to process the events, and when events enter it calculates the aggregations in-memory for its least granularity (usually seconds) using the IncrementalExecutor and maintains the running aggregation values in its BaseIncrementalValueStore . At each clock end time of the granularity (end of each second) IncrementalExecutor stores the summarized values to the associated granularity table and also passes the summarized values to the IncrementalExecutor of the next granularity level, which also follows the same methodology in processing the events. Through this approach each time granularities, the current time duration will be in-memory and all the historical time durations will be in stored in the tables. The aggregations results are calculated by IncrementalAttributeAggregator s and stored in such a way that allows proper data composition upon retrial, for example, avg() is stored as sum and count . This allows data composition across various granularity time durations when retrieving, for example, results for avg() composed by returning sum of sum s divided by the sum of count s. Aggregation can also work in a distributed manner and across system restarts. This is done by storing node specific IDs and granularity time duration information in the tables. To make sure tables do not go out of memory IncrementalDataPurger is used to purge old data. When aggregation is queried through join or store query for a given time granularity it reads the data from the in-memory BaseIncrementalValueStore and from the tables computes the composite results as described, and presents the results. Siddhi Event Formats Siddhi has three event formats. Event This is the format exposed to external systems when they send events via Input Handler and consume events via Stream Callback or Query Callback. This consists of a timestamp and an Object[] that contains all the values in accordance to the corresponding stream. StreamEvent (Subtype of ComplexEvent ) This is used within queries. This contains a timestamp and the following three Object[] s: beforeWindowData : This contains values that are only used in processors that are executed before the WindowProcessor . onAfterWindowData : This contains values that are only used by the WindowProcessor and the other processors that follow it, but not sent as output. outputData : This contains the values that are sent via the output stream of the query. In order to optimize the amount of data that is stored in the in-memory at windows, the content in beforeWindowData is cleared before the event enters the WindowProcessor . StreamEvents can also be chained by linking each other via the next property in them. StateEvent (Subtype of ComplexEvent ) This is used in joins, patterns and sequences queries when we need to associate events of multiple streams, tables, windows or aggregations together. This contains a timestamp , a collection of StreamEvent s representing different streams, tables, etc, that are used in the query, and an Object[] to contain outputData values that are needed for query output. The StreamEvent s within the StateEvent and the StateEvent themselves can be chained by linking each other with the next property in them. Event Chunks Event Chunks provide an easier way of manipulating the chain of StreamEvent s and StateEvent s so that they are be easily iterated, inserted and removed. Summary This article focuses on describing the architecture of Siddhi and rationalizing some of the architectural decisions made when implementing the system. It also explains the key features of Siddhi. We hope this will be a good starting point for new developers to understand Siddhi and to start contributing to it.","title":"Architecture"},{"location":"development/architecture/#siddhi-51-architecture","text":"Siddhi is an open source, cloud-native, stream processing and complex event processing engine. It can be utilized in any of the following ways: Run as a server on its own Run as a micro service on bare metal, VM, Docker and natively in Kubernetes Embedded into any Java or Python based application Run on an Android application Siddhi provides streaming data integration and data analytical operators. It connects multiple disparate live data sources, orchestrates data flows, calculates analytics, and also detects complex event patterns. This allows developers to build applications that collect data, perform data transformation and analytics, and publish the results to data sinks in real time. This section illustrates the architecture of the Siddhi Engine and guides you through its key functionality. We hope this article helps developers to understand Siddhi and its codebase better, and also help them to contribute and improve Siddhi.","title":"Siddhi 5.1 Architecture"},{"location":"development/architecture/#main-design-decisions","text":"Event-by-event processing of real-time streaming data to achieve low latency. Ease of use with Streaming SQL providing an intuitive way to express stream processing logic and complex event processing constructs such as Patterns. Achieve high performance by processing events in-memory and using data stores for long term data storage. Optimize performance by enforcing a strict event stream schema and by pre-compiling the queries. Optimize memory consumption by having only the absolutely necessary information in-memory and dropping the rest as soon as possible. Supporting multiple extension points to accommodate a diverse set of functionality such as supporting multiple sources, sinks, functions, aggregation operations, windows, etc.","title":"Main Design Decisions"},{"location":"development/architecture/#high-level-architecture","text":"At a high level, Siddhi consumes events from various events sources, processes them according to the defined Siddhi application, and produces results to the subscribed event sinks. Siddhi can store and consume events from in-memory tables or from external data stores such as RDBMS , MongoDB , Hazelcast in-memory grid, etc. (i.e., when configured to do so). Siddhi also allows applications and users to query Siddhi via its Store Query API to interactively retrieve data from in-memory and other stores.","title":"High-Level Architecture"},{"location":"development/architecture/#main-modules-in-siddhi-engine","text":"Siddhi Engine comprises four main modules, they are: Siddhi Query API : This allows users to define the execution logic of the Siddhi application as queries and definitions using POJOs (Plain Old Java Objects). Internally, Siddhi uses these objects to identify the logic that it is expected to perform. Siddhi Query Compiler : This allows users to define the Siddhi application using the Siddhi Streaming SQL, and it compiles the Streaming SQL script to Siddhi Query API POJOs so that Siddhi can execute them. Siddhi Core : This builds the execution runtime based on the defined Siddhi Application POJOs and processes the incoming events as and when they arrive. Siddhi Annotation : This is a helper module that allows all extensions to be annotated so that they can be picked by Siddhi Core for processing. This also helps Siddhi to generate the extension documentation.","title":"Main Modules in Siddhi Engine"},{"location":"development/architecture/#siddhi-component-architecture","text":"The following diagram illustrates the main components of Siddhi and how they work together. Here the Siddhi Core module maintains the execution logic. It also interacts with the external environment and systems for consuming, processing and publishing events. It uses the following components to achieve its tasks: SiddhiManager : This is a key component of Siddhi Core that manages Siddhi Application Runtimes and facilitates their functionality via Siddhi Context with periodic state persistence, statistics reporting and extension loading. It is recommended to use one Siddhi Manager for a single JVM. SiddhiAppRuntime : Siddhi Application Runtime can be generated for each Siddhi Application through the Siddhi Manager. Siddhi Application Runtimes provide an isolated execution environment for each defined Siddhi Application. These Siddhi Application Runtimes can have their own lifecycle and they execute based on the logic defined in their Siddhi Application. SiddhiContext : This is a shared object across all the Siddhi Application Runtimes within the same Siddhi manager. It contains references to the persistence store for periodic persistence, statistics manager to report performance statistics of Siddhi Application Runtimes, and extension holders for loading Siddhi extensions.","title":"Siddhi Component Architecture"},{"location":"development/architecture/#siddhi-application-creation","text":"Execution logic of the Siddhi Engine is composed as a Siddhi Application, and this is usually passed as a string to SiddhiManager to create the SiddhiAppRuntime for execution. When a Siddhi Application is passed to the SiddhiManager.createSiddhiAppRuntime() , it is processed internally with the SiddhiCompiler . Here, the SiddhiApp String is compiled to SiddhiApp object model by the SiddhiQLBaseVisitorImpl class. This validates the syntax of the given Siddhi Application. The model is then passed to the SiddhiAppParser to create the SiddhiAppRuntime . During this phase, the semantics of the Siddhi Application is validated and the execution logic of the Siddhi Application is optimized.","title":"Siddhi Application Creation"},{"location":"development/architecture/#siddhi-app-execution-flow","text":"Following diagram depicts the execution flow within a Siddhi App Runtime. The path taken by events within Siddhi Engine is indicated in blue. The components that are involved in handling the events are the following: StreamJunction This routes events of a particular stream to various components within the Siddhi App Runtime. A stream junction is generated for each defined or inferred Stream in the Siddhi Application. A stream junction by default uses the incoming event's thread and passes all the events to its subscribed components as soon as they arrive, but this behaviour can be altered by configuring @Async annotation to buffer the events at the and stream junction and to use another one or more threads to collect the events from the buffer and process the subsequent executions. InputHandler Input handler is used to push Event and Event[] objects into stream junctions from defined event sources, and from Java/Python programmes. StreamCallback This receives Event[] s from stream junction and passes them to event sinks to publish to external endpoints, and/or passes them to subscribed Java/Python programmes for further processing. Queries Partitions These components process events by filtering, transforming, aggregating, joining, pattern matching, etc. They consume events from one or more stream junctions, process them and publish the processed events into a set of stream junctions based on the defined queries or partitions. Source Sources consume events from external sources in various data formats, convert them into Siddhi events using SourceMapper s and pass them to corresponding stream junction via their associated input handlers. A source is generated for each @Source annotation defined above a stream definition. SourceMapper A source mapper is a sub-component of source, and it needs to be configured for each source in order to convert the incoming event into Siddhi event. The source mapper type can be configured using the @Map annotation within the @Source annotation. When the @Map annotation is not defined, Siddhi uses the PassThroughSourceMapper , where it assumes that the incoming message is already in the Siddhi Event format (i.e Event or Event[] ), and therefore makes no changes to the incoming event format. Sink Sinks consumes events from its associated stream junction, convert them to various data formats via SinkMapper and publish them to external endpoints as defined in the @Sink annotation. A sink is generated for each @Sink annotation defined above a stream definition. SinkMapper A sink mapper is a sub-component of sink. and its need to be configured for each sink in order to map the Siddhi events to the specified data format so that they can be published via the sink. The sink mapper type can be configured using the @Map annotation within the @Sink annotation. When the @Map annotation is not defined, Siddhi uses PassThroughSinkMapper , where it passes the Siddhi Event (i.e Event or Event[] ) without any formatting to the Sink. Table Tables are used to store events. When tables are defined by default, Siddhi uses the InMemoryTable implementation to store events in-memory. When @Store annotation is used on top of the table definition, it loads the associated external data store connector based on the defined store type. Most table implementations are extended from either AbstractRecordTable or AbstractQueryableRecordTable abstract classes the former provides the functionality to query external data store based on a given filtering condition, and the latter queries external data store by providing projection, limits, and ordering parameters in addition to data filter condition. Window Windows store events as and when they arrive and automatically expire/clean them based on the given window constraint. Multiple types of windows are can be implemented by extending the WindowProcessor abstract class. IncrementalAggregation Long running time series aggregates defined via the aggregation definition is calculated in an incremental manner using the Incremental Aggregation Processor for the defined time periods. Incremental aggregation functions can be implemented by extending IncrementalAttributeAggregator . By default, incremental aggregations aggregate all the values in-memory, but when it is associated with a store by adding @store annotation it uses in-memory to aggregate partial results and uses data stores to persist those increments. When requested for aggregate results it retrieves data from data stores and (if needed from) in-memory, computes combined aggregate results and provides as the output. Trigger A trigger triggers events at a given interval as given in the trigger definition. The triggered events are pushed to a stream junction having the same name as the trigger. QueryCallback A query callback taps into the events that are emitted by a particular query. It notifies the event occurrence timestamp and classifies the output events into currentEvents , and expiredEvents .","title":"Siddhi App Execution Flow"},{"location":"development/architecture/#siddhi-query-execution","text":"Siddhi QueryRuntimes can be categorized into three main types: SingleInputStream : Queries that consist of query types such as filters and windows. JoinInputStream : Queries that consist of joins. StateInputStream : Queries that consist of patterns and sequences. The following section explains the internals of each query type.","title":"Siddhi Query Execution"},{"location":"development/architecture/#singleinputstream-query-runtime-filter-windows","text":"A single input stream query runtime is generated for filter and window queries. They consume events from a stream junction or a window and convert the incoming events according to the expected output stream format at the ProcessStreamReceiver by dropping all the unrelated incoming stream attributes. Then the converted events are passed through a few Processors such as FilterProcessor , StreamProcessor , StreamFunctionProcessor , WindowProcessor , and QuerySelector . Here, the StreamProcessor , StreamFunctionProcessor , and WindowProcessor can be extended with various stream processing capabilities. The last processor of the chain of processors must always be a QuerySelector and it can't appear anywhere else. When the query runtime consumes events from a stream, its processor chain can maximum contain one WindowProcessor , and when query runtime consumes events from a window, its chain of processors cannot contain any WindowProcessor . The FilterProcessor is implemented using expressions that return a boolean value. ExpressionExecutor is used to process conditions, mathematical operations, unary operations, constant values, variables, and functions. Expressions have a tree structure, and they are processed based using the Depth First search algorithm. To achieve high performance, Siddhi currently depends on the user to formulate the least successful case in the leftmost side of the condition, thereby increasing the chance of early false detection. The condition expression price = 100 and ( Symbol == 'IBM' or Symbol == 'MSFT' ) is represented as shown below. These expressions also support the execution of user-defined functions (UDFs), and they can be implemented by extending the FunctionExecutor class. After getting processed by all the processors, events reach the QuerySelector for transformation. At the QuerySelector , events are transformed based on the select clause of the query. The select clause produces one AttributeProcessor for each output stream attribute, and these AttributeProcessor s contain expressions defining data transformation including constant values, variables, user-defined functions, etc. They can also contain AttributeAggregatorExecutor s to process aggregation operations such as sum , count , etc. If there is a Group By clause defined, then the GroupByKeyGenerator is used to identify the composite group-by key, and then for each key, an AttributeAggregatorExecutor state is generated to maintain per group-by key aggregations. When each time AttributeProcessor is executed the AttributeAggregatorExecutor calculates per group-by aggregation results and output the values. When AttributeAggregatorExecutor group-by states become obsolete, they are destroyed and automatically cleaned. After an event is transformed to the output format through the above process, it is evaluated against the having condition executor if a having clause is provided. The succeeding events are then ordered, and limited based on order by , limit and offset clauses before they pushed to the OutputRateLimiter . At OutputRateLimiter , the event output is controlled before sending the events to the stream junction or to the query callback. When the output clause is not defined, the PassThroughOutputRateLimiter is used by passing all the events without any rate limiting.","title":"SingleInputStream Query Runtime (Filter &amp; Windows)"},{"location":"development/architecture/#temporal-processing-with-windows","text":"The temporal event processing aspect is achieved via Window and AttributeAggregators To achieve temporal processing, Siddhi uses the following four type of events: Current Events : Events that are newly arriving to the query from streams. Expired Events : Events that have expired from a window. Timer Events : Events that inform the query about an update of execution time. These events are usually generated by schedulers. Reset Events : Events that resets the Siddhi query states. In Siddhi, when an event comes into a WindowProcessor , it creates an appropriate expired event corresponding to the incoming current event with the expiring timestamp, and stores that event in the window. At the same time, WindowProcessor also forwards the current event to the next processor for further processing. It uses a scheduler or some other counting approach to determine when to emit the events that are stored in in-memory. When the expired events meet the condition for expiry based on the window contains, it emits the expired events to the next processor. At times like in window.timeBatch() there can be cases that need emitting all the events in-memory at once and the output does not need individual expired events values, in this cases the window emits a single reset event instead of sending one expired event for each event it has stored, so that it can reset the states in one go. For the QuerySelector aggregations to work correctly the window must emit a corresponding expired event for each current event it has emitted or it must send a reset event . In the QuerySelector , the arrived current events increase the aggregation values, expired events decrease the values, and reset events reset the aggregation calculation to produce correct query output. For example, the sliding TimeWindow ( window.time() ) creates a corresponding expired event for each current event that arrives, adds the expired event s to the window, adds an entry to the scheduler to notify when that event need to be expired, and finally sends the current event to the next processor for subsequent processing. The scheduler notifies the window by sending a timer event , and when the window receives an indication that the expected expiry time has come for the oldest event in the window via a timer event or by other means, it removes the expired event from the window and passes that to the next processor.","title":"Temporal Processing with Windows"},{"location":"development/architecture/#joininputstream-query-runtime-join","text":"Join input stream query runtime is generated for join queries. This can consume events from two stream junctions and perform a join operation as depicted above. It can also perform a join by consuming events from one stream junction and join against itself, or it can also join against a table, window or an aggregation. When a join is performed with a table, window or aggregation, the WindowProcessor in the above image is replaced with the corresponding table, window or aggregation and no basic processors are used on their side. The joining operation is triggered by the events that arrive from the stream junction. Here, when an event from one stream reaches the pre JoinProcessor , it matches against all the available events of the other stream's WindowProcessor . When a match is found, those matched events are sent to the QuerySelector as current events , and at the same time, the original event is added to the WindowProcessor where it remains until it expires. Similarly, when an event expires from the WindowProcessor , it matches against all the available events of the other stream's WindowProcessor , and when a match is found, those matched events are sent to the QuerySelector as expired events . Note Despite the optimizations, a join query is quite expensive when it comes to performance. This is because the WindowProcessor is locked during the matching process to avoid race conditions and to achieve accuracy while joining. Therefore, when possible avoid matching large (time or length) windows in high volume streams.","title":"JoinInputStream Query Runtime (Join)"},{"location":"development/architecture/#stateinputstream-query-runtime-pattern-sequence","text":"The state input stream query runtime is generated for pattern and sequence queries. This consumes events from one or more stream junctions via ProcessStreamReceiver s and checks whether the events match each pattern or sequence condition by processing the set of basic processors associated with each ProcessStreamReceiver . The PreStateProcessor s usually contains lists of state events that are already matched by previous conditions, and if its the first condition then it will have an empty state event in its list. When ProcessStreamReceiver consumes an event, it passes the event to the PreStateProcessor which updates the list of state events it has with the incoming event and executes the condition by passing the events to the basic processors. The state events that match the conditions reach the PostStateProcessor which will then stores the events to the state event list of the following PreStateProcessor . If it is the final condition's PostStateProcessor , then it will pass the state event to the QuerySelector to generate and emit the output.","title":"StateInputStream Query Runtime (Pattern &amp; Sequence)"},{"location":"development/architecture/#siddhi-partition-execution","text":"A partition is a wrapper around one or more Siddhi queries and inner streams that connect them. A partition is implemented in Siddhi as a PartitionRuntime which contains multiple QueryRuntime s and inner stream junctions. Each partitioned stream entering the partition goes through a designated PartitionStreamReceiver . The PartitionExecutor of PartitionStreamReceiver evaluates the incoming events to identify their associated partition-key using either RangePartitionExecutor or ValuePartitionExecutor . The identified partition-key is then set as thread local variable and the event is passed to the QueryRuntime s of processing. The QueryRuntime s process events by maintaining separate states for each partition-key such that producing separate output per partition. When a partition query consumes a non-partitioned global stream, the QueryRuntime s are executed for each available partition-key in the system such that allowing all partitions to receive the same event. When the partitions are obsolete PartitionRuntime deletes all the partition states from its QueryRuntime s.","title":"Siddhi Partition Execution"},{"location":"development/architecture/#siddhi-aggregation","text":"Siddhi supports long duration time series aggregations via its aggregation definition. AggregationRuntime implements this by the use of streaming lambda architecture , where it processes part of the data in-memory and gets part of the data from data stores. AggregationRuntime creates an in-memory table or external store for each time granularity (i.e seconds, minutes, days, etc) it has to process the events, and when events enter it calculates the aggregations in-memory for its least granularity (usually seconds) using the IncrementalExecutor and maintains the running aggregation values in its BaseIncrementalValueStore . At each clock end time of the granularity (end of each second) IncrementalExecutor stores the summarized values to the associated granularity table and also passes the summarized values to the IncrementalExecutor of the next granularity level, which also follows the same methodology in processing the events. Through this approach each time granularities, the current time duration will be in-memory and all the historical time durations will be in stored in the tables. The aggregations results are calculated by IncrementalAttributeAggregator s and stored in such a way that allows proper data composition upon retrial, for example, avg() is stored as sum and count . This allows data composition across various granularity time durations when retrieving, for example, results for avg() composed by returning sum of sum s divided by the sum of count s. Aggregation can also work in a distributed manner and across system restarts. This is done by storing node specific IDs and granularity time duration information in the tables. To make sure tables do not go out of memory IncrementalDataPurger is used to purge old data. When aggregation is queried through join or store query for a given time granularity it reads the data from the in-memory BaseIncrementalValueStore and from the tables computes the composite results as described, and presents the results.","title":"Siddhi Aggregation"},{"location":"development/architecture/#siddhi-event-formats","text":"Siddhi has three event formats. Event This is the format exposed to external systems when they send events via Input Handler and consume events via Stream Callback or Query Callback. This consists of a timestamp and an Object[] that contains all the values in accordance to the corresponding stream. StreamEvent (Subtype of ComplexEvent ) This is used within queries. This contains a timestamp and the following three Object[] s: beforeWindowData : This contains values that are only used in processors that are executed before the WindowProcessor . onAfterWindowData : This contains values that are only used by the WindowProcessor and the other processors that follow it, but not sent as output. outputData : This contains the values that are sent via the output stream of the query. In order to optimize the amount of data that is stored in the in-memory at windows, the content in beforeWindowData is cleared before the event enters the WindowProcessor . StreamEvents can also be chained by linking each other via the next property in them. StateEvent (Subtype of ComplexEvent ) This is used in joins, patterns and sequences queries when we need to associate events of multiple streams, tables, windows or aggregations together. This contains a timestamp , a collection of StreamEvent s representing different streams, tables, etc, that are used in the query, and an Object[] to contain outputData values that are needed for query output. The StreamEvent s within the StateEvent and the StateEvent themselves can be chained by linking each other with the next property in them. Event Chunks Event Chunks provide an easier way of manipulating the chain of StreamEvent s and StateEvent s so that they are be easily iterated, inserted and removed.","title":"Siddhi Event Formats"},{"location":"development/architecture/#summary","text":"This article focuses on describing the architecture of Siddhi and rationalizing some of the architectural decisions made when implementing the system. It also explains the key features of Siddhi. We hope this will be a good starting point for new developers to understand Siddhi and to start contributing to it.","title":"Summary"},{"location":"development/build/","text":"Building Siddhi 5.1 Repos Building Java Repos Prerequisites Oracle JDK 8 , OpenJDK 8 , or JDK 11 (Java 8 should be used for building in order to support both Java 8 and Java 11 at runtime) Maven 3.5.x or later version Steps to Build Get a clone or download source from Github repo, E.g. 1 git clone https://github.com/siddhi-io/siddhi.git Run the Maven command mvn clean install from the root directory Command Description mvn clean install Build and install the artifacts into the local repository. mvn clean install -Dmaven.test.skip=true Build and install the artifacts into the local repository, without running any of the unit tests.","title":"Build"},{"location":"development/build/#building-siddhi-51-repos","text":"","title":"Building Siddhi 5.1 Repos"},{"location":"development/build/#building-java-repos","text":"","title":"Building Java Repos"},{"location":"development/build/#prerequisites","text":"Oracle JDK 8 , OpenJDK 8 , or JDK 11 (Java 8 should be used for building in order to support both Java 8 and Java 11 at runtime) Maven 3.5.x or later version","title":"Prerequisites"},{"location":"development/build/#steps-to-build","text":"Get a clone or download source from Github repo, E.g. 1 git clone https://github.com/siddhi-io/siddhi.git Run the Maven command mvn clean install from the root directory Command Description mvn clean install Build and install the artifacts into the local repository. mvn clean install -Dmaven.test.skip=true Build and install the artifacts into the local repository, without running any of the unit tests.","title":"Steps to Build"},{"location":"development/source/","text":"Siddhi 5.1 Source Code Project Source Code Siddhi Core Java Library https://github.com/siddhi-io/siddhi (Java) Siddhi repo, containing the core Java libraries of Siddhi. PySiddhi https://github.com/siddhi-io/pysiddhi (Python) The Python wrapper for Siddhi core Java libraries. This depends on the siddhi-io/siddhi repo. Siddhi Local Microservice Distribution https://github.com/siddhi-io/distribution (Java) The Microservice distribution of the Siddhi Tooling and Siddhi Runtime. This depends on the siddhi-io/siddhi repo. Siddhi Docker Microservice Distribution https://github.com/siddhi-io/docker-siddhi (Docker) The Docker wrapper for the Siddhi Tooling and Siddhi Runtime. This depends on the siddhi-io/siddhi and siddhi-io/distribution repos. Siddhi Kubernetes Operator https://github.com/siddhi-io/siddhi-operator (Go) The Siddhi Kubernetes CRD repo deploying Siddhi on Kubernetes. This depends on the siddhi-io/siddhi , siddhi-io/distribution and siddhi-io/docker-siddhi repos. Siddhi Extensions Find the supported Siddhi extensions and source here","title":"Source"},{"location":"development/source/#siddhi-51-source-code","text":"","title":"Siddhi 5.1 Source Code"},{"location":"development/source/#project-source-code","text":"","title":"Project Source Code"},{"location":"development/source/#siddhi-core-java-library","text":"https://github.com/siddhi-io/siddhi (Java) Siddhi repo, containing the core Java libraries of Siddhi.","title":"Siddhi Core Java Library"},{"location":"development/source/#pysiddhi","text":"https://github.com/siddhi-io/pysiddhi (Python) The Python wrapper for Siddhi core Java libraries. This depends on the siddhi-io/siddhi repo.","title":"PySiddhi"},{"location":"development/source/#siddhi-local-microservice-distribution","text":"https://github.com/siddhi-io/distribution (Java) The Microservice distribution of the Siddhi Tooling and Siddhi Runtime. This depends on the siddhi-io/siddhi repo.","title":"Siddhi Local Microservice Distribution"},{"location":"development/source/#siddhi-docker-microservice-distribution","text":"https://github.com/siddhi-io/docker-siddhi (Docker) The Docker wrapper for the Siddhi Tooling and Siddhi Runtime. This depends on the siddhi-io/siddhi and siddhi-io/distribution repos.","title":"Siddhi Docker Microservice Distribution"},{"location":"development/source/#siddhi-kubernetes-operator","text":"https://github.com/siddhi-io/siddhi-operator (Go) The Siddhi Kubernetes CRD repo deploying Siddhi on Kubernetes. This depends on the siddhi-io/siddhi , siddhi-io/distribution and siddhi-io/docker-siddhi repos.","title":"Siddhi Kubernetes Operator"},{"location":"development/source/#siddhi-extensions","text":"Find the supported Siddhi extensions and source here","title":"Siddhi Extensions"},{"location":"docs/","text":"Siddhi 5.1 User Guide This section provides information on using and running Siddhi. Checkout the Siddhi features to get and idea on what it can do in brief. Writing Siddhi Applications Writing steam processing logic in Siddhi is all about building Siddhi Applications. A Siddhi Application is a script with .siddhi file extension having self-contained stream processing logic. The stream processing constructs, such as streams and queries, defined within a Siddhi App is not visible even to the other Siddhi Apps running in the same JVM. It is recommended to have different business usecase in separate Siddhi Applications, where it allow users to selectively deploy the applications based on business needs. It is also recommended to move the repeated steam processing logics that exist in multiple Siddhi Applications, such as message retrieval and preprocessing, to a common Siddhi Application, whereby reducing code duplication and improving maintainability. In this case, to pass the events from one Siddhi App to another, users can configure common topic using In-Memory Sink and In-Memory Source in order to communicate between them. To write Siddhi Applications using Siddhi Streaming SQL refer Siddhi Query Guide for details. For specific API information on Siddhi functions and features refer Siddhi API Guide . Find out about the supported Siddhi extensions and their versions here . Executing Siddhi Applications Siddhi can run in multiple environments as follows. As a Java Library As a Local Microservice As a Docker Microservice As a Kubernetes Microservice As a Python Library (WIP) Siddhi Configurations Refer the Siddhi Config Guide for information on advance Siddhi execution configurations. System Requirements For all Siddhi execution modes following are the general system requirements. Memory - 128 MB (minimum), 500 MB (recommended), higher memory might be needed based on in-memory data stored for processing Cores - 2 cores (recommended), use lower number of cores after testing Siddhi Apps for performance JDK - 8 or 11 To build Siddhi from the Source distribution, it is necessary that you have JDK version 8 or 11 and Maven 3.0.4 or later","title":"Introduction"},{"location":"docs/#siddhi-51-user-guide","text":"This section provides information on using and running Siddhi. Checkout the Siddhi features to get and idea on what it can do in brief.","title":"Siddhi 5.1 User Guide"},{"location":"docs/#writing-siddhi-applications","text":"Writing steam processing logic in Siddhi is all about building Siddhi Applications. A Siddhi Application is a script with .siddhi file extension having self-contained stream processing logic. The stream processing constructs, such as streams and queries, defined within a Siddhi App is not visible even to the other Siddhi Apps running in the same JVM. It is recommended to have different business usecase in separate Siddhi Applications, where it allow users to selectively deploy the applications based on business needs. It is also recommended to move the repeated steam processing logics that exist in multiple Siddhi Applications, such as message retrieval and preprocessing, to a common Siddhi Application, whereby reducing code duplication and improving maintainability. In this case, to pass the events from one Siddhi App to another, users can configure common topic using In-Memory Sink and In-Memory Source in order to communicate between them. To write Siddhi Applications using Siddhi Streaming SQL refer Siddhi Query Guide for details. For specific API information on Siddhi functions and features refer Siddhi API Guide . Find out about the supported Siddhi extensions and their versions here .","title":"Writing Siddhi Applications"},{"location":"docs/#executing-siddhi-applications","text":"Siddhi can run in multiple environments as follows. As a Java Library As a Local Microservice As a Docker Microservice As a Kubernetes Microservice As a Python Library (WIP)","title":"Executing Siddhi Applications"},{"location":"docs/#siddhi-configurations","text":"Refer the Siddhi Config Guide for information on advance Siddhi execution configurations.","title":"Siddhi Configurations"},{"location":"docs/#system-requirements","text":"For all Siddhi execution modes following are the general system requirements. Memory - 128 MB (minimum), 500 MB (recommended), higher memory might be needed based on in-memory data stored for processing Cores - 2 cores (recommended), use lower number of cores after testing Siddhi Apps for performance JDK - 8 or 11 To build Siddhi from the Source distribution, it is necessary that you have JDK version 8 or 11 and Maven 3.0.4 or later","title":"System Requirements"},{"location":"docs/config-guide/","text":"Siddhi 5.1 Config Guide Configuring Databases Applicable only for Local, Docker, and Kubernetes modes. This section is not applicable for Java and Python modes. It is recommended to configure RDBMS databases as datasources under datasources section of Siddhi configuration yaml, and pass it during startup, this will allow database to reuse connections across multiple Siddhi Apps. By default Siddhi stores product-specific data in predefined embedded H2 database located in SIDDHI_RUNNER_HOME /wso2/runner/database directory. Here, the default H2 database is only suitable for development, testing, and some production environments which do not store data. However, for most production environments we recommend using industry-standard RDBMS such as Oracle, PostgreSQL, MySQL, or MSSQL. In this case users are expected to add the relevant database drivers to Siddhi's class-path. Including database drivers. The database driver corresponding to the database should be an OSGi bundle and it need to be added to SIDDHI_RUNNER_HOME /lib/ directory. If the driver is a jar then this should be converted to an OSGi bundle before adding . Converting Non OSGi drivers. If the database driver is not an OSGi bundle, then it should be converted to OSGi. Please refer Converting Jars to OSGi Bundles documentation for details. The necessary table schemas are self generated by the features themselves, other than the tables needed for statistics reporting via databases . Below are the sample datasource configuration for each supported database types: MySQL 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 dataSources : - name : SIDDHI_TEST_DB description : The datasource used for test database jndiConfig : name : jdbc/SIDDHI_TEST_DB definition : type : RDBMS configuration : jdbcUrl : jdbc:mysql://hostname:port/testdb username : root password : root driverClassName : com.mysql.jdbc.Driver maxPoolSize : 10 idleTimeout : 60000 connectionTestQuery : SELECT 1 validationTimeout : 30000 isAutoCommit : false Oracle There are two ways to configure Oracle. If you have a System Identifier (SID), use this (older) format: jdbc:oracle:thin:@[HOST][:PORT]:SID 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 dataSources : - name : SIDDHI_TEST_DB description : The datasource used for test database jndiConfig : name : jdbc/SIDDHI_TEST_DB definition : type : RDBMS configuration : jdbcUrl : jdbc:oracle:thin:@hostname:port:SID username : testdb password : root driverClassName : oracle.jdbc.driver.OracleDriver maxPoolSize : 10 idleTimeout : 60000 connectionTestQuery : SELECT 1 validationTimeout : 30000 isAutoCommit : false If you have an Oracle service name, use this (newer) format: jdbc:oracle:thin:@//[HOST][:PORT]/SERVICE 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 dataSources : - name : SIDDHI_TEST_DB description : The datasource used for test database jndiConfig : name : jdbc/SIDDHI_TEST_DB definition : type : RDBMS configuration : jdbcUrl : jdbc:oracle:thin:@hostname:port/SERVICE username : testdb password : root driverClassName : oracle.jdbc.driver.OracleDriver maxPoolSize : 50 idleTimeout : 60000 connectionTestQuery : SELECT 1 validationTimeout : 30000 isAutoCommit : false PostgreSQL 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 dataSources : - name : SIDDHI_TEST_DB description : The datasource used for test database jndiConfig : name : jdbc/SIDDHI_TEST_DB definition : type : RDBMS configuration : jdbcUrl : jdbc:postgresql://hostname:port/testdb username : root password : root driverClassName : org.postgresql.Driver maxPoolSize : 10 idleTimeout : 60000 connectionTestQuery : SELECT 1 validationTimeout : 30000 isAutoCommit : false MSSQL 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 dataSources : - name : SIDDHI_TEST_DB description : The datasource used for test database jndiConfig : name : jdbc/SIDDHI_TEST_DB definition : type : RDBMS configuration : jdbcUrl : jdbc:sqlserver://hostname:port;databaseName=testdb username : root password : root driverClassName : com.microsoft.sqlserver.jdbc.SQLServerDriver maxPoolSize : 10 idleTimeout : 60000 connectionTestQuery : SELECT 1 validationTimeout : 30000 isAutoCommit : false Configuring Periodic State Persistence Applicable only for Local, Docker, and Kubernetes modes. This section is not applicable for Java and Python modes. This explains how to periodically persisting the state of Siddhi either into a database system or file system, in order to prevent data losses that can result from a system failure. Persistence on Database To perform periodic state persistence on a database, the database should be configured as a datasource and the relevant jdbc drivers should be added to Siddhi's class-path. Refer Database Configuration section for more information. To configure database based periodic data persistence, add statePersistence section with the following properties on the Siddhi configuration yaml, and pass that during startup. Parameter Purpose Required Value enabled This enables data persistence. true intervalInMin The time interval in minutes that defines the interval in which state of Siddhi applications should be persisted 1 revisionsToKeep The number of revisions to keep in the system. Here when a new persistence takes place, the older revisions are removed. 3 persistenceStore The persistence store io.siddhi.distribution.core.persistence.DBPersistenceStore config datasource The datasource to be used in persisting the state. The datasource should be defined in the Siddhi configuration yaml. For detailed instructions of how to configure a datasource, see Database Configuration . SIDDHI_PERSISTENCE_DB (A datasource that is defined in datasources in Siddhi configuration yaml) config table The table that should be created and used for persisting states. PERSISTENCE_TABLE The following is a sample configuration for database based state persistence. 1 2 3 4 5 6 7 8 statePersistence : enabled : true intervalInMin : 1 revisionsToKeep : 3 persistenceStore : io.siddhi.distribution.core.persistence.DBPersistenceStore config : datasource : DATASOURCE NAME # A datasource with this name should be defined in datasources namespace table : TABLE NAME Persistence on File System To configure file system based periodic data persistence, add statePersistence section with the following properties on the Siddhi configuration yaml, and pass that during startup. Parameter Purpose Required Value enabled This enables data persistence. true intervalInMin The time interval in minutes that defines the interval in which state of Siddhi applications should be persisted 1 revisionsToKeep The number of revisions to keep in the system. Here when a new persistence takes place, the older revisions are removed. 3 persistenceStore The persistence store io.siddhi.distribution.core.persistence.FileSystemPersistenceStore config location A fully qualified folder location to where the revision files should be persisted. siddhi-app-persistence The following is a sample configuration for file system based state persistence. 1 2 3 4 5 6 7 statePersistence : enabled : true intervalInMin : 1 revisionsToKeep : 2 persistenceStore : io.siddhi.distribution.core.persistence.FileSystemPersistenceStore config : location : siddhi-app-persistence Configuring Siddhi Elements Applicable only for Local, Docker, and Kubernetes modes. This section is not applicable for Java and Python modes. You can configure some of there environment specific configurations in the Siddhi Configuration yaml rather than configuring in-line, such that your Siddhi Application can become potable between environments. Configuring Sources, Sinks and Stores References Multiple sources, sinks, and stores could be defined in Siddhi Configuration yaml as ref , and referred by several Siddhi Applications as described below. The following is the syntax for the configuration. 1 2 3 4 5 6 7 8 refs : - ref : name : name type : type properties : property1 : value1 property2 : value2 For each separate refs you want to configure, add a sub-section named ref under the refs subsection. The ref configured in Siddhi Configuration yaml can be referred from a Siddhi Application Source as follows. 1 2 3 @ Source ( ref = name , @ map ( type = json , @ attributes ( name = $.name , amount = $.quantity ))) define stream SweetProductionStream ( name string , amount double ); Similarly Sinks and Store Tables can also be configured and referred from Siddhi Apps. Example : Configuring http source using ref Following configuration defines the url and details about basic.auth , in the Siddhi Configuration yaml. 1 2 3 4 5 6 7 8 refs : - ref : name : http-passthrough type : http properties : receiver.url : http://0.0.0.0:8008/sweet-production basic.auth.enabled : false This can be referred in the Siddhi Applications as follows. 1 2 3 @ Source ( ref = http-passthrough , @ map ( type = json , @ attributes ( name = $.name , amount = $.quantity ))) define stream SweetProductionStream ( name string , amount double ); Configuring Extensions System Parameters Siddhi extensions cater use-case specific logic that is not available by default in Siddhi. Some of these extensions have system parameter configurations to define/modify their behavior. These extensions usually have default values for the parameters, but when needed, they can be overridden by configuring the parameters in Siddhi Configuration yaml and passing it at startup. The following is the syntax for the configuration. 1 2 3 4 5 6 7 extensions : - extension : name : extension name namespace : extension namespace properties : key : value For each separate extension you want to configure, add a sub-section named extension under the extensions subsection. Following are some examples on overriding default system properties via Siddhi Configuration yaml Example 1 : Defining service host and port for the TCP source 1 2 3 4 5 6 7 extensions : - extension : name : tcp namespace : source properties : host : 0.0.0.0 port : 5511 Example 2 : Overwriting the default RDBMS extension configuration 1 2 3 4 5 6 7 8 9 10 extensions : - extension : name : rdbms namespace : store properties : mysql.batchEnable : true mysql.batchSize : 1000 mysql.indexCreateQuery : CREATE INDEX {{TABLE_NAME}}_INDEX ON {{TABLE_NAME}} ({{INDEX_COLUMNS}}) mysql.recordDeleteQuery : DELETE FROM {{TABLE_NAME}} {{CONDITION}} mysql.recordExistsQuery : SELECT 1 FROM {{TABLE_NAME}} {{CONDITION}} LIMIT 1 Configuring Siddhi Properties Siddhi supports setting following properties to be specify distribution based behaviours, for instance all Named Aggregation in the distribution can be changed to Distributed Named Aggregation with the following siddhi properties. System Property Description Possible Values Optional Default Value shardId The id of the shard one of the distributed aggregation is running in. This should be unique to a single shard Any string No partitionById This allows user to enable/disable distributed aggregation for all aggregations running in one siddhi manager .(Available from v4.3.3) true/false Yes false Following is the example of setting Distributed Named Aggregation 1 2 3 properties : partitionById : true shardId : shard1 Configuring Authentication Applicable only for Local, Docker, and Kubernetes modes. This section is not applicable for Java and Python modes. Siddhi is configured with user name admin , and password admin . This can be updated by adding related user management configuration as authentication to the Siddhi Configuration yaml, and pass it at startup. A sample authentication is as follows. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 # Authentication configuration authentication : type : local # Type of the IdP client used userManager : adminRole : admin # Admin role which is granted all permissions userStore : # User store users : - user : username : admin password : YWRtaW4= roles : 1 roles : - role : id : 1 displayName : admin Adding Extensions and Third Party Dependencies Applicable for all modes. For certain use-cases, Siddhi might require extensions and/or third party dependencies to fulfill some characteristics that it does not provide by default. This section provides details on how to add or update extension and/or third party dependencies that is needed by Siddhi. Adding to Siddhi Java Program When running Siddhi as a Java library, the extension jars and/or third-party dependencies needed for Siddhi can be simply added to Siddhi class-path. When Maven is used as the build tool add them to the pom.xml file along with the other mandatory jars needed by Siddhi as given is Using Siddhi as a library guide. A sample on adding siddhi-io-http extension to the Maven pom.xml is as follows. 1 2 3 4 5 6 !--HTTP extension-- dependency groupId org.wso2.extension.siddhi.io.http /groupId artifactId siddhi-io-http /artifactId version ${siddhi.io.http.version} /version /dependency Refer guide for more details on using Siddhi as a Java Library. Adding to Siddhi Local Microservice The most used Siddhi extensions are packed by default with the Siddhi Local Microservice distribution. To add or update Siddhi extensions and/or third-party dependencies, you can use SIDDHI_RUNNER_HOME /jars and SIDDHI_RUNNER_HOME /bundles directories. SIDDHI_RUNNER_HOME /jars directory : Maintained for Jar files which may not have their corresponding OSGi bundle implementation. These Jars will be converted as OSGI bundles and copied to Siddhi Runner distribution during server startup. SIDDHI_RUNNER_HOME /bundles directory : Maintained for OSGI bundles which you need to copy to Siddhi Runner distribution during server startup. Updates to these directories will be adapted after a server restart. Refer guide for more details on using Siddhi as Local Microservice. Adding to Siddhi Docker Microservice The most used Siddhi extensions are packed by default with the Siddhi Docker Microservice distribution. To add or update Siddhi extensions and/or third-party dependencies, a new docker image has to be built from either siddhi-runner-base-ubuntu or siddhi-runner-base-alpine images. These images contain Linux OS, JDK and the Siddhi distribution. Sample docker file using siddhi-runner-base-alpine is as follows. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 # use siddhi-runner-base FROM siddhiio/siddhi-runner-base-alpine:5.1.0-alpha MAINTAINER Siddhi IO Docker Maintainers siddhi-dev@googlegroups.com ARG HOST_BUNDLES_DIR = ./files/bundles ARG HOST_JARS_DIR = ./files/jars ARG JARS = ${ RUNTIME_SERVER_HOME } /jars ARG BUNDLES = ${ RUNTIME_SERVER_HOME } /bundles # copy entrypoint bash script to user home COPY --chown = siddhi_user:siddhi_io init.sh ${ WORKING_DIRECTORY } / # copy bundles jars to the siddhi-runner distribution COPY --chown = siddhi_user:siddhi_io ${ HOST_BUNDLES_DIR } / ${ BUNDLES } COPY --chown = siddhi_user:siddhi_io ${ HOST_JARS_DIR } / ${ JARS } # expose ports EXPOSE 9090 9443 9712 9612 7711 7611 7070 7443 RUN bash ${ RUNTIME_SERVER_HOME } /bin/install-jars.sh STOPSIGNAL SIGINT ENTRYPOINT [ /home/siddhi_user/init.sh , -- ] Find the necessary artifacts to build the docker from docker-siddhi repository. DOCKERFILE_HOME gt/siddhi-runner/files contains two directories (bundles and jars directories) where you can copy the Jars and Bundles you need to bundle into the docker image. Jars directory - Maintained for Jar files which may not have their corresponding OSGi bundle implementation. These Jars will be converted as OSGI bundles and copied to Siddhi Runner docker image during docker build phase. Bundles directory - Maintained for OSGI bundles which you need to copy to Siddhi Runner docker image directory during docker build phase. Refer guide for more details on using Siddhi as Docker Microservice. Adding to Siddhi Kubernetes Microservice To add or update Siddhi extensions and/or third-party dependencies, a custom docker image has to be created using the steps described in Adding to Siddhi Docker Microservice documentation including the necessary extensions and dependencies. The created image can be then referenced in the sepc.pod subsection in the SiddhiProcess Kubernetes artifact created to deploy Siddhi in Kubernetes. For details on creating the Kubernetes artifacts refer Using Siddhi as Kubernetes Microservice documentation. Configuring Statistics Applicable only for Local, Docker, and Kubernetes modes. This section is not applicable for Java and Python modes. Siddhi uses dropwizard metrics library to calculate Siddhi and JVM statistics, and it can report the results via JMX Mbeans, console or database. To enable statistics, the relevant configuration under metrics section should be added to the Siddhi Configuration yaml as follows, and at the same time the statistics collection should be enabled in the Siddhi Application which is being monitored. Refer Siddhi Application Statistics documentation for enabling Siddhi Application level statistics. Configuring Metrics reporting level. To modify the statistics reporting, relevant metric names can be added under the metrics.levels subsection in the Siddhi Configurations yaml, along with the metrics level (i.e., OFF, INFO, DEBUG, TRACE, or ALL) as given below. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 metrics : # Metrics Levels are organized from most specific to least: # OFF (most specific, no metrics) # INFO # DEBUG # TRACE (least specific, a lot of data) # ALL (least specific, all data) levels : # The root level configured for Metrics rootLevel : INFO # Metric Levels levels : jvm.buffers : OFF jvm.class-loading : INFO jvm.gc : DEBUG jvm.memory : INFO The available metrics reporting options are as follows. Reporting via JMX Mbeans JMX Mbeans is the default statistics reporting option of Siddhi. To enable stats with the default configuration add the metric-related properties under metrics section in the Siddhi Configurations yaml file, and pass that during startup. A sample configuration is as follows. 1 2 metrics : enabled : true This will report JMX Mbeans in the name of org.wso2.carbon.metrics . However, in this default configuration the JVM metrics will not be measured. A detail JMX configuration along with the metrics reporting level is as follows. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 metrics : # Enable Metrics enabled : true jmx : # Register MBean when initializing Metrics registerMBean : true # MBean Name name : org.wso2.carbon:type=Metrics # Metrics Levels are organized from most specific to least: # OFF (most specific, no metrics) # INFO # DEBUG # TRACE (least specific, a lot of data) # ALL (least specific, all data) levels : # The root level configured for Metrics rootLevel : INFO # Metric Levels levels : jvm.buffers : OFF jvm.class-loading : INFO jvm.gc : DEBUG jvm.memory : INFO Reporting via Console To enable statistics by periodically printing the metrics on console add the following configuration to the the Siddhi Configurations yaml file, and pass that during startup. 1 2 3 4 5 6 7 8 9 10 11 12 13 # This is the main configuration for metrics metrics : # Enable Metrics enabled : false reporting : console : - # The name for the Console Reporter name : Console # Enable Console Reporter enabled : false # Polling Period in seconds. # This is the period for polling metrics from the metric registry and printing in the console pollingPeriod : 5 Reporting via Database To enable JDBC reporting and to periodically clean up the outdated statistics from the database, first a datasource should be created with the relevant database configurations and then the related metrics properties as given below should be added to in the Siddhi Configurations yaml file, and pass that during startup. The below sample is referring to the datasource with JNDI name jdbc/SiddhiMetricsDB , hence the datasource configuration in yaml should have jndiConfig.name as jdbc/SiddhiMetricsDB . For detailed instructions on configuring a datasource, refer Configuring Databases . . The scripts to create these tables are provided in the SIDDHI_RUNNER_HOME /wso2/runner/dbscripts directory. Sample configuration of reporting via database. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 metrics : enabled : true jdbc : # Data Source Configurations for JDBC Reporters dataSource : - JDBC01 dataSourceName : java:comp/env/jdbc/SiddhiMetricsDB scheduledCleanup : enabled : false daysToKeep : 7 scheduledCleanupPeriod : 86400 reporting : jdbc : - # The name for the JDBC Reporter name : JDBC enabled : true dataSource : *JDBC01 pollingPeriod : 60 Metrics history and reporting interval If the metrics.reporting.jdbc subsection is not enabled, the information relating to metrics history will not be persisted for future references. Also note the that the reporting will only start to update the database after the given pollingPeriod time has elapsed. Information about the parameters configured under the jdbc.dataSource subsection in the Siddhi Configuration yaml is as follows. Parameter Default Value Description dataSourceName java:comp/env/jdbc/SiddhiMetricsDB java:comp/env/ datasource JNDI name . The JNDI name of the datasource used to store metric data. scheduledCleanup.enabled false If this is set to true, metrics data stored in the database is cleared periodically based on scheduled time interval. scheduledCleanup.daysToKeep 3 If scheduled clean-up of metric data is enabled, all metric data in the database that are older than the number of days specified in this parameter are deleted. scheduledCleanup.scheduledCleanupPeriod 86400 The parameter specifies the time interval in seconds at which metric data should be cleaned. Converting Jars to OSGi Bundles To convert jar files to OSGi bundles, first download and save the non-OSGi jar it in a preferred directory in your machine. Then from the CLI, navigate to the SIDDHI_RUNNER_HOME /bin directory, and issue the following command. 1 ./jartobundle.sh path to non OSGi jar ../lib This converts the Jar to OSGi bundles and place it in SIDDHI_RUNNER_HOME /lib directory. Encrypt sensitive deployment configurations Cipher tool is used to encrypt sensitive data in deployment configurations. This tool works in conjunction with Secure Vault to replace sensitive data that is in plain text with an alias. The actual value is then encrypted and securely stored in the SecureVault. At runtime, the actual value is retrieved from the alias and used. For more information, see Secure Vault . Below is the default configurations for Secure Vault 1 2 3 4 5 6 7 8 9 10 11 12 # Secure Vault Configuration securevault : secretRepository : type : org.wso2.carbon.secvault.repository.DefaultSecretRepository parameters : privateKeyAlias : wso2carbon keystoreLocation : ${SIDDHI_RUNNER_HOME}/resources/security/securevault.jks secretPropertiesFile : ${SIDDHI_RUNNER_HOME}/conf/runner/secrets.properties masterKeyReader : type : org.wso2.carbon.secvault.reader.DefaultMasterKeyReader parameters : masterKeyReaderFile : ${SIDDHI_RUNNER_HOME}/conf/runner/master-keys.yaml Information about the parameters configured under the securevault subsection in the Siddhi Configuration yaml is as follows. Parameter Default Value Description secretRepository type org.wso2.carbon.secvault.repository.DefaultSecretRepository The default implementation of Secret Repository is based on the passwords and aliases given in the secrets.properties file and the JKS that is configured in the secure-vault.yaml file secretPropertiesFile ${SIDDHI_RUNNER_HOME}/conf/runner/secrets.properties Location of the secrect.properties file which matches alias with encrypted data secretPropertiesFile ${SIDDHI_RUNNER_HOME}/resources/security/securevault.jks Keystore which contains the certificate to encrypt sensitive data privateKeyAlias wso2carbon Alias of the certificate in the key store used for encryption masterKeyReader type org.wso2.carbon.secvault.reader.DefaultMasterKeyReader The default implementation of MasterKeyReader gets a list of required passwords from the Secret Repository and provides the values for those passwords by reading system properties, environment variables and the master-keys.yaml file. masterKeyReaderFile ${SIDDHI_RUNNER_HOME\\}/conf/runner/master-keys.yaml Location of master-keys.yaml file which contains password used to access the key store to decrypt the encrypted passwords at runtime Configuring server properties Siddhi runner and tooling distribution is based on WSO2 Carbon 5 Kernel platform. The properties for the server can be configure under wso2.carbon namespace. Sample configurations is as follows, 1 2 3 wso2.carbon : id : siddhi-runner name : Siddhi Runner Distribution Configure port offset Port offset defines the number by which all ports defined in the runtime such as the HTTP/S ports will be offset. For example, if the default HTTP port is 9090 and the ports offset is 1, the effective HTTP port will be 9091. This configuration allows to change ports in a uniform manner across the transports. Below is the sample configurations for offsets, 1 2 3 4 5 wso2.carbon : id : siddhi-runner name : Siddhi Runner Distribution ports : offset : 1 Configuring Databridge Transport Siddhi uses Databridge transport to send and receive events over Thrift/Binary protocols, This can be used through siddhi-io-wso2event extension. Sample Configuration is as follows, 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 transports : databridge : # Configuration used for the databridge communication listenerConfigurations : workerThreads : 10 . . . senderConfigurations : # Configuration of the Data Agents - to publish events through databridge agents : agentConfiguration : name : Thrift dataEndpointClass : org.wso2.carbon.databridge.agent.endpoint.thrift.ThriftDataEndpoint . . . Here, transports databridge includes listenerConfigurations, to configure databridge receiver in WSO2Event Source, and senderConfigurations, to configure agents used to publish events over databridge in WSO2Event Sink Configuring databridge listener Sample configuration for databridge listener and properties are as follows, 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 transports : databridge : listenerConfigurations : workerThreads : 10 maxEventBufferCapacity : 10 eventBufferSize : 2000 keyStoreLocation : ${sys:carbon.home}/resources/security/wso2carbon.jks keyStorePassword : wso2carbon clientTimeoutMin : 30 # Data receiver configurations dataReceivers : - dataReceiver : type : Thrift properties : tcpPort : 7611 sslPort : 7711 - dataReceiver : type : Binary properties : tcpPort : 9611 sslPort : 9711 tcpReceiverThreadPoolSize : 100 sslReceiverThreadPoolSize : 100 hostName : 0.0.0.0 Parameter Default Value Description workerThreads 10 No of worker threads to consume events maxEventBufferCapacity 10 Maximum amount of messages that can be queued internally in Message Buffer eventBufferSize 2000 Maximum number of events that can be stored in the queue clientTimeoutMin 30 Session timeout value in minutes keyStoreLocation ${SIDDHIRUNNER_HOME}/resources/security/wso2carbon.jks Keystore file path Keystore password wo2carbon Keystore password dataReceivers Generalised configuration for different types of data receivers dataReceivers dataReceiver type Type of the data receiver Parameters for Thrift data receiver, Parameter Default Value Description tcpPort 7611 TCP port for the Thrift data receiver sslPort 7711 SSL port for the Thrift data receiver Parameters for Binary data receiver, Parameter Default Value Description tcpPort 7611 TCP port for the Binary data receiver sslPort 7711 SSL port for the Binary data receiver tcpReceiverThreadPoolSize 100 Receiver pool size for Thrift TCP protocol sslReceiverThreadPoolSize 100 Receiver pool size for Thrift SSL protocol hostname 0.0.0.0 Hostname for the Thrift receiver Configuring databridge publisher Note By default both Thrift and Binary agents will be started. Sample configuration for databridge agent(publisher) and properties are as follows, 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 transports : databridge : senderConfigurations : agents : - agentConfiguration : name : Thrift dataEndpointClass : org.wso2.carbon.databridge.agent.endpoint.thrift.ThriftDataEndpoint publishingStrategy : async trustStorePath : ${sys:carbon.home}/resources/security/client-truststore.jks trustStorePassword : wso2carbon queueSize : 32768 batchSize : 200 corePoolSize : 1 socketTimeoutMS : 30000 maxPoolSize : 1 keepAliveTimeInPool : 20 reconnectionInterval : 30 maxTransportPoolSize : 250 maxIdleConnections : 250 evictionTimePeriod : 5500 minIdleTimeInPool : 5000 secureMaxTransportPoolSize : 250 secureMaxIdleConnections : 250 secureEvictionTimePeriod : 5500 secureMinIdleTimeInPool : 5000 sslEnabledProtocols : TLSv1.1,TLSv1.2 ciphers : TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256,TLS_DHE_RSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA,TLS_DHE_RSA_WITH_AES_128_CBC_SHA,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_DHE_RSA_WITH_AES_128_GCM_SHA256 - agentConfiguration : name : Binary dataEndpointClass : org.wso2.carbon.databridge.agent.endpoint.binary.BinaryDataEndpoint publishingStrategy : async trustStorePath : ${sys:carbon.home}/resources/security/client-truststore.jks trustStorePassword : wso2carbon queueSize : 32768 batchSize : 200 corePoolSize : 1 socketTimeoutMS : 30000 maxPoolSize : 1 keepAliveTimeInPool : 20 reconnectionInterval : 30 maxTransportPoolSize : 250 maxIdleConnections : 250 evictionTimePeriod : 5500 minIdleTimeInPool : 5000 secureMaxTransportPoolSize : 250 secureMaxIdleConnections : 250 secureEvictionTimePeriod : 5500 secureMinIdleTimeInPool : 5000 sslEnabledProtocols : TLSv1.1,TLSv1.2 ciphers : TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256,TLS_DHE_RSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA,TLS_DHE_RSA_WITH_AES_128_CBC_SHA,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_DHE_RSA_WITH_AES_128_GCM_SHA256 Parameter Default Value Description name Thrift / Binary Name of the databridge agent dataEndpointClass org.wso2.carbon.databridge.agent.endpoint.thrift.ThriftDataEndpoint / org.wso2.carbon.databridge.agent.endpoint.thrift.ThriftDataEndpoint Class of the databridge agent initialised publishingStrategy async Strategy used for publishing. Can be either sync or async trustStorePath ${sys:carbon.home\\}/resources/security/client-truststore.jks Truststore file path trustStorePassword wso2carbon Trust store password queueSize 32768 Queue size used to hold events before publishing batchSize 200 Size of a publishing batch of events corePoolSize 1 Pool size of the threads used to buffer before publishing maxPoolSize 1 Maximum pool size for threads used to buffer before publishing socketTimeoutMS 30000 Time for socket to timeout in Milliseconds keepAliveTimeInPool 20 Time used to keep the threads live reconnectionInterval 30 Reconnection interval in case of lost transmission maxTransportPoolSize 250 Transport threads used for publishing maxIdleConnections 250 Maximum idle connections maintained in the databridge evictionTimePeriod 5500 Eviction time interval minIdleTimeInPool 5500 Min idle time in pool secureMaxTransportPoolSize 250 Max transport pool size in SSL publishing secureMaxIdleConnections 250 Max idle connections in SSL publishing secureEvictionTimePeriod 5500 Eviction time period in SSL publishing secureMinIdleTimeInPool 5500 Min idle time in pool in SSL publishing sslEnabledProtocols TLSv1.1,TLSv1.2 SSL enabled protocols ciphers TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256, TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256, TLS_DHE_RSA_WITH_AES_128_CBC_SHA256, TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA, TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA, TLS_DHE_RSA_WITH_AES_128_CBC_SHA, TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256, TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256, TLS_DHE_RSA_WITH_AES_128_GCM_SHA256 Ciphers used in transmission Configuring Admin REST APIs Admin API can be configured under the namespace transports http . Sample Config and the parameters are as follows, 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 transports : http : listenerConfigurations : - id : default host : 0.0.0.0 port : 9090 - id : msf4j-https host : 0.0.0.0 port : 9443 scheme : https sslConfig : keyStore : ${carbon.home}/resources/security/wso2carbon.jks keyStorePassword : wso2carbon transportProperties : - name : server.bootstrap.socket.timeout value : 60 - name : latency.metrics.enabled value : false Parameter Default Value Description id default Id of the server host 0.0.0.0 Hostname of the server port 8080 Port of the APIs scheme http Scheme of the APIs. It can be either http or https httpTraceLogEnabled false Enable HTTP trace logs httpAccessLogEnabled false Enable HTTP access logs socketIdleTimeout 0 Timeout for socket for which requests received. Not set by default. SSL configurations (listenerConfigurations sslConfig) Parameter Default Value Description keyStore ${carbon.home}/resources/security/wso2carbon.jks The file containing the private key of the client keyStorePass wso2carbon Password of the private key if it is encrypted enableProtocols All SSL/TLS protocols to be enabled (e.g.: TLSv1,TLSv1.1,TLSv1.2) cipherSuites All List of ciphers to be used eg: TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256, TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA enableSessionCreation Enable/disable new SSL session creation sessionTimeOut 0 SSL session time out. Not set by default. handshakeTimeOut 0 SSL handshake time out. Not set by default. Transport Properties (transportProperties) Parameter Default Value Description server.bootstrap.connect.timeout 15000 Timout in millisecond to establish connection server.bootstrap.socket.timeout 60 Socket connection timeouts latency.metrics.enabled false Enable/Disable latency metrics by carbon metrics component","title":"Configuration Guide"},{"location":"docs/config-guide/#siddhi-51-config-guide","text":"","title":"Siddhi 5.1 Config Guide"},{"location":"docs/config-guide/#configuring-databases","text":"Applicable only for Local, Docker, and Kubernetes modes. This section is not applicable for Java and Python modes. It is recommended to configure RDBMS databases as datasources under datasources section of Siddhi configuration yaml, and pass it during startup, this will allow database to reuse connections across multiple Siddhi Apps. By default Siddhi stores product-specific data in predefined embedded H2 database located in SIDDHI_RUNNER_HOME /wso2/runner/database directory. Here, the default H2 database is only suitable for development, testing, and some production environments which do not store data. However, for most production environments we recommend using industry-standard RDBMS such as Oracle, PostgreSQL, MySQL, or MSSQL. In this case users are expected to add the relevant database drivers to Siddhi's class-path. Including database drivers. The database driver corresponding to the database should be an OSGi bundle and it need to be added to SIDDHI_RUNNER_HOME /lib/ directory. If the driver is a jar then this should be converted to an OSGi bundle before adding . Converting Non OSGi drivers. If the database driver is not an OSGi bundle, then it should be converted to OSGi. Please refer Converting Jars to OSGi Bundles documentation for details. The necessary table schemas are self generated by the features themselves, other than the tables needed for statistics reporting via databases . Below are the sample datasource configuration for each supported database types: MySQL 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 dataSources : - name : SIDDHI_TEST_DB description : The datasource used for test database jndiConfig : name : jdbc/SIDDHI_TEST_DB definition : type : RDBMS configuration : jdbcUrl : jdbc:mysql://hostname:port/testdb username : root password : root driverClassName : com.mysql.jdbc.Driver maxPoolSize : 10 idleTimeout : 60000 connectionTestQuery : SELECT 1 validationTimeout : 30000 isAutoCommit : false Oracle There are two ways to configure Oracle. If you have a System Identifier (SID), use this (older) format: jdbc:oracle:thin:@[HOST][:PORT]:SID 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 dataSources : - name : SIDDHI_TEST_DB description : The datasource used for test database jndiConfig : name : jdbc/SIDDHI_TEST_DB definition : type : RDBMS configuration : jdbcUrl : jdbc:oracle:thin:@hostname:port:SID username : testdb password : root driverClassName : oracle.jdbc.driver.OracleDriver maxPoolSize : 10 idleTimeout : 60000 connectionTestQuery : SELECT 1 validationTimeout : 30000 isAutoCommit : false If you have an Oracle service name, use this (newer) format: jdbc:oracle:thin:@//[HOST][:PORT]/SERVICE 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 dataSources : - name : SIDDHI_TEST_DB description : The datasource used for test database jndiConfig : name : jdbc/SIDDHI_TEST_DB definition : type : RDBMS configuration : jdbcUrl : jdbc:oracle:thin:@hostname:port/SERVICE username : testdb password : root driverClassName : oracle.jdbc.driver.OracleDriver maxPoolSize : 50 idleTimeout : 60000 connectionTestQuery : SELECT 1 validationTimeout : 30000 isAutoCommit : false PostgreSQL 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 dataSources : - name : SIDDHI_TEST_DB description : The datasource used for test database jndiConfig : name : jdbc/SIDDHI_TEST_DB definition : type : RDBMS configuration : jdbcUrl : jdbc:postgresql://hostname:port/testdb username : root password : root driverClassName : org.postgresql.Driver maxPoolSize : 10 idleTimeout : 60000 connectionTestQuery : SELECT 1 validationTimeout : 30000 isAutoCommit : false MSSQL 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 dataSources : - name : SIDDHI_TEST_DB description : The datasource used for test database jndiConfig : name : jdbc/SIDDHI_TEST_DB definition : type : RDBMS configuration : jdbcUrl : jdbc:sqlserver://hostname:port;databaseName=testdb username : root password : root driverClassName : com.microsoft.sqlserver.jdbc.SQLServerDriver maxPoolSize : 10 idleTimeout : 60000 connectionTestQuery : SELECT 1 validationTimeout : 30000 isAutoCommit : false","title":"Configuring Databases"},{"location":"docs/config-guide/#configuring-periodic-state-persistence","text":"Applicable only for Local, Docker, and Kubernetes modes. This section is not applicable for Java and Python modes. This explains how to periodically persisting the state of Siddhi either into a database system or file system, in order to prevent data losses that can result from a system failure.","title":"Configuring Periodic State Persistence"},{"location":"docs/config-guide/#persistence-on-database","text":"To perform periodic state persistence on a database, the database should be configured as a datasource and the relevant jdbc drivers should be added to Siddhi's class-path. Refer Database Configuration section for more information. To configure database based periodic data persistence, add statePersistence section with the following properties on the Siddhi configuration yaml, and pass that during startup. Parameter Purpose Required Value enabled This enables data persistence. true intervalInMin The time interval in minutes that defines the interval in which state of Siddhi applications should be persisted 1 revisionsToKeep The number of revisions to keep in the system. Here when a new persistence takes place, the older revisions are removed. 3 persistenceStore The persistence store io.siddhi.distribution.core.persistence.DBPersistenceStore config datasource The datasource to be used in persisting the state. The datasource should be defined in the Siddhi configuration yaml. For detailed instructions of how to configure a datasource, see Database Configuration . SIDDHI_PERSISTENCE_DB (A datasource that is defined in datasources in Siddhi configuration yaml) config table The table that should be created and used for persisting states. PERSISTENCE_TABLE The following is a sample configuration for database based state persistence. 1 2 3 4 5 6 7 8 statePersistence : enabled : true intervalInMin : 1 revisionsToKeep : 3 persistenceStore : io.siddhi.distribution.core.persistence.DBPersistenceStore config : datasource : DATASOURCE NAME # A datasource with this name should be defined in datasources namespace table : TABLE NAME","title":"Persistence on Database"},{"location":"docs/config-guide/#persistence-on-file-system","text":"To configure file system based periodic data persistence, add statePersistence section with the following properties on the Siddhi configuration yaml, and pass that during startup. Parameter Purpose Required Value enabled This enables data persistence. true intervalInMin The time interval in minutes that defines the interval in which state of Siddhi applications should be persisted 1 revisionsToKeep The number of revisions to keep in the system. Here when a new persistence takes place, the older revisions are removed. 3 persistenceStore The persistence store io.siddhi.distribution.core.persistence.FileSystemPersistenceStore config location A fully qualified folder location to where the revision files should be persisted. siddhi-app-persistence The following is a sample configuration for file system based state persistence. 1 2 3 4 5 6 7 statePersistence : enabled : true intervalInMin : 1 revisionsToKeep : 2 persistenceStore : io.siddhi.distribution.core.persistence.FileSystemPersistenceStore config : location : siddhi-app-persistence","title":"Persistence on File System"},{"location":"docs/config-guide/#configuring-siddhi-elements","text":"Applicable only for Local, Docker, and Kubernetes modes. This section is not applicable for Java and Python modes. You can configure some of there environment specific configurations in the Siddhi Configuration yaml rather than configuring in-line, such that your Siddhi Application can become potable between environments.","title":"Configuring Siddhi Elements"},{"location":"docs/config-guide/#configuring-sources-sinks-and-stores-references","text":"Multiple sources, sinks, and stores could be defined in Siddhi Configuration yaml as ref , and referred by several Siddhi Applications as described below. The following is the syntax for the configuration. 1 2 3 4 5 6 7 8 refs : - ref : name : name type : type properties : property1 : value1 property2 : value2 For each separate refs you want to configure, add a sub-section named ref under the refs subsection. The ref configured in Siddhi Configuration yaml can be referred from a Siddhi Application Source as follows. 1 2 3 @ Source ( ref = name , @ map ( type = json , @ attributes ( name = $.name , amount = $.quantity ))) define stream SweetProductionStream ( name string , amount double ); Similarly Sinks and Store Tables can also be configured and referred from Siddhi Apps. Example : Configuring http source using ref Following configuration defines the url and details about basic.auth , in the Siddhi Configuration yaml. 1 2 3 4 5 6 7 8 refs : - ref : name : http-passthrough type : http properties : receiver.url : http://0.0.0.0:8008/sweet-production basic.auth.enabled : false This can be referred in the Siddhi Applications as follows. 1 2 3 @ Source ( ref = http-passthrough , @ map ( type = json , @ attributes ( name = $.name , amount = $.quantity ))) define stream SweetProductionStream ( name string , amount double );","title":"Configuring Sources, Sinks and Stores References"},{"location":"docs/config-guide/#configuring-extensions-system-parameters","text":"Siddhi extensions cater use-case specific logic that is not available by default in Siddhi. Some of these extensions have system parameter configurations to define/modify their behavior. These extensions usually have default values for the parameters, but when needed, they can be overridden by configuring the parameters in Siddhi Configuration yaml and passing it at startup. The following is the syntax for the configuration. 1 2 3 4 5 6 7 extensions : - extension : name : extension name namespace : extension namespace properties : key : value For each separate extension you want to configure, add a sub-section named extension under the extensions subsection. Following are some examples on overriding default system properties via Siddhi Configuration yaml Example 1 : Defining service host and port for the TCP source 1 2 3 4 5 6 7 extensions : - extension : name : tcp namespace : source properties : host : 0.0.0.0 port : 5511 Example 2 : Overwriting the default RDBMS extension configuration 1 2 3 4 5 6 7 8 9 10 extensions : - extension : name : rdbms namespace : store properties : mysql.batchEnable : true mysql.batchSize : 1000 mysql.indexCreateQuery : CREATE INDEX {{TABLE_NAME}}_INDEX ON {{TABLE_NAME}} ({{INDEX_COLUMNS}}) mysql.recordDeleteQuery : DELETE FROM {{TABLE_NAME}} {{CONDITION}} mysql.recordExistsQuery : SELECT 1 FROM {{TABLE_NAME}} {{CONDITION}} LIMIT 1","title":"Configuring Extensions System Parameters"},{"location":"docs/config-guide/#configuring-siddhi-properties","text":"Siddhi supports setting following properties to be specify distribution based behaviours, for instance all Named Aggregation in the distribution can be changed to Distributed Named Aggregation with the following siddhi properties. System Property Description Possible Values Optional Default Value shardId The id of the shard one of the distributed aggregation is running in. This should be unique to a single shard Any string No partitionById This allows user to enable/disable distributed aggregation for all aggregations running in one siddhi manager .(Available from v4.3.3) true/false Yes false Following is the example of setting Distributed Named Aggregation 1 2 3 properties : partitionById : true shardId : shard1","title":"Configuring Siddhi Properties"},{"location":"docs/config-guide/#configuring-authentication","text":"Applicable only for Local, Docker, and Kubernetes modes. This section is not applicable for Java and Python modes. Siddhi is configured with user name admin , and password admin . This can be updated by adding related user management configuration as authentication to the Siddhi Configuration yaml, and pass it at startup. A sample authentication is as follows. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 # Authentication configuration authentication : type : local # Type of the IdP client used userManager : adminRole : admin # Admin role which is granted all permissions userStore : # User store users : - user : username : admin password : YWRtaW4= roles : 1 roles : - role : id : 1 displayName : admin","title":"Configuring Authentication"},{"location":"docs/config-guide/#adding-extensions-and-third-party-dependencies","text":"Applicable for all modes. For certain use-cases, Siddhi might require extensions and/or third party dependencies to fulfill some characteristics that it does not provide by default. This section provides details on how to add or update extension and/or third party dependencies that is needed by Siddhi.","title":"Adding Extensions and Third Party Dependencies"},{"location":"docs/config-guide/#adding-to-siddhi-java-program","text":"When running Siddhi as a Java library, the extension jars and/or third-party dependencies needed for Siddhi can be simply added to Siddhi class-path. When Maven is used as the build tool add them to the pom.xml file along with the other mandatory jars needed by Siddhi as given is Using Siddhi as a library guide. A sample on adding siddhi-io-http extension to the Maven pom.xml is as follows. 1 2 3 4 5 6 !--HTTP extension-- dependency groupId org.wso2.extension.siddhi.io.http /groupId artifactId siddhi-io-http /artifactId version ${siddhi.io.http.version} /version /dependency Refer guide for more details on using Siddhi as a Java Library.","title":"Adding to Siddhi Java Program"},{"location":"docs/config-guide/#adding-to-siddhi-local-microservice","text":"The most used Siddhi extensions are packed by default with the Siddhi Local Microservice distribution. To add or update Siddhi extensions and/or third-party dependencies, you can use SIDDHI_RUNNER_HOME /jars and SIDDHI_RUNNER_HOME /bundles directories. SIDDHI_RUNNER_HOME /jars directory : Maintained for Jar files which may not have their corresponding OSGi bundle implementation. These Jars will be converted as OSGI bundles and copied to Siddhi Runner distribution during server startup. SIDDHI_RUNNER_HOME /bundles directory : Maintained for OSGI bundles which you need to copy to Siddhi Runner distribution during server startup. Updates to these directories will be adapted after a server restart. Refer guide for more details on using Siddhi as Local Microservice.","title":"Adding to Siddhi Local Microservice"},{"location":"docs/config-guide/#adding-to-siddhi-docker-microservice","text":"The most used Siddhi extensions are packed by default with the Siddhi Docker Microservice distribution. To add or update Siddhi extensions and/or third-party dependencies, a new docker image has to be built from either siddhi-runner-base-ubuntu or siddhi-runner-base-alpine images. These images contain Linux OS, JDK and the Siddhi distribution. Sample docker file using siddhi-runner-base-alpine is as follows. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 # use siddhi-runner-base FROM siddhiio/siddhi-runner-base-alpine:5.1.0-alpha MAINTAINER Siddhi IO Docker Maintainers siddhi-dev@googlegroups.com ARG HOST_BUNDLES_DIR = ./files/bundles ARG HOST_JARS_DIR = ./files/jars ARG JARS = ${ RUNTIME_SERVER_HOME } /jars ARG BUNDLES = ${ RUNTIME_SERVER_HOME } /bundles # copy entrypoint bash script to user home COPY --chown = siddhi_user:siddhi_io init.sh ${ WORKING_DIRECTORY } / # copy bundles jars to the siddhi-runner distribution COPY --chown = siddhi_user:siddhi_io ${ HOST_BUNDLES_DIR } / ${ BUNDLES } COPY --chown = siddhi_user:siddhi_io ${ HOST_JARS_DIR } / ${ JARS } # expose ports EXPOSE 9090 9443 9712 9612 7711 7611 7070 7443 RUN bash ${ RUNTIME_SERVER_HOME } /bin/install-jars.sh STOPSIGNAL SIGINT ENTRYPOINT [ /home/siddhi_user/init.sh , -- ] Find the necessary artifacts to build the docker from docker-siddhi repository. DOCKERFILE_HOME gt/siddhi-runner/files contains two directories (bundles and jars directories) where you can copy the Jars and Bundles you need to bundle into the docker image. Jars directory - Maintained for Jar files which may not have their corresponding OSGi bundle implementation. These Jars will be converted as OSGI bundles and copied to Siddhi Runner docker image during docker build phase. Bundles directory - Maintained for OSGI bundles which you need to copy to Siddhi Runner docker image directory during docker build phase. Refer guide for more details on using Siddhi as Docker Microservice.","title":"Adding to Siddhi Docker Microservice"},{"location":"docs/config-guide/#adding-to-siddhi-kubernetes-microservice","text":"To add or update Siddhi extensions and/or third-party dependencies, a custom docker image has to be created using the steps described in Adding to Siddhi Docker Microservice documentation including the necessary extensions and dependencies. The created image can be then referenced in the sepc.pod subsection in the SiddhiProcess Kubernetes artifact created to deploy Siddhi in Kubernetes. For details on creating the Kubernetes artifacts refer Using Siddhi as Kubernetes Microservice documentation.","title":"Adding to Siddhi Kubernetes Microservice"},{"location":"docs/config-guide/#configuring-statistics","text":"Applicable only for Local, Docker, and Kubernetes modes. This section is not applicable for Java and Python modes. Siddhi uses dropwizard metrics library to calculate Siddhi and JVM statistics, and it can report the results via JMX Mbeans, console or database. To enable statistics, the relevant configuration under metrics section should be added to the Siddhi Configuration yaml as follows, and at the same time the statistics collection should be enabled in the Siddhi Application which is being monitored. Refer Siddhi Application Statistics documentation for enabling Siddhi Application level statistics. Configuring Metrics reporting level. To modify the statistics reporting, relevant metric names can be added under the metrics.levels subsection in the Siddhi Configurations yaml, along with the metrics level (i.e., OFF, INFO, DEBUG, TRACE, or ALL) as given below. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 metrics : # Metrics Levels are organized from most specific to least: # OFF (most specific, no metrics) # INFO # DEBUG # TRACE (least specific, a lot of data) # ALL (least specific, all data) levels : # The root level configured for Metrics rootLevel : INFO # Metric Levels levels : jvm.buffers : OFF jvm.class-loading : INFO jvm.gc : DEBUG jvm.memory : INFO The available metrics reporting options are as follows.","title":"Configuring Statistics"},{"location":"docs/config-guide/#reporting-via-jmx-mbeans","text":"JMX Mbeans is the default statistics reporting option of Siddhi. To enable stats with the default configuration add the metric-related properties under metrics section in the Siddhi Configurations yaml file, and pass that during startup. A sample configuration is as follows. 1 2 metrics : enabled : true This will report JMX Mbeans in the name of org.wso2.carbon.metrics . However, in this default configuration the JVM metrics will not be measured. A detail JMX configuration along with the metrics reporting level is as follows. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 metrics : # Enable Metrics enabled : true jmx : # Register MBean when initializing Metrics registerMBean : true # MBean Name name : org.wso2.carbon:type=Metrics # Metrics Levels are organized from most specific to least: # OFF (most specific, no metrics) # INFO # DEBUG # TRACE (least specific, a lot of data) # ALL (least specific, all data) levels : # The root level configured for Metrics rootLevel : INFO # Metric Levels levels : jvm.buffers : OFF jvm.class-loading : INFO jvm.gc : DEBUG jvm.memory : INFO","title":"Reporting via JMX Mbeans"},{"location":"docs/config-guide/#reporting-via-console","text":"To enable statistics by periodically printing the metrics on console add the following configuration to the the Siddhi Configurations yaml file, and pass that during startup. 1 2 3 4 5 6 7 8 9 10 11 12 13 # This is the main configuration for metrics metrics : # Enable Metrics enabled : false reporting : console : - # The name for the Console Reporter name : Console # Enable Console Reporter enabled : false # Polling Period in seconds. # This is the period for polling metrics from the metric registry and printing in the console pollingPeriod : 5","title":"Reporting via Console"},{"location":"docs/config-guide/#reporting-via-database","text":"To enable JDBC reporting and to periodically clean up the outdated statistics from the database, first a datasource should be created with the relevant database configurations and then the related metrics properties as given below should be added to in the Siddhi Configurations yaml file, and pass that during startup. The below sample is referring to the datasource with JNDI name jdbc/SiddhiMetricsDB , hence the datasource configuration in yaml should have jndiConfig.name as jdbc/SiddhiMetricsDB . For detailed instructions on configuring a datasource, refer Configuring Databases . . The scripts to create these tables are provided in the SIDDHI_RUNNER_HOME /wso2/runner/dbscripts directory. Sample configuration of reporting via database. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 metrics : enabled : true jdbc : # Data Source Configurations for JDBC Reporters dataSource : - JDBC01 dataSourceName : java:comp/env/jdbc/SiddhiMetricsDB scheduledCleanup : enabled : false daysToKeep : 7 scheduledCleanupPeriod : 86400 reporting : jdbc : - # The name for the JDBC Reporter name : JDBC enabled : true dataSource : *JDBC01 pollingPeriod : 60 Metrics history and reporting interval If the metrics.reporting.jdbc subsection is not enabled, the information relating to metrics history will not be persisted for future references. Also note the that the reporting will only start to update the database after the given pollingPeriod time has elapsed. Information about the parameters configured under the jdbc.dataSource subsection in the Siddhi Configuration yaml is as follows. Parameter Default Value Description dataSourceName java:comp/env/jdbc/SiddhiMetricsDB java:comp/env/ datasource JNDI name . The JNDI name of the datasource used to store metric data. scheduledCleanup.enabled false If this is set to true, metrics data stored in the database is cleared periodically based on scheduled time interval. scheduledCleanup.daysToKeep 3 If scheduled clean-up of metric data is enabled, all metric data in the database that are older than the number of days specified in this parameter are deleted. scheduledCleanup.scheduledCleanupPeriod 86400 The parameter specifies the time interval in seconds at which metric data should be cleaned.","title":"Reporting via Database"},{"location":"docs/config-guide/#converting-jars-to-osgi-bundles","text":"To convert jar files to OSGi bundles, first download and save the non-OSGi jar it in a preferred directory in your machine. Then from the CLI, navigate to the SIDDHI_RUNNER_HOME /bin directory, and issue the following command. 1 ./jartobundle.sh path to non OSGi jar ../lib This converts the Jar to OSGi bundles and place it in SIDDHI_RUNNER_HOME /lib directory.","title":"Converting Jars to OSGi Bundles"},{"location":"docs/config-guide/#encrypt-sensitive-deployment-configurations","text":"Cipher tool is used to encrypt sensitive data in deployment configurations. This tool works in conjunction with Secure Vault to replace sensitive data that is in plain text with an alias. The actual value is then encrypted and securely stored in the SecureVault. At runtime, the actual value is retrieved from the alias and used. For more information, see Secure Vault . Below is the default configurations for Secure Vault 1 2 3 4 5 6 7 8 9 10 11 12 # Secure Vault Configuration securevault : secretRepository : type : org.wso2.carbon.secvault.repository.DefaultSecretRepository parameters : privateKeyAlias : wso2carbon keystoreLocation : ${SIDDHI_RUNNER_HOME}/resources/security/securevault.jks secretPropertiesFile : ${SIDDHI_RUNNER_HOME}/conf/runner/secrets.properties masterKeyReader : type : org.wso2.carbon.secvault.reader.DefaultMasterKeyReader parameters : masterKeyReaderFile : ${SIDDHI_RUNNER_HOME}/conf/runner/master-keys.yaml Information about the parameters configured under the securevault subsection in the Siddhi Configuration yaml is as follows. Parameter Default Value Description secretRepository type org.wso2.carbon.secvault.repository.DefaultSecretRepository The default implementation of Secret Repository is based on the passwords and aliases given in the secrets.properties file and the JKS that is configured in the secure-vault.yaml file secretPropertiesFile ${SIDDHI_RUNNER_HOME}/conf/runner/secrets.properties Location of the secrect.properties file which matches alias with encrypted data secretPropertiesFile ${SIDDHI_RUNNER_HOME}/resources/security/securevault.jks Keystore which contains the certificate to encrypt sensitive data privateKeyAlias wso2carbon Alias of the certificate in the key store used for encryption masterKeyReader type org.wso2.carbon.secvault.reader.DefaultMasterKeyReader The default implementation of MasterKeyReader gets a list of required passwords from the Secret Repository and provides the values for those passwords by reading system properties, environment variables and the master-keys.yaml file. masterKeyReaderFile ${SIDDHI_RUNNER_HOME\\}/conf/runner/master-keys.yaml Location of master-keys.yaml file which contains password used to access the key store to decrypt the encrypted passwords at runtime","title":"Encrypt sensitive deployment configurations"},{"location":"docs/config-guide/#configuring-server-properties","text":"Siddhi runner and tooling distribution is based on WSO2 Carbon 5 Kernel platform. The properties for the server can be configure under wso2.carbon namespace. Sample configurations is as follows, 1 2 3 wso2.carbon : id : siddhi-runner name : Siddhi Runner Distribution","title":"Configuring server properties"},{"location":"docs/config-guide/#configure-port-offset","text":"Port offset defines the number by which all ports defined in the runtime such as the HTTP/S ports will be offset. For example, if the default HTTP port is 9090 and the ports offset is 1, the effective HTTP port will be 9091. This configuration allows to change ports in a uniform manner across the transports. Below is the sample configurations for offsets, 1 2 3 4 5 wso2.carbon : id : siddhi-runner name : Siddhi Runner Distribution ports : offset : 1","title":"Configure port offset"},{"location":"docs/config-guide/#configuring-databridge-transport","text":"Siddhi uses Databridge transport to send and receive events over Thrift/Binary protocols, This can be used through siddhi-io-wso2event extension. Sample Configuration is as follows, 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 transports : databridge : # Configuration used for the databridge communication listenerConfigurations : workerThreads : 10 . . . senderConfigurations : # Configuration of the Data Agents - to publish events through databridge agents : agentConfiguration : name : Thrift dataEndpointClass : org.wso2.carbon.databridge.agent.endpoint.thrift.ThriftDataEndpoint . . . Here, transports databridge includes listenerConfigurations, to configure databridge receiver in WSO2Event Source, and senderConfigurations, to configure agents used to publish events over databridge in WSO2Event Sink","title":"Configuring Databridge Transport"},{"location":"docs/config-guide/#configuring-databridge-listener","text":"Sample configuration for databridge listener and properties are as follows, 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 transports : databridge : listenerConfigurations : workerThreads : 10 maxEventBufferCapacity : 10 eventBufferSize : 2000 keyStoreLocation : ${sys:carbon.home}/resources/security/wso2carbon.jks keyStorePassword : wso2carbon clientTimeoutMin : 30 # Data receiver configurations dataReceivers : - dataReceiver : type : Thrift properties : tcpPort : 7611 sslPort : 7711 - dataReceiver : type : Binary properties : tcpPort : 9611 sslPort : 9711 tcpReceiverThreadPoolSize : 100 sslReceiverThreadPoolSize : 100 hostName : 0.0.0.0 Parameter Default Value Description workerThreads 10 No of worker threads to consume events maxEventBufferCapacity 10 Maximum amount of messages that can be queued internally in Message Buffer eventBufferSize 2000 Maximum number of events that can be stored in the queue clientTimeoutMin 30 Session timeout value in minutes keyStoreLocation ${SIDDHIRUNNER_HOME}/resources/security/wso2carbon.jks Keystore file path Keystore password wo2carbon Keystore password dataReceivers Generalised configuration for different types of data receivers dataReceivers dataReceiver type Type of the data receiver Parameters for Thrift data receiver, Parameter Default Value Description tcpPort 7611 TCP port for the Thrift data receiver sslPort 7711 SSL port for the Thrift data receiver Parameters for Binary data receiver, Parameter Default Value Description tcpPort 7611 TCP port for the Binary data receiver sslPort 7711 SSL port for the Binary data receiver tcpReceiverThreadPoolSize 100 Receiver pool size for Thrift TCP protocol sslReceiverThreadPoolSize 100 Receiver pool size for Thrift SSL protocol hostname 0.0.0.0 Hostname for the Thrift receiver","title":"Configuring databridge listener"},{"location":"docs/config-guide/#configuring-databridge-publisher","text":"Note By default both Thrift and Binary agents will be started. Sample configuration for databridge agent(publisher) and properties are as follows, 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 transports : databridge : senderConfigurations : agents : - agentConfiguration : name : Thrift dataEndpointClass : org.wso2.carbon.databridge.agent.endpoint.thrift.ThriftDataEndpoint publishingStrategy : async trustStorePath : ${sys:carbon.home}/resources/security/client-truststore.jks trustStorePassword : wso2carbon queueSize : 32768 batchSize : 200 corePoolSize : 1 socketTimeoutMS : 30000 maxPoolSize : 1 keepAliveTimeInPool : 20 reconnectionInterval : 30 maxTransportPoolSize : 250 maxIdleConnections : 250 evictionTimePeriod : 5500 minIdleTimeInPool : 5000 secureMaxTransportPoolSize : 250 secureMaxIdleConnections : 250 secureEvictionTimePeriod : 5500 secureMinIdleTimeInPool : 5000 sslEnabledProtocols : TLSv1.1,TLSv1.2 ciphers : TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256,TLS_DHE_RSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA,TLS_DHE_RSA_WITH_AES_128_CBC_SHA,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_DHE_RSA_WITH_AES_128_GCM_SHA256 - agentConfiguration : name : Binary dataEndpointClass : org.wso2.carbon.databridge.agent.endpoint.binary.BinaryDataEndpoint publishingStrategy : async trustStorePath : ${sys:carbon.home}/resources/security/client-truststore.jks trustStorePassword : wso2carbon queueSize : 32768 batchSize : 200 corePoolSize : 1 socketTimeoutMS : 30000 maxPoolSize : 1 keepAliveTimeInPool : 20 reconnectionInterval : 30 maxTransportPoolSize : 250 maxIdleConnections : 250 evictionTimePeriod : 5500 minIdleTimeInPool : 5000 secureMaxTransportPoolSize : 250 secureMaxIdleConnections : 250 secureEvictionTimePeriod : 5500 secureMinIdleTimeInPool : 5000 sslEnabledProtocols : TLSv1.1,TLSv1.2 ciphers : TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256,TLS_DHE_RSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA,TLS_DHE_RSA_WITH_AES_128_CBC_SHA,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_DHE_RSA_WITH_AES_128_GCM_SHA256 Parameter Default Value Description name Thrift / Binary Name of the databridge agent dataEndpointClass org.wso2.carbon.databridge.agent.endpoint.thrift.ThriftDataEndpoint / org.wso2.carbon.databridge.agent.endpoint.thrift.ThriftDataEndpoint Class of the databridge agent initialised publishingStrategy async Strategy used for publishing. Can be either sync or async trustStorePath ${sys:carbon.home\\}/resources/security/client-truststore.jks Truststore file path trustStorePassword wso2carbon Trust store password queueSize 32768 Queue size used to hold events before publishing batchSize 200 Size of a publishing batch of events corePoolSize 1 Pool size of the threads used to buffer before publishing maxPoolSize 1 Maximum pool size for threads used to buffer before publishing socketTimeoutMS 30000 Time for socket to timeout in Milliseconds keepAliveTimeInPool 20 Time used to keep the threads live reconnectionInterval 30 Reconnection interval in case of lost transmission maxTransportPoolSize 250 Transport threads used for publishing maxIdleConnections 250 Maximum idle connections maintained in the databridge evictionTimePeriod 5500 Eviction time interval minIdleTimeInPool 5500 Min idle time in pool secureMaxTransportPoolSize 250 Max transport pool size in SSL publishing secureMaxIdleConnections 250 Max idle connections in SSL publishing secureEvictionTimePeriod 5500 Eviction time period in SSL publishing secureMinIdleTimeInPool 5500 Min idle time in pool in SSL publishing sslEnabledProtocols TLSv1.1,TLSv1.2 SSL enabled protocols ciphers TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256, TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256, TLS_DHE_RSA_WITH_AES_128_CBC_SHA256, TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA, TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA, TLS_DHE_RSA_WITH_AES_128_CBC_SHA, TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256, TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256, TLS_DHE_RSA_WITH_AES_128_GCM_SHA256 Ciphers used in transmission","title":"Configuring databridge publisher"},{"location":"docs/config-guide/#configuring-admin-rest-apis","text":"Admin API can be configured under the namespace transports http . Sample Config and the parameters are as follows, 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 transports : http : listenerConfigurations : - id : default host : 0.0.0.0 port : 9090 - id : msf4j-https host : 0.0.0.0 port : 9443 scheme : https sslConfig : keyStore : ${carbon.home}/resources/security/wso2carbon.jks keyStorePassword : wso2carbon transportProperties : - name : server.bootstrap.socket.timeout value : 60 - name : latency.metrics.enabled value : false Parameter Default Value Description id default Id of the server host 0.0.0.0 Hostname of the server port 8080 Port of the APIs scheme http Scheme of the APIs. It can be either http or https httpTraceLogEnabled false Enable HTTP trace logs httpAccessLogEnabled false Enable HTTP access logs socketIdleTimeout 0 Timeout for socket for which requests received. Not set by default. SSL configurations (listenerConfigurations sslConfig) Parameter Default Value Description keyStore ${carbon.home}/resources/security/wso2carbon.jks The file containing the private key of the client keyStorePass wso2carbon Password of the private key if it is encrypted enableProtocols All SSL/TLS protocols to be enabled (e.g.: TLSv1,TLSv1.1,TLSv1.2) cipherSuites All List of ciphers to be used eg: TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256, TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA enableSessionCreation Enable/disable new SSL session creation sessionTimeOut 0 SSL session time out. Not set by default. handshakeTimeOut 0 SSL handshake time out. Not set by default. Transport Properties (transportProperties) Parameter Default Value Description server.bootstrap.connect.timeout 15000 Timout in millisecond to establish connection server.bootstrap.socket.timeout 60 Socket connection timeouts latency.metrics.enabled false Enable/Disable latency metrics by carbon metrics component","title":"Configuring Admin REST APIs"},{"location":"docs/extensions/","text":"Siddhi Extensions Following are some supported Siddhi extensions; Extensions released under Apache 2.0 License Execution Extensions Name Description Latest Tested Version execution-string Provides basic string handling capabilities such as concat, length, replace all, etc. 5.0.4 execution-regex Provides basic RegEx execution capabilities. 5.0.5 execution-math Provides useful mathematical functions. 5.0.3 execution-time Provides time related functionality such as getting current time, current date, manipulating/formatting dates, etc. 5.0.3 execution-map Provides the capability to generate and manipulate map data objects. 5.0.4 execution-json Provides the capability to retrieve, insert, and modify JSON elements. 2.0.2 execution-unitconversion Converts various units such as length, mass, time and volume. 2.0.2 execution-reorder Orders out-of-order event arrivals using algorithms such as K-Slack and alpha K-Stack. 5.0.3 execution-unique Retains and process unique events based on the given parameters. 5.0.3 execution-streamingml Performs streaming machine learning (clustering, classification and regression) on event streams. 2.0.2 execution-tensorflow provides support for running pre-built TensorFlow models. 2.0.1 Input/Output Extensions Name Description Latest Tested Version io-http Receives and publishes events via http and https transports, calls external services, and serves incoming requests and provide synchronous responses. 2.1.1 io-nats Receives and publishes events from/to NATS. 2.0.4 io-kafka Receives and publishes events from/to Kafka. 5.0.3 io-email Receives and publishes events via email using smtp , pop3 and imap protocols. 2.0.3 io-cdc Captures change data from databases such as MySQL, MS SQL, Postgresql, H2 and Oracle. 2.0.3 io-tcp Receives and publishes events through TCP transport. 3.0.3 io-googlepubsub Receives and publishes events through Google Pub/Sub. 2.0.1 io-file Receives and publishes event data from/to files. 2.0.1 io-jms Receives and publishes events via Java Message Service (JMS), supporting Message brokers such as ActiveMQ 2.0.2 io-prometheus Consumes and expose Prometheus metrics from/to Prometheus server. 2.0.1 Data Mapping Extensions Name Description Latest Tested Version map-json Converts JSON messages to/from Siddhi events. 5.0.4 map-xml Converts XML messages to/from Siddhi events. 5.0.3 map-text Converts text messages to/from Siddhi events. 2.0.4 map-avro Converts AVRO messages to/from Siddhi events. 2.0.2 map-keyvalue Converts events having Key-Value maps to/from Siddhi events. 2.0.3 map-csv Converts messages with CSV format to/from Siddhi events. 2.0.2 map-binary Converts binary events that adheres to Siddhi format to/from Siddhi events. 2.0.2 Store Extensions Name Description Latest Tested Version store-rdbms Persist and retrieve events to/from RDBMS databases such as MySQL, MS SQL, Postgresql, H2 and Oracle. 6.0.3 store-mongodb Persist and retrieve events to/from MongoDB. 2.0.1 store-redis Persist and retrieve events to/from Redis. 3.1.1 store-elasticsearch Persist and retrieve events to/from Elasticsearch. 3.1.0 Script Extensions Name Description Latest Tested Version script-js Allows writing user defined JavaScript functions within Siddhi Applications to process events. 5.0.1","title":"Extensions"},{"location":"docs/extensions/#siddhi-extensions","text":"Following are some supported Siddhi extensions;","title":"Siddhi Extensions"},{"location":"docs/extensions/#extensions-released-under-apache-20-license","text":"","title":"Extensions released under Apache 2.0 License"},{"location":"docs/extensions/#execution-extensions","text":"Name Description Latest Tested Version execution-string Provides basic string handling capabilities such as concat, length, replace all, etc. 5.0.4 execution-regex Provides basic RegEx execution capabilities. 5.0.5 execution-math Provides useful mathematical functions. 5.0.3 execution-time Provides time related functionality such as getting current time, current date, manipulating/formatting dates, etc. 5.0.3 execution-map Provides the capability to generate and manipulate map data objects. 5.0.4 execution-json Provides the capability to retrieve, insert, and modify JSON elements. 2.0.2 execution-unitconversion Converts various units such as length, mass, time and volume. 2.0.2 execution-reorder Orders out-of-order event arrivals using algorithms such as K-Slack and alpha K-Stack. 5.0.3 execution-unique Retains and process unique events based on the given parameters. 5.0.3 execution-streamingml Performs streaming machine learning (clustering, classification and regression) on event streams. 2.0.2 execution-tensorflow provides support for running pre-built TensorFlow models. 2.0.1","title":"Execution Extensions"},{"location":"docs/extensions/#inputoutput-extensions","text":"Name Description Latest Tested Version io-http Receives and publishes events via http and https transports, calls external services, and serves incoming requests and provide synchronous responses. 2.1.1 io-nats Receives and publishes events from/to NATS. 2.0.4 io-kafka Receives and publishes events from/to Kafka. 5.0.3 io-email Receives and publishes events via email using smtp , pop3 and imap protocols. 2.0.3 io-cdc Captures change data from databases such as MySQL, MS SQL, Postgresql, H2 and Oracle. 2.0.3 io-tcp Receives and publishes events through TCP transport. 3.0.3 io-googlepubsub Receives and publishes events through Google Pub/Sub. 2.0.1 io-file Receives and publishes event data from/to files. 2.0.1 io-jms Receives and publishes events via Java Message Service (JMS), supporting Message brokers such as ActiveMQ 2.0.2 io-prometheus Consumes and expose Prometheus metrics from/to Prometheus server. 2.0.1","title":"Input/Output Extensions"},{"location":"docs/extensions/#data-mapping-extensions","text":"Name Description Latest Tested Version map-json Converts JSON messages to/from Siddhi events. 5.0.4 map-xml Converts XML messages to/from Siddhi events. 5.0.3 map-text Converts text messages to/from Siddhi events. 2.0.4 map-avro Converts AVRO messages to/from Siddhi events. 2.0.2 map-keyvalue Converts events having Key-Value maps to/from Siddhi events. 2.0.3 map-csv Converts messages with CSV format to/from Siddhi events. 2.0.2 map-binary Converts binary events that adheres to Siddhi format to/from Siddhi events. 2.0.2","title":"Data Mapping Extensions"},{"location":"docs/extensions/#store-extensions","text":"Name Description Latest Tested Version store-rdbms Persist and retrieve events to/from RDBMS databases such as MySQL, MS SQL, Postgresql, H2 and Oracle. 6.0.3 store-mongodb Persist and retrieve events to/from MongoDB. 2.0.1 store-redis Persist and retrieve events to/from Redis. 3.1.1 store-elasticsearch Persist and retrieve events to/from Elasticsearch. 3.1.0","title":"Store Extensions"},{"location":"docs/extensions/#script-extensions","text":"Name Description Latest Tested Version script-js Allows writing user defined JavaScript functions within Siddhi Applications to process events. 5.0.1","title":"Script Extensions"},{"location":"docs/features/","text":"Siddhi 5.1 Features Retrieving Events From various data sources supporting multiple message formats Mapping Events Mapping events with various data formats to Stream for processing Mapping streams to multiple data formats for publishing Processing Streams Filter Filtering stream based on conditions Window Support for sliding and batch (tumbling) and many other type of windows Aggregation Supporting Avg , Sum , Min , Max , etc For long running aggregations and aggregation over windows Ability to perform aggregate processing with Group by and filter aggregated data with Having conditions Incremental Aggregation Support for processing and retrieving long running Aggregation Supports data processing in seconds, minutes, hours, days, months, and years granularity Table and Stores For storing events for future processing and retrieving them on demand Supporting storage in in-memory, RDBMs, Solr, mongoDb, etc Join Joining two streams, two windows based on conditions Joining stream/window with table or incremental aggregation based on conditions Supports inner joins, and left, right full outer joins Pattern Identifies event occurrence patterns among streams over time Identify non occurrence of events Supports repetitive matches of event pattern occurrences with logical conditions and time bound Sequence processing Identifies continuous sequence of events from streams Supports zero to many, one to many, and zero to one event matching conditions Partitions Grouping queries and based on keywords or value ranges for isolated parallel processing Scripting Support writing scripts like JavaScript, Scala and R within Siddhi Queries Process Based on event time Whole execution driven by the event time Publishing Events To various data sources with various message formats Supporting load balancing and failover data publishing Error handling Support errors and exceptions through error streams Automatic backoff retries to external data stores, sources and sinks. Parallel processing Support parallel processing through asynchronous multithreading at streams Snapshot and restore Support for periodic state persistence and restore capabilities to allow state restore during failures","title":"Features"},{"location":"docs/features/#siddhi-51-features","text":"Retrieving Events From various data sources supporting multiple message formats Mapping Events Mapping events with various data formats to Stream for processing Mapping streams to multiple data formats for publishing Processing Streams Filter Filtering stream based on conditions Window Support for sliding and batch (tumbling) and many other type of windows Aggregation Supporting Avg , Sum , Min , Max , etc For long running aggregations and aggregation over windows Ability to perform aggregate processing with Group by and filter aggregated data with Having conditions Incremental Aggregation Support for processing and retrieving long running Aggregation Supports data processing in seconds, minutes, hours, days, months, and years granularity Table and Stores For storing events for future processing and retrieving them on demand Supporting storage in in-memory, RDBMs, Solr, mongoDb, etc Join Joining two streams, two windows based on conditions Joining stream/window with table or incremental aggregation based on conditions Supports inner joins, and left, right full outer joins Pattern Identifies event occurrence patterns among streams over time Identify non occurrence of events Supports repetitive matches of event pattern occurrences with logical conditions and time bound Sequence processing Identifies continuous sequence of events from streams Supports zero to many, one to many, and zero to one event matching conditions Partitions Grouping queries and based on keywords or value ranges for isolated parallel processing Scripting Support writing scripts like JavaScript, Scala and R within Siddhi Queries Process Based on event time Whole execution driven by the event time Publishing Events To various data sources with various message formats Supporting load balancing and failover data publishing Error handling Support errors and exceptions through error streams Automatic backoff retries to external data stores, sources and sinks. Parallel processing Support parallel processing through asynchronous multithreading at streams Snapshot and restore Support for periodic state persistence and restore capabilities to allow state restore during failures","title":"Siddhi 5.1 Features"},{"location":"docs/query-guide/","text":"Siddhi 5.1 Streaming SQL Guide Introduction Siddhi Streaming SQL is designed to process streams of events. It can be used to implement streaming data integration, streaming analytics, rule based and adaptive decision making use cases. It is an evolution of Complex Event Processing (CEP) and Stream Processing systems, hence it can also be used to process stateful computations, detecting of complex event patterns, and sending notifications in real-time. Siddhi Streaming SQL uses SQL like syntax, and annotations to consume events from diverse event sources with various data formats, process then using stateful and stateless operators and send outputs to multiple endpoints according to their accepted event formats. It also supports exposing rule based and adaptive decision making as service endpoints such that external programs and systems can synchronously get decision support form Siddhi. The following sections explains how to write processing logic using Siddhi Streaming SQL. Siddhi Application The processing logic for your program can be written using the Streaming SQL and put together as a single file with .siddhi extension. This file is called as the Siddhi Application or the SiddhiApp . SiddhiApps are named by adding @app:name(' name ') annotation on the top of the SiddhiApp file. When the annotation is not added Siddhi assigns a random UUID as the name of the SiddhiApp. Purpose SiddhiApp provides an isolated execution environment for your processing logic that allows you to deploy and execute processing logic independent of other SiddhiApp in the system. Therefore it's always recommended to have a processing logic related to single use case in a single SiddhiApp. This will help you to group processing logic and easily manage addition and removal of various use cases. The following diagram depicts some of the key Siddhi Streaming SQL elements of Siddhi Application and how event flows through the elements. Below table provides brief description of a few key elements in the Siddhi Streaming SQL Language. Elements Description Stream A logical series of events ordered in time with a uniquely identifiable name, and a defined set of typed attributes defining its schema. Event An event is a single event object associated with a stream. All events of a stream contains a timestamp and an identical set of typed attributes based on the schema of the stream they belong to. Table A structured representation of data stored with a defined schema. Stored data can be backed by In-Memory , or external data stores such as RDBMS , MongoDB , etc. The tables can be accessed and manipulated at runtime. Named Window A structured representation of data stored with a defined schema and eviction policy. Window data is stored In-Memory and automatically cleared by the named window constrain. Other siddhi elements can only query the values in windows at runtime but they cannot modify them. Named Aggregation A structured representation of data that's incrementally aggregated and stored with a defined schema and aggregation granularity such as seconds, minutes, hours, etc. Aggregation data is stored both In-Memory and in external data stores such as RDBMS . Other siddhi elements can only query the values in windows at runtime but they cannot modify them. Query A logical construct that processes events in streaming manner by by consuming data from one or more streams, tables, windows and aggregations, and publishes output events into a stream, table or a window. Source A construct that consumes data from external sources (such as TCP , Kafka , HTTP , etc) with various event formats such as XML , JSON , binary , etc, convert then to Siddhi events, and passes into streams for processing. Sink A construct that consumes events arriving at a stream, maps them to a predefined data format (such as XML , JSON , binary , etc), and publishes them to external endpoints (such as E-mail , TCP , Kafka , HTTP , etc). Input Handler A mechanism to programmatically inject events into streams. Stream/Query Callback A mechanism to programmatically consume output events from streams or queries. Partition A logical container that isolates the processing of queries based on the partition keys derived from the events. Inner Stream A positionable stream that connects portioned queries with each other within the partition. Grammar SiddhiApp is a collection of Siddhi Streaming SQL elements composed together as a script. Here each Siddhi element must be separated by a semicolon ; . Hight level syntax of SiddhiApp is as follows. 1 2 3 4 5 siddhi app : app annotation * ( stream definition | table definition | ... ) + ( query | partition ) + ; Example Siddhi Application with name Temperature-Analytics defined with a stream named TempStream and a query named 5minAvgQuery . 1 2 3 4 5 6 7 8 9 @ app : name ( Temperature-Analytics ) define stream TempStream ( deviceID long , roomNo int , temp double ); @ info ( name = 5minAvgQuery ) from TempStream # window . time ( 5 min ) select roomNo , avg ( temp ) as avgTemp group by roomNo insert into OutputStream ; Stream A stream is a logical series of events ordered in time. Its schema is defined via the stream definition . A stream definition contains the stream name and a set of attributes with specific types and uniquely identifiable names within the stream. All events associated to the stream will have the same schema (i.e., have the same attributes in the same order). Purpose Stream groups common types of events together with a schema. This helps in various ways such as, processing all events together in queries and performing data format transformations together when they are consumed and published via sources and sinks. Syntax The syntax for defining a new stream is as follows. 1 2 define stream stream name ( attribute name attribute type , attribute name attribute type , ... ); The following parameters are used to configure a stream definition. Parameter Description stream name The name of the stream created. (It is recommended to define a stream name in PascalCase .) attribute name Uniquely identifiable name of the stream attribute. (It is recommended to define attribute names in camelCase .) attribute type The type of each attribute defined in the schema. This can be STRING , INT , LONG , DOUBLE , FLOAT , BOOL or OBJECT . To use and refer stream and attribute names that do not follow [a-zA-Z_][a-zA-Z_0-9]* format enclose them in ` . E.g. `$test(0)` . To make the stream process events in multi-threading and asynchronous way use the @Async annotation as shown in Multi-threading and Asynchronous Processing configuration section. Example 1 define stream TempStream ( deviceID long , roomNo int , temp double ); The above creates a stream with name TempStream having the following attributes. deviceID of type long roomNo of type int temp of type double Source Sources receive events via multiple transports and in various data formats, and direct them into streams for processing. A source configuration allows to define a mapping in order to convert each incoming event from its native data format to a Siddhi event. When customizations to such mappings are not provided, Siddhi assumes that the arriving event adheres to the predefined format based on the stream definition and the configured message mapping type. Purpose Source provides a way to consume events from external systems and convert them to be processed by the associated stream. Syntax To configure a stream that consumes events via a source, add the source configuration to a stream definition by adding the @source annotation with the required parameter values. The source syntax is as follows: 1 2 3 4 5 6 @ source ( type = source type , static . key = value , static . key = value , @ map ( type = map type , static . key = value , static . key = value , @ attributes ( attribute1 = attribute mapping , attributeN = attribute mapping ) ) ) define stream stream name ( attribute1 type , attributeN type ); This syntax includes the following annotations. Source The type parameter of @source annotation defines the source type that receives events. The other parameters of @source annotation depends upon the selected source type, and here some of its parameters can be optional. For detailed information about the supported parameters see the documentation of the relevant source. The following is the list of source types supported by Siddhi: Source type Description In-memory Allow SiddhiApp to consume events from other SiddhiApps running on the same JVM. HTTP Expose an HTTP service to consume messages. Kafka Subscribe to Kafka topic to consume events. TCP Expose a TCP service to consume messages. Email Consume emails via POP3 and IMAP protocols. JMS Subscribe to JMS topic or queue to consume events. File Reads files by tailing or as a whole to extract events out of them. CDC Perform change data capture on databases. Prometheus Consume data from Prometheus agent. In-memory is the only source inbuilt in Siddhi, and all other source types are implemented as extensions. Source Mapper Each @source configuration can have a mapping denoted by the @map annotation that defines how to convert the incoming event format to Siddhi events. The type parameter of the @map defines the map type to be used in converting the incoming events. The other parameters of @map annotation depends on the mapper selected, and some of its parameters can be optional. For detailed information about the parameters see the documentation of the relevant mapper. Map Attributes @attributes is an optional annotation used with @map to define custom mapping. When @attributes is not provided, each mapper assumes that the incoming events adheres to its own default message format and attempt to convert the events from that format. By adding the @attributes annotation, users can selectively extract data from the incoming message and assign them to the attributes. There are two ways to configure @attributes . Define attribute names as keys, and mapping configurations as values: @attributes( attribute1 =' mapping ', attributeN =' mapping ') Define the mapping configurations in the same order as the attributes defined in stream definition: @attributes( ' mapping for attribute1 ', ' mapping for attributeN ') Supported Source Mapping Types The following is the list of source mapping types supported by Siddhi: Source mapping type Description PassThrough Omits data conversion on Siddhi events. JSON Converts JSON messages to Siddhi events. XML Converts XML messages to Siddhi events. TEXT Converts plain text messages to Siddhi events. Avro Converts Avro events to Siddhi events. Binary Converts Siddhi specific binary events to Siddhi events. Key Value Converts key-value HashMaps to Siddhi events. CSV Converts CSV like delimiter separated events to Siddhi events. Tip When the @map annotation is not provided @map(type='passThrough') is used as default, that passes the consumed Siddhi events directly to the streams without any data conversion. PassThrough is the only source mapper inbuilt in Siddhi, and all other source mappers are implemented as extensions. Example 1 Receive JSON messages by exposing an HTTP service, and direct them to InputStream stream for processing. Here the HTTP service will be secured with basic authentication, receives events on all network interfaces on port 8080 and context /foo . The service expects the JSON messages to be on the default data format that's supported by the JSON mapper as follows. 1 2 3 4 5 { name : Paul , age : 20 , country : UK } The configuration of the HTTP source and JSON source mapper to achieve the above is as follows. 1 2 3 @ source ( type = http , receiver . url = http://0.0.0.0:8080/foo , @ map ( type = json )) define stream InputStream ( name string , age int , country string ); Example 2 Receive JSON messages by exposing an HTTP service, and direct them to StockStream stream for processing. Here the incoming JSON , as given below, do not adhere to the default data format that's supported by the JSON mapper. 1 2 3 4 5 6 7 8 9 10 11 { portfolio :{ stock :{ volume : 100 , company :{ symbol : FB }, price : 55.6 } } } The configuration of the HTTP source and the custom JSON source mapping to achieve the above is as follows. 1 2 3 4 5 @ source ( type = http , receiver . url = http://0.0.0.0:8080/foo , @ map ( type = json , enclosing . element = $.portfolio , @ attributes ( symbol = stock.company.symbol , price = stock.price , volume = stock.volume ))) define stream StockStream ( symbol string , price float , volume long ); The same can also be configured by omitting the attribute names as below. 1 2 3 4 @ source ( type = http , receiver . url = http://0.0.0.0:8080/foo , @ map ( type = json , enclosing . element = $.portfolio , @ attributes ( stock.company.symbol , stock.price , stock.volume ))) define stream StockStream ( symbol string , price float , volume long ); Sink Sinks consumes events from streams and publish them via multiple transports to external endpoints in various data formats. A sink configuration allows users to define a mapping to convert the Siddhi events in to the required output data format (such as JSON , TEXT , XML , etc.) and publish the events to the configured endpoints. When customizations to such mappings are not provided, Siddhi converts events to the predefined event format based on the stream definition and the configured message mapper type before publishing the events. Purpose Sink provides a way to publish Siddhi events of a stream to external systems by converting events to their supported format. Syntax To configure a stream to publish events via a sink, add the sink configuration to a stream definition by adding the @sink annotation with the required parameter values. The sink syntax is as follows: 1 2 3 4 5 6 @ sink ( type = sink type , static . key = value , dynamic . key = {{ value }} , @ map ( type = map type , static . key = value , dynamic . key = {{ value }} , @ payload ( payload mapping ) ) ) define stream stream name ( attribute1 type , attributeN type ); Dynamic Properties The sink and sink mapper properties that are categorized as dynamic have the ability to absorb attribute values dynamically from the Siddhi events of their associated streams. This can be configured by enclosing the relevant attribute names in double curly braces as {{...}} , and using it within the property values. Some valid dynamic properties values are: '{{attribute1}}' 'This is {{attribute1}}' {{attribute1}} {{attributeN}} Here the attribute names in the double curly braces will be replaced with the values from the events before they are published. This syntax includes the following annotations. Sink The type parameter of the @sink annotation defines the sink type that publishes the events. The other parameters of the @sink annotation depends upon the selected sink type, and here some of its parameters can be optional and/or dynamic. For detailed information about the supported parameters see documentation of the relevant sink. The following is a list of sink types supported by Siddhi: Source type Description In-memory Allow SiddhiApp to publish events to other SiddhiApps running on the same JVM. Log Logs the events appearing on the streams. HTTP Publish events to an HTTP endpoint. Kafka Publish events to Kafka topic. TCP Publish events to a TCP service. Email Send emails via SMTP protocols. JMS Publish events to JMS topics or queues. File Writes events to files. Prometheus Expose data through Prometheus agent. Distributed Sink Distributed Sinks publish events from a defined stream to multiple endpoints using load balancing or partitioning strategies. Any sink can be used as a distributed sink. A distributed sink configuration allows users to define a common mapping to convert and send the Siddhi events for all its destination endpoints. Purpose Distributed sink provides a way to publish Siddhi events to multiple endpoints in the configured event format. Syntax To configure distributed sink add the sink configuration to a stream definition by adding the @sink annotation and add the configuration parameters that are common of all the destination endpoints inside it, along with the common parameters also add the @distribution annotation specifying the distribution strategy (i.e. roundRobin or partitioned ) and @destination annotations providing each endpoint specific configurations. The distributed sink syntax is as follows: RoundRobin Distributed Sink Publishes events to defined destinations in a round robin manner. 1 2 3 4 5 6 7 8 9 @ sink ( type = sink type , common . static . key = value , common . dynamic . key = {{ value }} , @ map ( type = map type , static . key = value , dynamic . key = {{ value }} , @ payload ( payload mapping ) ) @ distribution ( strategy = roundRobin , @ destination ( destination . specific . key = value ), @ destination ( destination . specific . key = value ))) ) define stream stream name ( attribute1 type , attributeN type ); Partitioned Distributed Sink Publishes events to defined destinations by partitioning them based on the partitioning key. 1 2 3 4 5 6 7 8 9 @ sink ( type = sink type , common . static . key = value , common . dynamic . key = {{ value }} , @ map ( type = map type , static . key = value , dynamic . key = {{ value }} , @ payload ( payload mapping ) ) @ distribution ( strategy = partitioned , partitionKey = partition key , @ destination ( destination . specific . key = value ), @ destination ( destination . specific . key = value ))) ) define stream stream name ( attribute1 type , attributeN type ); Sink Mapper Each @sink configuration can have a mapping denoted by the @map annotation that defines how to convert Siddhi events to outgoing messages with the defined format. The type parameter of the @map defines the map type to be used in converting the outgoing events. The other parameters of @map annotation depends on the mapper selected, and some of its parameters can be optional and/or dynamic. For detailed information about the parameters see the documentation of the relevant mapper. Map Payload @payload is an optional annotation used with @map to define custom mapping. When the @payload annotation is not provided, each mapper maps the outgoing events to its own default event format. The @payload annotation allow users to configure mappers to produce the output payload of their choice, and by using dynamic properties within the payload they can selectively extract and add data from the published Siddhi events. There are two ways you to configure @payload annotation. Some mappers such as XML , JSON , and Test only accept one output payload: @payload( 'This is a test message from {{user}}.') Some mappers such key-value accept series of mapping values: @payload( key1='mapping_1', 'key2'='user : {{user}}') Here, the keys of payload mapping can be defined using the dot notation as a.b.c , or using any constant string value as '$abc' . Supported Sink Mapping Types The following is a list of sink mapping types supported by Siddhi: Sink mapping type Description PassThrough Omits data conversion on outgoing Siddhi events. JSON Converts Siddhi events to JSON messages. XML Converts Siddhi events to XML messages. TEXT Converts Siddhi events to plain text messages. Avro Converts Siddhi events to Avro Events. Binary Converts Siddhi events to Siddhi specific binary events. Key Value Converts Siddhi events to key-value HashMaps. CSV Converts Siddhi events to CSV like delimiter separated events. Tip When the @map annotation is not provided @map(type='passThrough') is used as default, that passes the outgoing Siddhi events directly to the sinks without any data conversion. PassThrough is the only sink mapper inbuilt in Siddhi, and all other sink mappers are implemented as extensions. Example 1 Publishes OutputStream events by converting them to JSON messages with the default format, and by sending to an HTTP endpoint http://localhost:8005/endpoint1 , using POST method, Accept header, and basic authentication having admin is both username and password. The configuration of the HTTP sink and JSON sink mapper to achieve the above is as follows. 1 2 3 4 5 6 @ sink ( type = http , publisher . url = http://localhost:8005/endpoint , method = POST , headers = Accept-Date:20/02/2017 , basic . auth . enabled = true , basic . auth . username = admin , basic . auth . password = admin , @ map ( type = json )) define stream OutputStream ( name string , age int , country string ); This will publish a JSON message on the following format: 1 2 3 4 5 6 7 { event :{ name : Paul , age : 20 , country : UK } } Example 2 Publishes StockStream events by converting them to user defined JSON messages, and by sending to an HTTP endpoint http://localhost:8005/stocks . The configuration of the HTTP sink and custom JSON sink mapping to achieve the above is as follows. 1 2 3 4 @ sink ( type = http , publisher . url = http://localhost:8005/stocks , @ map ( type = json , validate . json = true , enclosing . element = $.Portfolio , @ payload ( { StockData :{ Symbol : {{ symbol }} , Price :{{price}} }} ))) define stream StockStream ( symbol string , price float , volume long ); This will publish a single event as the JSON message on the following format: 1 2 3 4 5 6 7 8 { Portfolio :{ StockData :{ Symbol : GOOG , Price : 55.6 } } } This can also publish multiple events together as a JSON message on the following format: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 { Portfolio :[ { StockData :{ Symbol : GOOG , Price : 55.6 } }, { StockData :{ Symbol : FB , Price : 57.0 } } ] } Example 3 Publishes events from the OutputStream stream to multiple the HTTP endpoints using a partitioning strategy. Here the events are sent to either http://localhost:8005/endpoint1 or http://localhost:8006/endpoint2 based on the partitioning key country . It uses default JSON mapping, POST method, and used admin as both the username and the password when publishing to both the endpoints. The configuration of the distributed HTTP sink and JSON sink mapper to achieve the above is as follows. 1 2 3 4 5 6 7 @ sink ( type = http , method = POST , basic . auth . enabled = true , basic . auth . username = admin , basic . auth . password = admin , @ map ( type = json ), @ distribution ( strategy = partitioned , partitionKey = country , @ destination ( publisher . url = http://localhost:8005/endpoint1 ), @ destination ( publisher . url = http://localhost:8006/endpoint2 ))) define stream OutputStream ( name string , age int , country string ); This will partition the outgoing events and publish all events with the same country attribute value to the same endpoint. The JSON message published will be on the following format: 1 2 3 4 5 6 7 { event :{ name : Paul , age : 20 , country : UK } } Error Handling Errors in Siddhi can be handled at the Streams and in Sinks. Error Handling at Stream When errors are thrown by Siddhi elements subscribed to the stream, the error gets propagated up to the stream that delivered the event to those Siddhi elements. By default the error is logged and dropped at the stream, but this behavior can be altered by by adding @OnError annotation to the corresponding stream definition. @OnError annotation can help users to capture the error and the associated event, and handle them gracefully by sending them to a fault stream. The @OnError annotation and the required action to be specified as below. 1 2 3 @ OnError ( action = on error action ) define stream stream name ( attribute name attribute type , attribute name attribute type , ... ); The action parameter of the @OnError annotation defines the action to be executed during failure scenarios. The following actions can be specified to @OnError annotation to handle erroneous scenarios. LOG : Logs the event with the error, and drops the event. This is the default action performed even when @OnError annotation is not defined. STREAM : Creates a fault stream and redirects the event and the error to it. The created fault stream will have all the attributes defined in the base stream to capture the error causing event, and in addition it also contains _error attribute of type object to containing the error information. The fault stream can be referred by adding ! in front of the base stream name as ! stream name . Example Handle errors in TempStream by redirecting the errors to a fault stream. The configuration of TempStream stream and @OnError annotation is as follows. 1 2 @ OnError ( action = STREAM ) define stream TempStream ( deviceID long , roomNo int , temp double ); Siddhi will infer and automatically defines the fault stream of TempStream as given below. 1 define stream ! TempStream ( deviceID long , roomNo int , temp double , _error object ); The SiddhiApp extending the above the use-case by adding failure generation and error handling with the use of queries is as follows. Note: Details on writing processing logics via queries will be explained in later sections. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 -- Define fault stream to handle error occurred at TempStream subscribers @ OnError ( action = STREAM ) define stream TempStream ( deviceID long , roomNo int , temp double ); -- Error generation through a custom function `createError()` @ info ( name = error-generation ) from TempStream # custom : createError () insert into IgnoreStream1 ; -- Handling error by simply logging the event and error. @ info ( name = handle-error ) from ! TempStream # log ( Error Occurred! ) select deviceID , roomNo , temp , _error insert into IgnoreStream2 ; Error Handling at Sink There can be cases where external systems becoming unavailable or coursing errors when the events are published to them. By default sinks log and drop the events causing event losses, and this can be handled gracefully by configuring on.error parameter of the @sink annotation. The on.error parameter of the @sink annotation can be specified as below. 1 2 3 @ sink ( type = sink type , on . error = on error action , key = value , ...) define stream stream name ( attribute name attribute type , attribute name attribute type , ... ); The following actions can be specified to on.error parameter of @sink annotation to handle erroneous scenarios. LOG : Logs the event with the error, and drops the event. This is the default action performed even when on.error parameter is not defined on the @sink annotation. WAIT : Publishing threads wait in back-off and re-trying mode, and only send the events when the connection is re-established. During this time the threads will not consume any new messages causing the systems to introduce back pressure on the systems that publishes to it. STREAM : Pushes the failed events with the corresponding error to the associated fault stream the sink belongs to. Example 1 Introduce back pressure on the threads who bring events via TempStream when the system cannot connect to Kafka. The configuration of TempStream stream and @sink Kafka annotation with on.error property is as follows. 1 2 3 4 @ sink ( type = kafka , on . error = WAIT , topic = {{roomNo}} , bootstrap . servers = localhost:9092 , @ map ( type = xml )) define stream TempStream ( deviceID long , roomNo int , temp double ); Example 2 Send events to the fault stream of TempStream when the system cannot connect to Kafka. The configuration of TempStream stream with associated fault stream, @sink Kafka annotation with on.error property and a queries to handle the error is as follows. Note: Details on writing processing logics via queries will be explained in later sections. 1 2 3 4 5 6 7 8 9 10 11 @ OnError ( action = STREAM ) @ sink ( type = kafka , on . error = STREAM , topic = {{roomNo}} , bootstrap . servers = localhost:9092 , @ map ( type = xml )) define stream TempStream ( deviceID long , roomNo int , temp double ); -- Handling error by simply logging the event and error. @ info ( name = handle-error ) from ! TempStream # log ( Error Occurred! ) select deviceID , roomNo , temp , _error insert into IgnoreStream ; Query Query defines the processing logic in Siddhi. It consumes events from one or more streams, named-windows , tables , and/or named-aggregations , process the events in a streaming manner, and generate output events into a stream , named-window , or table . Purpose A query provides a way to process the events in the order they arrive and produce output using both stateful and stateless complex event processing and stream processing operations. Syntax The high level query syntax for defining processing logics is as follows: 1 2 3 4 @ info ( name = query name ) from input projection output action The following parameters are used to configure a stream definition. Parameter Description query name The name of the query. Since naming the query (i.e the @info(name = ' query name ') annotation) is optional, when the name is not provided Siddhi assign a system generated name for the query. input Defines the means of event consumption via streams , named-windows , tables , and/or named-aggregations , and defines the processing logic using filters , windows , stream-functions , joins , patterns and sequences . projection Generates output event attributes using select , functions , aggregation-functions , and group by operations, and filters the generated the output using having , limit offset , order by , and output rate limiting operations before sending them out. Here the projection is optional and when it is omitted all the input events will be sent to the output as it is. output action Defines output action (such as insert into , update , delete , etc) that needs to be performed by the generated events on a stream , named-window , or table Example A query consumes events from the TempStream stream and output only the roomNo and temp attributes to the RoomTempStream stream, from which another query consumes the events and sends all its attributes to AnotherRoomTempStream stream. 1 2 3 4 5 6 7 8 define stream TempStream ( deviceID long , roomNo int , temp double ); from TempStream select roomNo , temp insert into RoomTempStream ; from RoomTempStream insert into AnotherRoomTempStream ; Inferred Stream Here, the RoomTempStream and AnotherRoomTempStream streams are an inferred streams, which means their stream definitions are inferred from the queries and hence they can be used the same as any other defined streams without any restrictions. Value Values are typed data, which can be manipulated, transferred, and stored. Values can be referred by the attributes defined in definitions such as streams, and tables. Siddhi supports values of type STRING , INT (Integer), LONG , DOUBLE , FLOAT , BOOL (Boolean) and OBJECT . The syntax of each type and their example use as a constant value is as follows, Attribute Type Format Example int + 123 , -75 , +95 long +L 123000L , -750l , +154L float ( +)?('.' *)? (E(-|+)? +)?F 123.0f , -75.0e-10F , +95.789f double ( +)?('.' *)? (E(-|+)? +)?D? 123.0 , 123.0D , -75.0e-10D , +95.789d bool (true|false) true , false , TRUE , FALSE string '( char * !('|\"|\"\"\"| line ))' or \"( char * !(\"|\"\"\"| line ))\" or \"\"\"( char * !(\"\"\"))\"\"\" 'Any text.' , \"Text with 'single' quotes.\" , \"\"\" Text with 'single' quotes, \"double\" quotes, and new lines. \"\"\" Time Time is a special type of LONG value that denotes time using digits and their unit in the format ( digit + unit )+ . At execution, the time gets converted into milliseconds and returns a LONG value. Unit Syntax Year year | years Month month | months Week week | weeks Day day | days Hour hour | hours Minutes minute | minutes | min Seconds second | seconds | sec Milliseconds millisecond | milliseconds Example 1 hour and 25 minutes can by written as 1 hour and 25 minutes which is equal to the LONG value 5100000 . Select The select clause in Siddhi query defines the output event attributes of the query. Following are some basic query projection operations supported by select. Action Description Select specific attributes for projection Only select some of the input attributes as query output attributes. E.g., Select and output only roomNo and temp attributes from the TempStream stream. from TempStream select roomNo, temp insert into RoomTempStream; Select all attributes for projection Select all input attributes as query output attributes. This can be done by using asterisk ( * ) or by omitting the select clause itself. E.g., Both following queries select all attributes of TempStream input stream and output all attributes to NewTempStream stream. from TempStream select * insert into NewTempStream; or from TempStream insert into NewTempStream; Name attribute Provide a unique name for each output attribute generated by the query. This can help to rename the selected input attributes or assign an attribute name to a projection operation such as function, aggregate-function, mathematical operation, etc, using as keyword. E.g., Query that renames input attribute temp to temperature and function currentTimeMillis() as time . from TempStream select roomNo, temp as temperature, currentTimeMillis() as time insert into RoomTempStream; Constant values as attributes Creates output attributes with a constant value. Any constant value of type STRING , INT , LONG , DOUBLE , FLOAT , BOOL , and time as given in the values section can be defined. E.g., Query specifying 'C' as the constant value for the scale attribute. from TempStream select roomNo, temp, 'C' as scale insert into RoomTempStream; Mathematical and logical expressions in attributes Defines the mathematical and logical operations that need to be performed to generating output attribute values. These expressions are executed in the precedence order given below. Operator precedence Operator Distribution Example () Scope (cost + tax) * 0.05 IS NULL Null check deviceID is null NOT Logical NOT not (price > 10) * , / , % Multiplication, division, modulus temp * 9/5 + 32 + , - Addition, subtraction temp * 9/5 - 32 < , < = , > , >= Comparators: less-than, greater-than-equal, greater-than, less-than-equal totalCost >= price * quantity == , != Comparisons: equal, not equal totalCost != price * quantity IN Checks if value exist in the table roomNo in ServerRoomsTable AND Logical AND temp < 40 and humidity < 40 OR Logical OR humidity < 40 or humidity >= 60 E.g., Query converts temperature from Celsius to Fahrenheit, and identifies rooms with room number between 10 and 15 as server rooms. from TempStream select roomNo, temp * 9/5 + 32 as temp, 'F' as scale, roomNo > 10 and roomNo < 15 as isServerRoom insert into RoomTempStream; Function Functions are pre-configured operations that can consumes zero, or more parameters and always produce a single value as result. It can be used anywhere an attribute can be used. Purpose It encapsulate pre-configured reusable execution logic allowing users to execute the logic anywhere just by calling the function. This also make writing SiddhiApps simple and easy to understand. Syntax The syntax of function is as follows, 1 function name ( parameter * ) Here function name uniquely identifies the function. The parameter defined input parameters the function can accept. The input parameters can be attributes, constant values, results of other functions, results of mathematical or logical expressions, or time values. The number and type of parameters a function accepts depend on the function itself. Note Functions, mathematical expressions, and logical expressions can be used in a nested manner. Example 1 Function with name add accepting two input parameters, one being an attribute named input and other being a constant value 75 . 1 add(input, 75) Example 2 Function name alertAfter accepting two input parameters, one being a time value 1 hour and 25 minutes and the other a mathematical addition of startTime and 56 . 1 alertAfter(1 hour and 25 minutes, startTime + 56) Inbuilt functions Following are some inbuilt Siddhi functions, for more functions refer execution extensions . Inbuilt function Description eventTimestamp Returns event's timestamp. currentTimeMillis Returns current time of SiddhiApp runtime. default Returns a default value if the parameter is null. ifThenElse Returns parameters based on a conditional parameter. UUID Generates a UUID. cast Casts parameter type. convert Converts parameter type. coalesce Returns first not null input parameter. maximum Returns the maximum value of all parameters. minimum Returns the minimum value of all parameters. instanceOfBoolean Checks if the parameter is an instance of Boolean. instanceOfDouble Checks if the parameter is an instance of Double. instanceOfFloat Checks if the parameter is an instance of Float. instanceOfInteger Checks if the parameter is an instance of Integer. instanceOfLong Checks if the parameter is an instance of Long. instanceOfString Checks if the parameter is an instance of String. createSet Creates HashSet with given input parameters. sizeOfSet Returns number of items in the HashSet, that's passed as a parameter. Example Query to convert the roomNo to string using convert function, find the maximum temperature reading with maximum function, and to add a unique messageID using the UUID function. 1 2 3 4 5 from TempStream select convert ( roomNo , string ) as roomNo , maximum ( tempReading1 , tempReading2 ) as temp , UUID () as messageID insert into RoomTempStream ; Filter Filters filter events arriving on input streams based on specified conditions. They accept any type of condition including a combination of attributes, constants, functions, and others, that produces a Boolean result. Filters allow events to pass through if the condition results in true , and drops if it results in a false . Purpose Filter helps to select the events that are relevant for processing and omit the ones that are not. Syntax Filter conditions should be defined in square brackets ( [] ) next to the input stream as shown below. 1 2 3 from input stream [ filter condition ] select attribute name , attribute name , ... insert into output stream Example Query to filter TempStream stream events, having roomNo within the range of 100-210 and temperature greater than 40 degrees, and insert the filtered results into HighTempStream stream. 1 2 3 from TempStream [( roomNo = 100 and roomNo 210 ) and temp 40 ] select roomNo , temp insert into HighTempStream ; Window Windows capture a subset of events from input streams and retain them for a period of time based on a specified criterion. The criterion defines when and how the events should be evicted from the window. Such as events getting evicted based on time duration, or number of events in the window, and the way they get evicted is in sliding (one by one) or tumbling (batch) manner. In a query, each input stream can at most have only one window associated with it. Purpose Windows help to retain events based on a criterion, such that the values of those events can be aggregated, correlated or checked, if the event of interest is in the window. Syntax Window should be defined next to the input stream along the #window prefix as shown below. 1 2 3 from input stream # window . window name ( parameter , parameter , ... ) select attribute name , attribute name , ... insert ouput event type ? into output stream Note Filter conditions can be applied both before and/or after the window. Inbuilt windows Following are some inbuilt Siddhi windows, for more windows refer execution extensions . Inbuilt function Description time Retains events based on time in a sliding manner. timeBatch Retains events based on time in a tumbling/batch manner. length Retains events based on number of events in a sliding manner. lengthBatch Retains events based on number of events in a tumbling/batch manner. timeLength Retains events based on time and number of events in a sliding manner. session Retains events for each session based on session key. batch Retains events of last arrived event chunk. sort Retains top-k or bottom-k events based on a parameter value. cron Retains events based on cron time in a tumbling/batch manner. externalTime Retains events based on event time value passed as a parameter in a sliding manner. externalTimeBatch Retains events based on event time value passed as a parameter in a a tumbling/batch manner. delay Retains events and delays the output by the given time period in a sliding manner. Example 1 Query to find out the maximum temperature out of the last 10 events , using the window of length 10 and max() aggregation function, from the TempStream stream and insert the results into the MaxTempStream stream. 1 2 3 from TempStream # window . length ( 10 ) select max ( temp ) as maxTemp insert into MaxTempStream ; Here, the length window operates in a sliding manner where the following 3 event subsets are calculated and outputted when a list of 12 events are received in sequential order. Subset Event Range 1 1 - 10 2 2 - 11 3 3 - 12 Example 2 Query to find out the maximum temperature out of the every 10 events , using the window of lengthBatch 10 and max() aggregation function, from the TempStream stream and insert the results into the MaxTempStream stream. 1 2 3 from TempStream # window . lengthBatch ( 10 ) select max ( temp ) as maxTemp insert into MaxTempStream ; Here, the window operates in a batch/tumbling manner where the following 3 event subsets are calculated and outputted when a list of 30 events are received in a sequential order. Subset Event Range 1 1 - 10 2 11 - 20 3 21 - 30 Example 3 Query to find out the maximum temperature out of the events arrived during last 10 minutes , using the window of time 10 minutes and max() aggregation function, from the TempStream stream and insert the results into the MaxTempStream stream. 1 2 3 from TempStream # window . time ( 10 min ) select max ( temp ) as maxTemp insert into MaxTempStream ; Here, the time window operates in a sliding manner with millisecond accuracy, where it will process events in the following 3 time durations and output aggregated events when a list of events are received in a sequential order. Subset Time Range (in ms) 1 1:00:00.001 - 1:10:00.000 2 1:00:01.001 - 1:10:01.000 3 1:00:01.033 - 1:10:01.034 Example 4 Query to find out the maximum temperature out of the events arriving every 10 minutes , using the window of timeBatch 10 and max() aggregation function, from the TempStream stream and insert the results into the MaxTempStream stream. 1 2 3 from TempStream # window . timeBatch ( 10 min ) select max ( temp ) as maxTemp insert into MaxTempStream ; Here, the window operates in a batch/tumbling manner where the window will process evetns in the following 3 time durations and output aggregated events when a list of events are received in a sequential order. Subset Time Range (in ms) 1 1:00:00.001 - 1:10:00.000 2 1:10:00.001 - 1:20:00.000 3 1:20:00.001 - 1:30:00.000 Event Type Query output depends on the current and expired event types produced by the query based on its internal processing state. By default all queries produce current events upon event arrival. The queries containing windows additionally produce expired events when events expire from those windows. Purpose Event type helps to identify how the events were produced and to specify when a query should output such events to the output stream, such as output processed events only upon new event arrival to the query, upon event expiry from the window, or upon both cases. Syntax Event type should be defined in between insert and into keywords for insert queries as follows. 1 2 3 from input stream # window . window name ( parameter , parameter , ... ) select attribute name , attribute name , ... insert event type into output stream Event type should be defined next to the for keyword for delete queries as follows. 1 2 3 4 from input stream # window . window name ( parameter , parameter , ... ) select attribute name , attribute name , ... delete table ( for event type ) ? on condition Event type should be defined next to the for keyword for update queries as follows. 1 2 3 4 5 from input stream # window . window name ( parameter , parameter , ... ) select attribute name , attribute name , ... update table ( for event type ) ? set table . attribute name = ( attribute name | expression ) ? , table . attribute name = ( attribute name | expression ) ? , ... on condition Event type should be defined next to the for keyword for update or insert queries as follows. 1 2 3 4 5 from input stream # window . window name ( parameter , parameter , ... ) select attribute name , attribute name , ... update or insert into table ( for event type ) ? set table . attribute name = expression , table . attribute name = expression , ... on condition The event types can be defined using the following keywords to manipulate query output. Event types Description current events Outputs processed events only upon new event arrival to the query. This is default behavior when no specific event type is specified. expired events Outputs processed events only upon event expiry from the window. all events Outputs processed events when new events arrive to the query as well as when events expire from the window. Note Controlling query output based on the event types neither alters query execution nor its accuracy. Example Query to output processed events only upon event expiry from the 1 minute time window to the DelayedTempStream stream. This query helps to delay events by a minute. 1 2 3 from TempStream # window . time ( 1 min ) select * insert expired events into DelayedTempStream Note This is just to illustrate how expired events work, it is recommended to use delay window for use cases where we need to delay events by a given time period of time. Aggregate Function Aggregate functions are pre-configured aggregation operations that can consume zero, or more parameters from multiple events and produce a single value as result. They can be only used in query projection (as part of the select clause). When a query comprises a window, the aggregation will be constrained to the events in the window, and when it does not have a window, the aggregation is performed from the first event the query has received. Purpose Aggregate functions encapsulate pre-configured reusable aggregate logic allowing users to aggregate values of multiple events together. When used with batch/tumbling windows this will also reduce the number of output events produced. Syntax Aggregate function can be used in query projection (as part of the select clause) alone or as a part of another expression. In all cases, the output produced should be properly mapped to the output stream attribute of the query using the as keyword. The syntax of aggregate function is as follows, 1 2 3 from input stream # window . window name ( parameter , parameter , ... ) select aggregate function ( parameter , parameter , ... ) as attribute name , attribute2 name , ... insert into output stream ; Here aggregate function uniquely identifies the aggregate function. The parameter defined input parameters the aggregate function can accept. The input parameters can be attributes, constant values, results of other functions or aggregate functions, results of mathematical or logical expressions, or time values. The number and type of parameters an aggregate function accepts depend on the aggregate function itself. Inbuilt aggregate functions Following are some inbuilt aggregation functions, for more aggregate functions refer execution extensions . Inbuilt aggregate function Description sum Calculates the sum from a set of values. count Calculates the count from a set of values. distinctCount Calculates the distinct count based on a parameter from a set of values. avg Calculates the average from a set of values. max Finds the maximum value from a set of values. max Finds the minimum value from a set of values. maxForever Finds the maximum value from all events throughout its lifetime irrespective of the windows. minForever Finds the minimum value from all events throughout its lifetime irrespective of the windows. stdDev Calculates the standard deviation from a set of values. and Calculates boolean and from a set of values. or Calculates boolean or from a set of values. unionSet Constructs a Set by unioning set of values. Example Query to calculate average, maximum, and minimum values on temp attribute of the TempStream stream in a sliding manner, from the events arrived over the last 10 minutes and to produce output events with attributes avgTemp , maxTemp and minTemp respectively to the AvgTempStream stream. 1 2 3 from TempStream # window . time ( 10 min ) select avg ( temp ) as avgTemp , max ( temp ) as maxTemp , min ( temp ) as minTemp insert into AvgTempStream ; Group By Group By groups events based on one or more specified attributes to perform aggregate operations. Purpose Group By helps to perform aggregate functions independently for each given group-by key combination. Syntax The syntax for the Group By with aggregate function is as follows. 1 2 3 4 from input stream # window . window name (...) select aggregate function ( parameter , parameter , ...) as attribute1 name , attribute2 name , ... group by attribute1 name , attribute2 name , ... insert into output stream ; Here the group by attributes should be defined next to the group by keyword separating each attribute by a comma. Example Query to calculate the average temp per each roomNo and deviceID combination, from the events arrived from TempStream stream, during the last 10 minutes time-window in a sliding manner. 1 2 3 4 from TempStream # window . time ( 10 min ) select roomNo , deviceID , avg ( temp ) as avgTemp group by roomNo , deviceID insert into AvgTempStream ; Having Having filters events at the query output using a specified condition on query output stream attributes. It accepts any type of condition including a combination of output stream attributes, constants, and/or functions that produces a Boolean result. Having, allow events to passthrough if the condition results in true , and drops if it results in a false . Purpose Having helps to select the events that are relevant for the output based on the attributes those are produced by the select clause and omit the ones that are not. Syntax The syntax for the Having clause is as follows. 1 2 3 4 5 from input stream # window . window name ( ... ) select aggregate function ( parameter , parameter , ...) as attribute1 name , attribute2 name , ... group by attribute1 name , attribute2 name ... having condition insert into output stream ; Here the having condition should be defined next to the having keyword, and it can be used with or without group by clause. Example Query to calculate the average temp per roomNo for the events arrived on the last 10 minutes, and send alerts for each event having avgTemp more than 30 degrees. 1 2 3 4 5 from TempStream # window . time ( 10 min ) select roomNo , avg ( temp ) as avgTemp group by roomNo having avgTemp 30 insert into AlertStream ; Order By Order By, orders the query results in ascending or descending order based on one or more specified attributes. By default the order by attribute orders the events in ascending order, and by adding desc keyword, the events can be ordered in descending order. When more than one attribute is defined the attributes defined towards the left will have more precedence in ordering than the ones defined in right. Purpose Order By helps to sort the events in the query output chunks. Order By will only be effective when query outputs a lot of events together such as in batch windows than for sliding windows where events are emitted one at a time. Syntax The syntax for the Order By clause is as follows: 1 2 3 4 5 6 from input stream # window . window name ( ... ) select aggregate function ( parameter , parameter , ...) as attribute1 name , attribute2 name , ... group by attribute1 name , attribute2 name ... having condition order by attribute1 name ( asc | desc ) ? , attribute2 name ( asc | desc ) ? , ... insert into output stream ; Here, the order by attributes ( attributeN name ) should be defined next to the order by keyword separating each by a comma, and optionally the event ordering can be specified using asc (default) or desc keywords to respectively define ascending and descending. Example Query to calculate the average temp , per roomNo and deviceID combination, on every 10 minutes batches, and order the generated output events in ascending order by avgTemp and then in descending order of roomNo (if there are more events having the same avgTemp value) before emitting them to the AvgTempStream stream. 1 2 3 4 5 from TempStream # window . timeBatch ( 10 min ) select roomNo , deviceID , avg ( temp ) as avgTemp group by roomNo , deviceID order by avgTemp , roomNo desc insert into AvgTempStream ; Limit Offset These provide a way to select a limited number of events (via limit) from the desired index (using an offset) from the output event chunks produced by the query. Purpose Limit Offset helps to output only the selected set of events from large event batches. This will be very useful with Order By clause where one can order the output and extract the topK or bottomK events, and even use it to paginate through the dataset by obtaining set of events from the middle. Syntax The syntax for the Limit Offset clauses is as follows: 1 2 3 4 5 6 7 8 from input stream # window . window name ( ... ) select aggregate function ( parameter , parameter , ...) as attribute1 name , attribute2 name , ... group by attribute1 name , attribute2 name ... having condition order by attribute1 name ( asc | desc ) ? , attribute2 name ( ascend / descend ) ? , ... limit positive integer ? offset positive integer ? insert into output stream ; Here both limit and offset are optional and both can be defined by adding a positive integer next to their keywords, when limit is omitted the query will output all the events, and when offset is omitted 0 is taken as the default offset value. Example 1 Query to calculate the average temp , per roomNo and deviceID combination, for every 10 minutes batches, from the events arriving at the TempStream stream, and emit only two events having the highest avgTemp value. 1 2 3 4 5 6 from TempStream # window . timeBatch ( 10 min ) select roomNo , deviceID , avg ( temp ) as avgTemp group by roomNo , deviceID order by avgTemp desc limit 2 insert into HighestAvgTempStream ; Example 2 Query to calculate the average temp , per roomNo and deviceID combination, for every 10 minutes batches, for the events arriving at the TempStream stream, and emits only the third, forth and fifth events when sorted in descending order based on their avgTemp value. 1 2 3 4 5 6 7 from TempStream # window . timeBatch ( 10 min ) select roomNo , deviceID , avg ( temp ) as avgTemp group by roomNo , deviceID order by avgTemp desc limit 3 offset 2 insert into HighestAvgTempStream ; Join (Stream) Joins combine events from two streams in real-time based on a specified condition. Purpose Join provides a way of correlating events of two steams and in addition aggregating them based on the defined windows. Two streams cannot directly join as they are stateless, and they do not retain events. Therefore, each stream needs to be associated with a window for joining as it can retain events. Join also accepts a condition to match events against each event stream window. During the joining process each incoming event of each stream is matched against all the events in the other stream's window based on the given condition, and the output events are generated for all the matching event pairs. When there is no window associated with the joining steam, window.lengthBatch(0) is assigned by default to the steam to enable the join process and to preserve stream's stateless nature. Note Join can also be performed with stored data , aggregation or externally named windows . Syntax The syntax for a join two streams is as follows: 1 2 3 4 5 from input stream ( # window . window name ( parameter , ... )) ? ( unidirectional ) ? ( as reference ) ? join type input stream ( # window . window name ( parameter , ... )) ? ( unidirectional ) ? ( as reference ) ? ( on join condition ) ? select attribute name , attribute name , ... insert into output stream Here, both the streams can have a window associated with them and have an optional join condition next to the on keyword to match events from both windows to generate combined output events. Window should be defined as the last element of each joining stream. Join query expects a window to be defined as the last element of each joining stream, therefore a filter cannot be defined after the window. join types Following are the supported join operations. Inner join (join) This is the default behavior of a join operation, and the join keyword is used to join both the streams. Here the output is generated only if there is a matching event in both the streams when either stream is triggering the join operation. Left outer join The left outer join keyword is used to join two streams while producing all left stream events to the output. Here, the output is generated when right stream triggers the join operation and finds matching events in the left stream window to perform the join, and in all cases where the left stream triggers the join operation. Here, when the left stream finds matching events in the right stream window, it uses them for the join, and if there are no matching events, then it uses null values for the join operation. Right outer join This is similar to a left outer join and the Right outer join keyword is used to join two streams while producing all right stream events to the output. It generate output in all cases where the right stream triggers the join operation even if there are no matching events in the left stream window. Full outer join The full outer join combines the results of left outer join and right outer join. The full outer join keyword is used to join the streams while producing both left and stream events to the output. Here, the output is generated in all cases where the left or right stream triggers the join operation, and when a stream finds matching events in the other stream window, it uses them for the join, and if there are no matching events, then it uses null values instead. Cross join In either of these cases when join condition is omitted, the triggering event will successfully match against all the events in the other stream window, producing a cross join behavior. Unidirectional join operation By default, events arriving on either stream trigger the join operation and generate the corresponding output. However, this join behavior can be controlled by adding the unidirectional keyword next to one of the streams as depicted in the join query syntax above. This enables only the stream with the unidirectional to trigger the join operation. Therefore the events arriving on the other stream will neither trigger the join operation nor produce any output, but rather they only update their stream's window state. The unidirectional keyword cannot be applied on both join streams. This is because the default behavior already allows both the streams to trigger the join operation. Example 1 (join) The query to generate output when there is a matching event having equal symbol and companyID combination from the events arrived in the last 10 minutes on StockStream stream and the events arrived in the last 20 minutes on TwitterStream stream. 1 2 3 4 5 6 7 8 define stream StockStream ( symbol string , price float , volume long ); define stream TwitterStream ( companyID string , tweet string ); from StockStream # window . time ( 10 min ) as S join TwitterStream # window . time ( 20 min ) as T on S . symbol == T . companyID select S . symbol as symbol , T . tweet , S . price insert into OutputStream ; Possible OutputStream outputs as follows 1 2 ( FB , FB is great! , 23.5f) ( GOOG , Its time to Google! , 54.5f) Example 2 (with no join condition) The query to generate output for all possible event combinations from the last 5 events of the StockStream stream and the events arrived in the last 1 minutes on TwitterStream stream. 1 2 3 4 5 6 7 define stream StockStream ( symbol string , price float , volume long ); define stream TwitterStream ( companyID string , tweet string ); from StockStream # window . length ( 5 ) as S join TwitterStream # window . time ( 1 min ) as T select S . symbol as symbol , T . tweet , S . price insert into OutputStream ; Possible OutputStream outputs as follows, 1 2 3 4 ( FB , FB is great! , 23.5f) ( FB , Its time to Google! , 23.5f) ( GOOG , FB is great! , 54.5f) ( GOOG , Its time to Google! , 54.5f) Example 3 (left outer join) The query to generate output for all events arriving in the StockStream stream regardless of whether there is a matching companyID for symbol exist in the events arrived in the last 20 minutes on TwitterStream stream, and generate output for the events arriving in the StockStream stream only when there is a matchine symbol and companyID combination exist in the events arrived in the last 10 minutes on StockStream stream. 1 2 3 4 5 6 7 8 define stream StockStream ( symbol string , price float , volume long ); define stream TwitterStream ( companyID string , tweet string ); from StockStream # window . time ( 10 min ) as S left outer join TwitterStream # window . time ( 20 min ) as T on S . symbol == T . companyID select S . symbol as symbol , T . tweet , S . price insert into OutputStream ; Possible OutputStream outputs as follows, 1 2 ( FB , FB is great! , 23.5f) ( GOOG , null, 54.5f) //when there are no matching event in TwitterStream Example 3 (full outer join) The query to generate output for all events arriving in the StockStream stream and in the TwitterStream stream regardless of whether there is a matching companyID for symbol exist in the other stream window or not. 1 2 3 4 5 6 7 8 define stream StockStream ( symbol string , price float , volume long ); define stream TwitterStream ( companyID string , tweet string ); from StockStream # window . time ( 10 min ) as S full outer join TwitterStream # window . time ( 20 min ) as T on S . symbol == T . companyID select S . symbol as symbol , T . tweet , S . price insert into OutputStream ; Possible OutputStream outputs as follows, 1 2 3 ( FB , FB is great! , 23.5f) ( GOOG , null, 54.5f) //when there are no matching event in TwitterStream (null, I like to tweet! , null) //when there are no matching event in StockStream Example 3 (unidirectional join) The query to generate output only when events arrive on StockStream stream find a matching event having equal symbol and companyID combination against the events arrived in the last 20 minutes on TwitterStream stream. 1 2 3 4 5 6 7 8 define stream StockStream ( symbol string , price float , volume long ); define stream TwitterStream ( companyID string , tweet string ); from StockStream # window . time ( 10 min ) unidirectional as S join TwitterStream # window . time ( 20 min ) as T on S . symbol == T . companyID select S . symbol as symbol , T . tweet , S . price insert into OutputStream ; Possible OutputStream outputs as follows, 1 2 ( FB , FB is great! , 23.5f) ( GOOG , Its time to Google! , 54.5f) Here both outputs will be initiated by events arriving on StockStream . Pattern The pattern is a state machine implementation that detects event occurrences from events arrived via one or more event streams over time. It can repetitively match patterns, count event occurrences, and use logical event ordering (using and , or , and not ). Purpose The pattern helps to achieve Complex Event Processing (CEP) capabilities by detecting various pre-defined event occurrence patterns in realtime. Pattern query does not expect the matching events to occur immediately after each other, and it can successfully correlate the events who are far apart and having other events in between. Syntax The syntax for a pattern query is as follows, 1 2 3 4 5 6 7 8 from ( ( every ) ? ( event reference = ) ? input stream [ filter condition ]( min count : max count ) ? | ( every ) ? ( event reference = ) ? input stream [ filter condition ] ( and | or ) ( event reference = ) ? input stream [ filter condition ] | ( every ) ? not input stream [ filter condition ] ( and event reference = input stream [ filter condition ] | for time gap ) ) - ... ( within time gap ) ? select event reference . attribute name , event reference . attribute name , ... insert into output stream Items Description -> Indicates an event will follow the given event. The subsequent event does not necessarily have to occur immediately after the preceding event. The condition to be met by the preceding event should be added before the -> , and the condition to be met by the subsequent event should be added after the -> . every An optional keyword defining when a new event matching state-machine should be initiated to repetitively match the pattern. When this keyword is not used, the event matching state-machine will be initiated only once. within time gap An optional within clause that defines the time duration within which all the matching events should occur. min count : max count Determines the number of minimum and maximum number of events that should the matched at the given condition. Possible values for the min and max count and their behavior is as follows, Syntex Description Example n1:n2 Matches n1 to n2 events (including n1 and not more than n2 ). 1:4 matches 1 to 4 events. n: Matches n or more events (including n ). 2:> matches 2 or more events. :n Matches up to n events (excluding n ). :5 matches up to 5 events. n Matches exactly n events. 5 matches exactly 5 events. and Allows both of its condition to be matched by two distinct events in any order. or Only expects one of its condition to be matched by an event. Here the event reference of the unmatched condition will be null . not condition1 and condition2 Detects the event matching condition2 before any event matching condition1 . not condition1> for time period> Detects no event matching on condition1 for the specified time period . event reference An optional reference to access the matching event for further processing. All conditions can be assigned to an event reference to collect the matching event occurrences, other than the condition used for not case (as there will not be any event matched against it). Non occurrence of events. Siddhi detects non-occurrence of events using the not keyword, and its effective non-occurrence checking period is bounded either by fulfillment of a condition associated by and or via an expiry time using time period . Logical correlation of multiple conditions. Siddhi can only logically correlate two conditions at a time using keywords such as and , or , and not . When more than two conditions need to be logically correlated, use multiple pattern queries in a chaining manner, at a time correlating two logical conditions and streaming the output to a downstream query to logically correlate the results with other logical conditions. Event selection The event reference in pattern queries is used to retrieve the matched events. When a pattern condition is intended to match only a single event, then its attributes can be retrieved by referring to its reference as event reference . attribute name . An example of this is as follows. e1.symbol , refers to the symbol attribute value of the matching event e1 . But when the pattern condition is associated with min count : max count , it is expected to match against on multiple events. Therefore, an event from the matched event collection should be retrieved using the event index from its reference. Here the indexes are specified in square brackets next to event reference, where index 0 referring to the first event, and a special index last referring to the last available event in the collection. Attribute values of all the events in the matching event collection can be accessed a list, by referring to their event reference without an index. Some possible indexes and their behavior is as follows. e1[0].symbol , refers to the symbol attribute value of the 1 st event in reference e1 . e1[3].price , refers to the price attribute value of the 4 th event in reference e1 . e1[last].symbol , refers to the symbol attribute value of the last event in reference e1 . e1[last - 1].symbol , refers to the symbol attribute value of one before the last event in reference e1 . e1.symbol , refers to the list of symbol attribute values of all events in the event collection in reference e1 , as a list object. The system returns null when accessing attribute values, when no matching event is assigned to the event reference (as in when two conditions are combined using or ) or when the provided index is greater than the last event index in the event collection. Example 1 (Every) A query to send an alerts when temperature of a room increases by 5 degrees within 10 min. 1 2 3 4 from every ( e1 = TempStream ) - e2 = TempStream [ e1 . roomNo == roomNo and ( e1 . temp + 5 ) = temp ] within 10 min select e1 . roomNo , e1 . temp as initialTemp , e2 . temp as finalTemp insert into AlertStream ; Here, the matching process begins for each event in the TempStream stream (as every is used with e1=TempStream ), and if another event arrives within 10 minutes with a value for temp attribute being greater than or equal to e1.temp + 5 of the initial event e1 , an output is generated via the AlertStream . Example 2 (Event collection) A query to find the temperature difference between two regulator events. 1 2 3 4 5 6 7 define stream TempStream ( deviceID long , roomNo int , temp double ); define stream RegulatorStream ( deviceID long , roomNo int , tempSet double , isOn bool ); from every e1 = RegulatorStream - e2 = TempStream [ e1 . roomNo == roomNo ] 1 : - e3 = RegulatorStream [ e1 . roomNo == roomNo ] select e1 . roomNo , e2 [ 0 ]. temp - e2 [ last ]. temp as tempDiff insert into TempDiffStream ; Here, one or more TempStream events having the same roomNo as of the RegulatorStream stream event matched in e1 is collected, and among them, the first and the last was retrieved to find the temperature difference. Example 3 (Logical or condition) Query to send the stop control action to the regulator via RegulatorActionStream when the key is removed from the hotel room. Here the key actions are monitored via RoomKeyStream stream, and the regulator state is monitored through RegulatorStateChangeStream stream. 1 2 3 4 5 6 7 8 9 define stream RegulatorStateChangeStream ( deviceID long , roomNo int , tempSet double , action string ); define stream RoomKeyStream ( deviceID long , roomNo int , action string ); from every e1 = RegulatorStateChangeStream [ action == on ] - e2 = RoomKeyStream [ e1 . roomNo == roomNo and action == removed ] or e3 = RegulatorStateChangeStream [ e1 . roomNo == roomNo and action == off ] select e1 . roomNo , ifThenElse ( e2 is null , none , stop ) as action having action != none insert into RegulatorActionStream ; Here, the query sends a stop action on RegulatorActionStream stream, if a removed action is triggered in the RoomKeyStream stream before the regulator state changing to off which is notified via RegulatorStateChangeStream stream. Example 4 (Logical not condition) Query to generate alerts if the regulator gets switched off before the temperature reaches 12 degrees. 1 2 3 4 5 6 7 8 define stream RegulatorStateChangeStream ( deviceID long , roomNo int , tempSet double , action string ); define stream TempStream ( deviceID long , roomNo int , temp double ); from every e1 = RegulatorStateChangeStream [ action == start ] - not TempStream [ e1 . roomNo == roomNo and temp = 12 ] and e2 = RegulatorStateChangeStream [ e1 . roomNo == roomNo and action == off ] select e1 . roomNo as roomNo insert into AlertStream ; Here, the query alerts the roomNo via AlertStream stream, when no temperature events having less than 12 arrived in the TempStream between the start and off actions of the regulator, notified via RegulatorActionStream stream. Example 5 (Logical not condition) Query to alert if the room temperature does not reduce to the set value within 5 minutes after switching on the regulator. 1 2 3 4 5 6 7 define stream RegulatorStateChangeStream ( deviceID long , roomNo int , tempSet double , action string ); define stream TempStream ( deviceID long , roomNo int , temp double ); from e1 = RegulatorStateChangeStream [ action == start ] - not TempStream [ e1 . roomNo == roomNo and temp = e1 . tempSet ] for 5 min select e1 . roomNo as roomNo insert into AlertStream ; Here, the query alerts the roomNo via AlertStream stream, when no temperature events having less than tempSet temperature arrived in the TempStream within 5 minutes of the regulator start action arrived via RegulatorActionStream stream. Example 6 (Detecting event non-occurrence) Following table presents some non-occurrence event matching scenarios that can be implemented using patterns. Pattern Description Sample Scenario not A for time period The non-occurrence of event A within time period after system start up. Alerts if the taxi has not reached its destination within 30 minutes, indicating that the passenger might be in danger. not A for time period and B Event A does not occur within time period , but event B occurs at some point in time. Alerts if the taxi has not reached its destination within 30 minutes, and the passenger has marked that he/she is in danger at some point in time. not A for time period or B Either event A does not occur within time period , or event B occurs at some point in time. Alerts if the taxi has not reached its destination within 30 minutes, or if the passenger has marked that he/she is in danger at some point in time. not A for time period 1 and not B for time period 2 Event A does not occur within time period 1 , and event B also does not occur within time period 2 . Alerts if the taxi has not reached its destination within 30 minutes, and the passenger has not marked himself/herself not in danger within the same time period. not A for time period 1 or not B for time period 2 Either event A does not occur within time period 1 , or event B occurs within time period 2 . Alerts if the taxi has not reached its destination A within 20 minutes, or reached its destination B within 30 minutes. A \u2192 not B for time period Event B does not occur within time period after the occurrence of event A. Alerts if the taxi has reached its destination, but it has been not followed by a payment record within 10 minutes. not A and B or A and not B Event A does not occur before event B. Alerts if the taxi is stated before activating the taxi fare calculator. Sequence The sequence is a state machine implementation that detects consecutive event occurrences from events arrived via one or more event streams over time. Here all matching events need to arrive consecutively , and there should not be any non-matching events in between the matching sequence of events. The sequence can repetitively match event sequences, count event occurrences, and use logical event ordering (using and , or , and not ). Purpose The sequence helps to achieve Complex Event Processing (CEP) capabilities by detecting various pre-defined consecutive event occurrence sequences in realtime. Sequence query does expect the matching events to occur immediately after each other, and it can successfully correlate the events who do not have other events in between. Syntax The syntax for a sequence query is as follows: 1 2 3 4 5 6 7 8 from ( ( every ) ? ( event reference = ) ? input stream [ filter condition ] ( +|*|? ) ? | ( event reference = ) ? input stream [ filter condition ] ( and | or ) ( event reference = ) ? input stream [ filter condition ] | not input stream [ filter condition ] ( and event reference = input stream [ filter condition ] | for time gap ) ), ... ( within time gap ) ? select event reference . attribute name , event reference . attribute name , ... insert into output stream Items Description , Indicates the immediate next event that follows the given event. The condition to be met by the preceding event should be added before the , , and the condition to be met by the subsequent event should be added after the , . every An optional keyword defining when a new event matching state-machine should be initiated to repetitively match the sequence. When this keyword is not used, the event matching state-machine will be initiated only once. within time gap An optional within clause that defines the time duration within which all the matching events should occur. + Matches **one or more** events to the given condition. * Matches **zero or more** events to the given condition. ? Matches **zero or one** events to the given condition. and Allows both of its condition to be matched by two distinct events in any order. or Only expects one of its condition to be matched by an event. Here the event reference of the unmatched condition will be null . not condition1 and condition2 Detects the event matching condition2 before any event matching condition1 . not condition1> for time period> Detects no event matching on condition1 for the specified time period . event reference An optional reference to access the matching event for further processing. All conditions can be assigned to an event reference to collect the matching event occurrences, other than the condition used for not case (as there will not be any event matched against it). Non occurrence of events. Siddhi detects non-occurrence of events using the not keyword, and its effective non-occurrence checking period is bounded either by fulfillment of a condition associated by and or via an expiry time using time period . Logical correlation of multiple conditions. Siddhi can only logically correlate two conditions at a time using keywords such as and , or , and not . When more than two conditions need to be logically correlated, use multiple pattern queries in a chaining manner, at a time correlating two logical conditions and streaming the output to a downstream query to logically correlate the results with other logical conditions. Event selection The event reference in sequence queries is used to retrieve the matched events. When a sequence condition is intended to match only a single event, then its attributes can be retrieved by referring to its reference as event reference . attribute name . An example of this is as follows. e1.symbol , refers to the symbol attribute value of the matching event e1 . But when the pattern condition is associated with min count : max count , it is expected to match against on multiple events. Therefore, an event from the matched event collection should be retrieved using the event index from its reference. Here the indexes are specified in square brackets next to event reference, where index 0 referring to the first event, and a special index last referring to the last available event in the collection. Attribute values of all the events in the matching event collection can be accessed a list, by referring to their event reference without an index. Some possible indexes and their behavior is as follows. e1[0].symbol , refers to the symbol attribute value of the 1 st event in reference e1 . e1[3].price , refers to the price attribute value of the 4 th event in reference e1 . e1[last].symbol , refers to the symbol attribute value of the last event in reference e1 . e1[last - 1].symbol , refers to the symbol attribute value of one before the last event in reference e1 . e1.symbol , refers to the list of symbol attribute values of all events in the event collection in reference e1 , as a list object. The system returns null when accessing attribute values, when no matching event is assigned to the event reference (as in when two conditions are combined using or ) or when the provided index is greater than the last event index in the event collection. Example 1 (Every) Query to send alerts when temperature increases at least by one degree between two consecutive temperature events. 1 2 3 from every e1 = TempStream , e2 = TempStream [ temp e1 . temp + 1 ] select e1 . temp as initialTemp , e2 . temp as finalTemp insert into AlertStream ; Here, the matching process begins for each event in the TempStream stream (as every is used with e1=TempStream ), and if the immediate next event with a value for temp attribute being greater than e1.temp + 1 of the initial event e1 , then an output is generated via the AlertStream . Example 2 (Every collection) Query to identify temperature peeks by monitoring continuous increases in temp attribute and alerts upon the first drop. 1 2 3 4 5 6 7 8 define stream TempStream ( deviceID long , roomNo int , temp double ); @ info ( name = query1 ) from every e1 = TempStream , e2 = TempStream [ ifThenElse ( e2 [ last ]. temp is null , e1 . temp = temp , e2 [ last ]. temp = temp )] + , e3 = TempStream [ e2 [ last ]. temp temp ] select e1 . temp as initialTemp , e2 [ last ]. temp as peekTemp , e3 . price as firstDropTemp insert into PeekTempStream ; Here, the matching process begins for each event in the TempStream stream (as every is used with e1=TempStream ). It checks if the temp attribute value of the second event is greater than or equal to the temp attribute value of the first event ( e1.temp ), then for all the following events, their temp attribute value is checked if they are greater than or equal to their previous event's temp attribute value ( e2[last].temp ), and when the temp attribute value becomes less than its previous events temp attribute value value an output is generated via the AlertStream stream. Example 3 (Logical and condition) A query to identify a regulator activation event immediately followed by both temperature sensor and humidity sensor activation events in either order. 1 2 3 4 5 6 7 define stream TempStream ( deviceID long , isActive bool ); define stream HumidStream ( deviceID long , isActive bool ); define stream RegulatorStream ( deviceID long , isOn bool ); from every e1 = RegulatorStream [ isOn == true ], e2 = TempStream and e3 = HumidStream select e2 . isActive as tempSensorActive , e3 . isActive as humidSensorActive insert into StateNotificationStream ; Here, the matching process begins for each event in the RegulatorStream stream having the isOn attribute true . It generates an output via the AlertStream stream when an event from both TempStream stream and HumidStream stream arrives immediately after the first event in either order. Output rate limiting Output rate-limiting limits the number of events emitted by the queries based on a specified criterion such as time, and number of events. Purpose Output rate-limiting helps to reduce the load on the subsequent executions such as query processing, I/O operations, and notifications by reducing the output frequency of the events. Syntax The syntax for output rate limiting is as follows: 1 2 3 4 from input stream ... select attribute name , attribute name , ... output rate limiting configuration insert into output stream Here, the output rate limiting configuration ( rate limiting configuration ) should be defined next to the output keyword and the supported output rate limiting types are explained in the following table: Rate limiting configuration Syntax Description Time based ( output event selection )? every time interval Outputs output event selection every time interval time interval. Number of events based ( output event selection )? every event interval events Outputs output event selection for every event interval number of events. Snapshot based snapshot every time interval Outputs all events currently in the query window (or outputs only the last event if no window is defined in the query) for every given time interval time interval. The output event selection specifies the event(s) that are selected to be outputted from the query, here when no output event selection is defined, all is used by default. The possible values for the output event selection and their behaviors are as follows: * first : The first query output is published as soon as it is generated and the subsequent events are dropped until the specified time interval or the number of events are reached before sending the next event as output. * last : Emits only the last output event generated during the specified time or event interval. * all : Emits all the output events together which are generated during the specified time or event interval. Example 1 (Time based first event) Query to calculate the average temp per roomNo for the events arrived on the last 10 minutes, and send alerts once every 15 minutes of the events having avgTemp more than 30 degrees. 1 2 3 4 5 6 7 8 define stream TempStream ( deviceID long , roomNo int , temp double ); from TempStream # window . time ( 10 min ) select roomNo , avg ( temp ) as avgTemp group by roomNo having avgTemp 30 output first every 15 min insert into AlertStream ; Here the first event having avgTemp 30 is emitted immediately and the next event is only emitted after 15 minutes. Example 2 (Event based first event) Query to output the initial event, and there onwards every 5 th event from TempStream stream events. 1 2 3 4 5 define stream TempStream ( deviceID long , roomNo int , temp double ); from TempStream output first every 5 events insert into FiveEventBatchStream ; Example 3 (Event based all events) Query to collect last 5 TempStream stream events and send them together as a single batch. 1 2 3 4 5 define stream TempStream ( deviceID long , roomNo int , temp double ); from TempStream output every 5 events insert into FiveEventBatchStream ; As no output event selection is defined, the behavior of all is applied in this case. Example 4 (Time based last event) Query to emit only the last event of TempStream stream for every 10 minute interval. 1 2 3 4 5 define stream TempStream ( deviceID long , roomNo int , temp double ); from TempStream output last every 10 min insert into FiveEventBatchStream ; Example 5 (Snapshot based) Query to emit the snapshot of events retained by its last 5 minutes window defined on TempStream stream, every second. 1 2 3 4 5 define stream TempStream ( deviceID long , roomNo int , temp double ); from TempStream # window . time ( 5 sec ) output snapshot every 1 sec insert into SnapshotTempStream ; Here, the query emits all the current events generated which do not have a corresponding expired event at the predefined time interval. Example 6 (Snapshot based) Query to emit the snapshot of events retained every second, when no window is defined on TempStream stream. 1 2 3 4 5 define stream TempStream ( deviceID long , roomNo int , temp double ); from TempStream output snapshot every 5 sec insert into SnapshotTempStream ; Here, the query outputs the last seen event at the end of each time interval as there are no events stored in no window defined. Partition Partition provides data parallelism by categorizing events into various isolated partition instance based on their attribute values and by processing each partition instance in isolation. Here each partition instance is tagged with a partition key, and they only process events that match to the corresponding partition key. Purpose Partition provide ways to segment events into groups and allow them to process the same set of queries in parallel and in isolation without redefining the queries for each segment. Here, events form multiple steams generating the same partition key will result in the same instance of the partition, and executed together. When a stream is used within the partition block without configuring a partition key, all of its events will be executed in all available partition instances. Syntax The syntax for a partition is as follows: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 @ purge ( enable = true , interval = purge interval , idle . period = idle period of partition instance ) partition with ( key selection of stream name , key selection of stream name , ... ) begin from stream name ... select attribute name , attribute name , ... insert into ( # ) ? stream name from ( # ) ? stream name ... select attribute name , attribute name , ... insert into stream name ... end ; Here, a new instance of a partition will be dynamically created for each unique partition key that is generated based on the key selection applied on the events of their associated streams ( stream name ). These created partition instances will exist in the system forever unless otherwise a purging policy is defined using the @purge annotation. The inner streams denoted by # stream name can be used to chain multiple queries within a partition block without leaving the isolation of the partition instance. The key selection defines the partition key for each event based on the event attribute value or using range expressions as listed below. Key selection type Syntax description Partition by value attribute name Attribute value of the event is used as its partition key. Partition by range compare condition as 'value' or compare condition as 'value' or ... Event is executed against all compare conditions , and the values associated with the matching conditions are used as its partition key. Here, when the event is matched against multiple conditions, it is processed on all the partition instances that are associated with those matching conditions. When there are multiple queries within a partition block, and they can be chained without leaving the isolation of the partition instance using the inner streams denoted by # . More information on inner Streams will be covered in the following secsions. Inner Stream Inner stream connects the queries inside a partition instance to one another while preserving partition isolation. These are denoted by a # placed before the stream name, and these streams cannot be accessed outside the partition block. Through this, without repartitioning the streams, the output of a query instance can be used as the input of another query instance that is also in the same partition instance. Using non inner streams to chain queries within a partition block. When the connecting stream is not an inner stream and if it is not configured to generate a partition key, then it outputs events to all available partition instances . However, when the non-inner stream is configured to generate a partition key, it only outputs to the partition instances that are selected based on the repartitioned partition key. Purge Partition Purge partition purges partitions that are not being used for a given period on a regular interval. This is because, by default, when partition instances are created for each unique partition key they exist forever if their queries contain stateful information, and there are use cases (such as partitioning events by date value) where an extremely large number of unique partition keys are used, which generates a large number of partition instances, and this eventually leading to system out of memory. The partition instances that will not be used anymore can purged using the @purge annotation. The elements of the annotation and their behavior is as follows. Purge partition configuration Description enable To enable partition purging. internal Periodic time interval to purge the purgeable partition instances. idle.period The idle period, a particular partition instance (for a given partition key) needs to be idle before it becomes purgeable. Example 1 (Partition by value) Query to calculate the maximum temperature of each deviceID , among its last 10 events. 1 2 3 4 5 6 partition with ( deviceID of TempStream ) begin from TempStream # window . length ( 10 ) select roomNo , deviceID , max ( temp ) as maxTemp insert into DeviceTempStream ; end ; Here, each unique deviceID will create a partition instance which retains the last 10 events arrived for its corresponding partition key and calculates the maximum values without interfering with the events of other partition instances. Example 2 (Partition by range) Query to calculate the average temperature for the last 10 minutes per each office area, where the office areas are identified based on the roomNo attribute ranges from the events of TempStream stream. 1 2 3 4 5 6 7 8 partition with ( roomNo = 1030 as serverRoom or roomNo 1030 and roomNo = 330 as officeRoom or roomNo 330 as lobby of TempStream ) begin from TempStream # window . time ( 10 min ) select roomNo , deviceID , avg ( temp ) as avgTemp insert into AreaTempStream end ; Here, partition instances are created for each office area type such as serverRoom , officeRoom , and lobby . Events are processed only in the partition instances which are associated with matching compare condition values that are satisfied by the event's roomNo attribute, and within each partition instance, the average tamp value is calculated based on the events arrived over the last 10 minutes. Example 3 (Inner streams) A partition to calculate the average temperature of every 10 events for each sensor, and send the output via the DeviceTempIncreasingStream stream if consecutive average temperature ( avgTemp ) values increase by more than 5 degrees. 1 2 3 4 5 6 7 8 9 10 partition with ( deviceID of TempStream ) begin from TempStream # window . lengthBatch ( 10 ) select roomNo , deviceID , avg ( temp ) as avgTemp insert into # AvgTempStream from every e1 =# AvgTempStream , e2 =# AvgTempStream [ e1 . avgTemp + 5 avgTemp ] select e1 . deviceID , e1 . avgTemp as initialAvgTemp , e2 . avgTemp as finalAvgTemp insert into DeviceTempIncreasingStream end ; Here, the first query calculates the avgTemp for every 10 events for each unique deviceID and passes the output via the inner stream #AvgTempStream to the second query that is also in the same partition instance. The second query then identifies a pair of consecutive events from #AvgTempStream , where the latter event having 5 degrees more on avgTemp value than its previous event. Example 4 (Purge partition) A partition to identify consecutive three login failure attempts for each session within 1 hour. Here, the number of sessions can be infinite. 1 2 3 4 5 6 7 8 9 10 11 12 define stream LoginStream ( sessionID string , loginSuccessful bool ); @ purge ( enable = true , interval = 10 sec , idle . period = 1 hour ) partition with ( sessionID of LoginStream ) begin from every e1 = LoginStream [ loginSuccessful == false ], e2 = LoginStream [ loginSuccessful == false ], e3 = LoginStream [ loginSuccessful == false ] within 1 hour select e1 . sessionID as sessionID insert into LoginFailureStream ; end ; Here, the events in LoginStream is partitioned by their sessionID attribute and matched for consecutive occurrences of events having loginSuccessful==false with 1 hour using a sequence query and inserts the matching pattern's sessionID to LoginFailureStream . As the number of sessions is infinite the @purge annotation is enabled to purge the partition instances. The instances are marked for purging if there are no events from a particular sessionID for the last 1 hour, and the marked instances are periodically purged once every 10 seconds. Table A table is a stored version of an stream or a table of events. Its schema is defined via the table definition that is similar to a stream definition. These events are by default stored in-memory , but Siddhi also provides store extensions to work with data/events stored in various data stores through the table abstraction. Purpose Tables allow Siddhi to work with stored events. By defining a schema for tables Siddhi enables them to be processed by queries using their defined attributes with the streaming data. You can also interactively query the state of the stored events in the table. Syntax The syntax for a new table definition is as follows: 1 define table table name ( attribute name attribute type , attribute name attribute type , ... ); The following parameters are configured in a table definition: Parameter Description table name The name of the table defined. ( PascalCase is used for table name as a convention.) attribute name The schema of the table is defined by its attributes with uniquely identifiable attribute names ( camelCase is used for attribute names as a convention.) attribute type The type of each attribute defined in the schema. This can be STRING , INT , LONG , DOUBLE , FLOAT , BOOL or OBJECT . Example The following defines a table named RoomTypeTable with roomNo and type attributes of data types int and string respectively. 1 define table RoomTypeTable ( roomNo int , type string ); Primary Keys Tables can be configured with primary keys to avoid the duplication of data. Primary keys are configured by including the @PrimaryKey( 'key1', 'key2' ) annotation to the table definition. Each event table configuration can have only one @PrimaryKey annotation. The number of attributes supported differ based on the table implementations. When more than one attribute is used for the primary key, the uniqueness of the events stored in the table is determined based on the combination of values for those attributes. Examples This query creates an event table with the symbol attribute as the primary key. Therefore each entry in this table must have a unique value for symbol attribute. 1 2 @ PrimaryKey ( symbol ) define table StockTable ( symbol string , price float , volume long ); Indexes Indexes allow tables to be searched/modified much faster. Indexes are configured by including the @Index( 'key1', 'key2' ) annotation to the table definition. Each event table configuration can have 0-1 @Index annotations. Support for the @Index annotation and the number of attributes supported differ based on the table implementations. When more then one attribute is used for index, each one of them is used to index the table for fast access of the data. Indexes can be configured together with primary keys. Examples This query creates an indexed event table named RoomTypeTable with the roomNo attribute as the index key. 1 2 @ Index ( roomNo ) define table RoomTypeTable ( roomNo int , type string ); Store Store is a table that refers to data/events stored in data stores outside of Siddhi such as RDBMS, Cassandra, etc. Store is defined via the @store annotation, and the store schema is defined via a table definition associated with it. Purpose Store allows Siddhi to search, retrieve and manipulate data stored in external data stores through Siddhi queries. Syntax The syntax for a defining store and it's associated table definition is as follows: 1 2 @ store ( type = store_type , static . option . key1 = static_option_value1 , static . option . keyN = static_option_valueN ) define table TableName ( attribute1 Type1 , attributeN TypeN ); Example The following defines a RDBMS data store pointing to a MySQL database with name hotel hosted in loacalhost:3306 having a table RoomTypeTable with columns roomNo of INTEGER and type of VARCHAR(255) mapped to Siddhi data types int and string respectively. 1 2 3 @ Store ( type = rdbms , jdbc . url = jdbc:mysql://localhost:3306/hotel , username = siddhi , password = 123 , jdbc . driver . name = com.mysql.jdbc.Driver ) define table RoomTypeTable ( roomNo int , type string ); Supported Store Types The following is a list of currently supported store types: RDBMS (MySQL, Oracle, SQL Server, PostgreSQL, DB2, H2) MongoDB Caching in Memory Store tables are persisted in high i/o latency storage. Hence, it is beneficial to maintain a cache of store tables in memory which has low latency. Siddhi supports caching of store tables through @cache annotation. It should be used within @store annotation in a nested fashion as shown below. 1 2 3 @ store ( type = store_type , static . option . key1 = static_option_value1 , static . option . keyN = static_option_valueN , @ cache ( size = 10 , cache . policy = FIFO )) define table TableName ( attribute1 Type1 , attributeN TypeN ); In the above example we have defined a cache with a maximum size of 10 rows with first-in first-out cache policy. The following table contains the cache parameters. Parameter Mandatory/Optional Default Value Description size Mandatory - maximum number of rows to be cached cache.policy Optional FIFO policy to free up cache when cache miss occurs. There are 3 allowed policies. 1. FIFO - First-In, First-Out 2. LRU - Least Recently Used 3. LFU - Least Frequently Used retention.period Optional - If user specifies this parameter then cache expiry is enabled. For example if this is 5 min, rows older than 5 mins will be removed and in some cases reloaded from store purge.interval optional equal to retention period When cache expiry is enabled, a thread will be created for every purge.interval which will check for expired rows and remove them. The following is an example of caching with expiry. 1 2 3 @ store ( type = store_type , static . option . key1 = static_option_value1 , static . option . keyN = static_option_valueN , @ cache ( size = 10 , retention . period = 5 min , purge . interval = 1 min )) define table TableName ( attribute1 Type1 , attributeN TypeN ); The above query will define and create a store table of given type and a cache with a max size of 10. A thread will be created every 1 minute which will check the entire cache table for rows added earlier than 5 minutes and expire them. Cache Behavior Cache behavior changes profoundly based on the size of store table relative to maximum cache size defined. Since memory is a limited resource we don't allow cache to grow more than the user specified maximum size. Case 1 \\ When store table is smaller than maximum cache size defined we keep the entire content of store table in memory in cache table. All types of queries are routed to cache and cache results are directly sent out to the user. Every time the expiry thread finds that cache events were loaded earlier than retention period entire cache table will be deleted and reloaded from store. In addition, when siddhi app starts, the entire store table, if it exists, will be loaded into cache. Case 2 \\ When store table is bigger than maximum cache size only the queries satisfying the following 2 conditions are sent to cache. 1. the query contains all the primary keys of the table 2. the query contains only == type of comparison. Only for the above types of queries we can establish if the cache is hit or missed. Subject to these conditions if the cache is hit the results from cache is sent out. If the cache is missed then store is checked. If the above conditions are not met by a query it is directly sent to the store table. In addition, please note that if the store table is pre existing when siddhi app is started and it is bigger than max cache size, cache preloading will take only upto max size and put it in cache. For example if store table has 50 entries when the siddhi app is defined with cache size of 10, only the first 10 rows will be cached. When cache miss occurs we look for the answer in the store table. If there is a result from the store table it is added to cache. One element from cache is removed using the user given cache policy prior to adding. When it comes to cache expiry, since not all rows are loaded at once in this case there may be some expired rows and some unexpired rows at any time. So for every purge interval a thread will be generated which looks for rows that were loaded earlier than retention period and delete only those rows. No reloading is done. Operators on Table (and Store) The following operators can be performed on tables (and stores). Insert This allows events to be inserted into tables. This is similar to inserting events into streams. Warning If the table is defined with primary keys, and if you insert duplicate data, primary key constrain violations can occur. In such cases use the update or insert into operation. Syntax 1 2 3 from input stream select attribute name , attribute name , ... insert into table Similar to streams, you need to use the current events , expired events or the all events keyword between insert and into keywords in order to insert only the specific event types. For more information, see Event Type Example This query inserts all the events from the TempStream stream to the TempTable table. 1 2 3 from TempStream select * insert into TempTable ; Join (Table) This allows a stream to retrieve information from a table in a streaming manner. Note Joins can also be performed with two streams , aggregation or against externally named windows . Syntax 1 2 3 4 from input stream join table on condition select ( input stream | table ). attribute name , ( input stream | table ). attribute name , ... insert into output stream Note A table can only be joint with a stream. Two tables cannot be joint because there must be at least one active entity to trigger the join operation. Example This Siddhi App performs a join to retrieve the room type from RoomTypeTable table based on the room number, so that it can filter the events related to server-room s. 1 2 3 4 5 6 7 8 define table RoomTypeTable ( roomNo int , type string ); define stream TempStream ( deviceID long , roomNo int , temp double ); from TempStream join RoomTypeTable on RoomTypeTable . roomNo == TempStream . roomNo select deviceID , RoomTypeTable . type as roomType , type , temp having roomType == server-room insert into ServerRoomTempStream ; Supported join types Table join supports following join operations. Inner join (join) This is the default behavior of a join operation. join is used as the keyword to join the stream with the table. The output is generated only if there is a matching event in both the stream and the table. Left outer join The left outer join operation allows you to join a stream on left side with a table on the right side based on a condition. Here, it returns all the events of left stream even if there are no matching events in the right table by having null values for the attributes of the right table. Right outer join This is similar to a left outer join . right outer join is used as the keyword to join a stream on right side with a table on the left side based on a condition. It returns all the events of the right stream even if there are no matching events in the left table. Delete To delete selected events that are stored in a table. Syntax 1 2 3 4 from input stream select attribute name , attribute name , ... delete table ( for event type ) ? on condition The condition element specifies the basis on which events are selected to be deleted. When specifying the condition, table attributes should be referred to with the table name. To execute delete for specific event types, use the current events , expired events or the all events keyword with for as shown in the syntax. For more information, see Event Type Note Table attributes must be always referred to with the table name as follows: table name . attibute name Example In this example, the script deletes a record in the RoomTypeTable table if it has a value for the roomNo attribute that matches the value for the roomNumber attribute of an event in the DeleteStream stream. 1 2 3 4 5 6 7 define table RoomTypeTable ( roomNo int , type string ); define stream DeleteStream ( roomNumber int ); from DeleteStream delete RoomTypeTable on RoomTypeTable . roomNo == roomNumber ; Update This operator updates selected event attributes stored in a table based on a condition. Syntax 1 2 3 4 5 from input stream select attribute name , attribute name , ... update table ( for event type ) ? set table . attribute name = ( attribute name | expression ) ? , table . attribute name = ( attribute name | expression ) ? , ... on condition The condition element specifies the basis on which events are selected to be updated. When specifying the condition , table attributes must be referred to with the table name. You can use the set keyword to update selected attributes from the table. Here, for each assignment, the attribute specified in the left must be the table attribute, and the one specified in the right can be a stream/table attribute a mathematical operation, or other. When the set clause is not provided, all the attributes in the table are updated. To execute an update for specific event types use the current events , expired events or the all events keyword with for as shown in the syntax. For more information, see Event Type . Note Table attributes must be always referred to with the table name as shown below: table name . attibute name . Example This Siddhi application updates the room occupancy in the RoomOccupancyTable table for each room number based on new arrivals and exits from the UpdateStream stream. 1 2 3 4 5 6 7 8 define table RoomOccupancyTable ( roomNo int , people int ); define stream UpdateStream ( roomNumber int , arrival int , exit int ); from UpdateStream select * update RoomOccupancyTable set RoomOccupancyTable . people = RoomOccupancyTable . people + arrival - exit on RoomOccupancyTable . roomNo == roomNumber ; Update or Insert This allows you update if the event attributes already exist in the table based on a condition, or else insert the entry as a new attribute. Syntax 1 2 3 4 5 from input stream select attribute name , attribute name , ... update or insert into table ( for event type ) ? set table . attribute name = expression , table . attribute name = expression , ... on condition The condition element specifies the basis on which events are selected for update. When specifying the condition , table attributes should be referred to with the table name. If a record that matches the condition does not already exist in the table, the arriving event is inserted into the table. The set clause is only used when an update is performed during the insert/update operation. When set clause is used, the attribute to the left is always a table attribute, and the attribute to the right can be a stream/table attribute, mathematical operation or other. The attribute to the left (i.e., the attribute in the event table) is updated with the value of the attribute to the right if the given condition is met. When the set clause is not provided, all the attributes in the table are updated. Note When the attribute to the right is a table attribute, the operations supported differ based on the database type. To execute update upon specific event types use the current events , expired events or the all events keyword with for as shown in the syntax. To understand more see Event Type . Note Table attributes should be always referred to with the table name as table name . attibute name . Example The following query update for events in the UpdateTable event table that have room numbers that match the same in the UpdateStream stream. When such events are found in the event table, they are updated. When a room number available in the stream is not found in the event table, it is inserted from the stream. 1 2 3 4 5 6 7 8 define table RoomAssigneeTable ( roomNo int , type string , assignee string ); define stream RoomAssigneeStream ( roomNumber int , type string , assignee string ); from RoomAssigneeStream select roomNumber as roomNo , type , assignee update or insert into RoomAssigneeTable set RoomAssigneeTable . assignee = assignee on RoomAssigneeTable . roomNo == roomNo ; In This allows the stream to check whether the expected value exists in the table as a part of a conditional operation. Syntax 1 2 3 from input stream [ condition in table ] select attribute name , attribute name , ... insert into output stream The condition element specifies the basis on which events are selected to be compared. When constructing the condition , the table attribute must be always referred to with the table name as shown below: table . attibute name . Example This Siddhi application filters only room numbers that are listed in the ServerRoomTable table. 1 2 3 4 5 define table ServerRoomTable ( roomNo int ); define stream TempStream ( deviceID long , roomNo int , temp double ); from TempStream [ ServerRoomTable . roomNo == roomNo in ServerRoomTable ] insert into ServerRoomTempStream ; Named Aggregation Named aggregation allows you to obtain aggregates in an incremental manner for a specified set of time periods. This not only allows you to calculate aggregations with varied time granularity, but also allows you to access them in an interactive manner for reports, dashboards, and for further processing. Its schema is defined via the aggregation definition . Purpose Named aggregation allows you to retrieve the aggregate values for different time durations. That is, it allows you to obtain aggregates such as sum , count , avg , min , max , count and distinctCount of stream attributes for durations such as sec , min , hour , etc. This is of considerable importance in many Analytics scenarios because aggregate values are often needed for several time periods. Furthermore, this ensures that the aggregations are not lost due to unexpected system failures because aggregates can be stored in different persistence stores . Syntax 1 2 3 4 5 6 7 @ store ( type = store type , ...) @ purge ( enable = true or false , interval = purging interval , @ retentionPeriod ( granularity = retention period , ...) ) define aggregation aggregator name from input stream select attribute name , aggregate function ( attribute name ) as attribute name , ... group by attribute name aggregate by timestamp attribute every time periods ; The above syntax includes the following: Item Description @store This annotation is used to refer to the data store where the calculated aggregate results are stored. This annotation is optional. When no annotation is provided, the data is stored in the in-memory store. @purge This annotation is used to configure purging in aggregation granularities. If this annotation is not provided, the default purging mentioned above is applied. If you want to disable automatic data purging, you can use this annotation as follows: '@purge(enable=false) /You should disable data purging if the aggregation query in included in the Siddhi application for read-only purposes. @retentionPeriod This annotation is used to specify the length of time the data needs to be retained when carrying out data purging. If this annotation is not provided, the default retention period is applied. aggregator name This specifies a unique name for the aggregation so that it can be referred when accessing aggregate results. input stream The stream that feeds the aggregation. Note! this stream should be already defined. group by attribute name The group by clause is optional. If it is included in a Siddhi application, aggregate values are calculated per each group by attribute. If it is not used, all the events are aggregated together. by timestamp attribute This clause is optional. This defines the attribute that should be used as the timestamp. If this clause is not used, the event time is used by default. The timestamp could be given as either a string or a long value. If it is a long value, the unix timestamp in milliseconds is expected (e.g. 1496289950000 ). If it is a string value, the supported formats are yyyy - MM - dd HH : mm : ss (if time is in GMT) and yyyy - MM - dd HH : mm : ss Z (if time is not in GMT), here the ISO 8601 UTC offset must be provided for Z . (e.g., +05:30 , -11:00 ). time periods Time periods can be specified as a range where the minimum and the maximum value are separated by three dots, or as comma-separated values. e.g., A range can be specified as sec...year where aggregation is done per second, minute, hour, day, month and year. Comma-separated values can be specified as min, hour. Skipping time durations (e.g., min, day where the hour duration is skipped) when specifying comma-separated values is supported only from v4.1.1 onwards Aggregation's granularity data holders are automatically purged every 15 minutes. When carrying out data purging, the retention period you have specified for each granularity in the named aggregation query is taken into account. The retention period defined for a granularity needs to be greater than or equal to its minimum retention period as specified in the table below. If no valid retention period is defined for a granularity, the default retention period (as specified in the table below) is applied. Granularity Default retention Minimum retention second 120 seconds 120 seconds minute 24 hours 120 minutes hour 30 days 25 hours day 1 year 32 days month All 13 month year All none Note Aggregation is carried out at calendar start times for each granularity with the GMT timezone Note The same aggregation can be defined in multiple Siddhi apps for joining, however, only one siddhi app should carry out the processing (i.e. the aggregation input stream should only feed events to one aggregation definition). Example This Siddhi Application defines an aggregation named TradeAggregation to calculate the average and sum for the price attribute of events arriving at the TradeStream stream. These aggregates are calculated per every time granularity in the second-year range. 1 2 3 4 5 6 7 8 define stream TradeStream ( symbol string , price double , volume long , timestamp long ); @ purge ( enable = true , interval = 10 sec , @ retentionPeriod ( sec = 120 sec , min = 24 hours , hours = 30 days , days = 1 year , months = all , years = all )) define aggregation TradeAggregation from TradeStream select symbol , avg ( price ) as avgPrice , sum ( price ) as total group by symbol aggregate by timestamp every sec ... year ; Distributed Aggregation Distributed Aggregation allows you to partially process aggregations in different shards. This allows Siddhi app in one shard to be responsible only for processing a part of the aggregation. However for this, all aggregations must be based on a common physical database(@store). Syntax 1 2 3 4 5 6 7 @ store ( type = store type , ...) @ PartitionById define aggregation aggregator name from input stream select attribute name , aggregate function ( attribute name ) as attribute name , ... group by attribute name aggregate by timestamp attribute every time periods ; Following table includes the annotation to be used to enable distributed aggregation, Item Description @PartitionById If the annotation is given, then the distributed aggregation is enabled. Further this can be disabled by using enable element, @PartitionById(enable='false') . Further, following system properties are also available, System Property Description Possible Values Optional Default Value shardId The id of the shard one of the distributed aggregation is running in. This should be unique to a single shard Any string No partitionById This allows user to enable/disable distributed aggregation for all aggregations running in one siddhi manager .(Available from v4.3.3) true/false Yes false Note ShardIds should not be changed after the first configuration in order to keep data consistency. Join (Aggregation) This allows a stream to retrieve calculated aggregate values from the aggregation. Note A join can also be performed with two streams , with a table and a stream, or with a stream against externally named windows . Syntax A join with aggregation is similer to the join with table , but with additional within and per clauses. 1 2 3 4 5 6 from input stream join aggrigation on join condition within time range per time granularity select attribute name , attribute name , ... insert into output stream ; Apart from constructs of table join this includes the following. Please note that the 'on' condition is optional : Item Description within time range This allows you to specify the time interval for which the aggregate values need to be retrieved. This can be specified by providing the start and end time separated by a comma as string or long values, or by using the wildcard string specifying the data range. For details refer examples. per time granularity This specifies the time granularity by which the aggregate values must be grouped and returned. e.g., If you specify days , the retrieved aggregate values are grouped for each day within the selected time interval. within and per clauses also accept attribute values from the stream. The timestamp of the aggregations can be accessed through the AGG_TIMESTAMP attribute. Example Following aggregation definition will be used for the examples. 1 2 3 4 5 6 7 define stream TradeStream ( symbol string , price double , volume long , timestamp long ); define aggregation TradeAggregation from TradeStream select AGG_TIMESTAMP , symbol , avg ( price ) as avgPrice , sum ( price ) as total group by symbol aggregate by timestamp every sec ... year ; This query retrieves daily aggregations within the time range \"2014-02-15 00:00:00 +05:30\", \"2014-03-16 00:00:00 +05:30\" (Please note that +05:30 can be omitted if timezone is GMT) 1 2 3 4 5 6 7 8 define stream StockStream ( symbol string , value int ); from StockStream as S join TradeAggregation as T on S . symbol == T . symbol within 2014-02-15 00:00:00 +05:30 , 2014-03-16 00:00:00 +05:30 per days select S . symbol , T . total , T . avgPrice insert into AggregateStockStream ; This query retrieves hourly aggregations within the day 2014-02-15 . 1 2 3 4 5 6 7 8 define stream StockStream ( symbol string , value int ); from StockStream as S join TradeAggregation as T on S . symbol == T . symbol within 2014-02-15 **:**:** +05:30 per hours select S . symbol , T . total , T . avgPrice insert into AggregateStockStream ; This query retrieves all aggregations per perValue stream attribute within the time period between timestamps 1496200000000 and 1596434876000 . 1 2 3 4 5 6 7 8 define stream StockStream ( symbol string , value int , perValue string ); from StockStream as S join TradeAggregation as T on S . symbol == T . symbol within 1496200000000 L , 1596434876000 L per S . perValue select S . symbol , T . total , T . avgPrice insert into AggregateStockStream ; Supported join types Aggregation join supports following join operations. Inner join (join) This is the default behavior of a join operation. join is used as the keyword to join the stream with the aggregation. The output is generated only if there is a matching event in the stream and the aggregation. Left outer join The left outer join operation allows you to join a stream on left side with a aggregation on the right side based on a condition. Here, it returns all the events of left stream even if there are no matching events in the right aggregation by having null values for the attributes of the right aggregation. Right outer join This is similar to a left outer join . right outer join is used as the keyword to join a stream on right side with a aggregation on the left side based on a condition. It returns all the events of the right stream even if there are no matching events in the left aggregation. Named Window A named window is a window that can be shared across multiple queries. Events can be inserted to a named window from one or more queries and it can produce output events based on the named window type. Syntax The syntax for a named window is as follows: 1 define window window name ( attribute name attribute type , attribute name attribute type , ... ) window type ( parameter , parameter , \u2026 ) event type ; The following parameters are configured in a table definition: Parameter Description window name The name of the window defined. ( PascalCase is used for window names as a convention.) attribute name The schema of the window is defined by its attributes with uniquely identifiable attribute names ( camelCase is used for attribute names as a convention.) attribute type The type of each attribute defined in the schema. This can be STRING , INT , LONG , DOUBLE , FLOAT , BOOL or OBJECT . window type ( parameter , ...) The window type associated with the window and its parameters. output event type This is optional. Keywords such as current events , expired events and all events (the default) can be used to specify when the window output should be exposed. For more information, see Event Type . Examples Returning all output when events arrive and when events expire from the window. In this query, the event type is not specified. Therefore, it returns both current and expired events as the output. 1 define window SensorWindow ( name string , value float , roomNo int , deviceID string ) timeBatch ( 1 second ); Returning an output only when events expire from the window. In this query, the event type of the window is expired events . Therefore, it only returns the events that have expired from the window as the output. 1 define window SensorWindow ( name string , value float , roomNo int , deviceID string ) timeBatch ( 1 second ) output expired events ; Operators on Named Windows The following operators can be performed on named windows. Insert This allows events to be inserted into windows. This is similar to inserting events into streams. Syntax 1 2 3 from input stream select attribute name , attribute name , ... insert into window To insert only events of a specific event type, add the current events , expired events or the all events keyword between insert and into keywords (similar to how it is done for streams). For more information, see Event Type . Example This query inserts all events from the TempStream stream to the OneMinTempWindow window. 1 2 3 4 5 6 define stream TempStream ( tempId string , temp double ); define window OneMinTempWindow ( tempId string , temp double ) time ( 1 min ); from TempStream select * insert into OneMinTempWindow ; Join (Window) To allow a stream to retrieve information from a window based on a condition. Note A join can also be performed with two streams , aggregation or with tables tables . Syntax 1 2 3 4 from input stream join window on condition select ( input stream | window ). attribute name , ( input stream | window ). attribute name , ... insert into output stream Example This Siddhi Application performs a join count the number of temperature events having more then 40 degrees within the last 2 minutes. 1 2 3 4 5 6 7 define window TwoMinTempWindow ( roomNo int , temp double ) time ( 2 min ); define stream CheckStream ( requestId string ); from CheckStream as C join TwoMinTempWindow as T on T . temp 40 select requestId , count ( T . temp ) as count insert into HighTempCountStream ; Supported join types Window join supports following operations of a join clause. Inner join (join) This is the default behavior of a join operation. join is used as the keyword to join two windows or a stream with a window. The output is generated only if there is a matching event in both stream/window. Left outer join The left outer join operation allows you to join two windows or a stream with a window to be merged based on a condition. Here, it returns all the events of left stream/window even if there are no matching events in the right stream/window by having null values for the attributes of the right stream/window. Right outer join This is similar to a left outer join. Right outer join is used as the keyword to join two windows or a stream with a window. It returns all the events of the right stream/window even if there are no matching events in the left stream/window. Full outer join The full outer join combines the results of left outer join and right outer join . full outer join is used as the keyword to join two windows or a stream with a window. Here, output event are generated for each incoming event even if there are no matching events in the other stream/window. From A window can be an input to a query, similar to streams. Note !!! When window is used as an input to a query, another window cannot be applied on top of this. Syntax 1 2 3 from window select attribute name , attribute name , ... insert into output stream Example This Siddhi Application calculates the maximum temperature within the last 5 minutes. 1 2 3 4 5 6 define window FiveMinTempWindow ( roomNo int , temp double ) time ( 5 min ); from FiveMinTempWindow select max ( temp ) as maxValue , roomNo insert into MaxSensorReadingStream ; Trigger Triggers allow events to be periodically generated. Trigger definition can be used to define a trigger. A trigger also works like a stream with a predefined schema. Purpose For some use cases the system should be able to periodically generate events based on a specified time interval to perform some periodic executions. A trigger can be performed for a 'start' operation, for a given time interval , or for a given ' cron expression ' . Syntax The syntax for a trigger definition is as follows. 1 define trigger trigger name at ( start | every time interval | cron expression ); Similar to streams, triggers can be used as inputs. They adhere to the following stream definition and produce the triggered_time attribute of the long type. 1 define stream trigger name ( triggered_time long ); The following types of triggeres are currently supported: Trigger type Description 'start' An event is triggered when Siddhi is started. every time interval An event is triggered periodically at the given time interval. ' cron expression ' An event is triggered periodically based on the given cron expression. For configuration details, see quartz-scheduler . Examples Triggering events regularly at specific time intervals The following query triggers events every 5 minutes. 1 define trigger FiveMinTriggerStream at every 5 min ; Triggering events at a specific time on specified days The following query triggers an event at 10.15 AM on every weekdays. 1 define trigger FiveMinTriggerStream at 0 15 10 ? * MON-FRI ; Script Scripts allow you to write functions in other programming languages and execute them within Siddhi queries. Functions defined via scripts can be accessed in queries similar to any other inbuilt function. Function definitions can be used to define these scripts. Function parameters are passed into the function logic as Object[] and with the name data . Purpose Scripts allow you to define a function operation that is not provided in Siddhi core or its extension. It is not required to write an extension to define the function logic. Syntax The syntax for a Script definition is as follows. 1 2 3 define function function name [ language name ] return return type { operation of the function } ; The following parameters are configured when defining a script. Parameter Description function name The name of the function ( camelCase is used for the function name) as a convention. language name The name of the programming language used to define the script, such as javascript , r and scala . return type The attribute type of the function\u2019s return. This can be int , long , float , double , string , bool or object . Here the function implementer should be responsible for returning the output attribute on the defined return type for proper functionality. operation of the function Here, the execution logic of the function is added. This logic should be written in the language specified under the language name , and it should return the output in the data type specified via the return type parameter. Examples This query performs concatenation using JavaScript, and returns the output as a string. 1 2 3 4 5 6 7 8 9 10 11 12 13 define function concatFn [ javascript ] return string { var str1 = data [ 0 ]; var str2 = data [ 1 ]; var str3 = data [ 2 ]; var responce = str1 + str2 + str3 ; return responce ; } ; define stream TempStream ( deviceID long , roomNo int , temp double ); from TempStream select concatFn ( roomNo , - , deviceID ) as id , temp insert into DeviceTempStream ; Store Query Siddhi store queries are a set of on-demand queries that can be used to perform operations on Siddhi tables, windows, and aggregators. Purpose Store queries allow you to execute the following operations on Siddhi tables, windows, and aggregators without the intervention of streams. Queries supported for tables: SELECT INSERT DELETE UPDATE UPDATE OR INSERT Queries supported for windows and aggregators: SELECT This is be done by submitting the store query to the Siddhi application runtime using its query() method. In order to execute store queries, the Siddhi application of the Siddhi application runtime you are using, should have a store defined, which contains the table that needs to be queried. Example If you need to query the table named RoomTypeTable the it should have been defined in the Siddhi application. In order to execute a store query on RoomTypeTable , you need to submit the store query using query() method of SiddhiAppRuntime instance as below. 1 siddhiAppRuntime . query ( store query ); (Table/Window) Select The SELECT store query retrieves records from the specified table or window, based on the given condition. Syntax 1 2 3 4 5 6 7 from table / window on condition ? select attribute name , attribute name , ... group by ? having ? order by ? limit ? Example This query retrieves room numbers and types of the rooms starting from room no 10. 1 2 3 from roomTypeTable on roomNo = 10 ; select roomNo , type (Aggregation) Select The SELECT store query retrieves records from the specified aggregation, based on the given condition, time range, and granularity. Syntax 1 2 3 4 5 6 7 8 9 from aggregation on condition ? within time range per time granularity select attribute name , attribute name , ... group by ? having ? order by ? limit ? Example Following aggregation definition will be used for the examples. 1 2 3 4 5 6 7 define stream TradeStream ( symbol string , price double , volume long , timestamp long ); define aggregation TradeAggregation from TradeStream select symbol , avg ( price ) as avgPrice , sum ( price ) as total group by symbol aggregate by timestamp every sec ... year ; This query retrieves daily aggregations within the time range \"2014-02-15 00:00:00 +05:30\", \"2014-03-16 00:00:00 +05:30\" (Please note that +05:30 can be omitted if timezone is GMT) 1 2 3 4 from TradeAggregation within 2014-02-15 00:00:00 +05:30 , 2014-03-16 00:00:00 +05:30 per days select symbol , total , avgPrice ; This query retrieves hourly aggregations of \"FB\" symbol within the day 2014-02-15 . 1 2 3 4 5 from TradeAggregation on symbol == FB within 2014-02-15 **:**:** +05:30 per hours select symbol , total , avgPrice ; Insert This allows you to insert a new record to the table with the attribute values you define in the select section. Syntax 1 2 select attribute name , attribute name , ... insert into table ; Example This store query inserts a new record to the table RoomOccupancyTable , with the specified attribute values. 1 2 select 10 as roomNo , 2 as people insert into RoomOccupancyTable Delete The DELETE store query deletes selected records from a specified table. Syntax 1 2 3 select ? delete table on conditional expresssion The condition element specifies the basis on which records are selected to be deleted. Note Table attributes must always be referred to with the table name as shown below: table name . attibute name . Example In this example, query deletes a record in the table named RoomTypeTable if it has value for the roomNo attribute that matches the value for the roomNumber attribute of the selection which has 10 as the actual value. 1 2 3 select 10 as roomNumber delete RoomTypeTable on RoomTypeTable . roomNo == roomNumber ; 1 2 delete RoomTypeTable on RoomTypeTable . roomNo == 10 ; Update The UPDATE store query updates selected attributes stored in a specific table, based on a given condition. Syntax 1 2 3 4 select attribute name , attribute name , ... ? update table set table . attribute name = ( attribute name | expression ) ? , table . attribute name = ( attribute name | expression ) ? , ... on condition The condition element specifies the basis on which records are selected to be updated. When specifying the condition , table attributes must be referred to with the table name. You can use the set keyword to update selected attributes from the table. Here, for each assignment, the attribute specified in the left must be the table attribute, and the one specified in the right can be a stream/table attribute a mathematical operation, or other. When the set clause is not provided, all the attributes in the table are updated. Note Table attributes must always be referred to with the table name as shown below: table name . attibute name . Example The following query updates the room occupancy by increasing the value of people by 1, in the RoomOccupancyTable table for each room number greater than 10. 1 2 3 4 select 10 as roomNumber , 1 as arrival update RoomTypeTable set RoomTypeTable . people = RoomTypeTable . people + arrival on RoomTypeTable . roomNo == roomNumber ; 1 2 3 update RoomTypeTable set RoomTypeTable . people = RoomTypeTable . people + 1 on RoomTypeTable . roomNo == 10 ; Update or Insert This allows you to update selected attributes if a record that meets the given conditions already exists in the specified table. If a matching record does not exist, the entry is inserted as a new record. Syntax 1 2 3 4 select attribute name , attribute name , ... update or insert into table set table . attribute name = expression , table . attribute name = expression , ... on condition The condition element specifies the basis on which records are selected for update. When specifying the condition , table attributes should be referred to with the table name. If a record that matches the condition does not already exist in the table, the arriving event is inserted into the table. The set clause is only used when an update is performed during the insert/update operation. When set clause is used, the attribute to the left is always a table attribute, and the attribute to the right can be a stream/table attribute, mathematical operation or other. The attribute to the left (i.e., the attribute in the event table) is updated with the value of the attribute to the right if the given condition is met. When the set clause is not provided, all the attributes in the table are updated. Note Table attributes must always be referred to with the table name as shown below: table name . attibute name . Example The following query tries to update the records in the RoomAssigneeTable table that have room numbers that match the same in the selection. If such records are not found, it inserts a new record based on the values provided in the selection. 1 2 3 4 select 10 as roomNo , single as type , abc as assignee update or insert into RoomAssigneeTable set RoomAssigneeTable . assignee = assignee on RoomAssigneeTable . roomNo == roomNo ; Extensions Siddhi supports an extension architecture to enhance its functionality by incorporating other libraries in a seamless manner. Purpose Extensions are supported because, Siddhi core cannot have all the functionality that's needed for all the use cases, mostly use cases require different type of functionality, and for some cases there can be gaps and you need to write the functionality by yourself. All extensions have a namespace. This is used to identify the relevant extensions together, and to let you specifically call the extension. Syntax Extensions follow the following syntax; 1 namespace : function name ( parameter , parameter , ... ) The following parameters are configured when referring a script function. Parameter Description namespace Allows Siddhi to identify the extension without conflict function name The name of the function referred. parameter The function input parameter for function execution. Extension Types Siddhi supports following extension types: Function For each event, it consumes zero or more parameters as input parameters and returns a single attribute. This can be used to manipulate existing event attributes to generate new attributes like any Function operation. This is implemented by extending io.siddhi.core.executor.function.FunctionExecutor . Example : math:sin(x) Here, the sin function of math extension returns the sin value for the x parameter. Aggregate Function For each event, it consumes zero or more parameters as input parameters and returns a single attribute with aggregated results. This can be used in conjunction with a window in order to find the aggregated results based on the given window like any Aggregate Function operation. This is implemented by extending io.siddhi.core.query.selector.attribute.aggregator.AttributeAggregatorExecutor . Example : custom:std(x) Here, the std aggregate function of custom extension returns the standard deviation of the x value based on its assigned window query. Window This allows events to be collected, generated, dropped and expired anytime without altering the event format based on the given input parameters, similar to any other Window operator. This is implemented by extending io.siddhi.core.query.processor.stream.window.WindowProcessor . Example : custom:unique(key) Here, the unique window of the custom extension retains one event for each unique key parameter. Stream Function This allows events to be generated or dropped only during event arrival and altered by adding one or more attributes to it. This is implemented by extending io.siddhi.core.query.processor.stream.function.StreamFunctionProcessor . Example : custom:pol2cart(theta,rho) Here, the pol2cart function of the custom extension returns all the events by calculating the cartesian coordinates x y and adding them as new attributes to the events. Stream Processor This allows events to be collected, generated, dropped and expired anytime by altering the event format by adding one or more attributes to it based on the given input parameters. Implemented by extending io.siddhi.core.query.processor.stream.StreamProcessor . Example : custom:perMinResults( parameter , parameter , ...) Here, the perMinResults function of the custom extension returns all events by adding one or more attributes to the events based on the conversion logic. Altered events are output every minute regardless of event arrivals. Sink Sinks provide a way to publish Siddhi events to external systems in the preferred data format. Sinks publish events from the streams via multiple transports to external endpoints in various data formats. Implemented by extending io.siddhi.core.stream.output.sink.Sink . Example : @sink(type='sink_type', static_option_key1='static_option_value1') To configure a stream to publish events via a sink, add the sink configuration to a stream definition by adding the @sink annotation with the required parameter values. The sink syntax is as above Source Source allows Siddhi to consume events from external systems , and map the events to adhere to the associated stream. Sources receive events via multiple transports and in various data formats, and direct them into streams for processing. Implemented by extending io.siddhi.core.stream.input.source.Source . Example : @source(type='source_type', static.option.key1='static_option_value1') To configure a stream that consumes events via a source, add the source configuration to a stream definition by adding the @source annotation with the required parameter values. The source syntax is as above Store You can use Store extension type to work with data/events stored in various data stores through the table abstraction . You can find more information about these extension types under the heading 'Extension types' in this document. Implemented by extending io.siddhi.core.table.record.AbstractRecordTable . Script Scripts allow you to define a function operation that is not provided in Siddhi core or its extension. It is not required to write an extension to define the function logic. Scripts allow you to write functions in other programming languages and execute them within Siddhi queries. Functions defined via scripts can be accessed in queries similar to any other inbuilt function. Implemented by extending io.siddhi.core.function.Script . Source Mapper Each @source configuration has a mapping denoted by the @map annotation that converts the incoming messages format to Siddhi events .The type parameter of the @map defines the map type to be used to map the data. The other parameters to be configured depends on the mapper selected. Some of these parameters are optional. Implemented by extending io.siddhi.core.stream.output.sink.SourceMapper . Example : @map(type='map_type', static_option_key1='static_option_value1') Sink Mapper Each @sink configuration has a mapping denoted by the @map annotation that converts the outgoing Siddhi events to configured messages format .The type parameter of the @map defines the map type to be used to map the data. The other parameters to be configured depends on the mapper selected. Some of these parameters are optional. Implemented by extending io.siddhi.core.stream.output.sink.SinkMapper . Example : @map(type='map_type', static_option_key1='static_option_value1') Example A window extension created with namespace foo and function name unique can be referred as follows: 1 2 3 from StockExchangeStream [ price = 20 ] # window . foo : unique ( symbol ) select symbol , price insert into StockQuote Available Extensions Siddhi currently has several pre written extensions that are available here We value your contribution on improving Siddhi and its extensions further. Writing Custom Extensions Custom extensions can be written in order to cater use case specific logic that are not available in Siddhi out of the box or as an existing extension. There are five types of Siddhi extensions that you can write to cater your specific use cases. These extension types and the related maven archetypes are given below. You can use these archetypes to generate Maven projects for each extension type. Follow the procedure for the required archetype, based on your project: Note When using the generated archetype please make sure you complete the @Extension annotation with proper values. This annotation will be used to identify and document the extension, hence your extension will not work without @Extension annotation. siddhi-execution Siddhi-execution provides following extension types: Function Aggregate Function Stream Function Stream Processor Window You can use one or more from above mentioned extension types and implement according to your requirement. For more information about these extension types, see Extension Types . To install and implement the siddhi-io extension archetype, follow the procedure below: Issue the following command from your CLI. 1 2 3 4 5 mvn archetype:generate -DarchetypeGroupId=io.siddhi.extension.archetype -DarchetypeArtifactId=siddhi-archetype-execution -DgroupId=io.siddhi.extension.execution -Dversion=1.0.0-SNAPSHOT Enter the mandatory properties prompted, please see the description for all properties below. Properties Description Mandatory Default Value _nameOfFunction Name of the custom function to be created Y - _nameSpaceOfFunction Namespace of the function, used to grouped similar custom functions Y - groupIdPostfix Namespace of the function is added as postfix to the groupId as a convention N artifactId Artifact Id of the project N siddhi-execution- classNameOfAggregateFunction Class name of the Aggregate Function N $ classNameOfFunction Class name of the Function N $ classNameOfStreamFunction Class name of the Stream Function N $ classNameOfStreamProcessor Class name of the Stream Processor N $ classNameOfWindow Class name of the Window N $ To confirm that all property values are correct, type Y in the console. If not, press N . siddhi-io Siddhi-io provides following extension types: Sink Source You can use one or more from above mentioned extension types and implement according to your requirement. siddhi-io is generally used to work with IO operations as follows: * The Source extension type gets inputs to your Siddhi application. * The Sink extension publishes outputs from your Siddhi application. For more information about these extension types, see Extension Types . To implement the siddhi-io extension archetype, follow the procedure below: Issue the following command from your CLI. 1 2 3 4 5 mvn archetype:generate -DarchetypeGroupId=io.siddhi.extension.archetype -DarchetypeArtifactId=siddhi-archetype-io -DgroupId=io.siddhi.extension.io -Dversion=1.0.0-SNAPSHOT Enter the mandatory properties prompted, please see the description for all properties below. Properties Description Mandatory Default Value _IOType Type of IO for which Siddhi-io extension is written Y - groupIdPostfix Type of the IO is added as postfix to the groupId as a convention N artifactId Artifact Id of the project N siddhi-io- classNameOfSink Class name of the Sink N classNameOfSource Class name of the Source N To confirm that all property values are correct, type Y in the console. If not, press N . siddhi-map Siddhi-map provides following extension types, Sink Mapper Source Mapper You can use one or more from above mentioned extension types and implement according to your requirement as follows. The Source Mapper maps events to a predefined data format (such as XML, JSON, binary, etc), and publishes them to external endpoints (such as E-mail, TCP, Kafka, HTTP, etc). The Sink Mapper also maps events to a predefined data format, but it does it at the time of publishing events from a Siddhi application. For more information about these extension types, see Extension Types . To implement the siddhi-map extension archetype, follow the procedure below: Issue the following command from your CLI. 1 2 3 4 5 mvn archetype:generate -DarchetypeGroupId=io.siddhi.extension.archetype -DarchetypeArtifactId=siddhi-archetype-map -DgroupId=io.siddhi.extension.map -Dversion=1.0.0-SNAPSHOT Enter the mandatory properties prompted, please see the description for all properties below. Properties Description Mandatory Default Value _mapType Type of Mapper for which Siddhi-map extension is written Y - groupIdPostfix Type of the Map is added as postfix to the groupId as a convention N artifactId Artifact Id of the project N siddhi-map- classNameOfSinkMapper Class name of the Sink Mapper N classNameOfSourceMapper Class name of the Source Mapper N To confirm that all property values are correct, type Y in the console. If not, press N . siddhi-script Siddhi-script provides the Script extension type. The script extension type allows you to write functions in other programming languages and execute them within Siddhi queries. Functions defined via scripts can be accessed in queries similar to any other inbuilt function. For more information about these extension types, see Extension Types . To implement the siddhi-script extension archetype, follow the procedure below: Issue the following command from your CLI. 1 2 3 4 5 mvn archetype:generate -DarchetypeGroupId=io.siddhi.extension.archetype -DarchetypeArtifactId=siddhi-archetype-script -DgroupId=io.siddhi.extension.script -Dversion=1.0.0-SNAPSHOT Enter the mandatory properties prompted, please see the description for all properties below. Properties Description Mandatory Default Value _nameOfScript Name of Custom Script for which Siddhi-script extension is written Y - groupIdPostfix Name of the Script is added as postfix to the groupId as a convention N artifactId Artifact Id of the project N siddhi-script- classNameOfScript Class name of the Script N Eval To confirm that all property values are correct, type Y in the console. If not, press N . siddhi-store Siddhi-store provides the Store extension type. The Store extension type allows you to work with data/events stored in various data stores through the table abstraction. For more information about these extension types, see Extension Types . To implement the siddhi-store extension archetype, follow the procedure below: Issue the following command from your CLI. 1 2 3 4 5 mvn archetype:generate -DarchetypeGroupId=io.siddhi.extension.archetype -DarchetypeArtifactId=siddhi-archetype-store -DgroupId=io.siddhi.extension.store -Dversion=1.0.0-SNAPSHOT Enter the mandatory properties prompted, please see the description for all properties below. Properties Description Mandatory Default Value _storeType Type of Store for which Siddhi-store extension is written Y - groupIdPostfix Type of the Store is added as postfix to the groupId as a convention N artifactId Artifact Id of the project N siddhi-store- className Class name of the Store N To confirm that all property values are correct, type Y in the console. If not, press N . Configuring and Monitoring Siddhi Applications Multi-threading and Asynchronous Processing When @Async annotation is added to the Streams it enable the Streams to introduce asynchronous and multi-threading behavior. 1 2 @ Async ( buffer . size = 256 , workers = 2 , batch . size . max = 5 ) define stream stream name ( attribute name attribute type , attribute name attribute type , ... ); The following elements are configured with this annotation. Annotation Description Default Value buffer.size The size of the event buffer that will be used to handover the execution to other threads. - workers Number of worker threads that will be be used to process the buffered events. 1 batch.size.max The maximum number of events that will be processed together by a worker thread at a given time. buffer.size Statistics Use @app:statistics app level annotation to evaluate the performance of an application, you can enable the statistics of a Siddhi application to be published. This is done via the @app:statistics annotation that can be added to a Siddhi application as shown in the following example. 1 @ app : statistics ( reporter = console ) The following elements are configured with this annotation. Annotation Description Default Value reporter The interface in which statistics for the Siddhi application are published. Possible values are as follows: console jmx console interval The time interval (in seconds) at which the statistics for the Siddhi application are reported. 60 include If this parameter is added, only the types of metrics you specify are included in the reporting. The required metric types can be specified as a comma-separated list. It is also possible to use wild cards All ( . ) The metrics are reported in the following format. io.siddhi.SiddhiApps. SiddhiAppName .Siddhi. Component Type . Component Name . Metrics name The following table lists the types of metrics supported for different Siddhi application component types. Component Type Metrics Type Stream Throughput The size of the buffer if parallel processing is enabled via the @async annotation. Trigger Throughput (Trigger and Stream) Source Throughput Sink Throughput Mapper Latency Input/output throughput Table Memory Throughput (For all operations) Throughput (For all operations) Query Memory Latency Window Throughput (For all operations) Latency (For all operation) Partition Throughput (For all operations) Latency (For all operation) e.g., the following is a Siddhi application that includes the @app annotation to report performance statistics. 1 2 3 4 5 6 7 8 @ App : name ( TestMetrics ) @ App : Statistics ( reporter = console ) define stream TestStream ( message string ); @ info ( name = logQuery ) from TestSream # log ( Message: ) insert into TempSream ; Statistics are reported for this Siddhi application as shown in the extract below. Click to view the extract 11/26/17 8:01:20 PM ============================================================ -- Gauges ---------------------------------------------------------------------- io.siddhi.SiddhiApps.TestMetrics.Siddhi.Queries.logQuery.memory value = 5760 io.siddhi.SiddhiApps.TestMetrics.Siddhi.Streams.TestStream.size value = 0 -- Meters ---------------------------------------------------------------------- io.siddhi.SiddhiApps.TestMetrics.Siddhi.Sources.TestStream.http.throughput count = 0 mean rate = 0.00 events/second 1-minute rate = 0.00 events/second 5-minute rate = 0.00 events/second 15-minute rate = 0.00 events/second io.siddhi.SiddhiApps.TestMetrics.Siddhi.Streams.TempSream.throughput count = 2 mean rate = 0.04 events/second 1-minute rate = 0.03 events/second 5-minute rate = 0.01 events/second 15-minute rate = 0.00 events/second io.siddhi.SiddhiApps.TestMetrics.Siddhi.Streams.TestStream.throughput count = 2 mean rate = 0.04 events/second 1-minute rate = 0.03 events/second 5-minute rate = 0.01 events/second 15-minute rate = 0.00 events/second -- Timers ---------------------------------------------------------------------- io.siddhi.SiddhiApps.TestMetrics.Siddhi.Queries.logQuery.latency count = 2 mean rate = 0.11 calls/second 1-minute rate = 0.34 calls/second 5-minute rate = 0.39 calls/second 15-minute rate = 0.40 calls/second min = 0.61 milliseconds max = 1.08 milliseconds mean = 0.84 milliseconds stddev = 0.23 milliseconds median = 0.61 milliseconds 75% < = 1.08 milliseconds 95% < = 1.08 milliseconds 98% < = 1.08 milliseconds 99% < = 1.08 milliseconds 99.9% < = 1.08 milliseconds Event Playback When @app:playback annotation is added to the app, the timestamp of the event (specified via an attribute) is treated as the current time. This results in events being processed faster. The following elements are configured with this annotation. Annotation Description idle.time If no events are received during a time interval specified (in milliseconds) via this element, the Siddhi system time is incremented by a number of seconds specified via the increment element. increment The number of seconds by which the Siddhi system time must be incremented if no events are received during the time interval specified via the idle.time element. e.g., In the following example, the Siddhi system time is incremented by two seconds if no events arrive for a time interval of 100 milliseconds. @app:playback(idle.time = '100 millisecond', increment = '2 sec')","title":"Query Guide"},{"location":"docs/query-guide/#siddhi-51-streaming-sql-guide","text":"","title":"Siddhi 5.1 Streaming SQL Guide"},{"location":"docs/query-guide/#introduction","text":"Siddhi Streaming SQL is designed to process streams of events. It can be used to implement streaming data integration, streaming analytics, rule based and adaptive decision making use cases. It is an evolution of Complex Event Processing (CEP) and Stream Processing systems, hence it can also be used to process stateful computations, detecting of complex event patterns, and sending notifications in real-time. Siddhi Streaming SQL uses SQL like syntax, and annotations to consume events from diverse event sources with various data formats, process then using stateful and stateless operators and send outputs to multiple endpoints according to their accepted event formats. It also supports exposing rule based and adaptive decision making as service endpoints such that external programs and systems can synchronously get decision support form Siddhi. The following sections explains how to write processing logic using Siddhi Streaming SQL.","title":"Introduction"},{"location":"docs/query-guide/#siddhi-application","text":"The processing logic for your program can be written using the Streaming SQL and put together as a single file with .siddhi extension. This file is called as the Siddhi Application or the SiddhiApp . SiddhiApps are named by adding @app:name(' name ') annotation on the top of the SiddhiApp file. When the annotation is not added Siddhi assigns a random UUID as the name of the SiddhiApp. Purpose SiddhiApp provides an isolated execution environment for your processing logic that allows you to deploy and execute processing logic independent of other SiddhiApp in the system. Therefore it's always recommended to have a processing logic related to single use case in a single SiddhiApp. This will help you to group processing logic and easily manage addition and removal of various use cases. The following diagram depicts some of the key Siddhi Streaming SQL elements of Siddhi Application and how event flows through the elements. Below table provides brief description of a few key elements in the Siddhi Streaming SQL Language. Elements Description Stream A logical series of events ordered in time with a uniquely identifiable name, and a defined set of typed attributes defining its schema. Event An event is a single event object associated with a stream. All events of a stream contains a timestamp and an identical set of typed attributes based on the schema of the stream they belong to. Table A structured representation of data stored with a defined schema. Stored data can be backed by In-Memory , or external data stores such as RDBMS , MongoDB , etc. The tables can be accessed and manipulated at runtime. Named Window A structured representation of data stored with a defined schema and eviction policy. Window data is stored In-Memory and automatically cleared by the named window constrain. Other siddhi elements can only query the values in windows at runtime but they cannot modify them. Named Aggregation A structured representation of data that's incrementally aggregated and stored with a defined schema and aggregation granularity such as seconds, minutes, hours, etc. Aggregation data is stored both In-Memory and in external data stores such as RDBMS . Other siddhi elements can only query the values in windows at runtime but they cannot modify them. Query A logical construct that processes events in streaming manner by by consuming data from one or more streams, tables, windows and aggregations, and publishes output events into a stream, table or a window. Source A construct that consumes data from external sources (such as TCP , Kafka , HTTP , etc) with various event formats such as XML , JSON , binary , etc, convert then to Siddhi events, and passes into streams for processing. Sink A construct that consumes events arriving at a stream, maps them to a predefined data format (such as XML , JSON , binary , etc), and publishes them to external endpoints (such as E-mail , TCP , Kafka , HTTP , etc). Input Handler A mechanism to programmatically inject events into streams. Stream/Query Callback A mechanism to programmatically consume output events from streams or queries. Partition A logical container that isolates the processing of queries based on the partition keys derived from the events. Inner Stream A positionable stream that connects portioned queries with each other within the partition. Grammar SiddhiApp is a collection of Siddhi Streaming SQL elements composed together as a script. Here each Siddhi element must be separated by a semicolon ; . Hight level syntax of SiddhiApp is as follows. 1 2 3 4 5 siddhi app : app annotation * ( stream definition | table definition | ... ) + ( query | partition ) + ; Example Siddhi Application with name Temperature-Analytics defined with a stream named TempStream and a query named 5minAvgQuery . 1 2 3 4 5 6 7 8 9 @ app : name ( Temperature-Analytics ) define stream TempStream ( deviceID long , roomNo int , temp double ); @ info ( name = 5minAvgQuery ) from TempStream # window . time ( 5 min ) select roomNo , avg ( temp ) as avgTemp group by roomNo insert into OutputStream ;","title":"Siddhi Application"},{"location":"docs/query-guide/#stream","text":"A stream is a logical series of events ordered in time. Its schema is defined via the stream definition . A stream definition contains the stream name and a set of attributes with specific types and uniquely identifiable names within the stream. All events associated to the stream will have the same schema (i.e., have the same attributes in the same order). Purpose Stream groups common types of events together with a schema. This helps in various ways such as, processing all events together in queries and performing data format transformations together when they are consumed and published via sources and sinks. Syntax The syntax for defining a new stream is as follows. 1 2 define stream stream name ( attribute name attribute type , attribute name attribute type , ... ); The following parameters are used to configure a stream definition. Parameter Description stream name The name of the stream created. (It is recommended to define a stream name in PascalCase .) attribute name Uniquely identifiable name of the stream attribute. (It is recommended to define attribute names in camelCase .) attribute type The type of each attribute defined in the schema. This can be STRING , INT , LONG , DOUBLE , FLOAT , BOOL or OBJECT . To use and refer stream and attribute names that do not follow [a-zA-Z_][a-zA-Z_0-9]* format enclose them in ` . E.g. `$test(0)` . To make the stream process events in multi-threading and asynchronous way use the @Async annotation as shown in Multi-threading and Asynchronous Processing configuration section. Example 1 define stream TempStream ( deviceID long , roomNo int , temp double ); The above creates a stream with name TempStream having the following attributes. deviceID of type long roomNo of type int temp of type double","title":"Stream"},{"location":"docs/query-guide/#source","text":"Sources receive events via multiple transports and in various data formats, and direct them into streams for processing. A source configuration allows to define a mapping in order to convert each incoming event from its native data format to a Siddhi event. When customizations to such mappings are not provided, Siddhi assumes that the arriving event adheres to the predefined format based on the stream definition and the configured message mapping type. Purpose Source provides a way to consume events from external systems and convert them to be processed by the associated stream. Syntax To configure a stream that consumes events via a source, add the source configuration to a stream definition by adding the @source annotation with the required parameter values. The source syntax is as follows: 1 2 3 4 5 6 @ source ( type = source type , static . key = value , static . key = value , @ map ( type = map type , static . key = value , static . key = value , @ attributes ( attribute1 = attribute mapping , attributeN = attribute mapping ) ) ) define stream stream name ( attribute1 type , attributeN type ); This syntax includes the following annotations. Source The type parameter of @source annotation defines the source type that receives events. The other parameters of @source annotation depends upon the selected source type, and here some of its parameters can be optional. For detailed information about the supported parameters see the documentation of the relevant source. The following is the list of source types supported by Siddhi: Source type Description In-memory Allow SiddhiApp to consume events from other SiddhiApps running on the same JVM. HTTP Expose an HTTP service to consume messages. Kafka Subscribe to Kafka topic to consume events. TCP Expose a TCP service to consume messages. Email Consume emails via POP3 and IMAP protocols. JMS Subscribe to JMS topic or queue to consume events. File Reads files by tailing or as a whole to extract events out of them. CDC Perform change data capture on databases. Prometheus Consume data from Prometheus agent. In-memory is the only source inbuilt in Siddhi, and all other source types are implemented as extensions.","title":"Source"},{"location":"docs/query-guide/#source-mapper","text":"Each @source configuration can have a mapping denoted by the @map annotation that defines how to convert the incoming event format to Siddhi events. The type parameter of the @map defines the map type to be used in converting the incoming events. The other parameters of @map annotation depends on the mapper selected, and some of its parameters can be optional. For detailed information about the parameters see the documentation of the relevant mapper. Map Attributes @attributes is an optional annotation used with @map to define custom mapping. When @attributes is not provided, each mapper assumes that the incoming events adheres to its own default message format and attempt to convert the events from that format. By adding the @attributes annotation, users can selectively extract data from the incoming message and assign them to the attributes. There are two ways to configure @attributes . Define attribute names as keys, and mapping configurations as values: @attributes( attribute1 =' mapping ', attributeN =' mapping ') Define the mapping configurations in the same order as the attributes defined in stream definition: @attributes( ' mapping for attribute1 ', ' mapping for attributeN ') Supported Source Mapping Types The following is the list of source mapping types supported by Siddhi: Source mapping type Description PassThrough Omits data conversion on Siddhi events. JSON Converts JSON messages to Siddhi events. XML Converts XML messages to Siddhi events. TEXT Converts plain text messages to Siddhi events. Avro Converts Avro events to Siddhi events. Binary Converts Siddhi specific binary events to Siddhi events. Key Value Converts key-value HashMaps to Siddhi events. CSV Converts CSV like delimiter separated events to Siddhi events. Tip When the @map annotation is not provided @map(type='passThrough') is used as default, that passes the consumed Siddhi events directly to the streams without any data conversion. PassThrough is the only source mapper inbuilt in Siddhi, and all other source mappers are implemented as extensions. Example 1 Receive JSON messages by exposing an HTTP service, and direct them to InputStream stream for processing. Here the HTTP service will be secured with basic authentication, receives events on all network interfaces on port 8080 and context /foo . The service expects the JSON messages to be on the default data format that's supported by the JSON mapper as follows. 1 2 3 4 5 { name : Paul , age : 20 , country : UK } The configuration of the HTTP source and JSON source mapper to achieve the above is as follows. 1 2 3 @ source ( type = http , receiver . url = http://0.0.0.0:8080/foo , @ map ( type = json )) define stream InputStream ( name string , age int , country string ); Example 2 Receive JSON messages by exposing an HTTP service, and direct them to StockStream stream for processing. Here the incoming JSON , as given below, do not adhere to the default data format that's supported by the JSON mapper. 1 2 3 4 5 6 7 8 9 10 11 { portfolio :{ stock :{ volume : 100 , company :{ symbol : FB }, price : 55.6 } } } The configuration of the HTTP source and the custom JSON source mapping to achieve the above is as follows. 1 2 3 4 5 @ source ( type = http , receiver . url = http://0.0.0.0:8080/foo , @ map ( type = json , enclosing . element = $.portfolio , @ attributes ( symbol = stock.company.symbol , price = stock.price , volume = stock.volume ))) define stream StockStream ( symbol string , price float , volume long ); The same can also be configured by omitting the attribute names as below. 1 2 3 4 @ source ( type = http , receiver . url = http://0.0.0.0:8080/foo , @ map ( type = json , enclosing . element = $.portfolio , @ attributes ( stock.company.symbol , stock.price , stock.volume ))) define stream StockStream ( symbol string , price float , volume long );","title":"Source Mapper"},{"location":"docs/query-guide/#sink","text":"Sinks consumes events from streams and publish them via multiple transports to external endpoints in various data formats. A sink configuration allows users to define a mapping to convert the Siddhi events in to the required output data format (such as JSON , TEXT , XML , etc.) and publish the events to the configured endpoints. When customizations to such mappings are not provided, Siddhi converts events to the predefined event format based on the stream definition and the configured message mapper type before publishing the events. Purpose Sink provides a way to publish Siddhi events of a stream to external systems by converting events to their supported format. Syntax To configure a stream to publish events via a sink, add the sink configuration to a stream definition by adding the @sink annotation with the required parameter values. The sink syntax is as follows: 1 2 3 4 5 6 @ sink ( type = sink type , static . key = value , dynamic . key = {{ value }} , @ map ( type = map type , static . key = value , dynamic . key = {{ value }} , @ payload ( payload mapping ) ) ) define stream stream name ( attribute1 type , attributeN type ); Dynamic Properties The sink and sink mapper properties that are categorized as dynamic have the ability to absorb attribute values dynamically from the Siddhi events of their associated streams. This can be configured by enclosing the relevant attribute names in double curly braces as {{...}} , and using it within the property values. Some valid dynamic properties values are: '{{attribute1}}' 'This is {{attribute1}}' {{attribute1}} {{attributeN}} Here the attribute names in the double curly braces will be replaced with the values from the events before they are published. This syntax includes the following annotations. Sink The type parameter of the @sink annotation defines the sink type that publishes the events. The other parameters of the @sink annotation depends upon the selected sink type, and here some of its parameters can be optional and/or dynamic. For detailed information about the supported parameters see documentation of the relevant sink. The following is a list of sink types supported by Siddhi: Source type Description In-memory Allow SiddhiApp to publish events to other SiddhiApps running on the same JVM. Log Logs the events appearing on the streams. HTTP Publish events to an HTTP endpoint. Kafka Publish events to Kafka topic. TCP Publish events to a TCP service. Email Send emails via SMTP protocols. JMS Publish events to JMS topics or queues. File Writes events to files. Prometheus Expose data through Prometheus agent.","title":"Sink"},{"location":"docs/query-guide/#distributed-sink","text":"Distributed Sinks publish events from a defined stream to multiple endpoints using load balancing or partitioning strategies. Any sink can be used as a distributed sink. A distributed sink configuration allows users to define a common mapping to convert and send the Siddhi events for all its destination endpoints. Purpose Distributed sink provides a way to publish Siddhi events to multiple endpoints in the configured event format. Syntax To configure distributed sink add the sink configuration to a stream definition by adding the @sink annotation and add the configuration parameters that are common of all the destination endpoints inside it, along with the common parameters also add the @distribution annotation specifying the distribution strategy (i.e. roundRobin or partitioned ) and @destination annotations providing each endpoint specific configurations. The distributed sink syntax is as follows: RoundRobin Distributed Sink Publishes events to defined destinations in a round robin manner. 1 2 3 4 5 6 7 8 9 @ sink ( type = sink type , common . static . key = value , common . dynamic . key = {{ value }} , @ map ( type = map type , static . key = value , dynamic . key = {{ value }} , @ payload ( payload mapping ) ) @ distribution ( strategy = roundRobin , @ destination ( destination . specific . key = value ), @ destination ( destination . specific . key = value ))) ) define stream stream name ( attribute1 type , attributeN type ); Partitioned Distributed Sink Publishes events to defined destinations by partitioning them based on the partitioning key. 1 2 3 4 5 6 7 8 9 @ sink ( type = sink type , common . static . key = value , common . dynamic . key = {{ value }} , @ map ( type = map type , static . key = value , dynamic . key = {{ value }} , @ payload ( payload mapping ) ) @ distribution ( strategy = partitioned , partitionKey = partition key , @ destination ( destination . specific . key = value ), @ destination ( destination . specific . key = value ))) ) define stream stream name ( attribute1 type , attributeN type );","title":"Distributed Sink"},{"location":"docs/query-guide/#sink-mapper","text":"Each @sink configuration can have a mapping denoted by the @map annotation that defines how to convert Siddhi events to outgoing messages with the defined format. The type parameter of the @map defines the map type to be used in converting the outgoing events. The other parameters of @map annotation depends on the mapper selected, and some of its parameters can be optional and/or dynamic. For detailed information about the parameters see the documentation of the relevant mapper. Map Payload @payload is an optional annotation used with @map to define custom mapping. When the @payload annotation is not provided, each mapper maps the outgoing events to its own default event format. The @payload annotation allow users to configure mappers to produce the output payload of their choice, and by using dynamic properties within the payload they can selectively extract and add data from the published Siddhi events. There are two ways you to configure @payload annotation. Some mappers such as XML , JSON , and Test only accept one output payload: @payload( 'This is a test message from {{user}}.') Some mappers such key-value accept series of mapping values: @payload( key1='mapping_1', 'key2'='user : {{user}}') Here, the keys of payload mapping can be defined using the dot notation as a.b.c , or using any constant string value as '$abc' . Supported Sink Mapping Types The following is a list of sink mapping types supported by Siddhi: Sink mapping type Description PassThrough Omits data conversion on outgoing Siddhi events. JSON Converts Siddhi events to JSON messages. XML Converts Siddhi events to XML messages. TEXT Converts Siddhi events to plain text messages. Avro Converts Siddhi events to Avro Events. Binary Converts Siddhi events to Siddhi specific binary events. Key Value Converts Siddhi events to key-value HashMaps. CSV Converts Siddhi events to CSV like delimiter separated events. Tip When the @map annotation is not provided @map(type='passThrough') is used as default, that passes the outgoing Siddhi events directly to the sinks without any data conversion. PassThrough is the only sink mapper inbuilt in Siddhi, and all other sink mappers are implemented as extensions. Example 1 Publishes OutputStream events by converting them to JSON messages with the default format, and by sending to an HTTP endpoint http://localhost:8005/endpoint1 , using POST method, Accept header, and basic authentication having admin is both username and password. The configuration of the HTTP sink and JSON sink mapper to achieve the above is as follows. 1 2 3 4 5 6 @ sink ( type = http , publisher . url = http://localhost:8005/endpoint , method = POST , headers = Accept-Date:20/02/2017 , basic . auth . enabled = true , basic . auth . username = admin , basic . auth . password = admin , @ map ( type = json )) define stream OutputStream ( name string , age int , country string ); This will publish a JSON message on the following format: 1 2 3 4 5 6 7 { event :{ name : Paul , age : 20 , country : UK } } Example 2 Publishes StockStream events by converting them to user defined JSON messages, and by sending to an HTTP endpoint http://localhost:8005/stocks . The configuration of the HTTP sink and custom JSON sink mapping to achieve the above is as follows. 1 2 3 4 @ sink ( type = http , publisher . url = http://localhost:8005/stocks , @ map ( type = json , validate . json = true , enclosing . element = $.Portfolio , @ payload ( { StockData :{ Symbol : {{ symbol }} , Price :{{price}} }} ))) define stream StockStream ( symbol string , price float , volume long ); This will publish a single event as the JSON message on the following format: 1 2 3 4 5 6 7 8 { Portfolio :{ StockData :{ Symbol : GOOG , Price : 55.6 } } } This can also publish multiple events together as a JSON message on the following format: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 { Portfolio :[ { StockData :{ Symbol : GOOG , Price : 55.6 } }, { StockData :{ Symbol : FB , Price : 57.0 } } ] } Example 3 Publishes events from the OutputStream stream to multiple the HTTP endpoints using a partitioning strategy. Here the events are sent to either http://localhost:8005/endpoint1 or http://localhost:8006/endpoint2 based on the partitioning key country . It uses default JSON mapping, POST method, and used admin as both the username and the password when publishing to both the endpoints. The configuration of the distributed HTTP sink and JSON sink mapper to achieve the above is as follows. 1 2 3 4 5 6 7 @ sink ( type = http , method = POST , basic . auth . enabled = true , basic . auth . username = admin , basic . auth . password = admin , @ map ( type = json ), @ distribution ( strategy = partitioned , partitionKey = country , @ destination ( publisher . url = http://localhost:8005/endpoint1 ), @ destination ( publisher . url = http://localhost:8006/endpoint2 ))) define stream OutputStream ( name string , age int , country string ); This will partition the outgoing events and publish all events with the same country attribute value to the same endpoint. The JSON message published will be on the following format: 1 2 3 4 5 6 7 { event :{ name : Paul , age : 20 , country : UK } }","title":"Sink Mapper"},{"location":"docs/query-guide/#error-handling","text":"Errors in Siddhi can be handled at the Streams and in Sinks. Error Handling at Stream When errors are thrown by Siddhi elements subscribed to the stream, the error gets propagated up to the stream that delivered the event to those Siddhi elements. By default the error is logged and dropped at the stream, but this behavior can be altered by by adding @OnError annotation to the corresponding stream definition. @OnError annotation can help users to capture the error and the associated event, and handle them gracefully by sending them to a fault stream. The @OnError annotation and the required action to be specified as below. 1 2 3 @ OnError ( action = on error action ) define stream stream name ( attribute name attribute type , attribute name attribute type , ... ); The action parameter of the @OnError annotation defines the action to be executed during failure scenarios. The following actions can be specified to @OnError annotation to handle erroneous scenarios. LOG : Logs the event with the error, and drops the event. This is the default action performed even when @OnError annotation is not defined. STREAM : Creates a fault stream and redirects the event and the error to it. The created fault stream will have all the attributes defined in the base stream to capture the error causing event, and in addition it also contains _error attribute of type object to containing the error information. The fault stream can be referred by adding ! in front of the base stream name as ! stream name . Example Handle errors in TempStream by redirecting the errors to a fault stream. The configuration of TempStream stream and @OnError annotation is as follows. 1 2 @ OnError ( action = STREAM ) define stream TempStream ( deviceID long , roomNo int , temp double ); Siddhi will infer and automatically defines the fault stream of TempStream as given below. 1 define stream ! TempStream ( deviceID long , roomNo int , temp double , _error object ); The SiddhiApp extending the above the use-case by adding failure generation and error handling with the use of queries is as follows. Note: Details on writing processing logics via queries will be explained in later sections. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 -- Define fault stream to handle error occurred at TempStream subscribers @ OnError ( action = STREAM ) define stream TempStream ( deviceID long , roomNo int , temp double ); -- Error generation through a custom function `createError()` @ info ( name = error-generation ) from TempStream # custom : createError () insert into IgnoreStream1 ; -- Handling error by simply logging the event and error. @ info ( name = handle-error ) from ! TempStream # log ( Error Occurred! ) select deviceID , roomNo , temp , _error insert into IgnoreStream2 ; Error Handling at Sink There can be cases where external systems becoming unavailable or coursing errors when the events are published to them. By default sinks log and drop the events causing event losses, and this can be handled gracefully by configuring on.error parameter of the @sink annotation. The on.error parameter of the @sink annotation can be specified as below. 1 2 3 @ sink ( type = sink type , on . error = on error action , key = value , ...) define stream stream name ( attribute name attribute type , attribute name attribute type , ... ); The following actions can be specified to on.error parameter of @sink annotation to handle erroneous scenarios. LOG : Logs the event with the error, and drops the event. This is the default action performed even when on.error parameter is not defined on the @sink annotation. WAIT : Publishing threads wait in back-off and re-trying mode, and only send the events when the connection is re-established. During this time the threads will not consume any new messages causing the systems to introduce back pressure on the systems that publishes to it. STREAM : Pushes the failed events with the corresponding error to the associated fault stream the sink belongs to. Example 1 Introduce back pressure on the threads who bring events via TempStream when the system cannot connect to Kafka. The configuration of TempStream stream and @sink Kafka annotation with on.error property is as follows. 1 2 3 4 @ sink ( type = kafka , on . error = WAIT , topic = {{roomNo}} , bootstrap . servers = localhost:9092 , @ map ( type = xml )) define stream TempStream ( deviceID long , roomNo int , temp double ); Example 2 Send events to the fault stream of TempStream when the system cannot connect to Kafka. The configuration of TempStream stream with associated fault stream, @sink Kafka annotation with on.error property and a queries to handle the error is as follows. Note: Details on writing processing logics via queries will be explained in later sections. 1 2 3 4 5 6 7 8 9 10 11 @ OnError ( action = STREAM ) @ sink ( type = kafka , on . error = STREAM , topic = {{roomNo}} , bootstrap . servers = localhost:9092 , @ map ( type = xml )) define stream TempStream ( deviceID long , roomNo int , temp double ); -- Handling error by simply logging the event and error. @ info ( name = handle-error ) from ! TempStream # log ( Error Occurred! ) select deviceID , roomNo , temp , _error insert into IgnoreStream ;","title":"Error Handling"},{"location":"docs/query-guide/#query","text":"Query defines the processing logic in Siddhi. It consumes events from one or more streams, named-windows , tables , and/or named-aggregations , process the events in a streaming manner, and generate output events into a stream , named-window , or table . Purpose A query provides a way to process the events in the order they arrive and produce output using both stateful and stateless complex event processing and stream processing operations. Syntax The high level query syntax for defining processing logics is as follows: 1 2 3 4 @ info ( name = query name ) from input projection output action The following parameters are used to configure a stream definition. Parameter Description query name The name of the query. Since naming the query (i.e the @info(name = ' query name ') annotation) is optional, when the name is not provided Siddhi assign a system generated name for the query. input Defines the means of event consumption via streams , named-windows , tables , and/or named-aggregations , and defines the processing logic using filters , windows , stream-functions , joins , patterns and sequences . projection Generates output event attributes using select , functions , aggregation-functions , and group by operations, and filters the generated the output using having , limit offset , order by , and output rate limiting operations before sending them out. Here the projection is optional and when it is omitted all the input events will be sent to the output as it is. output action Defines output action (such as insert into , update , delete , etc) that needs to be performed by the generated events on a stream , named-window , or table Example A query consumes events from the TempStream stream and output only the roomNo and temp attributes to the RoomTempStream stream, from which another query consumes the events and sends all its attributes to AnotherRoomTempStream stream. 1 2 3 4 5 6 7 8 define stream TempStream ( deviceID long , roomNo int , temp double ); from TempStream select roomNo , temp insert into RoomTempStream ; from RoomTempStream insert into AnotherRoomTempStream ; Inferred Stream Here, the RoomTempStream and AnotherRoomTempStream streams are an inferred streams, which means their stream definitions are inferred from the queries and hence they can be used the same as any other defined streams without any restrictions.","title":"Query"},{"location":"docs/query-guide/#value","text":"Values are typed data, which can be manipulated, transferred, and stored. Values can be referred by the attributes defined in definitions such as streams, and tables. Siddhi supports values of type STRING , INT (Integer), LONG , DOUBLE , FLOAT , BOOL (Boolean) and OBJECT . The syntax of each type and their example use as a constant value is as follows, Attribute Type Format Example int + 123 , -75 , +95 long +L 123000L , -750l , +154L float ( +)?('.' *)? (E(-|+)? +)?F 123.0f , -75.0e-10F , +95.789f double ( +)?('.' *)? (E(-|+)? +)?D? 123.0 , 123.0D , -75.0e-10D , +95.789d bool (true|false) true , false , TRUE , FALSE string '( char * !('|\"|\"\"\"| line ))' or \"( char * !(\"|\"\"\"| line ))\" or \"\"\"( char * !(\"\"\"))\"\"\" 'Any text.' , \"Text with 'single' quotes.\" , \"\"\" Text with 'single' quotes, \"double\" quotes, and new lines. \"\"\" Time Time is a special type of LONG value that denotes time using digits and their unit in the format ( digit + unit )+ . At execution, the time gets converted into milliseconds and returns a LONG value. Unit Syntax Year year | years Month month | months Week week | weeks Day day | days Hour hour | hours Minutes minute | minutes | min Seconds second | seconds | sec Milliseconds millisecond | milliseconds Example 1 hour and 25 minutes can by written as 1 hour and 25 minutes which is equal to the LONG value 5100000 .","title":"Value"},{"location":"docs/query-guide/#select","text":"The select clause in Siddhi query defines the output event attributes of the query. Following are some basic query projection operations supported by select. Action Description Select specific attributes for projection Only select some of the input attributes as query output attributes. E.g., Select and output only roomNo and temp attributes from the TempStream stream. from TempStream select roomNo, temp insert into RoomTempStream; Select all attributes for projection Select all input attributes as query output attributes. This can be done by using asterisk ( * ) or by omitting the select clause itself. E.g., Both following queries select all attributes of TempStream input stream and output all attributes to NewTempStream stream. from TempStream select * insert into NewTempStream; or from TempStream insert into NewTempStream; Name attribute Provide a unique name for each output attribute generated by the query. This can help to rename the selected input attributes or assign an attribute name to a projection operation such as function, aggregate-function, mathematical operation, etc, using as keyword. E.g., Query that renames input attribute temp to temperature and function currentTimeMillis() as time . from TempStream select roomNo, temp as temperature, currentTimeMillis() as time insert into RoomTempStream; Constant values as attributes Creates output attributes with a constant value. Any constant value of type STRING , INT , LONG , DOUBLE , FLOAT , BOOL , and time as given in the values section can be defined. E.g., Query specifying 'C' as the constant value for the scale attribute. from TempStream select roomNo, temp, 'C' as scale insert into RoomTempStream; Mathematical and logical expressions in attributes Defines the mathematical and logical operations that need to be performed to generating output attribute values. These expressions are executed in the precedence order given below. Operator precedence Operator Distribution Example () Scope (cost + tax) * 0.05 IS NULL Null check deviceID is null NOT Logical NOT not (price > 10) * , / , % Multiplication, division, modulus temp * 9/5 + 32 + , - Addition, subtraction temp * 9/5 - 32 < , < = , > , >= Comparators: less-than, greater-than-equal, greater-than, less-than-equal totalCost >= price * quantity == , != Comparisons: equal, not equal totalCost != price * quantity IN Checks if value exist in the table roomNo in ServerRoomsTable AND Logical AND temp < 40 and humidity < 40 OR Logical OR humidity < 40 or humidity >= 60 E.g., Query converts temperature from Celsius to Fahrenheit, and identifies rooms with room number between 10 and 15 as server rooms. from TempStream select roomNo, temp * 9/5 + 32 as temp, 'F' as scale, roomNo > 10 and roomNo < 15 as isServerRoom insert into RoomTempStream;","title":"Select"},{"location":"docs/query-guide/#function","text":"Functions are pre-configured operations that can consumes zero, or more parameters and always produce a single value as result. It can be used anywhere an attribute can be used. Purpose It encapsulate pre-configured reusable execution logic allowing users to execute the logic anywhere just by calling the function. This also make writing SiddhiApps simple and easy to understand. Syntax The syntax of function is as follows, 1 function name ( parameter * ) Here function name uniquely identifies the function. The parameter defined input parameters the function can accept. The input parameters can be attributes, constant values, results of other functions, results of mathematical or logical expressions, or time values. The number and type of parameters a function accepts depend on the function itself. Note Functions, mathematical expressions, and logical expressions can be used in a nested manner. Example 1 Function with name add accepting two input parameters, one being an attribute named input and other being a constant value 75 . 1 add(input, 75) Example 2 Function name alertAfter accepting two input parameters, one being a time value 1 hour and 25 minutes and the other a mathematical addition of startTime and 56 . 1 alertAfter(1 hour and 25 minutes, startTime + 56) Inbuilt functions Following are some inbuilt Siddhi functions, for more functions refer execution extensions . Inbuilt function Description eventTimestamp Returns event's timestamp. currentTimeMillis Returns current time of SiddhiApp runtime. default Returns a default value if the parameter is null. ifThenElse Returns parameters based on a conditional parameter. UUID Generates a UUID. cast Casts parameter type. convert Converts parameter type. coalesce Returns first not null input parameter. maximum Returns the maximum value of all parameters. minimum Returns the minimum value of all parameters. instanceOfBoolean Checks if the parameter is an instance of Boolean. instanceOfDouble Checks if the parameter is an instance of Double. instanceOfFloat Checks if the parameter is an instance of Float. instanceOfInteger Checks if the parameter is an instance of Integer. instanceOfLong Checks if the parameter is an instance of Long. instanceOfString Checks if the parameter is an instance of String. createSet Creates HashSet with given input parameters. sizeOfSet Returns number of items in the HashSet, that's passed as a parameter. Example Query to convert the roomNo to string using convert function, find the maximum temperature reading with maximum function, and to add a unique messageID using the UUID function. 1 2 3 4 5 from TempStream select convert ( roomNo , string ) as roomNo , maximum ( tempReading1 , tempReading2 ) as temp , UUID () as messageID insert into RoomTempStream ;","title":"Function"},{"location":"docs/query-guide/#filter","text":"Filters filter events arriving on input streams based on specified conditions. They accept any type of condition including a combination of attributes, constants, functions, and others, that produces a Boolean result. Filters allow events to pass through if the condition results in true , and drops if it results in a false . Purpose Filter helps to select the events that are relevant for processing and omit the ones that are not. Syntax Filter conditions should be defined in square brackets ( [] ) next to the input stream as shown below. 1 2 3 from input stream [ filter condition ] select attribute name , attribute name , ... insert into output stream Example Query to filter TempStream stream events, having roomNo within the range of 100-210 and temperature greater than 40 degrees, and insert the filtered results into HighTempStream stream. 1 2 3 from TempStream [( roomNo = 100 and roomNo 210 ) and temp 40 ] select roomNo , temp insert into HighTempStream ;","title":"Filter"},{"location":"docs/query-guide/#window","text":"Windows capture a subset of events from input streams and retain them for a period of time based on a specified criterion. The criterion defines when and how the events should be evicted from the window. Such as events getting evicted based on time duration, or number of events in the window, and the way they get evicted is in sliding (one by one) or tumbling (batch) manner. In a query, each input stream can at most have only one window associated with it. Purpose Windows help to retain events based on a criterion, such that the values of those events can be aggregated, correlated or checked, if the event of interest is in the window. Syntax Window should be defined next to the input stream along the #window prefix as shown below. 1 2 3 from input stream # window . window name ( parameter , parameter , ... ) select attribute name , attribute name , ... insert ouput event type ? into output stream Note Filter conditions can be applied both before and/or after the window. Inbuilt windows Following are some inbuilt Siddhi windows, for more windows refer execution extensions . Inbuilt function Description time Retains events based on time in a sliding manner. timeBatch Retains events based on time in a tumbling/batch manner. length Retains events based on number of events in a sliding manner. lengthBatch Retains events based on number of events in a tumbling/batch manner. timeLength Retains events based on time and number of events in a sliding manner. session Retains events for each session based on session key. batch Retains events of last arrived event chunk. sort Retains top-k or bottom-k events based on a parameter value. cron Retains events based on cron time in a tumbling/batch manner. externalTime Retains events based on event time value passed as a parameter in a sliding manner. externalTimeBatch Retains events based on event time value passed as a parameter in a a tumbling/batch manner. delay Retains events and delays the output by the given time period in a sliding manner. Example 1 Query to find out the maximum temperature out of the last 10 events , using the window of length 10 and max() aggregation function, from the TempStream stream and insert the results into the MaxTempStream stream. 1 2 3 from TempStream # window . length ( 10 ) select max ( temp ) as maxTemp insert into MaxTempStream ; Here, the length window operates in a sliding manner where the following 3 event subsets are calculated and outputted when a list of 12 events are received in sequential order. Subset Event Range 1 1 - 10 2 2 - 11 3 3 - 12 Example 2 Query to find out the maximum temperature out of the every 10 events , using the window of lengthBatch 10 and max() aggregation function, from the TempStream stream and insert the results into the MaxTempStream stream. 1 2 3 from TempStream # window . lengthBatch ( 10 ) select max ( temp ) as maxTemp insert into MaxTempStream ; Here, the window operates in a batch/tumbling manner where the following 3 event subsets are calculated and outputted when a list of 30 events are received in a sequential order. Subset Event Range 1 1 - 10 2 11 - 20 3 21 - 30 Example 3 Query to find out the maximum temperature out of the events arrived during last 10 minutes , using the window of time 10 minutes and max() aggregation function, from the TempStream stream and insert the results into the MaxTempStream stream. 1 2 3 from TempStream # window . time ( 10 min ) select max ( temp ) as maxTemp insert into MaxTempStream ; Here, the time window operates in a sliding manner with millisecond accuracy, where it will process events in the following 3 time durations and output aggregated events when a list of events are received in a sequential order. Subset Time Range (in ms) 1 1:00:00.001 - 1:10:00.000 2 1:00:01.001 - 1:10:01.000 3 1:00:01.033 - 1:10:01.034 Example 4 Query to find out the maximum temperature out of the events arriving every 10 minutes , using the window of timeBatch 10 and max() aggregation function, from the TempStream stream and insert the results into the MaxTempStream stream. 1 2 3 from TempStream # window . timeBatch ( 10 min ) select max ( temp ) as maxTemp insert into MaxTempStream ; Here, the window operates in a batch/tumbling manner where the window will process evetns in the following 3 time durations and output aggregated events when a list of events are received in a sequential order. Subset Time Range (in ms) 1 1:00:00.001 - 1:10:00.000 2 1:10:00.001 - 1:20:00.000 3 1:20:00.001 - 1:30:00.000","title":"Window"},{"location":"docs/query-guide/#event-type","text":"Query output depends on the current and expired event types produced by the query based on its internal processing state. By default all queries produce current events upon event arrival. The queries containing windows additionally produce expired events when events expire from those windows. Purpose Event type helps to identify how the events were produced and to specify when a query should output such events to the output stream, such as output processed events only upon new event arrival to the query, upon event expiry from the window, or upon both cases. Syntax Event type should be defined in between insert and into keywords for insert queries as follows. 1 2 3 from input stream # window . window name ( parameter , parameter , ... ) select attribute name , attribute name , ... insert event type into output stream Event type should be defined next to the for keyword for delete queries as follows. 1 2 3 4 from input stream # window . window name ( parameter , parameter , ... ) select attribute name , attribute name , ... delete table ( for event type ) ? on condition Event type should be defined next to the for keyword for update queries as follows. 1 2 3 4 5 from input stream # window . window name ( parameter , parameter , ... ) select attribute name , attribute name , ... update table ( for event type ) ? set table . attribute name = ( attribute name | expression ) ? , table . attribute name = ( attribute name | expression ) ? , ... on condition Event type should be defined next to the for keyword for update or insert queries as follows. 1 2 3 4 5 from input stream # window . window name ( parameter , parameter , ... ) select attribute name , attribute name , ... update or insert into table ( for event type ) ? set table . attribute name = expression , table . attribute name = expression , ... on condition The event types can be defined using the following keywords to manipulate query output. Event types Description current events Outputs processed events only upon new event arrival to the query. This is default behavior when no specific event type is specified. expired events Outputs processed events only upon event expiry from the window. all events Outputs processed events when new events arrive to the query as well as when events expire from the window. Note Controlling query output based on the event types neither alters query execution nor its accuracy. Example Query to output processed events only upon event expiry from the 1 minute time window to the DelayedTempStream stream. This query helps to delay events by a minute. 1 2 3 from TempStream # window . time ( 1 min ) select * insert expired events into DelayedTempStream Note This is just to illustrate how expired events work, it is recommended to use delay window for use cases where we need to delay events by a given time period of time.","title":"Event Type"},{"location":"docs/query-guide/#aggregate-function","text":"Aggregate functions are pre-configured aggregation operations that can consume zero, or more parameters from multiple events and produce a single value as result. They can be only used in query projection (as part of the select clause). When a query comprises a window, the aggregation will be constrained to the events in the window, and when it does not have a window, the aggregation is performed from the first event the query has received. Purpose Aggregate functions encapsulate pre-configured reusable aggregate logic allowing users to aggregate values of multiple events together. When used with batch/tumbling windows this will also reduce the number of output events produced. Syntax Aggregate function can be used in query projection (as part of the select clause) alone or as a part of another expression. In all cases, the output produced should be properly mapped to the output stream attribute of the query using the as keyword. The syntax of aggregate function is as follows, 1 2 3 from input stream # window . window name ( parameter , parameter , ... ) select aggregate function ( parameter , parameter , ... ) as attribute name , attribute2 name , ... insert into output stream ; Here aggregate function uniquely identifies the aggregate function. The parameter defined input parameters the aggregate function can accept. The input parameters can be attributes, constant values, results of other functions or aggregate functions, results of mathematical or logical expressions, or time values. The number and type of parameters an aggregate function accepts depend on the aggregate function itself. Inbuilt aggregate functions Following are some inbuilt aggregation functions, for more aggregate functions refer execution extensions . Inbuilt aggregate function Description sum Calculates the sum from a set of values. count Calculates the count from a set of values. distinctCount Calculates the distinct count based on a parameter from a set of values. avg Calculates the average from a set of values. max Finds the maximum value from a set of values. max Finds the minimum value from a set of values. maxForever Finds the maximum value from all events throughout its lifetime irrespective of the windows. minForever Finds the minimum value from all events throughout its lifetime irrespective of the windows. stdDev Calculates the standard deviation from a set of values. and Calculates boolean and from a set of values. or Calculates boolean or from a set of values. unionSet Constructs a Set by unioning set of values. Example Query to calculate average, maximum, and minimum values on temp attribute of the TempStream stream in a sliding manner, from the events arrived over the last 10 minutes and to produce output events with attributes avgTemp , maxTemp and minTemp respectively to the AvgTempStream stream. 1 2 3 from TempStream # window . time ( 10 min ) select avg ( temp ) as avgTemp , max ( temp ) as maxTemp , min ( temp ) as minTemp insert into AvgTempStream ;","title":"Aggregate Function"},{"location":"docs/query-guide/#group-by","text":"Group By groups events based on one or more specified attributes to perform aggregate operations. Purpose Group By helps to perform aggregate functions independently for each given group-by key combination. Syntax The syntax for the Group By with aggregate function is as follows. 1 2 3 4 from input stream # window . window name (...) select aggregate function ( parameter , parameter , ...) as attribute1 name , attribute2 name , ... group by attribute1 name , attribute2 name , ... insert into output stream ; Here the group by attributes should be defined next to the group by keyword separating each attribute by a comma. Example Query to calculate the average temp per each roomNo and deviceID combination, from the events arrived from TempStream stream, during the last 10 minutes time-window in a sliding manner. 1 2 3 4 from TempStream # window . time ( 10 min ) select roomNo , deviceID , avg ( temp ) as avgTemp group by roomNo , deviceID insert into AvgTempStream ;","title":"Group By"},{"location":"docs/query-guide/#having","text":"Having filters events at the query output using a specified condition on query output stream attributes. It accepts any type of condition including a combination of output stream attributes, constants, and/or functions that produces a Boolean result. Having, allow events to passthrough if the condition results in true , and drops if it results in a false . Purpose Having helps to select the events that are relevant for the output based on the attributes those are produced by the select clause and omit the ones that are not. Syntax The syntax for the Having clause is as follows. 1 2 3 4 5 from input stream # window . window name ( ... ) select aggregate function ( parameter , parameter , ...) as attribute1 name , attribute2 name , ... group by attribute1 name , attribute2 name ... having condition insert into output stream ; Here the having condition should be defined next to the having keyword, and it can be used with or without group by clause. Example Query to calculate the average temp per roomNo for the events arrived on the last 10 minutes, and send alerts for each event having avgTemp more than 30 degrees. 1 2 3 4 5 from TempStream # window . time ( 10 min ) select roomNo , avg ( temp ) as avgTemp group by roomNo having avgTemp 30 insert into AlertStream ;","title":"Having"},{"location":"docs/query-guide/#order-by","text":"Order By, orders the query results in ascending or descending order based on one or more specified attributes. By default the order by attribute orders the events in ascending order, and by adding desc keyword, the events can be ordered in descending order. When more than one attribute is defined the attributes defined towards the left will have more precedence in ordering than the ones defined in right. Purpose Order By helps to sort the events in the query output chunks. Order By will only be effective when query outputs a lot of events together such as in batch windows than for sliding windows where events are emitted one at a time. Syntax The syntax for the Order By clause is as follows: 1 2 3 4 5 6 from input stream # window . window name ( ... ) select aggregate function ( parameter , parameter , ...) as attribute1 name , attribute2 name , ... group by attribute1 name , attribute2 name ... having condition order by attribute1 name ( asc | desc ) ? , attribute2 name ( asc | desc ) ? , ... insert into output stream ; Here, the order by attributes ( attributeN name ) should be defined next to the order by keyword separating each by a comma, and optionally the event ordering can be specified using asc (default) or desc keywords to respectively define ascending and descending. Example Query to calculate the average temp , per roomNo and deviceID combination, on every 10 minutes batches, and order the generated output events in ascending order by avgTemp and then in descending order of roomNo (if there are more events having the same avgTemp value) before emitting them to the AvgTempStream stream. 1 2 3 4 5 from TempStream # window . timeBatch ( 10 min ) select roomNo , deviceID , avg ( temp ) as avgTemp group by roomNo , deviceID order by avgTemp , roomNo desc insert into AvgTempStream ;","title":"Order By"},{"location":"docs/query-guide/#limit-offset","text":"These provide a way to select a limited number of events (via limit) from the desired index (using an offset) from the output event chunks produced by the query. Purpose Limit Offset helps to output only the selected set of events from large event batches. This will be very useful with Order By clause where one can order the output and extract the topK or bottomK events, and even use it to paginate through the dataset by obtaining set of events from the middle. Syntax The syntax for the Limit Offset clauses is as follows: 1 2 3 4 5 6 7 8 from input stream # window . window name ( ... ) select aggregate function ( parameter , parameter , ...) as attribute1 name , attribute2 name , ... group by attribute1 name , attribute2 name ... having condition order by attribute1 name ( asc | desc ) ? , attribute2 name ( ascend / descend ) ? , ... limit positive integer ? offset positive integer ? insert into output stream ; Here both limit and offset are optional and both can be defined by adding a positive integer next to their keywords, when limit is omitted the query will output all the events, and when offset is omitted 0 is taken as the default offset value. Example 1 Query to calculate the average temp , per roomNo and deviceID combination, for every 10 minutes batches, from the events arriving at the TempStream stream, and emit only two events having the highest avgTemp value. 1 2 3 4 5 6 from TempStream # window . timeBatch ( 10 min ) select roomNo , deviceID , avg ( temp ) as avgTemp group by roomNo , deviceID order by avgTemp desc limit 2 insert into HighestAvgTempStream ; Example 2 Query to calculate the average temp , per roomNo and deviceID combination, for every 10 minutes batches, for the events arriving at the TempStream stream, and emits only the third, forth and fifth events when sorted in descending order based on their avgTemp value. 1 2 3 4 5 6 7 from TempStream # window . timeBatch ( 10 min ) select roomNo , deviceID , avg ( temp ) as avgTemp group by roomNo , deviceID order by avgTemp desc limit 3 offset 2 insert into HighestAvgTempStream ;","title":"Limit &amp; Offset"},{"location":"docs/query-guide/#join-stream","text":"Joins combine events from two streams in real-time based on a specified condition. Purpose Join provides a way of correlating events of two steams and in addition aggregating them based on the defined windows. Two streams cannot directly join as they are stateless, and they do not retain events. Therefore, each stream needs to be associated with a window for joining as it can retain events. Join also accepts a condition to match events against each event stream window. During the joining process each incoming event of each stream is matched against all the events in the other stream's window based on the given condition, and the output events are generated for all the matching event pairs. When there is no window associated with the joining steam, window.lengthBatch(0) is assigned by default to the steam to enable the join process and to preserve stream's stateless nature. Note Join can also be performed with stored data , aggregation or externally named windows . Syntax The syntax for a join two streams is as follows: 1 2 3 4 5 from input stream ( # window . window name ( parameter , ... )) ? ( unidirectional ) ? ( as reference ) ? join type input stream ( # window . window name ( parameter , ... )) ? ( unidirectional ) ? ( as reference ) ? ( on join condition ) ? select attribute name , attribute name , ... insert into output stream Here, both the streams can have a window associated with them and have an optional join condition next to the on keyword to match events from both windows to generate combined output events. Window should be defined as the last element of each joining stream. Join query expects a window to be defined as the last element of each joining stream, therefore a filter cannot be defined after the window. join types Following are the supported join operations. Inner join (join) This is the default behavior of a join operation, and the join keyword is used to join both the streams. Here the output is generated only if there is a matching event in both the streams when either stream is triggering the join operation. Left outer join The left outer join keyword is used to join two streams while producing all left stream events to the output. Here, the output is generated when right stream triggers the join operation and finds matching events in the left stream window to perform the join, and in all cases where the left stream triggers the join operation. Here, when the left stream finds matching events in the right stream window, it uses them for the join, and if there are no matching events, then it uses null values for the join operation. Right outer join This is similar to a left outer join and the Right outer join keyword is used to join two streams while producing all right stream events to the output. It generate output in all cases where the right stream triggers the join operation even if there are no matching events in the left stream window. Full outer join The full outer join combines the results of left outer join and right outer join. The full outer join keyword is used to join the streams while producing both left and stream events to the output. Here, the output is generated in all cases where the left or right stream triggers the join operation, and when a stream finds matching events in the other stream window, it uses them for the join, and if there are no matching events, then it uses null values instead. Cross join In either of these cases when join condition is omitted, the triggering event will successfully match against all the events in the other stream window, producing a cross join behavior. Unidirectional join operation By default, events arriving on either stream trigger the join operation and generate the corresponding output. However, this join behavior can be controlled by adding the unidirectional keyword next to one of the streams as depicted in the join query syntax above. This enables only the stream with the unidirectional to trigger the join operation. Therefore the events arriving on the other stream will neither trigger the join operation nor produce any output, but rather they only update their stream's window state. The unidirectional keyword cannot be applied on both join streams. This is because the default behavior already allows both the streams to trigger the join operation. Example 1 (join) The query to generate output when there is a matching event having equal symbol and companyID combination from the events arrived in the last 10 minutes on StockStream stream and the events arrived in the last 20 minutes on TwitterStream stream. 1 2 3 4 5 6 7 8 define stream StockStream ( symbol string , price float , volume long ); define stream TwitterStream ( companyID string , tweet string ); from StockStream # window . time ( 10 min ) as S join TwitterStream # window . time ( 20 min ) as T on S . symbol == T . companyID select S . symbol as symbol , T . tweet , S . price insert into OutputStream ; Possible OutputStream outputs as follows 1 2 ( FB , FB is great! , 23.5f) ( GOOG , Its time to Google! , 54.5f) Example 2 (with no join condition) The query to generate output for all possible event combinations from the last 5 events of the StockStream stream and the events arrived in the last 1 minutes on TwitterStream stream. 1 2 3 4 5 6 7 define stream StockStream ( symbol string , price float , volume long ); define stream TwitterStream ( companyID string , tweet string ); from StockStream # window . length ( 5 ) as S join TwitterStream # window . time ( 1 min ) as T select S . symbol as symbol , T . tweet , S . price insert into OutputStream ; Possible OutputStream outputs as follows, 1 2 3 4 ( FB , FB is great! , 23.5f) ( FB , Its time to Google! , 23.5f) ( GOOG , FB is great! , 54.5f) ( GOOG , Its time to Google! , 54.5f) Example 3 (left outer join) The query to generate output for all events arriving in the StockStream stream regardless of whether there is a matching companyID for symbol exist in the events arrived in the last 20 minutes on TwitterStream stream, and generate output for the events arriving in the StockStream stream only when there is a matchine symbol and companyID combination exist in the events arrived in the last 10 minutes on StockStream stream. 1 2 3 4 5 6 7 8 define stream StockStream ( symbol string , price float , volume long ); define stream TwitterStream ( companyID string , tweet string ); from StockStream # window . time ( 10 min ) as S left outer join TwitterStream # window . time ( 20 min ) as T on S . symbol == T . companyID select S . symbol as symbol , T . tweet , S . price insert into OutputStream ; Possible OutputStream outputs as follows, 1 2 ( FB , FB is great! , 23.5f) ( GOOG , null, 54.5f) //when there are no matching event in TwitterStream Example 3 (full outer join) The query to generate output for all events arriving in the StockStream stream and in the TwitterStream stream regardless of whether there is a matching companyID for symbol exist in the other stream window or not. 1 2 3 4 5 6 7 8 define stream StockStream ( symbol string , price float , volume long ); define stream TwitterStream ( companyID string , tweet string ); from StockStream # window . time ( 10 min ) as S full outer join TwitterStream # window . time ( 20 min ) as T on S . symbol == T . companyID select S . symbol as symbol , T . tweet , S . price insert into OutputStream ; Possible OutputStream outputs as follows, 1 2 3 ( FB , FB is great! , 23.5f) ( GOOG , null, 54.5f) //when there are no matching event in TwitterStream (null, I like to tweet! , null) //when there are no matching event in StockStream Example 3 (unidirectional join) The query to generate output only when events arrive on StockStream stream find a matching event having equal symbol and companyID combination against the events arrived in the last 20 minutes on TwitterStream stream. 1 2 3 4 5 6 7 8 define stream StockStream ( symbol string , price float , volume long ); define stream TwitterStream ( companyID string , tweet string ); from StockStream # window . time ( 10 min ) unidirectional as S join TwitterStream # window . time ( 20 min ) as T on S . symbol == T . companyID select S . symbol as symbol , T . tweet , S . price insert into OutputStream ; Possible OutputStream outputs as follows, 1 2 ( FB , FB is great! , 23.5f) ( GOOG , Its time to Google! , 54.5f) Here both outputs will be initiated by events arriving on StockStream .","title":"Join (Stream)"},{"location":"docs/query-guide/#pattern","text":"The pattern is a state machine implementation that detects event occurrences from events arrived via one or more event streams over time. It can repetitively match patterns, count event occurrences, and use logical event ordering (using and , or , and not ). Purpose The pattern helps to achieve Complex Event Processing (CEP) capabilities by detecting various pre-defined event occurrence patterns in realtime. Pattern query does not expect the matching events to occur immediately after each other, and it can successfully correlate the events who are far apart and having other events in between. Syntax The syntax for a pattern query is as follows, 1 2 3 4 5 6 7 8 from ( ( every ) ? ( event reference = ) ? input stream [ filter condition ]( min count : max count ) ? | ( every ) ? ( event reference = ) ? input stream [ filter condition ] ( and | or ) ( event reference = ) ? input stream [ filter condition ] | ( every ) ? not input stream [ filter condition ] ( and event reference = input stream [ filter condition ] | for time gap ) ) - ... ( within time gap ) ? select event reference . attribute name , event reference . attribute name , ... insert into output stream Items Description -> Indicates an event will follow the given event. The subsequent event does not necessarily have to occur immediately after the preceding event. The condition to be met by the preceding event should be added before the -> , and the condition to be met by the subsequent event should be added after the -> . every An optional keyword defining when a new event matching state-machine should be initiated to repetitively match the pattern. When this keyword is not used, the event matching state-machine will be initiated only once. within time gap An optional within clause that defines the time duration within which all the matching events should occur. min count : max count Determines the number of minimum and maximum number of events that should the matched at the given condition. Possible values for the min and max count and their behavior is as follows, Syntex Description Example n1:n2 Matches n1 to n2 events (including n1 and not more than n2 ). 1:4 matches 1 to 4 events. n: Matches n or more events (including n ). 2:> matches 2 or more events. :n Matches up to n events (excluding n ). :5 matches up to 5 events. n Matches exactly n events. 5 matches exactly 5 events. and Allows both of its condition to be matched by two distinct events in any order. or Only expects one of its condition to be matched by an event. Here the event reference of the unmatched condition will be null . not condition1 and condition2 Detects the event matching condition2 before any event matching condition1 . not condition1> for time period> Detects no event matching on condition1 for the specified time period . event reference An optional reference to access the matching event for further processing. All conditions can be assigned to an event reference to collect the matching event occurrences, other than the condition used for not case (as there will not be any event matched against it). Non occurrence of events. Siddhi detects non-occurrence of events using the not keyword, and its effective non-occurrence checking period is bounded either by fulfillment of a condition associated by and or via an expiry time using time period . Logical correlation of multiple conditions. Siddhi can only logically correlate two conditions at a time using keywords such as and , or , and not . When more than two conditions need to be logically correlated, use multiple pattern queries in a chaining manner, at a time correlating two logical conditions and streaming the output to a downstream query to logically correlate the results with other logical conditions. Event selection The event reference in pattern queries is used to retrieve the matched events. When a pattern condition is intended to match only a single event, then its attributes can be retrieved by referring to its reference as event reference . attribute name . An example of this is as follows. e1.symbol , refers to the symbol attribute value of the matching event e1 . But when the pattern condition is associated with min count : max count , it is expected to match against on multiple events. Therefore, an event from the matched event collection should be retrieved using the event index from its reference. Here the indexes are specified in square brackets next to event reference, where index 0 referring to the first event, and a special index last referring to the last available event in the collection. Attribute values of all the events in the matching event collection can be accessed a list, by referring to their event reference without an index. Some possible indexes and their behavior is as follows. e1[0].symbol , refers to the symbol attribute value of the 1 st event in reference e1 . e1[3].price , refers to the price attribute value of the 4 th event in reference e1 . e1[last].symbol , refers to the symbol attribute value of the last event in reference e1 . e1[last - 1].symbol , refers to the symbol attribute value of one before the last event in reference e1 . e1.symbol , refers to the list of symbol attribute values of all events in the event collection in reference e1 , as a list object. The system returns null when accessing attribute values, when no matching event is assigned to the event reference (as in when two conditions are combined using or ) or when the provided index is greater than the last event index in the event collection. Example 1 (Every) A query to send an alerts when temperature of a room increases by 5 degrees within 10 min. 1 2 3 4 from every ( e1 = TempStream ) - e2 = TempStream [ e1 . roomNo == roomNo and ( e1 . temp + 5 ) = temp ] within 10 min select e1 . roomNo , e1 . temp as initialTemp , e2 . temp as finalTemp insert into AlertStream ; Here, the matching process begins for each event in the TempStream stream (as every is used with e1=TempStream ), and if another event arrives within 10 minutes with a value for temp attribute being greater than or equal to e1.temp + 5 of the initial event e1 , an output is generated via the AlertStream . Example 2 (Event collection) A query to find the temperature difference between two regulator events. 1 2 3 4 5 6 7 define stream TempStream ( deviceID long , roomNo int , temp double ); define stream RegulatorStream ( deviceID long , roomNo int , tempSet double , isOn bool ); from every e1 = RegulatorStream - e2 = TempStream [ e1 . roomNo == roomNo ] 1 : - e3 = RegulatorStream [ e1 . roomNo == roomNo ] select e1 . roomNo , e2 [ 0 ]. temp - e2 [ last ]. temp as tempDiff insert into TempDiffStream ; Here, one or more TempStream events having the same roomNo as of the RegulatorStream stream event matched in e1 is collected, and among them, the first and the last was retrieved to find the temperature difference. Example 3 (Logical or condition) Query to send the stop control action to the regulator via RegulatorActionStream when the key is removed from the hotel room. Here the key actions are monitored via RoomKeyStream stream, and the regulator state is monitored through RegulatorStateChangeStream stream. 1 2 3 4 5 6 7 8 9 define stream RegulatorStateChangeStream ( deviceID long , roomNo int , tempSet double , action string ); define stream RoomKeyStream ( deviceID long , roomNo int , action string ); from every e1 = RegulatorStateChangeStream [ action == on ] - e2 = RoomKeyStream [ e1 . roomNo == roomNo and action == removed ] or e3 = RegulatorStateChangeStream [ e1 . roomNo == roomNo and action == off ] select e1 . roomNo , ifThenElse ( e2 is null , none , stop ) as action having action != none insert into RegulatorActionStream ; Here, the query sends a stop action on RegulatorActionStream stream, if a removed action is triggered in the RoomKeyStream stream before the regulator state changing to off which is notified via RegulatorStateChangeStream stream. Example 4 (Logical not condition) Query to generate alerts if the regulator gets switched off before the temperature reaches 12 degrees. 1 2 3 4 5 6 7 8 define stream RegulatorStateChangeStream ( deviceID long , roomNo int , tempSet double , action string ); define stream TempStream ( deviceID long , roomNo int , temp double ); from every e1 = RegulatorStateChangeStream [ action == start ] - not TempStream [ e1 . roomNo == roomNo and temp = 12 ] and e2 = RegulatorStateChangeStream [ e1 . roomNo == roomNo and action == off ] select e1 . roomNo as roomNo insert into AlertStream ; Here, the query alerts the roomNo via AlertStream stream, when no temperature events having less than 12 arrived in the TempStream between the start and off actions of the regulator, notified via RegulatorActionStream stream. Example 5 (Logical not condition) Query to alert if the room temperature does not reduce to the set value within 5 minutes after switching on the regulator. 1 2 3 4 5 6 7 define stream RegulatorStateChangeStream ( deviceID long , roomNo int , tempSet double , action string ); define stream TempStream ( deviceID long , roomNo int , temp double ); from e1 = RegulatorStateChangeStream [ action == start ] - not TempStream [ e1 . roomNo == roomNo and temp = e1 . tempSet ] for 5 min select e1 . roomNo as roomNo insert into AlertStream ; Here, the query alerts the roomNo via AlertStream stream, when no temperature events having less than tempSet temperature arrived in the TempStream within 5 minutes of the regulator start action arrived via RegulatorActionStream stream. Example 6 (Detecting event non-occurrence) Following table presents some non-occurrence event matching scenarios that can be implemented using patterns. Pattern Description Sample Scenario not A for time period The non-occurrence of event A within time period after system start up. Alerts if the taxi has not reached its destination within 30 minutes, indicating that the passenger might be in danger. not A for time period and B Event A does not occur within time period , but event B occurs at some point in time. Alerts if the taxi has not reached its destination within 30 minutes, and the passenger has marked that he/she is in danger at some point in time. not A for time period or B Either event A does not occur within time period , or event B occurs at some point in time. Alerts if the taxi has not reached its destination within 30 minutes, or if the passenger has marked that he/she is in danger at some point in time. not A for time period 1 and not B for time period 2 Event A does not occur within time period 1 , and event B also does not occur within time period 2 . Alerts if the taxi has not reached its destination within 30 minutes, and the passenger has not marked himself/herself not in danger within the same time period. not A for time period 1 or not B for time period 2 Either event A does not occur within time period 1 , or event B occurs within time period 2 . Alerts if the taxi has not reached its destination A within 20 minutes, or reached its destination B within 30 minutes. A \u2192 not B for time period Event B does not occur within time period after the occurrence of event A. Alerts if the taxi has reached its destination, but it has been not followed by a payment record within 10 minutes. not A and B or A and not B Event A does not occur before event B. Alerts if the taxi is stated before activating the taxi fare calculator.","title":"Pattern"},{"location":"docs/query-guide/#sequence","text":"The sequence is a state machine implementation that detects consecutive event occurrences from events arrived via one or more event streams over time. Here all matching events need to arrive consecutively , and there should not be any non-matching events in between the matching sequence of events. The sequence can repetitively match event sequences, count event occurrences, and use logical event ordering (using and , or , and not ). Purpose The sequence helps to achieve Complex Event Processing (CEP) capabilities by detecting various pre-defined consecutive event occurrence sequences in realtime. Sequence query does expect the matching events to occur immediately after each other, and it can successfully correlate the events who do not have other events in between. Syntax The syntax for a sequence query is as follows: 1 2 3 4 5 6 7 8 from ( ( every ) ? ( event reference = ) ? input stream [ filter condition ] ( +|*|? ) ? | ( event reference = ) ? input stream [ filter condition ] ( and | or ) ( event reference = ) ? input stream [ filter condition ] | not input stream [ filter condition ] ( and event reference = input stream [ filter condition ] | for time gap ) ), ... ( within time gap ) ? select event reference . attribute name , event reference . attribute name , ... insert into output stream Items Description , Indicates the immediate next event that follows the given event. The condition to be met by the preceding event should be added before the , , and the condition to be met by the subsequent event should be added after the , . every An optional keyword defining when a new event matching state-machine should be initiated to repetitively match the sequence. When this keyword is not used, the event matching state-machine will be initiated only once. within time gap An optional within clause that defines the time duration within which all the matching events should occur. + Matches **one or more** events to the given condition. * Matches **zero or more** events to the given condition. ? Matches **zero or one** events to the given condition. and Allows both of its condition to be matched by two distinct events in any order. or Only expects one of its condition to be matched by an event. Here the event reference of the unmatched condition will be null . not condition1 and condition2 Detects the event matching condition2 before any event matching condition1 . not condition1> for time period> Detects no event matching on condition1 for the specified time period . event reference An optional reference to access the matching event for further processing. All conditions can be assigned to an event reference to collect the matching event occurrences, other than the condition used for not case (as there will not be any event matched against it). Non occurrence of events. Siddhi detects non-occurrence of events using the not keyword, and its effective non-occurrence checking period is bounded either by fulfillment of a condition associated by and or via an expiry time using time period . Logical correlation of multiple conditions. Siddhi can only logically correlate two conditions at a time using keywords such as and , or , and not . When more than two conditions need to be logically correlated, use multiple pattern queries in a chaining manner, at a time correlating two logical conditions and streaming the output to a downstream query to logically correlate the results with other logical conditions. Event selection The event reference in sequence queries is used to retrieve the matched events. When a sequence condition is intended to match only a single event, then its attributes can be retrieved by referring to its reference as event reference . attribute name . An example of this is as follows. e1.symbol , refers to the symbol attribute value of the matching event e1 . But when the pattern condition is associated with min count : max count , it is expected to match against on multiple events. Therefore, an event from the matched event collection should be retrieved using the event index from its reference. Here the indexes are specified in square brackets next to event reference, where index 0 referring to the first event, and a special index last referring to the last available event in the collection. Attribute values of all the events in the matching event collection can be accessed a list, by referring to their event reference without an index. Some possible indexes and their behavior is as follows. e1[0].symbol , refers to the symbol attribute value of the 1 st event in reference e1 . e1[3].price , refers to the price attribute value of the 4 th event in reference e1 . e1[last].symbol , refers to the symbol attribute value of the last event in reference e1 . e1[last - 1].symbol , refers to the symbol attribute value of one before the last event in reference e1 . e1.symbol , refers to the list of symbol attribute values of all events in the event collection in reference e1 , as a list object. The system returns null when accessing attribute values, when no matching event is assigned to the event reference (as in when two conditions are combined using or ) or when the provided index is greater than the last event index in the event collection. Example 1 (Every) Query to send alerts when temperature increases at least by one degree between two consecutive temperature events. 1 2 3 from every e1 = TempStream , e2 = TempStream [ temp e1 . temp + 1 ] select e1 . temp as initialTemp , e2 . temp as finalTemp insert into AlertStream ; Here, the matching process begins for each event in the TempStream stream (as every is used with e1=TempStream ), and if the immediate next event with a value for temp attribute being greater than e1.temp + 1 of the initial event e1 , then an output is generated via the AlertStream . Example 2 (Every collection) Query to identify temperature peeks by monitoring continuous increases in temp attribute and alerts upon the first drop. 1 2 3 4 5 6 7 8 define stream TempStream ( deviceID long , roomNo int , temp double ); @ info ( name = query1 ) from every e1 = TempStream , e2 = TempStream [ ifThenElse ( e2 [ last ]. temp is null , e1 . temp = temp , e2 [ last ]. temp = temp )] + , e3 = TempStream [ e2 [ last ]. temp temp ] select e1 . temp as initialTemp , e2 [ last ]. temp as peekTemp , e3 . price as firstDropTemp insert into PeekTempStream ; Here, the matching process begins for each event in the TempStream stream (as every is used with e1=TempStream ). It checks if the temp attribute value of the second event is greater than or equal to the temp attribute value of the first event ( e1.temp ), then for all the following events, their temp attribute value is checked if they are greater than or equal to their previous event's temp attribute value ( e2[last].temp ), and when the temp attribute value becomes less than its previous events temp attribute value value an output is generated via the AlertStream stream. Example 3 (Logical and condition) A query to identify a regulator activation event immediately followed by both temperature sensor and humidity sensor activation events in either order. 1 2 3 4 5 6 7 define stream TempStream ( deviceID long , isActive bool ); define stream HumidStream ( deviceID long , isActive bool ); define stream RegulatorStream ( deviceID long , isOn bool ); from every e1 = RegulatorStream [ isOn == true ], e2 = TempStream and e3 = HumidStream select e2 . isActive as tempSensorActive , e3 . isActive as humidSensorActive insert into StateNotificationStream ; Here, the matching process begins for each event in the RegulatorStream stream having the isOn attribute true . It generates an output via the AlertStream stream when an event from both TempStream stream and HumidStream stream arrives immediately after the first event in either order.","title":"Sequence"},{"location":"docs/query-guide/#output-rate-limiting","text":"Output rate-limiting limits the number of events emitted by the queries based on a specified criterion such as time, and number of events. Purpose Output rate-limiting helps to reduce the load on the subsequent executions such as query processing, I/O operations, and notifications by reducing the output frequency of the events. Syntax The syntax for output rate limiting is as follows: 1 2 3 4 from input stream ... select attribute name , attribute name , ... output rate limiting configuration insert into output stream Here, the output rate limiting configuration ( rate limiting configuration ) should be defined next to the output keyword and the supported output rate limiting types are explained in the following table: Rate limiting configuration Syntax Description Time based ( output event selection )? every time interval Outputs output event selection every time interval time interval. Number of events based ( output event selection )? every event interval events Outputs output event selection for every event interval number of events. Snapshot based snapshot every time interval Outputs all events currently in the query window (or outputs only the last event if no window is defined in the query) for every given time interval time interval. The output event selection specifies the event(s) that are selected to be outputted from the query, here when no output event selection is defined, all is used by default. The possible values for the output event selection and their behaviors are as follows: * first : The first query output is published as soon as it is generated and the subsequent events are dropped until the specified time interval or the number of events are reached before sending the next event as output. * last : Emits only the last output event generated during the specified time or event interval. * all : Emits all the output events together which are generated during the specified time or event interval. Example 1 (Time based first event) Query to calculate the average temp per roomNo for the events arrived on the last 10 minutes, and send alerts once every 15 minutes of the events having avgTemp more than 30 degrees. 1 2 3 4 5 6 7 8 define stream TempStream ( deviceID long , roomNo int , temp double ); from TempStream # window . time ( 10 min ) select roomNo , avg ( temp ) as avgTemp group by roomNo having avgTemp 30 output first every 15 min insert into AlertStream ; Here the first event having avgTemp 30 is emitted immediately and the next event is only emitted after 15 minutes. Example 2 (Event based first event) Query to output the initial event, and there onwards every 5 th event from TempStream stream events. 1 2 3 4 5 define stream TempStream ( deviceID long , roomNo int , temp double ); from TempStream output first every 5 events insert into FiveEventBatchStream ; Example 3 (Event based all events) Query to collect last 5 TempStream stream events and send them together as a single batch. 1 2 3 4 5 define stream TempStream ( deviceID long , roomNo int , temp double ); from TempStream output every 5 events insert into FiveEventBatchStream ; As no output event selection is defined, the behavior of all is applied in this case. Example 4 (Time based last event) Query to emit only the last event of TempStream stream for every 10 minute interval. 1 2 3 4 5 define stream TempStream ( deviceID long , roomNo int , temp double ); from TempStream output last every 10 min insert into FiveEventBatchStream ; Example 5 (Snapshot based) Query to emit the snapshot of events retained by its last 5 minutes window defined on TempStream stream, every second. 1 2 3 4 5 define stream TempStream ( deviceID long , roomNo int , temp double ); from TempStream # window . time ( 5 sec ) output snapshot every 1 sec insert into SnapshotTempStream ; Here, the query emits all the current events generated which do not have a corresponding expired event at the predefined time interval. Example 6 (Snapshot based) Query to emit the snapshot of events retained every second, when no window is defined on TempStream stream. 1 2 3 4 5 define stream TempStream ( deviceID long , roomNo int , temp double ); from TempStream output snapshot every 5 sec insert into SnapshotTempStream ; Here, the query outputs the last seen event at the end of each time interval as there are no events stored in no window defined.","title":"Output rate limiting"},{"location":"docs/query-guide/#partition","text":"Partition provides data parallelism by categorizing events into various isolated partition instance based on their attribute values and by processing each partition instance in isolation. Here each partition instance is tagged with a partition key, and they only process events that match to the corresponding partition key. Purpose Partition provide ways to segment events into groups and allow them to process the same set of queries in parallel and in isolation without redefining the queries for each segment. Here, events form multiple steams generating the same partition key will result in the same instance of the partition, and executed together. When a stream is used within the partition block without configuring a partition key, all of its events will be executed in all available partition instances. Syntax The syntax for a partition is as follows: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 @ purge ( enable = true , interval = purge interval , idle . period = idle period of partition instance ) partition with ( key selection of stream name , key selection of stream name , ... ) begin from stream name ... select attribute name , attribute name , ... insert into ( # ) ? stream name from ( # ) ? stream name ... select attribute name , attribute name , ... insert into stream name ... end ; Here, a new instance of a partition will be dynamically created for each unique partition key that is generated based on the key selection applied on the events of their associated streams ( stream name ). These created partition instances will exist in the system forever unless otherwise a purging policy is defined using the @purge annotation. The inner streams denoted by # stream name can be used to chain multiple queries within a partition block without leaving the isolation of the partition instance. The key selection defines the partition key for each event based on the event attribute value or using range expressions as listed below. Key selection type Syntax description Partition by value attribute name Attribute value of the event is used as its partition key. Partition by range compare condition as 'value' or compare condition as 'value' or ... Event is executed against all compare conditions , and the values associated with the matching conditions are used as its partition key. Here, when the event is matched against multiple conditions, it is processed on all the partition instances that are associated with those matching conditions. When there are multiple queries within a partition block, and they can be chained without leaving the isolation of the partition instance using the inner streams denoted by # . More information on inner Streams will be covered in the following secsions. Inner Stream Inner stream connects the queries inside a partition instance to one another while preserving partition isolation. These are denoted by a # placed before the stream name, and these streams cannot be accessed outside the partition block. Through this, without repartitioning the streams, the output of a query instance can be used as the input of another query instance that is also in the same partition instance. Using non inner streams to chain queries within a partition block. When the connecting stream is not an inner stream and if it is not configured to generate a partition key, then it outputs events to all available partition instances . However, when the non-inner stream is configured to generate a partition key, it only outputs to the partition instances that are selected based on the repartitioned partition key. Purge Partition Purge partition purges partitions that are not being used for a given period on a regular interval. This is because, by default, when partition instances are created for each unique partition key they exist forever if their queries contain stateful information, and there are use cases (such as partitioning events by date value) where an extremely large number of unique partition keys are used, which generates a large number of partition instances, and this eventually leading to system out of memory. The partition instances that will not be used anymore can purged using the @purge annotation. The elements of the annotation and their behavior is as follows. Purge partition configuration Description enable To enable partition purging. internal Periodic time interval to purge the purgeable partition instances. idle.period The idle period, a particular partition instance (for a given partition key) needs to be idle before it becomes purgeable. Example 1 (Partition by value) Query to calculate the maximum temperature of each deviceID , among its last 10 events. 1 2 3 4 5 6 partition with ( deviceID of TempStream ) begin from TempStream # window . length ( 10 ) select roomNo , deviceID , max ( temp ) as maxTemp insert into DeviceTempStream ; end ; Here, each unique deviceID will create a partition instance which retains the last 10 events arrived for its corresponding partition key and calculates the maximum values without interfering with the events of other partition instances. Example 2 (Partition by range) Query to calculate the average temperature for the last 10 minutes per each office area, where the office areas are identified based on the roomNo attribute ranges from the events of TempStream stream. 1 2 3 4 5 6 7 8 partition with ( roomNo = 1030 as serverRoom or roomNo 1030 and roomNo = 330 as officeRoom or roomNo 330 as lobby of TempStream ) begin from TempStream # window . time ( 10 min ) select roomNo , deviceID , avg ( temp ) as avgTemp insert into AreaTempStream end ; Here, partition instances are created for each office area type such as serverRoom , officeRoom , and lobby . Events are processed only in the partition instances which are associated with matching compare condition values that are satisfied by the event's roomNo attribute, and within each partition instance, the average tamp value is calculated based on the events arrived over the last 10 minutes. Example 3 (Inner streams) A partition to calculate the average temperature of every 10 events for each sensor, and send the output via the DeviceTempIncreasingStream stream if consecutive average temperature ( avgTemp ) values increase by more than 5 degrees. 1 2 3 4 5 6 7 8 9 10 partition with ( deviceID of TempStream ) begin from TempStream # window . lengthBatch ( 10 ) select roomNo , deviceID , avg ( temp ) as avgTemp insert into # AvgTempStream from every e1 =# AvgTempStream , e2 =# AvgTempStream [ e1 . avgTemp + 5 avgTemp ] select e1 . deviceID , e1 . avgTemp as initialAvgTemp , e2 . avgTemp as finalAvgTemp insert into DeviceTempIncreasingStream end ; Here, the first query calculates the avgTemp for every 10 events for each unique deviceID and passes the output via the inner stream #AvgTempStream to the second query that is also in the same partition instance. The second query then identifies a pair of consecutive events from #AvgTempStream , where the latter event having 5 degrees more on avgTemp value than its previous event. Example 4 (Purge partition) A partition to identify consecutive three login failure attempts for each session within 1 hour. Here, the number of sessions can be infinite. 1 2 3 4 5 6 7 8 9 10 11 12 define stream LoginStream ( sessionID string , loginSuccessful bool ); @ purge ( enable = true , interval = 10 sec , idle . period = 1 hour ) partition with ( sessionID of LoginStream ) begin from every e1 = LoginStream [ loginSuccessful == false ], e2 = LoginStream [ loginSuccessful == false ], e3 = LoginStream [ loginSuccessful == false ] within 1 hour select e1 . sessionID as sessionID insert into LoginFailureStream ; end ; Here, the events in LoginStream is partitioned by their sessionID attribute and matched for consecutive occurrences of events having loginSuccessful==false with 1 hour using a sequence query and inserts the matching pattern's sessionID to LoginFailureStream . As the number of sessions is infinite the @purge annotation is enabled to purge the partition instances. The instances are marked for purging if there are no events from a particular sessionID for the last 1 hour, and the marked instances are periodically purged once every 10 seconds.","title":"Partition"},{"location":"docs/query-guide/#table","text":"A table is a stored version of an stream or a table of events. Its schema is defined via the table definition that is similar to a stream definition. These events are by default stored in-memory , but Siddhi also provides store extensions to work with data/events stored in various data stores through the table abstraction. Purpose Tables allow Siddhi to work with stored events. By defining a schema for tables Siddhi enables them to be processed by queries using their defined attributes with the streaming data. You can also interactively query the state of the stored events in the table. Syntax The syntax for a new table definition is as follows: 1 define table table name ( attribute name attribute type , attribute name attribute type , ... ); The following parameters are configured in a table definition: Parameter Description table name The name of the table defined. ( PascalCase is used for table name as a convention.) attribute name The schema of the table is defined by its attributes with uniquely identifiable attribute names ( camelCase is used for attribute names as a convention.) attribute type The type of each attribute defined in the schema. This can be STRING , INT , LONG , DOUBLE , FLOAT , BOOL or OBJECT . Example The following defines a table named RoomTypeTable with roomNo and type attributes of data types int and string respectively. 1 define table RoomTypeTable ( roomNo int , type string ); Primary Keys Tables can be configured with primary keys to avoid the duplication of data. Primary keys are configured by including the @PrimaryKey( 'key1', 'key2' ) annotation to the table definition. Each event table configuration can have only one @PrimaryKey annotation. The number of attributes supported differ based on the table implementations. When more than one attribute is used for the primary key, the uniqueness of the events stored in the table is determined based on the combination of values for those attributes. Examples This query creates an event table with the symbol attribute as the primary key. Therefore each entry in this table must have a unique value for symbol attribute. 1 2 @ PrimaryKey ( symbol ) define table StockTable ( symbol string , price float , volume long ); Indexes Indexes allow tables to be searched/modified much faster. Indexes are configured by including the @Index( 'key1', 'key2' ) annotation to the table definition. Each event table configuration can have 0-1 @Index annotations. Support for the @Index annotation and the number of attributes supported differ based on the table implementations. When more then one attribute is used for index, each one of them is used to index the table for fast access of the data. Indexes can be configured together with primary keys. Examples This query creates an indexed event table named RoomTypeTable with the roomNo attribute as the index key. 1 2 @ Index ( roomNo ) define table RoomTypeTable ( roomNo int , type string );","title":"Table"},{"location":"docs/query-guide/#store","text":"Store is a table that refers to data/events stored in data stores outside of Siddhi such as RDBMS, Cassandra, etc. Store is defined via the @store annotation, and the store schema is defined via a table definition associated with it. Purpose Store allows Siddhi to search, retrieve and manipulate data stored in external data stores through Siddhi queries. Syntax The syntax for a defining store and it's associated table definition is as follows: 1 2 @ store ( type = store_type , static . option . key1 = static_option_value1 , static . option . keyN = static_option_valueN ) define table TableName ( attribute1 Type1 , attributeN TypeN ); Example The following defines a RDBMS data store pointing to a MySQL database with name hotel hosted in loacalhost:3306 having a table RoomTypeTable with columns roomNo of INTEGER and type of VARCHAR(255) mapped to Siddhi data types int and string respectively. 1 2 3 @ Store ( type = rdbms , jdbc . url = jdbc:mysql://localhost:3306/hotel , username = siddhi , password = 123 , jdbc . driver . name = com.mysql.jdbc.Driver ) define table RoomTypeTable ( roomNo int , type string ); Supported Store Types The following is a list of currently supported store types: RDBMS (MySQL, Oracle, SQL Server, PostgreSQL, DB2, H2) MongoDB Caching in Memory Store tables are persisted in high i/o latency storage. Hence, it is beneficial to maintain a cache of store tables in memory which has low latency. Siddhi supports caching of store tables through @cache annotation. It should be used within @store annotation in a nested fashion as shown below. 1 2 3 @ store ( type = store_type , static . option . key1 = static_option_value1 , static . option . keyN = static_option_valueN , @ cache ( size = 10 , cache . policy = FIFO )) define table TableName ( attribute1 Type1 , attributeN TypeN ); In the above example we have defined a cache with a maximum size of 10 rows with first-in first-out cache policy. The following table contains the cache parameters. Parameter Mandatory/Optional Default Value Description size Mandatory - maximum number of rows to be cached cache.policy Optional FIFO policy to free up cache when cache miss occurs. There are 3 allowed policies. 1. FIFO - First-In, First-Out 2. LRU - Least Recently Used 3. LFU - Least Frequently Used retention.period Optional - If user specifies this parameter then cache expiry is enabled. For example if this is 5 min, rows older than 5 mins will be removed and in some cases reloaded from store purge.interval optional equal to retention period When cache expiry is enabled, a thread will be created for every purge.interval which will check for expired rows and remove them. The following is an example of caching with expiry. 1 2 3 @ store ( type = store_type , static . option . key1 = static_option_value1 , static . option . keyN = static_option_valueN , @ cache ( size = 10 , retention . period = 5 min , purge . interval = 1 min )) define table TableName ( attribute1 Type1 , attributeN TypeN ); The above query will define and create a store table of given type and a cache with a max size of 10. A thread will be created every 1 minute which will check the entire cache table for rows added earlier than 5 minutes and expire them. Cache Behavior Cache behavior changes profoundly based on the size of store table relative to maximum cache size defined. Since memory is a limited resource we don't allow cache to grow more than the user specified maximum size. Case 1 \\ When store table is smaller than maximum cache size defined we keep the entire content of store table in memory in cache table. All types of queries are routed to cache and cache results are directly sent out to the user. Every time the expiry thread finds that cache events were loaded earlier than retention period entire cache table will be deleted and reloaded from store. In addition, when siddhi app starts, the entire store table, if it exists, will be loaded into cache. Case 2 \\ When store table is bigger than maximum cache size only the queries satisfying the following 2 conditions are sent to cache. 1. the query contains all the primary keys of the table 2. the query contains only == type of comparison. Only for the above types of queries we can establish if the cache is hit or missed. Subject to these conditions if the cache is hit the results from cache is sent out. If the cache is missed then store is checked. If the above conditions are not met by a query it is directly sent to the store table. In addition, please note that if the store table is pre existing when siddhi app is started and it is bigger than max cache size, cache preloading will take only upto max size and put it in cache. For example if store table has 50 entries when the siddhi app is defined with cache size of 10, only the first 10 rows will be cached. When cache miss occurs we look for the answer in the store table. If there is a result from the store table it is added to cache. One element from cache is removed using the user given cache policy prior to adding. When it comes to cache expiry, since not all rows are loaded at once in this case there may be some expired rows and some unexpired rows at any time. So for every purge interval a thread will be generated which looks for rows that were loaded earlier than retention period and delete only those rows. No reloading is done. Operators on Table (and Store) The following operators can be performed on tables (and stores).","title":"Store"},{"location":"docs/query-guide/#insert","text":"This allows events to be inserted into tables. This is similar to inserting events into streams. Warning If the table is defined with primary keys, and if you insert duplicate data, primary key constrain violations can occur. In such cases use the update or insert into operation. Syntax 1 2 3 from input stream select attribute name , attribute name , ... insert into table Similar to streams, you need to use the current events , expired events or the all events keyword between insert and into keywords in order to insert only the specific event types. For more information, see Event Type Example This query inserts all the events from the TempStream stream to the TempTable table. 1 2 3 from TempStream select * insert into TempTable ;","title":"Insert"},{"location":"docs/query-guide/#join-table","text":"This allows a stream to retrieve information from a table in a streaming manner. Note Joins can also be performed with two streams , aggregation or against externally named windows . Syntax 1 2 3 4 from input stream join table on condition select ( input stream | table ). attribute name , ( input stream | table ). attribute name , ... insert into output stream Note A table can only be joint with a stream. Two tables cannot be joint because there must be at least one active entity to trigger the join operation. Example This Siddhi App performs a join to retrieve the room type from RoomTypeTable table based on the room number, so that it can filter the events related to server-room s. 1 2 3 4 5 6 7 8 define table RoomTypeTable ( roomNo int , type string ); define stream TempStream ( deviceID long , roomNo int , temp double ); from TempStream join RoomTypeTable on RoomTypeTable . roomNo == TempStream . roomNo select deviceID , RoomTypeTable . type as roomType , type , temp having roomType == server-room insert into ServerRoomTempStream ; Supported join types Table join supports following join operations. Inner join (join) This is the default behavior of a join operation. join is used as the keyword to join the stream with the table. The output is generated only if there is a matching event in both the stream and the table. Left outer join The left outer join operation allows you to join a stream on left side with a table on the right side based on a condition. Here, it returns all the events of left stream even if there are no matching events in the right table by having null values for the attributes of the right table. Right outer join This is similar to a left outer join . right outer join is used as the keyword to join a stream on right side with a table on the left side based on a condition. It returns all the events of the right stream even if there are no matching events in the left table.","title":"Join (Table)"},{"location":"docs/query-guide/#delete","text":"To delete selected events that are stored in a table. Syntax 1 2 3 4 from input stream select attribute name , attribute name , ... delete table ( for event type ) ? on condition The condition element specifies the basis on which events are selected to be deleted. When specifying the condition, table attributes should be referred to with the table name. To execute delete for specific event types, use the current events , expired events or the all events keyword with for as shown in the syntax. For more information, see Event Type Note Table attributes must be always referred to with the table name as follows: table name . attibute name Example In this example, the script deletes a record in the RoomTypeTable table if it has a value for the roomNo attribute that matches the value for the roomNumber attribute of an event in the DeleteStream stream. 1 2 3 4 5 6 7 define table RoomTypeTable ( roomNo int , type string ); define stream DeleteStream ( roomNumber int ); from DeleteStream delete RoomTypeTable on RoomTypeTable . roomNo == roomNumber ;","title":"Delete"},{"location":"docs/query-guide/#update","text":"This operator updates selected event attributes stored in a table based on a condition. Syntax 1 2 3 4 5 from input stream select attribute name , attribute name , ... update table ( for event type ) ? set table . attribute name = ( attribute name | expression ) ? , table . attribute name = ( attribute name | expression ) ? , ... on condition The condition element specifies the basis on which events are selected to be updated. When specifying the condition , table attributes must be referred to with the table name. You can use the set keyword to update selected attributes from the table. Here, for each assignment, the attribute specified in the left must be the table attribute, and the one specified in the right can be a stream/table attribute a mathematical operation, or other. When the set clause is not provided, all the attributes in the table are updated. To execute an update for specific event types use the current events , expired events or the all events keyword with for as shown in the syntax. For more information, see Event Type . Note Table attributes must be always referred to with the table name as shown below: table name . attibute name . Example This Siddhi application updates the room occupancy in the RoomOccupancyTable table for each room number based on new arrivals and exits from the UpdateStream stream. 1 2 3 4 5 6 7 8 define table RoomOccupancyTable ( roomNo int , people int ); define stream UpdateStream ( roomNumber int , arrival int , exit int ); from UpdateStream select * update RoomOccupancyTable set RoomOccupancyTable . people = RoomOccupancyTable . people + arrival - exit on RoomOccupancyTable . roomNo == roomNumber ;","title":"Update"},{"location":"docs/query-guide/#update-or-insert","text":"This allows you update if the event attributes already exist in the table based on a condition, or else insert the entry as a new attribute. Syntax 1 2 3 4 5 from input stream select attribute name , attribute name , ... update or insert into table ( for event type ) ? set table . attribute name = expression , table . attribute name = expression , ... on condition The condition element specifies the basis on which events are selected for update. When specifying the condition , table attributes should be referred to with the table name. If a record that matches the condition does not already exist in the table, the arriving event is inserted into the table. The set clause is only used when an update is performed during the insert/update operation. When set clause is used, the attribute to the left is always a table attribute, and the attribute to the right can be a stream/table attribute, mathematical operation or other. The attribute to the left (i.e., the attribute in the event table) is updated with the value of the attribute to the right if the given condition is met. When the set clause is not provided, all the attributes in the table are updated. Note When the attribute to the right is a table attribute, the operations supported differ based on the database type. To execute update upon specific event types use the current events , expired events or the all events keyword with for as shown in the syntax. To understand more see Event Type . Note Table attributes should be always referred to with the table name as table name . attibute name . Example The following query update for events in the UpdateTable event table that have room numbers that match the same in the UpdateStream stream. When such events are found in the event table, they are updated. When a room number available in the stream is not found in the event table, it is inserted from the stream. 1 2 3 4 5 6 7 8 define table RoomAssigneeTable ( roomNo int , type string , assignee string ); define stream RoomAssigneeStream ( roomNumber int , type string , assignee string ); from RoomAssigneeStream select roomNumber as roomNo , type , assignee update or insert into RoomAssigneeTable set RoomAssigneeTable . assignee = assignee on RoomAssigneeTable . roomNo == roomNo ;","title":"Update or Insert"},{"location":"docs/query-guide/#in","text":"This allows the stream to check whether the expected value exists in the table as a part of a conditional operation. Syntax 1 2 3 from input stream [ condition in table ] select attribute name , attribute name , ... insert into output stream The condition element specifies the basis on which events are selected to be compared. When constructing the condition , the table attribute must be always referred to with the table name as shown below: table . attibute name . Example This Siddhi application filters only room numbers that are listed in the ServerRoomTable table. 1 2 3 4 5 define table ServerRoomTable ( roomNo int ); define stream TempStream ( deviceID long , roomNo int , temp double ); from TempStream [ ServerRoomTable . roomNo == roomNo in ServerRoomTable ] insert into ServerRoomTempStream ;","title":"In"},{"location":"docs/query-guide/#named-aggregation","text":"Named aggregation allows you to obtain aggregates in an incremental manner for a specified set of time periods. This not only allows you to calculate aggregations with varied time granularity, but also allows you to access them in an interactive manner for reports, dashboards, and for further processing. Its schema is defined via the aggregation definition . Purpose Named aggregation allows you to retrieve the aggregate values for different time durations. That is, it allows you to obtain aggregates such as sum , count , avg , min , max , count and distinctCount of stream attributes for durations such as sec , min , hour , etc. This is of considerable importance in many Analytics scenarios because aggregate values are often needed for several time periods. Furthermore, this ensures that the aggregations are not lost due to unexpected system failures because aggregates can be stored in different persistence stores . Syntax 1 2 3 4 5 6 7 @ store ( type = store type , ...) @ purge ( enable = true or false , interval = purging interval , @ retentionPeriod ( granularity = retention period , ...) ) define aggregation aggregator name from input stream select attribute name , aggregate function ( attribute name ) as attribute name , ... group by attribute name aggregate by timestamp attribute every time periods ; The above syntax includes the following: Item Description @store This annotation is used to refer to the data store where the calculated aggregate results are stored. This annotation is optional. When no annotation is provided, the data is stored in the in-memory store. @purge This annotation is used to configure purging in aggregation granularities. If this annotation is not provided, the default purging mentioned above is applied. If you want to disable automatic data purging, you can use this annotation as follows: '@purge(enable=false) /You should disable data purging if the aggregation query in included in the Siddhi application for read-only purposes. @retentionPeriod This annotation is used to specify the length of time the data needs to be retained when carrying out data purging. If this annotation is not provided, the default retention period is applied. aggregator name This specifies a unique name for the aggregation so that it can be referred when accessing aggregate results. input stream The stream that feeds the aggregation. Note! this stream should be already defined. group by attribute name The group by clause is optional. If it is included in a Siddhi application, aggregate values are calculated per each group by attribute. If it is not used, all the events are aggregated together. by timestamp attribute This clause is optional. This defines the attribute that should be used as the timestamp. If this clause is not used, the event time is used by default. The timestamp could be given as either a string or a long value. If it is a long value, the unix timestamp in milliseconds is expected (e.g. 1496289950000 ). If it is a string value, the supported formats are yyyy - MM - dd HH : mm : ss (if time is in GMT) and yyyy - MM - dd HH : mm : ss Z (if time is not in GMT), here the ISO 8601 UTC offset must be provided for Z . (e.g., +05:30 , -11:00 ). time periods Time periods can be specified as a range where the minimum and the maximum value are separated by three dots, or as comma-separated values. e.g., A range can be specified as sec...year where aggregation is done per second, minute, hour, day, month and year. Comma-separated values can be specified as min, hour. Skipping time durations (e.g., min, day where the hour duration is skipped) when specifying comma-separated values is supported only from v4.1.1 onwards Aggregation's granularity data holders are automatically purged every 15 minutes. When carrying out data purging, the retention period you have specified for each granularity in the named aggregation query is taken into account. The retention period defined for a granularity needs to be greater than or equal to its minimum retention period as specified in the table below. If no valid retention period is defined for a granularity, the default retention period (as specified in the table below) is applied. Granularity Default retention Minimum retention second 120 seconds 120 seconds minute 24 hours 120 minutes hour 30 days 25 hours day 1 year 32 days month All 13 month year All none Note Aggregation is carried out at calendar start times for each granularity with the GMT timezone Note The same aggregation can be defined in multiple Siddhi apps for joining, however, only one siddhi app should carry out the processing (i.e. the aggregation input stream should only feed events to one aggregation definition). Example This Siddhi Application defines an aggregation named TradeAggregation to calculate the average and sum for the price attribute of events arriving at the TradeStream stream. These aggregates are calculated per every time granularity in the second-year range. 1 2 3 4 5 6 7 8 define stream TradeStream ( symbol string , price double , volume long , timestamp long ); @ purge ( enable = true , interval = 10 sec , @ retentionPeriod ( sec = 120 sec , min = 24 hours , hours = 30 days , days = 1 year , months = all , years = all )) define aggregation TradeAggregation from TradeStream select symbol , avg ( price ) as avgPrice , sum ( price ) as total group by symbol aggregate by timestamp every sec ... year ;","title":"Named Aggregation"},{"location":"docs/query-guide/#distributed-aggregation","text":"Distributed Aggregation allows you to partially process aggregations in different shards. This allows Siddhi app in one shard to be responsible only for processing a part of the aggregation. However for this, all aggregations must be based on a common physical database(@store). Syntax 1 2 3 4 5 6 7 @ store ( type = store type , ...) @ PartitionById define aggregation aggregator name from input stream select attribute name , aggregate function ( attribute name ) as attribute name , ... group by attribute name aggregate by timestamp attribute every time periods ; Following table includes the annotation to be used to enable distributed aggregation, Item Description @PartitionById If the annotation is given, then the distributed aggregation is enabled. Further this can be disabled by using enable element, @PartitionById(enable='false') . Further, following system properties are also available, System Property Description Possible Values Optional Default Value shardId The id of the shard one of the distributed aggregation is running in. This should be unique to a single shard Any string No partitionById This allows user to enable/disable distributed aggregation for all aggregations running in one siddhi manager .(Available from v4.3.3) true/false Yes false Note ShardIds should not be changed after the first configuration in order to keep data consistency.","title":"Distributed Aggregation"},{"location":"docs/query-guide/#join-aggregation","text":"This allows a stream to retrieve calculated aggregate values from the aggregation. Note A join can also be performed with two streams , with a table and a stream, or with a stream against externally named windows . Syntax A join with aggregation is similer to the join with table , but with additional within and per clauses. 1 2 3 4 5 6 from input stream join aggrigation on join condition within time range per time granularity select attribute name , attribute name , ... insert into output stream ; Apart from constructs of table join this includes the following. Please note that the 'on' condition is optional : Item Description within time range This allows you to specify the time interval for which the aggregate values need to be retrieved. This can be specified by providing the start and end time separated by a comma as string or long values, or by using the wildcard string specifying the data range. For details refer examples. per time granularity This specifies the time granularity by which the aggregate values must be grouped and returned. e.g., If you specify days , the retrieved aggregate values are grouped for each day within the selected time interval. within and per clauses also accept attribute values from the stream. The timestamp of the aggregations can be accessed through the AGG_TIMESTAMP attribute. Example Following aggregation definition will be used for the examples. 1 2 3 4 5 6 7 define stream TradeStream ( symbol string , price double , volume long , timestamp long ); define aggregation TradeAggregation from TradeStream select AGG_TIMESTAMP , symbol , avg ( price ) as avgPrice , sum ( price ) as total group by symbol aggregate by timestamp every sec ... year ; This query retrieves daily aggregations within the time range \"2014-02-15 00:00:00 +05:30\", \"2014-03-16 00:00:00 +05:30\" (Please note that +05:30 can be omitted if timezone is GMT) 1 2 3 4 5 6 7 8 define stream StockStream ( symbol string , value int ); from StockStream as S join TradeAggregation as T on S . symbol == T . symbol within 2014-02-15 00:00:00 +05:30 , 2014-03-16 00:00:00 +05:30 per days select S . symbol , T . total , T . avgPrice insert into AggregateStockStream ; This query retrieves hourly aggregations within the day 2014-02-15 . 1 2 3 4 5 6 7 8 define stream StockStream ( symbol string , value int ); from StockStream as S join TradeAggregation as T on S . symbol == T . symbol within 2014-02-15 **:**:** +05:30 per hours select S . symbol , T . total , T . avgPrice insert into AggregateStockStream ; This query retrieves all aggregations per perValue stream attribute within the time period between timestamps 1496200000000 and 1596434876000 . 1 2 3 4 5 6 7 8 define stream StockStream ( symbol string , value int , perValue string ); from StockStream as S join TradeAggregation as T on S . symbol == T . symbol within 1496200000000 L , 1596434876000 L per S . perValue select S . symbol , T . total , T . avgPrice insert into AggregateStockStream ; Supported join types Aggregation join supports following join operations. Inner join (join) This is the default behavior of a join operation. join is used as the keyword to join the stream with the aggregation. The output is generated only if there is a matching event in the stream and the aggregation. Left outer join The left outer join operation allows you to join a stream on left side with a aggregation on the right side based on a condition. Here, it returns all the events of left stream even if there are no matching events in the right aggregation by having null values for the attributes of the right aggregation. Right outer join This is similar to a left outer join . right outer join is used as the keyword to join a stream on right side with a aggregation on the left side based on a condition. It returns all the events of the right stream even if there are no matching events in the left aggregation.","title":"Join (Aggregation)"},{"location":"docs/query-guide/#named-window","text":"A named window is a window that can be shared across multiple queries. Events can be inserted to a named window from one or more queries and it can produce output events based on the named window type. Syntax The syntax for a named window is as follows: 1 define window window name ( attribute name attribute type , attribute name attribute type , ... ) window type ( parameter , parameter , \u2026 ) event type ; The following parameters are configured in a table definition: Parameter Description window name The name of the window defined. ( PascalCase is used for window names as a convention.) attribute name The schema of the window is defined by its attributes with uniquely identifiable attribute names ( camelCase is used for attribute names as a convention.) attribute type The type of each attribute defined in the schema. This can be STRING , INT , LONG , DOUBLE , FLOAT , BOOL or OBJECT . window type ( parameter , ...) The window type associated with the window and its parameters. output event type This is optional. Keywords such as current events , expired events and all events (the default) can be used to specify when the window output should be exposed. For more information, see Event Type . Examples Returning all output when events arrive and when events expire from the window. In this query, the event type is not specified. Therefore, it returns both current and expired events as the output. 1 define window SensorWindow ( name string , value float , roomNo int , deviceID string ) timeBatch ( 1 second ); Returning an output only when events expire from the window. In this query, the event type of the window is expired events . Therefore, it only returns the events that have expired from the window as the output. 1 define window SensorWindow ( name string , value float , roomNo int , deviceID string ) timeBatch ( 1 second ) output expired events ; Operators on Named Windows The following operators can be performed on named windows.","title":"Named Window"},{"location":"docs/query-guide/#insert_1","text":"This allows events to be inserted into windows. This is similar to inserting events into streams. Syntax 1 2 3 from input stream select attribute name , attribute name , ... insert into window To insert only events of a specific event type, add the current events , expired events or the all events keyword between insert and into keywords (similar to how it is done for streams). For more information, see Event Type . Example This query inserts all events from the TempStream stream to the OneMinTempWindow window. 1 2 3 4 5 6 define stream TempStream ( tempId string , temp double ); define window OneMinTempWindow ( tempId string , temp double ) time ( 1 min ); from TempStream select * insert into OneMinTempWindow ;","title":"Insert"},{"location":"docs/query-guide/#join-window","text":"To allow a stream to retrieve information from a window based on a condition. Note A join can also be performed with two streams , aggregation or with tables tables . Syntax 1 2 3 4 from input stream join window on condition select ( input stream | window ). attribute name , ( input stream | window ). attribute name , ... insert into output stream Example This Siddhi Application performs a join count the number of temperature events having more then 40 degrees within the last 2 minutes. 1 2 3 4 5 6 7 define window TwoMinTempWindow ( roomNo int , temp double ) time ( 2 min ); define stream CheckStream ( requestId string ); from CheckStream as C join TwoMinTempWindow as T on T . temp 40 select requestId , count ( T . temp ) as count insert into HighTempCountStream ; Supported join types Window join supports following operations of a join clause. Inner join (join) This is the default behavior of a join operation. join is used as the keyword to join two windows or a stream with a window. The output is generated only if there is a matching event in both stream/window. Left outer join The left outer join operation allows you to join two windows or a stream with a window to be merged based on a condition. Here, it returns all the events of left stream/window even if there are no matching events in the right stream/window by having null values for the attributes of the right stream/window. Right outer join This is similar to a left outer join. Right outer join is used as the keyword to join two windows or a stream with a window. It returns all the events of the right stream/window even if there are no matching events in the left stream/window. Full outer join The full outer join combines the results of left outer join and right outer join . full outer join is used as the keyword to join two windows or a stream with a window. Here, output event are generated for each incoming event even if there are no matching events in the other stream/window.","title":"Join (Window)"},{"location":"docs/query-guide/#from","text":"A window can be an input to a query, similar to streams. Note !!! When window is used as an input to a query, another window cannot be applied on top of this. Syntax 1 2 3 from window select attribute name , attribute name , ... insert into output stream Example This Siddhi Application calculates the maximum temperature within the last 5 minutes. 1 2 3 4 5 6 define window FiveMinTempWindow ( roomNo int , temp double ) time ( 5 min ); from FiveMinTempWindow select max ( temp ) as maxValue , roomNo insert into MaxSensorReadingStream ;","title":"From"},{"location":"docs/query-guide/#trigger","text":"Triggers allow events to be periodically generated. Trigger definition can be used to define a trigger. A trigger also works like a stream with a predefined schema. Purpose For some use cases the system should be able to periodically generate events based on a specified time interval to perform some periodic executions. A trigger can be performed for a 'start' operation, for a given time interval , or for a given ' cron expression ' . Syntax The syntax for a trigger definition is as follows. 1 define trigger trigger name at ( start | every time interval | cron expression ); Similar to streams, triggers can be used as inputs. They adhere to the following stream definition and produce the triggered_time attribute of the long type. 1 define stream trigger name ( triggered_time long ); The following types of triggeres are currently supported: Trigger type Description 'start' An event is triggered when Siddhi is started. every time interval An event is triggered periodically at the given time interval. ' cron expression ' An event is triggered periodically based on the given cron expression. For configuration details, see quartz-scheduler . Examples Triggering events regularly at specific time intervals The following query triggers events every 5 minutes. 1 define trigger FiveMinTriggerStream at every 5 min ; Triggering events at a specific time on specified days The following query triggers an event at 10.15 AM on every weekdays. 1 define trigger FiveMinTriggerStream at 0 15 10 ? * MON-FRI ;","title":"Trigger"},{"location":"docs/query-guide/#script","text":"Scripts allow you to write functions in other programming languages and execute them within Siddhi queries. Functions defined via scripts can be accessed in queries similar to any other inbuilt function. Function definitions can be used to define these scripts. Function parameters are passed into the function logic as Object[] and with the name data . Purpose Scripts allow you to define a function operation that is not provided in Siddhi core or its extension. It is not required to write an extension to define the function logic. Syntax The syntax for a Script definition is as follows. 1 2 3 define function function name [ language name ] return return type { operation of the function } ; The following parameters are configured when defining a script. Parameter Description function name The name of the function ( camelCase is used for the function name) as a convention. language name The name of the programming language used to define the script, such as javascript , r and scala . return type The attribute type of the function\u2019s return. This can be int , long , float , double , string , bool or object . Here the function implementer should be responsible for returning the output attribute on the defined return type for proper functionality. operation of the function Here, the execution logic of the function is added. This logic should be written in the language specified under the language name , and it should return the output in the data type specified via the return type parameter. Examples This query performs concatenation using JavaScript, and returns the output as a string. 1 2 3 4 5 6 7 8 9 10 11 12 13 define function concatFn [ javascript ] return string { var str1 = data [ 0 ]; var str2 = data [ 1 ]; var str3 = data [ 2 ]; var responce = str1 + str2 + str3 ; return responce ; } ; define stream TempStream ( deviceID long , roomNo int , temp double ); from TempStream select concatFn ( roomNo , - , deviceID ) as id , temp insert into DeviceTempStream ;","title":"Script"},{"location":"docs/query-guide/#store-query","text":"Siddhi store queries are a set of on-demand queries that can be used to perform operations on Siddhi tables, windows, and aggregators. Purpose Store queries allow you to execute the following operations on Siddhi tables, windows, and aggregators without the intervention of streams. Queries supported for tables: SELECT INSERT DELETE UPDATE UPDATE OR INSERT Queries supported for windows and aggregators: SELECT This is be done by submitting the store query to the Siddhi application runtime using its query() method. In order to execute store queries, the Siddhi application of the Siddhi application runtime you are using, should have a store defined, which contains the table that needs to be queried. Example If you need to query the table named RoomTypeTable the it should have been defined in the Siddhi application. In order to execute a store query on RoomTypeTable , you need to submit the store query using query() method of SiddhiAppRuntime instance as below. 1 siddhiAppRuntime . query ( store query );","title":"Store Query"},{"location":"docs/query-guide/#tablewindow-select","text":"The SELECT store query retrieves records from the specified table or window, based on the given condition. Syntax 1 2 3 4 5 6 7 from table / window on condition ? select attribute name , attribute name , ... group by ? having ? order by ? limit ? Example This query retrieves room numbers and types of the rooms starting from room no 10. 1 2 3 from roomTypeTable on roomNo = 10 ; select roomNo , type","title":"(Table/Window) Select"},{"location":"docs/query-guide/#aggregation-select","text":"The SELECT store query retrieves records from the specified aggregation, based on the given condition, time range, and granularity. Syntax 1 2 3 4 5 6 7 8 9 from aggregation on condition ? within time range per time granularity select attribute name , attribute name , ... group by ? having ? order by ? limit ? Example Following aggregation definition will be used for the examples. 1 2 3 4 5 6 7 define stream TradeStream ( symbol string , price double , volume long , timestamp long ); define aggregation TradeAggregation from TradeStream select symbol , avg ( price ) as avgPrice , sum ( price ) as total group by symbol aggregate by timestamp every sec ... year ; This query retrieves daily aggregations within the time range \"2014-02-15 00:00:00 +05:30\", \"2014-03-16 00:00:00 +05:30\" (Please note that +05:30 can be omitted if timezone is GMT) 1 2 3 4 from TradeAggregation within 2014-02-15 00:00:00 +05:30 , 2014-03-16 00:00:00 +05:30 per days select symbol , total , avgPrice ; This query retrieves hourly aggregations of \"FB\" symbol within the day 2014-02-15 . 1 2 3 4 5 from TradeAggregation on symbol == FB within 2014-02-15 **:**:** +05:30 per hours select symbol , total , avgPrice ;","title":"(Aggregation) Select"},{"location":"docs/query-guide/#insert_2","text":"This allows you to insert a new record to the table with the attribute values you define in the select section. Syntax 1 2 select attribute name , attribute name , ... insert into table ; Example This store query inserts a new record to the table RoomOccupancyTable , with the specified attribute values. 1 2 select 10 as roomNo , 2 as people insert into RoomOccupancyTable","title":"Insert"},{"location":"docs/query-guide/#delete_1","text":"The DELETE store query deletes selected records from a specified table. Syntax 1 2 3 select ? delete table on conditional expresssion The condition element specifies the basis on which records are selected to be deleted. Note Table attributes must always be referred to with the table name as shown below: table name . attibute name . Example In this example, query deletes a record in the table named RoomTypeTable if it has value for the roomNo attribute that matches the value for the roomNumber attribute of the selection which has 10 as the actual value. 1 2 3 select 10 as roomNumber delete RoomTypeTable on RoomTypeTable . roomNo == roomNumber ; 1 2 delete RoomTypeTable on RoomTypeTable . roomNo == 10 ;","title":"Delete"},{"location":"docs/query-guide/#update_1","text":"The UPDATE store query updates selected attributes stored in a specific table, based on a given condition. Syntax 1 2 3 4 select attribute name , attribute name , ... ? update table set table . attribute name = ( attribute name | expression ) ? , table . attribute name = ( attribute name | expression ) ? , ... on condition The condition element specifies the basis on which records are selected to be updated. When specifying the condition , table attributes must be referred to with the table name. You can use the set keyword to update selected attributes from the table. Here, for each assignment, the attribute specified in the left must be the table attribute, and the one specified in the right can be a stream/table attribute a mathematical operation, or other. When the set clause is not provided, all the attributes in the table are updated. Note Table attributes must always be referred to with the table name as shown below: table name . attibute name . Example The following query updates the room occupancy by increasing the value of people by 1, in the RoomOccupancyTable table for each room number greater than 10. 1 2 3 4 select 10 as roomNumber , 1 as arrival update RoomTypeTable set RoomTypeTable . people = RoomTypeTable . people + arrival on RoomTypeTable . roomNo == roomNumber ; 1 2 3 update RoomTypeTable set RoomTypeTable . people = RoomTypeTable . people + 1 on RoomTypeTable . roomNo == 10 ;","title":"Update"},{"location":"docs/query-guide/#update-or-insert_1","text":"This allows you to update selected attributes if a record that meets the given conditions already exists in the specified table. If a matching record does not exist, the entry is inserted as a new record. Syntax 1 2 3 4 select attribute name , attribute name , ... update or insert into table set table . attribute name = expression , table . attribute name = expression , ... on condition The condition element specifies the basis on which records are selected for update. When specifying the condition , table attributes should be referred to with the table name. If a record that matches the condition does not already exist in the table, the arriving event is inserted into the table. The set clause is only used when an update is performed during the insert/update operation. When set clause is used, the attribute to the left is always a table attribute, and the attribute to the right can be a stream/table attribute, mathematical operation or other. The attribute to the left (i.e., the attribute in the event table) is updated with the value of the attribute to the right if the given condition is met. When the set clause is not provided, all the attributes in the table are updated. Note Table attributes must always be referred to with the table name as shown below: table name . attibute name . Example The following query tries to update the records in the RoomAssigneeTable table that have room numbers that match the same in the selection. If such records are not found, it inserts a new record based on the values provided in the selection. 1 2 3 4 select 10 as roomNo , single as type , abc as assignee update or insert into RoomAssigneeTable set RoomAssigneeTable . assignee = assignee on RoomAssigneeTable . roomNo == roomNo ;","title":"Update or Insert"},{"location":"docs/query-guide/#extensions","text":"Siddhi supports an extension architecture to enhance its functionality by incorporating other libraries in a seamless manner. Purpose Extensions are supported because, Siddhi core cannot have all the functionality that's needed for all the use cases, mostly use cases require different type of functionality, and for some cases there can be gaps and you need to write the functionality by yourself. All extensions have a namespace. This is used to identify the relevant extensions together, and to let you specifically call the extension. Syntax Extensions follow the following syntax; 1 namespace : function name ( parameter , parameter , ... ) The following parameters are configured when referring a script function. Parameter Description namespace Allows Siddhi to identify the extension without conflict function name The name of the function referred. parameter The function input parameter for function execution. Extension Types Siddhi supports following extension types: Function For each event, it consumes zero or more parameters as input parameters and returns a single attribute. This can be used to manipulate existing event attributes to generate new attributes like any Function operation. This is implemented by extending io.siddhi.core.executor.function.FunctionExecutor . Example : math:sin(x) Here, the sin function of math extension returns the sin value for the x parameter. Aggregate Function For each event, it consumes zero or more parameters as input parameters and returns a single attribute with aggregated results. This can be used in conjunction with a window in order to find the aggregated results based on the given window like any Aggregate Function operation. This is implemented by extending io.siddhi.core.query.selector.attribute.aggregator.AttributeAggregatorExecutor . Example : custom:std(x) Here, the std aggregate function of custom extension returns the standard deviation of the x value based on its assigned window query. Window This allows events to be collected, generated, dropped and expired anytime without altering the event format based on the given input parameters, similar to any other Window operator. This is implemented by extending io.siddhi.core.query.processor.stream.window.WindowProcessor . Example : custom:unique(key) Here, the unique window of the custom extension retains one event for each unique key parameter. Stream Function This allows events to be generated or dropped only during event arrival and altered by adding one or more attributes to it. This is implemented by extending io.siddhi.core.query.processor.stream.function.StreamFunctionProcessor . Example : custom:pol2cart(theta,rho) Here, the pol2cart function of the custom extension returns all the events by calculating the cartesian coordinates x y and adding them as new attributes to the events. Stream Processor This allows events to be collected, generated, dropped and expired anytime by altering the event format by adding one or more attributes to it based on the given input parameters. Implemented by extending io.siddhi.core.query.processor.stream.StreamProcessor . Example : custom:perMinResults( parameter , parameter , ...) Here, the perMinResults function of the custom extension returns all events by adding one or more attributes to the events based on the conversion logic. Altered events are output every minute regardless of event arrivals. Sink Sinks provide a way to publish Siddhi events to external systems in the preferred data format. Sinks publish events from the streams via multiple transports to external endpoints in various data formats. Implemented by extending io.siddhi.core.stream.output.sink.Sink . Example : @sink(type='sink_type', static_option_key1='static_option_value1') To configure a stream to publish events via a sink, add the sink configuration to a stream definition by adding the @sink annotation with the required parameter values. The sink syntax is as above Source Source allows Siddhi to consume events from external systems , and map the events to adhere to the associated stream. Sources receive events via multiple transports and in various data formats, and direct them into streams for processing. Implemented by extending io.siddhi.core.stream.input.source.Source . Example : @source(type='source_type', static.option.key1='static_option_value1') To configure a stream that consumes events via a source, add the source configuration to a stream definition by adding the @source annotation with the required parameter values. The source syntax is as above Store You can use Store extension type to work with data/events stored in various data stores through the table abstraction . You can find more information about these extension types under the heading 'Extension types' in this document. Implemented by extending io.siddhi.core.table.record.AbstractRecordTable . Script Scripts allow you to define a function operation that is not provided in Siddhi core or its extension. It is not required to write an extension to define the function logic. Scripts allow you to write functions in other programming languages and execute them within Siddhi queries. Functions defined via scripts can be accessed in queries similar to any other inbuilt function. Implemented by extending io.siddhi.core.function.Script . Source Mapper Each @source configuration has a mapping denoted by the @map annotation that converts the incoming messages format to Siddhi events .The type parameter of the @map defines the map type to be used to map the data. The other parameters to be configured depends on the mapper selected. Some of these parameters are optional. Implemented by extending io.siddhi.core.stream.output.sink.SourceMapper . Example : @map(type='map_type', static_option_key1='static_option_value1') Sink Mapper Each @sink configuration has a mapping denoted by the @map annotation that converts the outgoing Siddhi events to configured messages format .The type parameter of the @map defines the map type to be used to map the data. The other parameters to be configured depends on the mapper selected. Some of these parameters are optional. Implemented by extending io.siddhi.core.stream.output.sink.SinkMapper . Example : @map(type='map_type', static_option_key1='static_option_value1') Example A window extension created with namespace foo and function name unique can be referred as follows: 1 2 3 from StockExchangeStream [ price = 20 ] # window . foo : unique ( symbol ) select symbol , price insert into StockQuote Available Extensions Siddhi currently has several pre written extensions that are available here We value your contribution on improving Siddhi and its extensions further.","title":"Extensions"},{"location":"docs/query-guide/#writing-custom-extensions","text":"Custom extensions can be written in order to cater use case specific logic that are not available in Siddhi out of the box or as an existing extension. There are five types of Siddhi extensions that you can write to cater your specific use cases. These extension types and the related maven archetypes are given below. You can use these archetypes to generate Maven projects for each extension type. Follow the procedure for the required archetype, based on your project: Note When using the generated archetype please make sure you complete the @Extension annotation with proper values. This annotation will be used to identify and document the extension, hence your extension will not work without @Extension annotation. siddhi-execution Siddhi-execution provides following extension types: Function Aggregate Function Stream Function Stream Processor Window You can use one or more from above mentioned extension types and implement according to your requirement. For more information about these extension types, see Extension Types . To install and implement the siddhi-io extension archetype, follow the procedure below: Issue the following command from your CLI. 1 2 3 4 5 mvn archetype:generate -DarchetypeGroupId=io.siddhi.extension.archetype -DarchetypeArtifactId=siddhi-archetype-execution -DgroupId=io.siddhi.extension.execution -Dversion=1.0.0-SNAPSHOT Enter the mandatory properties prompted, please see the description for all properties below. Properties Description Mandatory Default Value _nameOfFunction Name of the custom function to be created Y - _nameSpaceOfFunction Namespace of the function, used to grouped similar custom functions Y - groupIdPostfix Namespace of the function is added as postfix to the groupId as a convention N artifactId Artifact Id of the project N siddhi-execution- classNameOfAggregateFunction Class name of the Aggregate Function N $ classNameOfFunction Class name of the Function N $ classNameOfStreamFunction Class name of the Stream Function N $ classNameOfStreamProcessor Class name of the Stream Processor N $ classNameOfWindow Class name of the Window N $ To confirm that all property values are correct, type Y in the console. If not, press N . siddhi-io Siddhi-io provides following extension types: Sink Source You can use one or more from above mentioned extension types and implement according to your requirement. siddhi-io is generally used to work with IO operations as follows: * The Source extension type gets inputs to your Siddhi application. * The Sink extension publishes outputs from your Siddhi application. For more information about these extension types, see Extension Types . To implement the siddhi-io extension archetype, follow the procedure below: Issue the following command from your CLI. 1 2 3 4 5 mvn archetype:generate -DarchetypeGroupId=io.siddhi.extension.archetype -DarchetypeArtifactId=siddhi-archetype-io -DgroupId=io.siddhi.extension.io -Dversion=1.0.0-SNAPSHOT Enter the mandatory properties prompted, please see the description for all properties below. Properties Description Mandatory Default Value _IOType Type of IO for which Siddhi-io extension is written Y - groupIdPostfix Type of the IO is added as postfix to the groupId as a convention N artifactId Artifact Id of the project N siddhi-io- classNameOfSink Class name of the Sink N classNameOfSource Class name of the Source N To confirm that all property values are correct, type Y in the console. If not, press N . siddhi-map Siddhi-map provides following extension types, Sink Mapper Source Mapper You can use one or more from above mentioned extension types and implement according to your requirement as follows. The Source Mapper maps events to a predefined data format (such as XML, JSON, binary, etc), and publishes them to external endpoints (such as E-mail, TCP, Kafka, HTTP, etc). The Sink Mapper also maps events to a predefined data format, but it does it at the time of publishing events from a Siddhi application. For more information about these extension types, see Extension Types . To implement the siddhi-map extension archetype, follow the procedure below: Issue the following command from your CLI. 1 2 3 4 5 mvn archetype:generate -DarchetypeGroupId=io.siddhi.extension.archetype -DarchetypeArtifactId=siddhi-archetype-map -DgroupId=io.siddhi.extension.map -Dversion=1.0.0-SNAPSHOT Enter the mandatory properties prompted, please see the description for all properties below. Properties Description Mandatory Default Value _mapType Type of Mapper for which Siddhi-map extension is written Y - groupIdPostfix Type of the Map is added as postfix to the groupId as a convention N artifactId Artifact Id of the project N siddhi-map- classNameOfSinkMapper Class name of the Sink Mapper N classNameOfSourceMapper Class name of the Source Mapper N To confirm that all property values are correct, type Y in the console. If not, press N . siddhi-script Siddhi-script provides the Script extension type. The script extension type allows you to write functions in other programming languages and execute them within Siddhi queries. Functions defined via scripts can be accessed in queries similar to any other inbuilt function. For more information about these extension types, see Extension Types . To implement the siddhi-script extension archetype, follow the procedure below: Issue the following command from your CLI. 1 2 3 4 5 mvn archetype:generate -DarchetypeGroupId=io.siddhi.extension.archetype -DarchetypeArtifactId=siddhi-archetype-script -DgroupId=io.siddhi.extension.script -Dversion=1.0.0-SNAPSHOT Enter the mandatory properties prompted, please see the description for all properties below. Properties Description Mandatory Default Value _nameOfScript Name of Custom Script for which Siddhi-script extension is written Y - groupIdPostfix Name of the Script is added as postfix to the groupId as a convention N artifactId Artifact Id of the project N siddhi-script- classNameOfScript Class name of the Script N Eval To confirm that all property values are correct, type Y in the console. If not, press N . siddhi-store Siddhi-store provides the Store extension type. The Store extension type allows you to work with data/events stored in various data stores through the table abstraction. For more information about these extension types, see Extension Types . To implement the siddhi-store extension archetype, follow the procedure below: Issue the following command from your CLI. 1 2 3 4 5 mvn archetype:generate -DarchetypeGroupId=io.siddhi.extension.archetype -DarchetypeArtifactId=siddhi-archetype-store -DgroupId=io.siddhi.extension.store -Dversion=1.0.0-SNAPSHOT Enter the mandatory properties prompted, please see the description for all properties below. Properties Description Mandatory Default Value _storeType Type of Store for which Siddhi-store extension is written Y - groupIdPostfix Type of the Store is added as postfix to the groupId as a convention N artifactId Artifact Id of the project N siddhi-store- className Class name of the Store N To confirm that all property values are correct, type Y in the console. If not, press N .","title":"Writing Custom Extensions"},{"location":"docs/query-guide/#configuring-and-monitoring-siddhi-applications","text":"","title":"Configuring and Monitoring Siddhi Applications"},{"location":"docs/query-guide/#multi-threading-and-asynchronous-processing","text":"When @Async annotation is added to the Streams it enable the Streams to introduce asynchronous and multi-threading behavior. 1 2 @ Async ( buffer . size = 256 , workers = 2 , batch . size . max = 5 ) define stream stream name ( attribute name attribute type , attribute name attribute type , ... ); The following elements are configured with this annotation. Annotation Description Default Value buffer.size The size of the event buffer that will be used to handover the execution to other threads. - workers Number of worker threads that will be be used to process the buffered events. 1 batch.size.max The maximum number of events that will be processed together by a worker thread at a given time. buffer.size","title":"Multi-threading and Asynchronous Processing"},{"location":"docs/query-guide/#statistics","text":"Use @app:statistics app level annotation to evaluate the performance of an application, you can enable the statistics of a Siddhi application to be published. This is done via the @app:statistics annotation that can be added to a Siddhi application as shown in the following example. 1 @ app : statistics ( reporter = console ) The following elements are configured with this annotation. Annotation Description Default Value reporter The interface in which statistics for the Siddhi application are published. Possible values are as follows: console jmx console interval The time interval (in seconds) at which the statistics for the Siddhi application are reported. 60 include If this parameter is added, only the types of metrics you specify are included in the reporting. The required metric types can be specified as a comma-separated list. It is also possible to use wild cards All ( . ) The metrics are reported in the following format. io.siddhi.SiddhiApps. SiddhiAppName .Siddhi. Component Type . Component Name . Metrics name The following table lists the types of metrics supported for different Siddhi application component types. Component Type Metrics Type Stream Throughput The size of the buffer if parallel processing is enabled via the @async annotation. Trigger Throughput (Trigger and Stream) Source Throughput Sink Throughput Mapper Latency Input/output throughput Table Memory Throughput (For all operations) Throughput (For all operations) Query Memory Latency Window Throughput (For all operations) Latency (For all operation) Partition Throughput (For all operations) Latency (For all operation) e.g., the following is a Siddhi application that includes the @app annotation to report performance statistics. 1 2 3 4 5 6 7 8 @ App : name ( TestMetrics ) @ App : Statistics ( reporter = console ) define stream TestStream ( message string ); @ info ( name = logQuery ) from TestSream # log ( Message: ) insert into TempSream ; Statistics are reported for this Siddhi application as shown in the extract below. Click to view the extract 11/26/17 8:01:20 PM ============================================================ -- Gauges ---------------------------------------------------------------------- io.siddhi.SiddhiApps.TestMetrics.Siddhi.Queries.logQuery.memory value = 5760 io.siddhi.SiddhiApps.TestMetrics.Siddhi.Streams.TestStream.size value = 0 -- Meters ---------------------------------------------------------------------- io.siddhi.SiddhiApps.TestMetrics.Siddhi.Sources.TestStream.http.throughput count = 0 mean rate = 0.00 events/second 1-minute rate = 0.00 events/second 5-minute rate = 0.00 events/second 15-minute rate = 0.00 events/second io.siddhi.SiddhiApps.TestMetrics.Siddhi.Streams.TempSream.throughput count = 2 mean rate = 0.04 events/second 1-minute rate = 0.03 events/second 5-minute rate = 0.01 events/second 15-minute rate = 0.00 events/second io.siddhi.SiddhiApps.TestMetrics.Siddhi.Streams.TestStream.throughput count = 2 mean rate = 0.04 events/second 1-minute rate = 0.03 events/second 5-minute rate = 0.01 events/second 15-minute rate = 0.00 events/second -- Timers ---------------------------------------------------------------------- io.siddhi.SiddhiApps.TestMetrics.Siddhi.Queries.logQuery.latency count = 2 mean rate = 0.11 calls/second 1-minute rate = 0.34 calls/second 5-minute rate = 0.39 calls/second 15-minute rate = 0.40 calls/second min = 0.61 milliseconds max = 1.08 milliseconds mean = 0.84 milliseconds stddev = 0.23 milliseconds median = 0.61 milliseconds 75% < = 1.08 milliseconds 95% < = 1.08 milliseconds 98% < = 1.08 milliseconds 99% < = 1.08 milliseconds 99.9% < = 1.08 milliseconds","title":"Statistics"},{"location":"docs/query-guide/#event-playback","text":"When @app:playback annotation is added to the app, the timestamp of the event (specified via an attribute) is treated as the current time. This results in events being processed faster. The following elements are configured with this annotation. Annotation Description idle.time If no events are received during a time interval specified (in milliseconds) via this element, the Siddhi system time is incremented by a number of seconds specified via the increment element. increment The number of seconds by which the Siddhi system time must be incremented if no events are received during the time interval specified via the idle.time element. e.g., In the following example, the Siddhi system time is incremented by two seconds if no events arrive for a time interval of 100 milliseconds. @app:playback(idle.time = '100 millisecond', increment = '2 sec')","title":"Event Playback"},{"location":"docs/siddhi-as-a-docker-microservice/","text":"Siddhi 5.1 as a Docker Microservice This section provides information on running Siddhi Apps on Docker. Siddhi Microservice can run one or more Siddhi Applications with required system configurations. Here, the Siddhi application ( .siddhi file) contains stream processing logic and the necessary system configurations can be passed via the Siddhi configuration .yaml file. Steps to Run Siddhi Docker Microservice is as follows. Pull the the latest Siddhi Runner image from Siddhiio Docker Hub . 1 docker pull siddhiio/siddhi-runner-alpine:latest Start SiddhiApps with the runner config by executing the following docker command. 1 docker run -it -v local-siddhi-file-path : siddhi-file-mount-path -v local-conf-file-path : conf-file-mount-path siddhiio/siddhi-runner-alpine:latest -Dapps= siddhi-file-mount-path -Dconfig= conf-file-mount-path E.g., 1 docker run -it -v /home/me/siddhi-apps:/apps -v /home/me/siddhi-configs:/configs siddhiio/siddhi-runner-alpine:latest -Dapps=/apps/Foo.siddhi -Dconfig=/configs/siddhi-config.yaml Running multiple SiddhiApps in one runner instance. To run multiple SiddhiApps in one runtime instance, have all SiddhiApps in a directory, mount the directory and pass its location through -Dapps parameter as follows, -Dapps= siddhi-apps-directory Always use absolute path for SiddhiApps and runner configs. Providing absolute path of SiddhiApp file, or directory in -Dapps parameter, and when providing the Siddhi runner config yaml on -Dconfig parameter while starting Siddhi runner. Siddhi Tooling You can also use the powerful Siddhi Editor to implement and test steam processing applications. Configuring Siddhi To configure databases, extensions, authentication, periodic state persistence, and statistics for Siddhi as Docker Microservice refer Siddhi Config Guide . Samples Running Siddhi App Following SiddhiApp collects events via HTTP and logs the number of events arrived during last 15 seconds. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 @ App : name ( CountOverTime ) @ App : description ( Receive events via HTTP, and logs the number of events received during last 15 seconds ) @ source ( type = http , receiver . url = http://0.0.0.0:8006/production , @ map ( type = json )) define stream ProductionStream ( name string , amount double ); @ sink ( type = log ) define stream TotalCountStream ( totalCount long ); -- Count the incoming events @ info ( name = query1 ) from ProductionStream # window . time ( 15 sec ) select count () as totalCount insert into TotalCountStream ; Always listen on 0.0.0.0 with the Siddhi Application running inside a docker container. If you listen on localhost inside the container, nothing outside the container can connect to your application. That includes blocking port forwarding from the docker host and container to container networking. Copy the above SiddhiApp, and create the SiddhiApp file CountOverTime.siddhi . Run the SiddhiApp by executing following commands from the distribution directory docker run -it -p 8006:8006 -v local-absolute-siddhi-file-path /CountOverTime.siddhi:/apps/CountOverTime.siddhi siddhiio/siddhi-runner-alpine -Dapps=/apps/CountOverTime.siddhi Test the SiddhiApp by calling the HTTP endpoint using curl or Postman as follows Publish events with curl command: Publish few json to the http endpoint as follows, curl -X POST http://localhost:8006/production \\ --header \"Content-Type:application/json\" \\ -d '{\"event\":{\"name\":\"Cake\",\"amount\":20.12}}' Publish events with Postman: Install 'Postman' application from Chrome web store Launch the application Make a 'Post' request to 'http://localhost:8006/production' endpoint. Set the Content-Type to 'application/json' and set the request body in json format as follows, { \"event\": { \"name\": \"Cake\", \"amount\": 20.12 } } Runner logs the total count on the console. Note, how the count increments with every event sent. [2019-04-11 13:36:03,517] INFO {io.siddhi.core.stream.output.sink.LogSink} - CountOverTime : TotalCountStream : Event{timestamp=1554969963512, data=[1], isExpired=false} [2019-04-11 13:36:10,267] INFO {io.siddhi.core.stream.output.sink.LogSink} - CountOverTime : TotalCountStream : Event{timestamp=1554969970267, data=[2], isExpired=false} [2019-04-11 13:36:41,694] INFO {io.siddhi.core.stream.output.sink.LogSink} - CountOverTime : TotalCountStream : Event{timestamp=1554970001694, data=[1], isExpired=false} Running with runner config When running SiddhiApps users can optionally provide a config yaml to Siddhi runner to manage configurations such as state persistence, databases connections and secure vault. Following SiddhiApp collects events via HTTP and store them in H2 Database. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 @ App : name ( ConsumeAndStore ) @ App : description ( Consume events from HTTP and write to TEST_DB ) @ source ( type = http , receiver . url = http://0.0.0.0:8006/production , @ map ( type = json )) define stream ProductionStream ( name string , amount double ); @ store ( type = rdbms , datasource = TEST_DB ) define table ProductionTable ( name string , amount double ); -- Store all events to the table @ info ( name = query1 ) from ProductionStream insert into ProductionTable ; The runner config can be configured with the relevant datasource information and passed when starting the runner 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 wso2.datasources : dataSources : - name : TEST_DB description : The datasource used for testing definition : type : RDBMS configuration : jdbcUrl : jdbc:h2:${sys:carbon.home}/wso2/${sys:wso2.runtime}/database/TEST_DB;DB_CLOSE_ON_EXIT=FALSE;LOCK_TIMEOUT=60000 username : admin password : admin driverClassName : org.h2.Driver maxPoolSize : 10 idleTimeout : 60000 connectionTestQuery : SELECT 1 validationTimeout : 30000 isAutoCommit : false Copy the above SiddhiApp, & config yaml, and create corresponding the SiddhiApp file ConsumeAndStore.siddhi and TestDb.yaml files. Run the SiddhiApp by executing following command docker run -it -p 8006:8006 -p 9443:9443 -v local-absolute-siddhi-file-path /ConsumeAndStore.siddhi:/apps/ConsumeAndStore.siddhi -v local-absolute-config-yaml-path /TestDb.yaml:/conf/TestDb.yaml siddhiio/siddhi-runner-alpine -Dapps=/apps/ConsumeAndStore.siddhi -Dconfig=/conf/TestDb.yaml Test the SiddhiApp by calling the HTTP endpoint using curl or Postman as follows Publish events with curl command: Publish few json to the http endpoint as follows, curl -X POST http://localhost:8006/production \\ --header \"Content-Type:application/json\" \\ -d '{\"event\":{\"name\":\"Cake\",\"amount\":20.12}}' Publish events with Postman: Install 'Postman' application from Chrome web store Launch the application Make a 'Post' request to 'http://localhost:8006/production' endpoint. Set the Content-Type to 'application/json' and set the request body in json format as follows, { \"event\": { \"name\": \"Cake\", \"amount\": 20.12 } } Query Siddhi Store APIs to retrieve 10 records from the table. Query stored events with curl command: Publish few json to the http endpoint as follows, curl -X POST https://localhost:9443/stores/query \\ -H \"content-type: application/json\" \\ -u \"admin:admin\" \\ -d '{\"appName\" : \"ConsumeAndStore\", \"query\" : \"from ProductionTable select * limit 10;\" }' -k Query stored events with Postman: Install 'Postman' application from Chrome web store Launch the application Make a 'Post' request to 'https://localhost:9443/stores/query' endpoint. Set the Content-Type to 'application/json' and set the request body in json format as follows, { \"appName\" : \"ConsumeAndStore\", \"query\" : \"from ProductionTable select * limit 10;\" } The results of the query will be as follows, { \"records\":[ [\"Cake\",20.12] ] } Running with environmental/system variables Templating SiddhiApps allows users to provide environment/system variables to siddhiApps at runtime. This can help users to migrate SiddhiApps from one environment to another (E.g from dev, test and to prod). Following templated SiddhiApp collects events via HTTP, filters them based on amount greater than a given threshold value, and only sends the filtered events via email. Here the THRESHOLD value, and TO_EMAIL are templated in the TemplatedFilterAndEmail.siddhi SiddhiApp. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 @ App : name ( TemplatedFilterAndEmail ) @ App : description ( Consumes events from HTTP, filters them based on amount greater than a templated threshold value, and sends filtered events via email. ) @ source ( type = http , receiver . url = http://0.0.0.0:8006/production , @ map ( type = json )) define stream ProductionStream ( name string , amount double ); @ sink ( ref = email-sink , subject = High {{name}} production! , to = ${TO_EMAIL} , content . type = text/html , @ map ( type = text , @ payload ( Hi, br/ br/ High production of b {{name}}, /b with amount b {{amount}} /b identified. br/ br/ For more information please contact production department. br/ br/ Thank you ))) define stream FilteredProductionStream ( name string , amount double ); -- Filters the events based on threshold @ info ( name = query1 ) from ProductionStream [ amount ${ THRESHOLD } ] insert into FilteredProductionStream ; The runner config is configured with a gmail account to send email messages in EmailConfig.yaml by templating sending EMAIL_ADDRESS , EMAIL_USERNAME and EMAIL_PASSWORD . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 siddhi : refs : - ref : name : email-sink type : email properties : port : 465 host : smtp.gmail.com ssl.enable : true auth : true ssl.enable : true # User your gmail configurations here address : ${EMAIL_ADDRESS} #E.g. test@gmail.com username : ${EMAIL_USERNAME} #E.g. test password : ${EMAIL_PASSWORD} #E.g. password Copy the above SiddhiApp, & config yaml, and create corresponding the SiddhiApp file TemplatedFilterAndEmail.siddhi and EmailConfig.yaml files. Set the below environment variables by passing them during the docker run command: THRESHOLD=20 TO_EMAIL= to email address EMAIL_ADDRESS= gmail address EMAIL_USERNAME= gmail username EMAIL_PASSWORD= gmail password Or they can also be passed as system variables by adding them to the end of the docker run command . -DTHRESHOLD=20 -DTO_EMAIL= to email address -DEMAIL_ADDRESS= gmail address -DEMAIL_USERNAME= gmail username -DEMAIL_PASSWORD= gmail password Run the SiddhiApp by executing following command. docker run -it -p 8006:8006 -v local-absolute-siddhi-file-path /TemplatedFilterAndEmail.siddhi:/apps/TemplatedFilterAndEmail.siddhi -v local-absolute-config-yaml-path /EmailConfig.yaml:/conf/EmailConfig.yaml -e THRESHOLD=20 -e TO_EMAIL= to email address -e EMAIL_ADDRESS= gmail address -e EMAIL_USERNAME= gmail username -e EMAIL_PASSWORD= gmail password siddhiio/siddhi-runner-alpine -Dapps=/apps/TemplatedFilterAndEmail.siddhi -Dconfig=/conf/EmailConfig.yaml Test the SiddhiApp by calling the HTTP endpoint using curl or Postman as follows Publish events with curl command: Publish few json to the http endpoint as follows, curl -X POST http://localhost:8006/production \\ --header \"Content-Type:application/json\" \\ -d '{\"event\":{\"name\":\"Cake\",\"amount\":2000.0}}' Publish events with Postman: Install 'Postman' application from Chrome web store Launch the application Make a 'Post' request to 'http://localhost:8006/production' endpoint. Set the Content-Type to 'application/json' and set the request body in json format as follows, { \"event\": { \"name\": \"Cake\", \"amount\": 2000.0 } } Check the to.email for the published email message, which will look as follows, Subject : High Cake production! Hi, High production of Cake, with amount 2000.0 identified. For more information please contact production department. Thank you","title":"Siddhi Docker Microservice"},{"location":"docs/siddhi-as-a-docker-microservice/#siddhi-51-as-a-docker-microservice","text":"This section provides information on running Siddhi Apps on Docker. Siddhi Microservice can run one or more Siddhi Applications with required system configurations. Here, the Siddhi application ( .siddhi file) contains stream processing logic and the necessary system configurations can be passed via the Siddhi configuration .yaml file. Steps to Run Siddhi Docker Microservice is as follows. Pull the the latest Siddhi Runner image from Siddhiio Docker Hub . 1 docker pull siddhiio/siddhi-runner-alpine:latest Start SiddhiApps with the runner config by executing the following docker command. 1 docker run -it -v local-siddhi-file-path : siddhi-file-mount-path -v local-conf-file-path : conf-file-mount-path siddhiio/siddhi-runner-alpine:latest -Dapps= siddhi-file-mount-path -Dconfig= conf-file-mount-path E.g., 1 docker run -it -v /home/me/siddhi-apps:/apps -v /home/me/siddhi-configs:/configs siddhiio/siddhi-runner-alpine:latest -Dapps=/apps/Foo.siddhi -Dconfig=/configs/siddhi-config.yaml Running multiple SiddhiApps in one runner instance. To run multiple SiddhiApps in one runtime instance, have all SiddhiApps in a directory, mount the directory and pass its location through -Dapps parameter as follows, -Dapps= siddhi-apps-directory Always use absolute path for SiddhiApps and runner configs. Providing absolute path of SiddhiApp file, or directory in -Dapps parameter, and when providing the Siddhi runner config yaml on -Dconfig parameter while starting Siddhi runner. Siddhi Tooling You can also use the powerful Siddhi Editor to implement and test steam processing applications. Configuring Siddhi To configure databases, extensions, authentication, periodic state persistence, and statistics for Siddhi as Docker Microservice refer Siddhi Config Guide .","title":"Siddhi 5.1 as a Docker Microservice"},{"location":"docs/siddhi-as-a-docker-microservice/#samples","text":"","title":"Samples"},{"location":"docs/siddhi-as-a-docker-microservice/#running-siddhi-app","text":"Following SiddhiApp collects events via HTTP and logs the number of events arrived during last 15 seconds. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 @ App : name ( CountOverTime ) @ App : description ( Receive events via HTTP, and logs the number of events received during last 15 seconds ) @ source ( type = http , receiver . url = http://0.0.0.0:8006/production , @ map ( type = json )) define stream ProductionStream ( name string , amount double ); @ sink ( type = log ) define stream TotalCountStream ( totalCount long ); -- Count the incoming events @ info ( name = query1 ) from ProductionStream # window . time ( 15 sec ) select count () as totalCount insert into TotalCountStream ; Always listen on 0.0.0.0 with the Siddhi Application running inside a docker container. If you listen on localhost inside the container, nothing outside the container can connect to your application. That includes blocking port forwarding from the docker host and container to container networking. Copy the above SiddhiApp, and create the SiddhiApp file CountOverTime.siddhi . Run the SiddhiApp by executing following commands from the distribution directory docker run -it -p 8006:8006 -v local-absolute-siddhi-file-path /CountOverTime.siddhi:/apps/CountOverTime.siddhi siddhiio/siddhi-runner-alpine -Dapps=/apps/CountOverTime.siddhi Test the SiddhiApp by calling the HTTP endpoint using curl or Postman as follows Publish events with curl command: Publish few json to the http endpoint as follows, curl -X POST http://localhost:8006/production \\ --header \"Content-Type:application/json\" \\ -d '{\"event\":{\"name\":\"Cake\",\"amount\":20.12}}' Publish events with Postman: Install 'Postman' application from Chrome web store Launch the application Make a 'Post' request to 'http://localhost:8006/production' endpoint. Set the Content-Type to 'application/json' and set the request body in json format as follows, { \"event\": { \"name\": \"Cake\", \"amount\": 20.12 } } Runner logs the total count on the console. Note, how the count increments with every event sent. [2019-04-11 13:36:03,517] INFO {io.siddhi.core.stream.output.sink.LogSink} - CountOverTime : TotalCountStream : Event{timestamp=1554969963512, data=[1], isExpired=false} [2019-04-11 13:36:10,267] INFO {io.siddhi.core.stream.output.sink.LogSink} - CountOverTime : TotalCountStream : Event{timestamp=1554969970267, data=[2], isExpired=false} [2019-04-11 13:36:41,694] INFO {io.siddhi.core.stream.output.sink.LogSink} - CountOverTime : TotalCountStream : Event{timestamp=1554970001694, data=[1], isExpired=false}","title":"Running Siddhi App"},{"location":"docs/siddhi-as-a-docker-microservice/#running-with-runner-config","text":"When running SiddhiApps users can optionally provide a config yaml to Siddhi runner to manage configurations such as state persistence, databases connections and secure vault. Following SiddhiApp collects events via HTTP and store them in H2 Database. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 @ App : name ( ConsumeAndStore ) @ App : description ( Consume events from HTTP and write to TEST_DB ) @ source ( type = http , receiver . url = http://0.0.0.0:8006/production , @ map ( type = json )) define stream ProductionStream ( name string , amount double ); @ store ( type = rdbms , datasource = TEST_DB ) define table ProductionTable ( name string , amount double ); -- Store all events to the table @ info ( name = query1 ) from ProductionStream insert into ProductionTable ; The runner config can be configured with the relevant datasource information and passed when starting the runner 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 wso2.datasources : dataSources : - name : TEST_DB description : The datasource used for testing definition : type : RDBMS configuration : jdbcUrl : jdbc:h2:${sys:carbon.home}/wso2/${sys:wso2.runtime}/database/TEST_DB;DB_CLOSE_ON_EXIT=FALSE;LOCK_TIMEOUT=60000 username : admin password : admin driverClassName : org.h2.Driver maxPoolSize : 10 idleTimeout : 60000 connectionTestQuery : SELECT 1 validationTimeout : 30000 isAutoCommit : false Copy the above SiddhiApp, & config yaml, and create corresponding the SiddhiApp file ConsumeAndStore.siddhi and TestDb.yaml files. Run the SiddhiApp by executing following command docker run -it -p 8006:8006 -p 9443:9443 -v local-absolute-siddhi-file-path /ConsumeAndStore.siddhi:/apps/ConsumeAndStore.siddhi -v local-absolute-config-yaml-path /TestDb.yaml:/conf/TestDb.yaml siddhiio/siddhi-runner-alpine -Dapps=/apps/ConsumeAndStore.siddhi -Dconfig=/conf/TestDb.yaml Test the SiddhiApp by calling the HTTP endpoint using curl or Postman as follows Publish events with curl command: Publish few json to the http endpoint as follows, curl -X POST http://localhost:8006/production \\ --header \"Content-Type:application/json\" \\ -d '{\"event\":{\"name\":\"Cake\",\"amount\":20.12}}' Publish events with Postman: Install 'Postman' application from Chrome web store Launch the application Make a 'Post' request to 'http://localhost:8006/production' endpoint. Set the Content-Type to 'application/json' and set the request body in json format as follows, { \"event\": { \"name\": \"Cake\", \"amount\": 20.12 } } Query Siddhi Store APIs to retrieve 10 records from the table. Query stored events with curl command: Publish few json to the http endpoint as follows, curl -X POST https://localhost:9443/stores/query \\ -H \"content-type: application/json\" \\ -u \"admin:admin\" \\ -d '{\"appName\" : \"ConsumeAndStore\", \"query\" : \"from ProductionTable select * limit 10;\" }' -k Query stored events with Postman: Install 'Postman' application from Chrome web store Launch the application Make a 'Post' request to 'https://localhost:9443/stores/query' endpoint. Set the Content-Type to 'application/json' and set the request body in json format as follows, { \"appName\" : \"ConsumeAndStore\", \"query\" : \"from ProductionTable select * limit 10;\" } The results of the query will be as follows, { \"records\":[ [\"Cake\",20.12] ] }","title":"Running with runner config"},{"location":"docs/siddhi-as-a-docker-microservice/#running-with-environmentalsystem-variables","text":"Templating SiddhiApps allows users to provide environment/system variables to siddhiApps at runtime. This can help users to migrate SiddhiApps from one environment to another (E.g from dev, test and to prod). Following templated SiddhiApp collects events via HTTP, filters them based on amount greater than a given threshold value, and only sends the filtered events via email. Here the THRESHOLD value, and TO_EMAIL are templated in the TemplatedFilterAndEmail.siddhi SiddhiApp. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 @ App : name ( TemplatedFilterAndEmail ) @ App : description ( Consumes events from HTTP, filters them based on amount greater than a templated threshold value, and sends filtered events via email. ) @ source ( type = http , receiver . url = http://0.0.0.0:8006/production , @ map ( type = json )) define stream ProductionStream ( name string , amount double ); @ sink ( ref = email-sink , subject = High {{name}} production! , to = ${TO_EMAIL} , content . type = text/html , @ map ( type = text , @ payload ( Hi, br/ br/ High production of b {{name}}, /b with amount b {{amount}} /b identified. br/ br/ For more information please contact production department. br/ br/ Thank you ))) define stream FilteredProductionStream ( name string , amount double ); -- Filters the events based on threshold @ info ( name = query1 ) from ProductionStream [ amount ${ THRESHOLD } ] insert into FilteredProductionStream ; The runner config is configured with a gmail account to send email messages in EmailConfig.yaml by templating sending EMAIL_ADDRESS , EMAIL_USERNAME and EMAIL_PASSWORD . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 siddhi : refs : - ref : name : email-sink type : email properties : port : 465 host : smtp.gmail.com ssl.enable : true auth : true ssl.enable : true # User your gmail configurations here address : ${EMAIL_ADDRESS} #E.g. test@gmail.com username : ${EMAIL_USERNAME} #E.g. test password : ${EMAIL_PASSWORD} #E.g. password Copy the above SiddhiApp, & config yaml, and create corresponding the SiddhiApp file TemplatedFilterAndEmail.siddhi and EmailConfig.yaml files. Set the below environment variables by passing them during the docker run command: THRESHOLD=20 TO_EMAIL= to email address EMAIL_ADDRESS= gmail address EMAIL_USERNAME= gmail username EMAIL_PASSWORD= gmail password Or they can also be passed as system variables by adding them to the end of the docker run command . -DTHRESHOLD=20 -DTO_EMAIL= to email address -DEMAIL_ADDRESS= gmail address -DEMAIL_USERNAME= gmail username -DEMAIL_PASSWORD= gmail password Run the SiddhiApp by executing following command. docker run -it -p 8006:8006 -v local-absolute-siddhi-file-path /TemplatedFilterAndEmail.siddhi:/apps/TemplatedFilterAndEmail.siddhi -v local-absolute-config-yaml-path /EmailConfig.yaml:/conf/EmailConfig.yaml -e THRESHOLD=20 -e TO_EMAIL= to email address -e EMAIL_ADDRESS= gmail address -e EMAIL_USERNAME= gmail username -e EMAIL_PASSWORD= gmail password siddhiio/siddhi-runner-alpine -Dapps=/apps/TemplatedFilterAndEmail.siddhi -Dconfig=/conf/EmailConfig.yaml Test the SiddhiApp by calling the HTTP endpoint using curl or Postman as follows Publish events with curl command: Publish few json to the http endpoint as follows, curl -X POST http://localhost:8006/production \\ --header \"Content-Type:application/json\" \\ -d '{\"event\":{\"name\":\"Cake\",\"amount\":2000.0}}' Publish events with Postman: Install 'Postman' application from Chrome web store Launch the application Make a 'Post' request to 'http://localhost:8006/production' endpoint. Set the Content-Type to 'application/json' and set the request body in json format as follows, { \"event\": { \"name\": \"Cake\", \"amount\": 2000.0 } } Check the to.email for the published email message, which will look as follows, Subject : High Cake production! Hi, High production of Cake, with amount 2000.0 identified. For more information please contact production department. Thank you","title":"Running with environmental/system variables"},{"location":"docs/siddhi-as-a-java-library/","text":"Siddhi 5.1 as a Java library Siddhi can be used as a library in any Java program (including in OSGi runtimes) just by adding Siddhi and its extension jars as dependencies. Find a sample Siddhi project that's implemented as a Java program using Maven here , this can be used as a reference for any based implementation. Following are the mandatory dependencies that need to be added to the Maven pom.xml file (or to the program classpath). 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 dependency groupId io.siddhi /groupId artifactId siddhi-core /artifactId version 5.x.x /version /dependency dependency groupId io.siddhi /groupId artifactId siddhi-query-api /artifactId version 5.x.x /version /dependency dependency groupId io.siddhi /groupId artifactId siddhi-query-compiler /artifactId version 5.x.x /version /dependency dependency groupId io.siddhi /groupId artifactId siddhi-annotations /artifactId version 5.x.x /version /dependency Sample Sample Java class using Siddhi is as follows. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 package io.siddhi.sample ; import io.siddhi.core.SiddhiAppRuntime ; import io.siddhi.core.SiddhiManager ; import io.siddhi.core.event.Event ; import io.siddhi.core.stream.input.InputHandler ; import io.siddhi.core.stream.output.StreamCallback ; import io.siddhi.core.util.EventPrinter ; /** * The sample demonstrate how to use Siddhi within another Java program. * This sample contains a simple filter query. */ public class SimpleFilterSample { public static void main ( String [] args ) throws InterruptedException { // Create Siddhi Manager SiddhiManager siddhiManager = new SiddhiManager (); //Siddhi Application String siddhiApp = + define stream StockStream (symbol string, price float, volume long); + + @info(name = query1 ) + from StockStream[volume 150] + select symbol, price + insert into OutputStream; ; //Generate runtime SiddhiAppRuntime siddhiAppRuntime = siddhiManager . createSiddhiAppRuntime ( siddhiApp ); //Adding callback to retrieve output events from stream siddhiAppRuntime . addCallback ( OutputStream , new StreamCallback () { @Override public void receive ( Event [] events ) { EventPrinter . print ( events ); //To convert and print event as a map //EventPrinter.print(toMap(events)); } }); //Get InputHandler to push events into Siddhi InputHandler inputHandler = siddhiAppRuntime . getInputHandler ( StockStream ); //Start processing siddhiAppRuntime . start (); //Sending events to Siddhi inputHandler . send ( new Object []{ IBM , 700 f , 100L }); inputHandler . send ( new Object []{ WSO2 , 60.5f , 200L }); inputHandler . send ( new Object []{ GOOG , 50 f , 30L }); inputHandler . send ( new Object []{ IBM , 76.6f , 400L }); inputHandler . send ( new Object []{ WSO2 , 45.6f , 50L }); Thread . sleep ( 500 ); //Shutdown runtime siddhiAppRuntime . shutdown (); //Shutdown Siddhi Manager siddhiManager . shutdown (); } }","title":"Siddhi Java library"},{"location":"docs/siddhi-as-a-java-library/#siddhi-51-as-a-java-library","text":"Siddhi can be used as a library in any Java program (including in OSGi runtimes) just by adding Siddhi and its extension jars as dependencies. Find a sample Siddhi project that's implemented as a Java program using Maven here , this can be used as a reference for any based implementation. Following are the mandatory dependencies that need to be added to the Maven pom.xml file (or to the program classpath). 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 dependency groupId io.siddhi /groupId artifactId siddhi-core /artifactId version 5.x.x /version /dependency dependency groupId io.siddhi /groupId artifactId siddhi-query-api /artifactId version 5.x.x /version /dependency dependency groupId io.siddhi /groupId artifactId siddhi-query-compiler /artifactId version 5.x.x /version /dependency dependency groupId io.siddhi /groupId artifactId siddhi-annotations /artifactId version 5.x.x /version /dependency","title":"Siddhi 5.1 as a Java library"},{"location":"docs/siddhi-as-a-java-library/#sample","text":"Sample Java class using Siddhi is as follows. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 package io.siddhi.sample ; import io.siddhi.core.SiddhiAppRuntime ; import io.siddhi.core.SiddhiManager ; import io.siddhi.core.event.Event ; import io.siddhi.core.stream.input.InputHandler ; import io.siddhi.core.stream.output.StreamCallback ; import io.siddhi.core.util.EventPrinter ; /** * The sample demonstrate how to use Siddhi within another Java program. * This sample contains a simple filter query. */ public class SimpleFilterSample { public static void main ( String [] args ) throws InterruptedException { // Create Siddhi Manager SiddhiManager siddhiManager = new SiddhiManager (); //Siddhi Application String siddhiApp = + define stream StockStream (symbol string, price float, volume long); + + @info(name = query1 ) + from StockStream[volume 150] + select symbol, price + insert into OutputStream; ; //Generate runtime SiddhiAppRuntime siddhiAppRuntime = siddhiManager . createSiddhiAppRuntime ( siddhiApp ); //Adding callback to retrieve output events from stream siddhiAppRuntime . addCallback ( OutputStream , new StreamCallback () { @Override public void receive ( Event [] events ) { EventPrinter . print ( events ); //To convert and print event as a map //EventPrinter.print(toMap(events)); } }); //Get InputHandler to push events into Siddhi InputHandler inputHandler = siddhiAppRuntime . getInputHandler ( StockStream ); //Start processing siddhiAppRuntime . start (); //Sending events to Siddhi inputHandler . send ( new Object []{ IBM , 700 f , 100L }); inputHandler . send ( new Object []{ WSO2 , 60.5f , 200L }); inputHandler . send ( new Object []{ GOOG , 50 f , 30L }); inputHandler . send ( new Object []{ IBM , 76.6f , 400L }); inputHandler . send ( new Object []{ WSO2 , 45.6f , 50L }); Thread . sleep ( 500 ); //Shutdown runtime siddhiAppRuntime . shutdown (); //Shutdown Siddhi Manager siddhiManager . shutdown (); } }","title":"Sample"},{"location":"docs/siddhi-as-a-kubernetes-microservice/","text":"Siddhi 5.1 as a Kubernetes Microservice This section provides information on running Siddhi Apps natively in Kubernetes via Siddhi Kubernetes Operator. Siddhi can be configured using SiddhiProcess kind and passed to the Siddhi operator for deployment. Here, the Siddhi applications containing stream processing logic can be written inline in SiddhiProcess yaml or passed as .siddhi files via contig maps. SiddhiProcess yaml can also be configured with the necessary system configurations. Prerequisites A Kubernetes cluster v1.10.11 or higher. Minikube Google Kubernetes Engine(GKE) Cluster Docker for Mac Or any other Kubernetes cluster Distributed deployment of Siddhi apps need NATS operator and NATS streaming operator . Admin privileges to install Siddhi operator Minikube Siddhi operator automatically creates NGINX ingress. Therefore it to work we can either enable ingress on Minikube using the following command. minikube addons enable ingress or disable Siddhi operator's automatically ingress creation . Google Kubernetes Engine (GKE) Cluster To install Siddhi operator, you have to give cluster admin permission to your account. In order to do that execute the following command (by replacing \"your-address@email.com\" with your account email address). kubectl create clusterrolebinding user-cluster-admin-binding --clusterrole=cluster-admin --user=your-address@email.com Docker for Mac Siddhi operator automatically creates NGINX ingress. Therefore it to work we can either enable ingress on Docker for mac following the official documentation or disable Siddhi operator's automatically ingress creation . Install Siddhi Operator To install the Siddhi Kubernetes operator run the following commands. 1 2 kubectl apply -f https://github.com/siddhi-io/siddhi-operator/releases/download/v0.2.0-alpha/00-prereqs.yaml kubectl apply -f https://github.com/siddhi-io/siddhi-operator/releases/download/v0.2.0-alpha/01-siddhi-operator.yaml You can verify the installation by making sure the following deployments are running in your Kubernetes cluster. 1 2 3 4 $ kubectl get deployment NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE siddhi-operator 1 1 1 1 1m Using a custom-built Siddhi runner image If you need to use a custom-built siddhi-runner image for all the SiddhiProcess deployments, you have to configure siddhiRunnerImage entry in siddhi-operator-config config map. Refer the documentation on creating custom Siddhi runner images bundling additional JARs here . If you are pulling the custom-built image from a private Docker registry/repository, specify the corresponding kubernetes secret as siddhiRunnerImageSecret entry in siddhi-operator-config config map. For more details on using docker images from private registries/repositories refer this documentation . Deploy and run Siddhi App Siddhi applications can be deployed on Kubernetes using the Siddhi operator. Here we will be creating a very simple Siddhi stream processing application that receives power consumption from several devices in a house. If the power consumption of dryer exceeds the consumption limit of 6000W then that Siddhi app sends an alert from printing a log. This can be created using a SiddhiProcess YAML file as given below. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 apiVersion : siddhi.io/v1alpha2 kind : SiddhiProcess metadata : name : power-surge-app spec : apps : - script : | @App:name( PowerSurgeDetection ) @App:description( App consume events from HTTP as a JSON message of { deviceType : dryer , power : 6000 } format and inserts the events into DevicePowerStream, and alerts the user if the power level is greater than or equal to 600 by printing a message in the log. ) /* Input: deviceType string and powerConsuption int(Watt) Output: Alert user from printing a log, if there is a power surge in the dryer. In other words, notify when power is greater than or equal to 600W. */ @source( type= http , receiver.url= ${RECEIVER_URL} , basic.auth.enabled= false , @map(type= json ) ) define stream DevicePowerStream(deviceType string, power int); @sink(type= log , prefix= LOGGER ) define stream PowerSurgeAlertStream(deviceType string, power int); @info(name= power-filter ) from DevicePowerStream[deviceType == dryer and power = 600] select deviceType, power insert into PowerSurgeAlertStream; container : env : - name : RECEIVER_URL value : http://0.0.0.0:8080/checkPower image : siddhiio/siddhi-runner-ubuntu:5.1.0-alpha Always listen on 0.0.0.0 with the Siddhi Application running inside a container environment. If you listen on localhost inside the container, nothing outside the container can connect to your application. Siddhi Tooling You can also use the powerful Siddhi Editor to implement and test steam processing applications. Configuring Siddhi To configure databases, extensions, authentication, periodic state persistence, and statistics for Siddhi as Kubernetes Microservice refer Siddhi Config Guide . To deploy the above Siddhi app in your Kubernetes cluster, copy above YAML to a file with name power-surge-app.yaml and execute the following command. 1 kubectl create -f absolute-yaml-file-path /power-surge-app.yaml TLS secret Within the SiddhiProcess, a TLS secret named siddhi-tls is configured. If a Kubernetes secret with the same name does not exist in the Kubernetes cluster, the NGINX will ignore it and use a self-generated certificate. Configuring a secret will be necessary for calling HTTPS endpoints, refer deploy and run Siddhi apps with HTTPS section for more details. If the power-surge-app is deployed successfully, it should create SiddhiProcess, deployment, service, and ingress as following. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 $ kubectl get SiddhiProcesses NAME STATUS READY AGE power-surge-app Running 1 /1 2m $ kubectl get deployment NAME READY UP-TO-DATE AVAILABLE AGE power-surge-app-0 1 /1 1 1 2m siddhi-operator 1 /1 1 1 2m $ kubectl get service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE kubernetes ClusterIP 10 .96.0.1 none 443 /TCP 2d power-surge-app-0 ClusterIP 10 .96.44.182 none 8080 /TCP 2m siddhi-operator ClusterIP 10 .98.78.238 none 8383 /TCP 2m $ kubectl get ingress NAME HOSTS ADDRESS PORTS AGE siddhi siddhi 10 .0.2.15 80 2m Using a custom-built Siddhi runner image If you need to use a custom-built siddhi-runner image for a specific SiddhiProcess deployment, you have to configure container.image spec in the power-surge-app.yaml . Refer the documentation on creating custom Siddhi runner images bundling additional JARs here . If you are pulling the custom-built image from a private Docker registry/repository, specify the corresponding kubernetes secret as imagePullSecret argument in the power-surge-app.yaml file. For more details on using docker images from private registries/repositories refer this documentation . Invoke Siddhi Applications To invoke the Siddhi App, obtain the external IP of the ingress load balancer using kubectl get ingress command as following. 1 2 3 $ kubectl get ingress NAME HOSTS ADDRESS PORTS AGE siddhi siddhi 10 .0.2.15 80 2m Then, add the host siddhi and related external IP ( ADDRESS ) to the /etc/hosts file in your machine. Minikube For Minikube, you have to use Minikube IP as the external IP. Hence, run minikube ip command to get the IP of the Minikube cluster. Docker for Mac For Docker for Mac, you have to use 0.0.0.0 as the external IP. Use the following CURL command to send events to power-surge-app deployed in Kubernetes. 1 2 3 4 5 6 7 8 9 curl -X POST \\ http://siddhi/power-surge-app-0/8080/checkPower \\ -H Accept: */* \\ -H Content-Type: application/json \\ -H Host: siddhi \\ -d { deviceType : dryer , power : 60000 } View Siddhi Process Logs Since the output of power-surge-app is logged, you can see the output by monitoring the associated pod's logs. To find the power-surge-app pod use the kubectl get pods command. This will list down all the deployed pods. 1 2 3 4 5 $ kubectl get pods NAME READY STATUS RESTARTS AGE power-surge-app-0-646c4f9dd5-rxzkq 1 /1 Running 0 4m siddhi-operator-6698d8f69d-6rfb6 1 /1 Running 0 4m Here, the pod starting with the SiddhiProcess name (in this case power-surge-app- ) is the pod we need to monitor. To view the logs, run the kubectl logs pod name command. This will show all the Siddhi process logs, along with the filtered output events as given below. 1 2 3 4 5 6 7 $ kubectl logs power-surge-app-0-646c4f9dd5-rxzkq ... [ 2019 -07-12 07 :12:48,925 ] INFO { org.wso2.transport.http.netty.contractimpl.listener.ServerConnectorBootstrap $HttpServerConnector } - HTTP ( S ) Interface starting on host 0 .0.0.0 and port 9443 [ 2019 -07-12 07 :12:48,927 ] INFO { org.wso2.transport.http.netty.contractimpl.listener.ServerConnectorBootstrap $HttpServerConnector } - HTTP ( S ) Interface starting on host 0 .0.0.0 and port 9090 [ 2019 -07-12 07 :12:48,941 ] INFO { org.wso2.carbon.kernel.internal.CarbonStartupHandler } - Siddhi Runner Distribution started in 6 .853 sec [ 2019 -07-12 07 :17:22,219 ] INFO { io.siddhi.core.stream.output.sink.LogSink } - LOGGER : Event { timestamp = 1562915842182 , data =[ dryer, 60000 ] , isExpired = false } Get Siddhi process status List Siddhi processes List the Siddhi process using the kubectl get sps or kubectl get SiddhiProcesses commands as follows. 1 2 3 4 5 6 7 $ kubectl get sps NAME STATUS READY AGE power-surge-app Running 1 /1 5m $ kubectl get SiddhiProcesses NAME STATUS READY AGE power-surge-app Running 1 /1 5m View Siddhi process configs Describe the Siddhi process configuration details using kubectl describe sp command as follows. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 $ kubectl describe sp power-surge-app Name: power-surge-app Namespace: default Labels: none Annotations: kubectl.kubernetes.io/last-applied-configuration ={ apiVersion : siddhi.io/v1alpha2 , kind : SiddhiProcess , metadata : { annotations : {} , name : power-surge-app , namespace : default } , spec : { apps : [ ... API Version: siddhi.io/v1alpha2 Kind: SiddhiProcess Metadata: Creation Timestamp: 2019 -07-12T07:12:35Z Generation: 1 Resource Version: 148205 Self Link: /apis/siddhi.io/v1alpha2/namespaces/default/siddhiprocesses/power-surge-app UID: 6c6d90a4-a474-11e9-a05b-080027f4eb25 Spec: Apps: Script: @App:name ( PowerSurgeDetection ) @App:description ( App consume events from HTTP as a JSON message of { deviceType : dryer , power : 6000 } format and inserts the events into DevicePowerStream, and alerts the user if the power level is greater than or equal to 600 by printing a message in the log. ) /* Input: deviceType string and powerConsuption int ( Watt ) Output: Alert user from printing a log, if there is a power surge in the dryer. In other words, notify when power is greater than or equal to 600W. */ @source ( type = http , receiver.url = ${RECEIVER_URL} , basic.auth.enabled = false , @map ( type = json ) ) define stream DevicePowerStream ( deviceType string, power int ) ; @sink ( type = log , prefix = LOGGER ) define stream PowerSurgeAlertStream ( deviceType string, power int ) ; @info ( name = power-filter ) from DevicePowerStream [ deviceType == dryer and power = 600 ] select deviceType, power insert into PowerSurgeAlertStream ; Container: Env: Name: RECEIVER_URL Value: http://0.0.0.0:8080/checkPower Name: BASIC_AUTH_ENABLED Value: false Image: siddhiio/siddhi-runner-ubuntu:5.1.0-alpha Status: Nodes: nil Ready: 1 /1 Status: Running Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal DeploymentCreated 11m siddhiprocess-controller power-surge-app-0 deployment created successfully Normal ServiceCreated 11m siddhiprocess-controller power-surge-app-0 service created successfully View Siddhi process logs To view the Siddhi process logs, first get the Siddhi process pods using the kubectl get pods command as follows. 1 2 3 4 5 $ kubectl get pods NAME READY STATUS RESTARTS AGE power-surge-app-0-646c4f9dd5-rxzkq 1 /1 Running 0 4m siddhi-operator-6698d8f69d-6rfb6 1 /1 Running 0 4m Then to retrieve the Siddhi process logs, run kubectl logs pod name command. Here pod name should be replaced with the name of the pod that starts with the relevant SiddhiProcess's name. A sample output logs are of this command is as follows. 1 2 3 4 5 6 7 $ kubectl logs power-surge-app-0-646c4f9dd5-rxzkq ... [ 2019 -07-12 07 :12:48,925 ] INFO { org.wso2.transport.http.netty.contractimpl.listener.ServerConnectorBootstrap $HttpServerConnector } - HTTP ( S ) Interface starting on host 0 .0.0.0 and port 9443 [ 2019 -07-12 07 :12:48,927 ] INFO { org.wso2.transport.http.netty.contractimpl.listener.ServerConnectorBootstrap $HttpServerConnector } - HTTP ( S ) Interface starting on host 0 .0.0.0 and port 9090 [ 2019 -07-12 07 :12:48,941 ] INFO { org.wso2.carbon.kernel.internal.CarbonStartupHandler } - Siddhi Runner Distribution started in 6 .853 sec [ 2019 -07-12 07 :17:22,219 ] INFO { io.siddhi.core.stream.output.sink.LogSink } - LOGGER : Event { timestamp = 1562915842182 , data =[ dryer, 60000 ] , isExpired = false } Deploy and run Siddhi App using config maps Siddhi operator allows you to deploy Siddhi app configurations via config maps instead of just adding them inline. Through this, you can also run multiple Siddhi Apps in a single SiddhiProcess. This can be done by passing the config maps containing Siddhi app files to the SiddhiProcess's apps configuration as follows. 1 2 3 apps : - configMap : power-surge-cm1 - configMap : power-surge-cm2 Sample on deploying and running Siddhi Apps via config maps Here we will be creating a very simple Siddhi stream processing application that receives power consumption from several devices in a house. If the power consumption of dryer exceeds the consumption limit of 6000W then that Siddhi app sends an alert from printing a log. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 @App:name( PowerSurgeDetection ) @App:description( App consume events from HTTP as a JSON message of { deviceType : dryer , power : 6000 } format and inserts the events into DevicePowerStream, and alerts the user if the power level is greater than or equal to 600 by printing a message in the log. ) /* Input: deviceType string and powerConsuption int(Watt) Output: Alert user from printing a log, if there is a power surge in the dryer. In other words, notify when power is greater than or equal to 600W. */ @source( type= http , receiver.url= ${RECEIVER_URL} , basic.auth.enabled= false , @map(type= json ) ) define stream DevicePowerStream(deviceType string, power int); @sink(type= log , prefix= LOGGER ) define stream PowerSurgeAlertStream(deviceType string, power int); @info(name= power-filter ) from DevicePowerStream[deviceType == dryer and power = 600] select deviceType, power insert into PowerSurgeAlertStream; Siddhi Tooling You can also use the powerful Siddhi Editor to implement and test steam processing applications. Save the above Siddhi App file as PowerSurgeDetection.siddhi , and use this file to create a Kubernetes config map with the name power-surge-cm . This can be achieved by running the following command. 1 kubectl create configmap power-surge-cm --from-file = absolute-file-path /PowerSurgeDetection.siddhi The created config map can be added to SiddhiProcess YAML under the apps entry as follows. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 apiVersion : siddhi.io/v1alpha2 kind : SiddhiProcess metadata : name : power-surge-app spec : apps : - configMap : power-surge-cm container : env : - name : RECEIVER_URL value : http://0.0.0.0:8080/checkPower image : siddhiio/siddhi-runner-ubuntu:5.1.0-alpha Save the YAML file as power-surge-app.yaml , and use the following command to deploy the SiddhiProcess. 1 kubectl create -f absolute-yaml-file-path /power-surge-app.yaml Using a config, created from a directory containing multiple Siddhi files SiddhiProcess's apps.configMap configuration also supports a config map that is created from a directory containing multiple Siddhi files. Use kubectl create configmap siddhi-apps --from-file= DIRECTORY_PATH command to create a config map from a directory. Invoke Siddhi Applications To invoke the Siddhi App, first obtain the external IP of the ingress load balancer using kubectl get ingress command as follows. 1 2 3 $ kubectl get ingress NAME HOSTS ADDRESS PORTS AGE siddhi siddhi 10 .0.2.15 80 2m Then, add the host siddhi and related external IP ( ADDRESS ) to the /etc/hosts file in your machine. Minikube For Minikube, you have to use Minikube IP as the external IP. Hence, run minikube ip command to get the IP of the Minikube cluster. Use the following CURL command to send events to power-surge-app deployed in Kubernetes. 1 2 3 4 5 6 7 8 9 10 curl -X POST \\ http://siddhi/power-surge-app-0/8080/checkPower \\ -H Accept: */* \\ -H Content-Type: application/json \\ -H Host: siddhi \\ -H cache-control: no-cache \\ -d { deviceType : dryer , power : 60000 } View Siddhi Process Logs Since the output of power-surge-app is logged, you can see the output by monitoring the associated pod's logs. To find the power-surge-app pod use the kubectl get pods command. This will list down all the deployed pods. 1 2 3 4 5 $ kubectl get pods NAME READY STATUS RESTARTS AGE power-surge-app-0-646c4f9dd5-tns7l 1 /1 Running 0 2m siddhi-operator-6698d8f69d-6rfb6 1 /1 Running 0 8m Here, the pod starting with the SiddhiProcess name (in this case power-surge-app- ) is the pod we need to monitor. To view the logs, run the kubectl logs pod name command. This will show all the Siddhi process logs, along with the filtered output events as given below. 1 2 3 4 5 6 7 $ kubectl logs power-surge-app-0-646c4f9dd5-tns7l ... [ 2019 -07-12 07 :50:32,861 ] INFO { org.wso2.carbon.kernel.internal.CarbonStartupHandler } - Siddhi Runner Distribution started in 8 .048 sec [ 2019 -07-12 07 :50:32,864 ] INFO { org.wso2.transport.http.netty.contractimpl.listener.ServerConnectorBootstrap $HttpServerConnector } - HTTP ( S ) Interface starting on host 0 .0.0.0 and port 9443 [ 2019 -07-12 07 :50:32,866 ] INFO { org.wso2.transport.http.netty.contractimpl.listener.ServerConnectorBootstrap $HttpServerConnector } - HTTP ( S ) Interface starting on host 0 .0.0.0 and port 9090 [ 2019 -07-12 07 :51:42,488 ] INFO { io.siddhi.core.stream.output.sink.LogSink } - LOGGER : Event { timestamp = 1562917902484 , data =[ dryer, 60000 ] , isExpired = false } Deploy Siddhi Apps without Ingress creation By default, Siddhi operator creates an NGINX ingress and exposes your HTTP/HTTPS through that ingress. If you need to disable automatic ingress creation, you have to change the autoIngressCreation value in the Siddhi siddhi-operator-config config map to false or null as below. 1 2 3 4 5 6 7 8 9 10 # This config map used to parse configurations to the Siddhi operator. apiVersion : v1 kind : ConfigMap metadata : name : siddhi-operator-config data : siddhiHome : /home/siddhi_user/siddhi-runner/ siddhiProfile : runner siddhiImage : siddhiio/siddhi-runner-alpine:5.1.0-alpha autoIngressCreation : false Deploy and run Siddhi App with HTTPS Configuring TLS will allow Siddhi ingress NGINX to expose HTTPS endpoints of your Siddhi Apps. To do so, create a Kubernetes secret( siddhi-tls ) and add that to the TLS configuration in siddhi-operator-config config map as given below. 1 ingressTLS : siddhi-tls Sample on deploying and running Siddhi App with HTTPS First, you need to create a certificate using the following commands. For more details about the certificate creation refers this . 1 openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout siddhi.key -out siddhi.crt -subj /CN=siddhi/O=siddhi After that, create a kubernetes secret called siddhi-tls , which we intended to add to the TLS configurations using the following command. 1 kubectl create secret tls siddhi-tls --key siddhi.key --cert siddhi.crt The created secret then need to be added to the siddhi-operator-config config map as follow. 1 2 3 4 5 6 7 8 9 10 apiVersion : v1 kind : ConfigMap metadata : name : siddhi-operator-config data : siddhiHome : /home/siddhi_user/siddhi-runner/ siddhiProfile : runner siddhiImage : siddhiio/siddhi-runner-alpine:5.1.0-alpha autoIngressCreation : true ingressTLS : siddhi-tls When this is done Siddhi operator will now enable TLS support via the NGINX ingress, and you will be able to access all the HTTPS endpoints. Invoke Siddhi Applications You can use now send the events to following HTTPS endpoint. 1 https://siddhi/power-surge-app-0/8080/checkPower Further, you can use the following CURL command to send a request to the deployed Siddhi applications via HTTPS. 1 2 3 4 5 6 7 8 9 10 curl --cacert siddhi.crt -X POST \\ https://siddhi/power-surge-app-0/8080/checkPower \\ -H Accept: */* \\ -H Content-Type: application/json \\ -H Host: siddhi \\ -H cache-control: no-cache \\ -d { deviceType : dryer , power : 60000 } View Siddhi Process Logs The output logs show the event that you sent using the previous CURL command. 1 2 3 4 5 6 7 8 9 10 11 $ kubectl get pods NAME READY STATUS RESTARTS AGE power-surge-app-0-646c4f9dd5-kk5md 1/1 Running 0 2m siddhi-operator-6698d8f69d-6rfb6 1/1 Running 0 10m $ kubectl logs monitor-app-667c97c898-rrtfs ... [2019-07-12 09:06:15,173] INFO {org.wso2.transport.http.netty.contractimpl.listener.ServerConnectorBootstrap$HttpServerConnector} - HTTP(S) Interface starting on host 0.0.0.0 and port 9443 [2019-07-12 09:06:15,184] INFO {org.wso2.transport.http.netty.contractimpl.listener.ServerConnectorBootstrap$HttpServerConnector} - HTTP(S) Interface starting on host 0.0.0.0 and port 9090 [2019-07-12 09:06:15,187] INFO {org.wso2.carbon.kernel.internal.CarbonStartupHandler} - Siddhi Runner Distribution started in 10.819 sec [2019-07-12 09:07:50,098] INFO {io.siddhi.core.stream.output.sink.LogSink} - LOGGER : Event{timestamp=1562922470093, data=[dryer, 60000], isExpired=false} Deploy and run Siddhi App in Distributed Mode Siddhi apps can be in two different types. Stateless Siddhi apps Stateful Siddhi apps The deployment of the stateful Siddhi apps follows distributed architecture to ensure high availability. The fully distributed scenario of Siddhi deployments handle using Siddhi distributed annotations . Without Messaging System With Messaging System Without Distributed Annotations Case 1 : The given Siddhi app will be deployed in a stateless mode in a single kubernetes deployment. Case 2 : If given Siddhi app contains stateful queries then the Siddhi app divided into two partial Siddhi apps (passthrough and process) and deployed in two kubernetes deployments. Use the configured messaging system to communicate between two apps. With Distributed Annotations Case 3 : WIP(Work In Progress) Case 4 : WIP(Work In Progress) The previously described Siddhi app deployments fall under this Case 1 category. The following sample will cover the Siddhi app deployments which fall under Case 2. Sample on deploying and running Siddhi App with a Messaging System The Siddhi operator currently supports NATS as the messaging system. Therefore it is prerequisite to deploying NATS operator and NATS streaming operator in your kubernetes cluster before you install the Siddhi app. Refer this documentation to install NATS operator and NATS streaming operator. Install the Siddhi operator . Create a persistence volume in your cluster. Now we need a NATS cluster and NATS streaming cluster to run the Siddhi app deployment. For this, there are two cases handled by the operator. User can create NATS cluster and NATS streaming cluster as described in this documentation . Specify cluster details in the YAML file like following. 1 2 3 4 5 6 messagingSystem : type : nats config : bootstrapServers : - nats://example-nats:4222 clusterId : example-stan If the user only specifies messaging system as NATS like below then Siddhi operator will automatically create NATS cluster( siddhi-nats ) and NATS streaming cluster( siddhi-stan ), and connect two partial apps. 1 2 messagingSystem : type : nats Before installing a Siddhi app you have to check that all prerequisites(Siddhi-operator, nats-operator, and nats-streaming-operator) up and running perfectly like below. 1 2 3 4 5 6 $ kubectl get deployments NAME READY UP-TO-DATE AVAILABLE AGE nats-operator 1 /1 1 1 5m nats-streaming-operator 1 /1 1 1 5m siddhi-operator 1 /1 1 1 5m Now you need to specify a YAML file like below to create stateful Siddhi app deployment. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 apiVersion : siddhi.io/v1alpha2 kind : SiddhiProcess metadata : name : power-consume-app spec : apps : - script : | @App:name( PowerConsumptionSurgeDetection ) @App:description( App consumes events from HTTP as a JSON message of { deviceType : dryer , power : 6000 } format and inserts the events into DevicePowerStream, and alerts the user if the power consumption in 1 minute is greater than or equal to 10000W by printing a message in the log for every 30 seconds. ) /* Input: deviceType string and powerConsuption int(Joules) Output: Alert user from printing a log, if there is a power surge in the dryer within 1 minute period. Notify the user in every 30 seconds when total power consumption is greater than or equal to 10000W in 1 minute time period. */ @source( type= http , receiver.url= ${RECEIVER_URL} , basic.auth.enabled= false , @map(type= json ) ) define stream DevicePowerStream(deviceType string, power int); @sink(type= log , prefix= LOGGER ) define stream PowerSurgeAlertStream(deviceType string, powerConsumed long); @info(name= power-consumption-window ) from DevicePowerStream#window.time(1 min) select deviceType, sum(power) as powerConsumed group by deviceType having powerConsumed 10000 output every 30 sec insert into PowerSurgeAlertStream; container : env : - name : RECEIVER_URL value : http://0.0.0.0:8080/checkPower - name : BASIC_AUTH_ENABLED value : false image : siddhiio/siddhi-runner-ubuntu:5.1.0 messagingSystem : type : nats persistentVolumeClaim : accessModes : - ReadWriteOnce resources : requests : storage : 1Gi storageClassName : standard volumeMode : Filesystem runner : | state.persistence: enabled: true intervalInMin: 1 revisionsToKeep: 2 persistenceStore: io.siddhi.distribution.core.persistence.FileSystemPersistenceStore config: location: siddhi-app-persistence Save this YAML as power-consume-app.yaml as use kubectl to deploy the app. 1 kubectl apply -f power-consume-app.yaml This kubectl execution in the Siddhi operator will do the following tasks. Create a NATS cluster and streaming cluster since the user did not specify it. Parse the given Siddhi app and create two partial Siddhi apps(passthrough and process). Then deploy both apps in separate deployments to distribute I/O time. Check health of the Siddhi runner and make deployments up and running. Create a service for passthrough app. Create an ingress rule that maps to passthrough service. After a successful deployment, your kubernetes cluster should have these artifacts. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 $ kubectl get SiddhiProcesses NAME STATUS READY AGE power-consume-app Running 2 /2 5m $ kubectl get deployments NAME READY UP-TO-DATE AVAILABLE AGE nats-operator 1 /1 1 1 10m nats-streaming-operator 1 /1 1 1 10m power-consume-app-0 1 /1 1 1 5m power-consume-app-1 1 /1 1 1 5m siddhi-operator 1 /1 1 1 10m $ kubectl get service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE kubernetes ClusterIP 10 .96.0.1 none 443 /TCP 2d7h power-consume-app-0 ClusterIP 10 .105.67.227 none 8080 /TCP 5m siddhi-nats ClusterIP 10 .100.205.21 none 4222 /TCP 10m siddhi-nats-mgmt ClusterIP None none 6222 /TCP,8222/TCP,7777/TCP 10m siddhi-operator ClusterIP 10 .103.229.109 none 8383 /TCP 10m $ kubectl get ingress NAME HOSTS ADDRESS PORTS AGE siddhi siddhi 10 .0.2.15 80 10m $ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE siddhi-pv 1Gi RWO Recycle Bound default/power-consume-app-1-pvc standard 10m $ kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE power-consume-app-1-pvc Bound siddhi-pv 1Gi RWO standard 5m Here power-consume-app-0 is the passthrough deployment and power-consume-app-1 is the process deployment. Now you can send an HTTP request to the passthrough app. 1 2 3 4 5 6 7 8 9 curl -X POST \\ http://siddhi/power-consume-app-0/8080/checkPower \\ -H Accept: */* \\ -H Content-Type: application/json \\ -H Host: siddhi \\ -d { deviceType : dryer , power : 60000 } The process app logs will show that event. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 $ kubectl get pods NAME READY STATUS RESTARTS AGE nats-operator-dd7f4945f-x4vf8 1 /1 Running 0 10m nats-streaming-operator-6fbb6695ff-9rmlx 1 /1 Running 0 10m power-consume-app-0-7486b87979-6tccx 1 /1 Running 0 5m power-consume-app-1-588996fcfb-prncj 1 /1 Running 0 5m siddhi-nats-1 1 /1 Running 0 5m siddhi-operator-6698d8f69d-w2kvj 1 /1 Running 0 10m siddhi-stan-1 1 /1 Running 1 5m $ kubectl logs power-consume-app-1-588996fcfb-prncj JAVA_HOME environment variable is set to /opt/java/openjdk CARBON_HOME environment variable is set to /home/siddhi_user/siddhi-runner RUNTIME_HOME environment variable is set to /home/siddhi_user/siddhi-runner/wso2/runner Picked up JAVA_TOOL_OPTIONS: -XX:+UnlockExperimentalVMOptions -XX:+UseCGroupMemoryLimitForHeap [ 2019 -07-12 14 :09:16,648 ] INFO { org.wso2.carbon.launcher.extensions.OSGiLibBundleDeployerUtils updateOSGiLib } - Successfully updated the OSGi bundle information of Carbon Runtime: runner ... [ 2019 -07-12 14 :12:04,969 ] INFO { io.siddhi.core.stream.output.sink.LogSink } - LOGGER : Event { timestamp = 1562940716559 , data =[ dryer, 60000 ] , isExpired = false }","title":"Siddhi Kubernetes Microservice"},{"location":"docs/siddhi-as-a-kubernetes-microservice/#siddhi-51-as-a-kubernetes-microservice","text":"This section provides information on running Siddhi Apps natively in Kubernetes via Siddhi Kubernetes Operator. Siddhi can be configured using SiddhiProcess kind and passed to the Siddhi operator for deployment. Here, the Siddhi applications containing stream processing logic can be written inline in SiddhiProcess yaml or passed as .siddhi files via contig maps. SiddhiProcess yaml can also be configured with the necessary system configurations.","title":"Siddhi 5.1 as a Kubernetes Microservice"},{"location":"docs/siddhi-as-a-kubernetes-microservice/#prerequisites","text":"A Kubernetes cluster v1.10.11 or higher. Minikube Google Kubernetes Engine(GKE) Cluster Docker for Mac Or any other Kubernetes cluster Distributed deployment of Siddhi apps need NATS operator and NATS streaming operator . Admin privileges to install Siddhi operator Minikube Siddhi operator automatically creates NGINX ingress. Therefore it to work we can either enable ingress on Minikube using the following command. minikube addons enable ingress or disable Siddhi operator's automatically ingress creation . Google Kubernetes Engine (GKE) Cluster To install Siddhi operator, you have to give cluster admin permission to your account. In order to do that execute the following command (by replacing \"your-address@email.com\" with your account email address). kubectl create clusterrolebinding user-cluster-admin-binding --clusterrole=cluster-admin --user=your-address@email.com Docker for Mac Siddhi operator automatically creates NGINX ingress. Therefore it to work we can either enable ingress on Docker for mac following the official documentation or disable Siddhi operator's automatically ingress creation .","title":"Prerequisites"},{"location":"docs/siddhi-as-a-kubernetes-microservice/#install-siddhi-operator","text":"To install the Siddhi Kubernetes operator run the following commands. 1 2 kubectl apply -f https://github.com/siddhi-io/siddhi-operator/releases/download/v0.2.0-alpha/00-prereqs.yaml kubectl apply -f https://github.com/siddhi-io/siddhi-operator/releases/download/v0.2.0-alpha/01-siddhi-operator.yaml You can verify the installation by making sure the following deployments are running in your Kubernetes cluster. 1 2 3 4 $ kubectl get deployment NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE siddhi-operator 1 1 1 1 1m Using a custom-built Siddhi runner image If you need to use a custom-built siddhi-runner image for all the SiddhiProcess deployments, you have to configure siddhiRunnerImage entry in siddhi-operator-config config map. Refer the documentation on creating custom Siddhi runner images bundling additional JARs here . If you are pulling the custom-built image from a private Docker registry/repository, specify the corresponding kubernetes secret as siddhiRunnerImageSecret entry in siddhi-operator-config config map. For more details on using docker images from private registries/repositories refer this documentation .","title":"Install Siddhi Operator"},{"location":"docs/siddhi-as-a-kubernetes-microservice/#deploy-and-run-siddhi-app","text":"Siddhi applications can be deployed on Kubernetes using the Siddhi operator. Here we will be creating a very simple Siddhi stream processing application that receives power consumption from several devices in a house. If the power consumption of dryer exceeds the consumption limit of 6000W then that Siddhi app sends an alert from printing a log. This can be created using a SiddhiProcess YAML file as given below. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 apiVersion : siddhi.io/v1alpha2 kind : SiddhiProcess metadata : name : power-surge-app spec : apps : - script : | @App:name( PowerSurgeDetection ) @App:description( App consume events from HTTP as a JSON message of { deviceType : dryer , power : 6000 } format and inserts the events into DevicePowerStream, and alerts the user if the power level is greater than or equal to 600 by printing a message in the log. ) /* Input: deviceType string and powerConsuption int(Watt) Output: Alert user from printing a log, if there is a power surge in the dryer. In other words, notify when power is greater than or equal to 600W. */ @source( type= http , receiver.url= ${RECEIVER_URL} , basic.auth.enabled= false , @map(type= json ) ) define stream DevicePowerStream(deviceType string, power int); @sink(type= log , prefix= LOGGER ) define stream PowerSurgeAlertStream(deviceType string, power int); @info(name= power-filter ) from DevicePowerStream[deviceType == dryer and power = 600] select deviceType, power insert into PowerSurgeAlertStream; container : env : - name : RECEIVER_URL value : http://0.0.0.0:8080/checkPower image : siddhiio/siddhi-runner-ubuntu:5.1.0-alpha Always listen on 0.0.0.0 with the Siddhi Application running inside a container environment. If you listen on localhost inside the container, nothing outside the container can connect to your application. Siddhi Tooling You can also use the powerful Siddhi Editor to implement and test steam processing applications. Configuring Siddhi To configure databases, extensions, authentication, periodic state persistence, and statistics for Siddhi as Kubernetes Microservice refer Siddhi Config Guide . To deploy the above Siddhi app in your Kubernetes cluster, copy above YAML to a file with name power-surge-app.yaml and execute the following command. 1 kubectl create -f absolute-yaml-file-path /power-surge-app.yaml TLS secret Within the SiddhiProcess, a TLS secret named siddhi-tls is configured. If a Kubernetes secret with the same name does not exist in the Kubernetes cluster, the NGINX will ignore it and use a self-generated certificate. Configuring a secret will be necessary for calling HTTPS endpoints, refer deploy and run Siddhi apps with HTTPS section for more details. If the power-surge-app is deployed successfully, it should create SiddhiProcess, deployment, service, and ingress as following. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 $ kubectl get SiddhiProcesses NAME STATUS READY AGE power-surge-app Running 1 /1 2m $ kubectl get deployment NAME READY UP-TO-DATE AVAILABLE AGE power-surge-app-0 1 /1 1 1 2m siddhi-operator 1 /1 1 1 2m $ kubectl get service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE kubernetes ClusterIP 10 .96.0.1 none 443 /TCP 2d power-surge-app-0 ClusterIP 10 .96.44.182 none 8080 /TCP 2m siddhi-operator ClusterIP 10 .98.78.238 none 8383 /TCP 2m $ kubectl get ingress NAME HOSTS ADDRESS PORTS AGE siddhi siddhi 10 .0.2.15 80 2m Using a custom-built Siddhi runner image If you need to use a custom-built siddhi-runner image for a specific SiddhiProcess deployment, you have to configure container.image spec in the power-surge-app.yaml . Refer the documentation on creating custom Siddhi runner images bundling additional JARs here . If you are pulling the custom-built image from a private Docker registry/repository, specify the corresponding kubernetes secret as imagePullSecret argument in the power-surge-app.yaml file. For more details on using docker images from private registries/repositories refer this documentation . Invoke Siddhi Applications To invoke the Siddhi App, obtain the external IP of the ingress load balancer using kubectl get ingress command as following. 1 2 3 $ kubectl get ingress NAME HOSTS ADDRESS PORTS AGE siddhi siddhi 10 .0.2.15 80 2m Then, add the host siddhi and related external IP ( ADDRESS ) to the /etc/hosts file in your machine. Minikube For Minikube, you have to use Minikube IP as the external IP. Hence, run minikube ip command to get the IP of the Minikube cluster. Docker for Mac For Docker for Mac, you have to use 0.0.0.0 as the external IP. Use the following CURL command to send events to power-surge-app deployed in Kubernetes. 1 2 3 4 5 6 7 8 9 curl -X POST \\ http://siddhi/power-surge-app-0/8080/checkPower \\ -H Accept: */* \\ -H Content-Type: application/json \\ -H Host: siddhi \\ -d { deviceType : dryer , power : 60000 } View Siddhi Process Logs Since the output of power-surge-app is logged, you can see the output by monitoring the associated pod's logs. To find the power-surge-app pod use the kubectl get pods command. This will list down all the deployed pods. 1 2 3 4 5 $ kubectl get pods NAME READY STATUS RESTARTS AGE power-surge-app-0-646c4f9dd5-rxzkq 1 /1 Running 0 4m siddhi-operator-6698d8f69d-6rfb6 1 /1 Running 0 4m Here, the pod starting with the SiddhiProcess name (in this case power-surge-app- ) is the pod we need to monitor. To view the logs, run the kubectl logs pod name command. This will show all the Siddhi process logs, along with the filtered output events as given below. 1 2 3 4 5 6 7 $ kubectl logs power-surge-app-0-646c4f9dd5-rxzkq ... [ 2019 -07-12 07 :12:48,925 ] INFO { org.wso2.transport.http.netty.contractimpl.listener.ServerConnectorBootstrap $HttpServerConnector } - HTTP ( S ) Interface starting on host 0 .0.0.0 and port 9443 [ 2019 -07-12 07 :12:48,927 ] INFO { org.wso2.transport.http.netty.contractimpl.listener.ServerConnectorBootstrap $HttpServerConnector } - HTTP ( S ) Interface starting on host 0 .0.0.0 and port 9090 [ 2019 -07-12 07 :12:48,941 ] INFO { org.wso2.carbon.kernel.internal.CarbonStartupHandler } - Siddhi Runner Distribution started in 6 .853 sec [ 2019 -07-12 07 :17:22,219 ] INFO { io.siddhi.core.stream.output.sink.LogSink } - LOGGER : Event { timestamp = 1562915842182 , data =[ dryer, 60000 ] , isExpired = false }","title":"Deploy and run Siddhi App"},{"location":"docs/siddhi-as-a-kubernetes-microservice/#get-siddhi-process-status","text":"","title":"Get Siddhi process status"},{"location":"docs/siddhi-as-a-kubernetes-microservice/#list-siddhi-processes","text":"List the Siddhi process using the kubectl get sps or kubectl get SiddhiProcesses commands as follows. 1 2 3 4 5 6 7 $ kubectl get sps NAME STATUS READY AGE power-surge-app Running 1 /1 5m $ kubectl get SiddhiProcesses NAME STATUS READY AGE power-surge-app Running 1 /1 5m","title":"List Siddhi processes"},{"location":"docs/siddhi-as-a-kubernetes-microservice/#view-siddhi-process-configs","text":"Describe the Siddhi process configuration details using kubectl describe sp command as follows. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 $ kubectl describe sp power-surge-app Name: power-surge-app Namespace: default Labels: none Annotations: kubectl.kubernetes.io/last-applied-configuration ={ apiVersion : siddhi.io/v1alpha2 , kind : SiddhiProcess , metadata : { annotations : {} , name : power-surge-app , namespace : default } , spec : { apps : [ ... API Version: siddhi.io/v1alpha2 Kind: SiddhiProcess Metadata: Creation Timestamp: 2019 -07-12T07:12:35Z Generation: 1 Resource Version: 148205 Self Link: /apis/siddhi.io/v1alpha2/namespaces/default/siddhiprocesses/power-surge-app UID: 6c6d90a4-a474-11e9-a05b-080027f4eb25 Spec: Apps: Script: @App:name ( PowerSurgeDetection ) @App:description ( App consume events from HTTP as a JSON message of { deviceType : dryer , power : 6000 } format and inserts the events into DevicePowerStream, and alerts the user if the power level is greater than or equal to 600 by printing a message in the log. ) /* Input: deviceType string and powerConsuption int ( Watt ) Output: Alert user from printing a log, if there is a power surge in the dryer. In other words, notify when power is greater than or equal to 600W. */ @source ( type = http , receiver.url = ${RECEIVER_URL} , basic.auth.enabled = false , @map ( type = json ) ) define stream DevicePowerStream ( deviceType string, power int ) ; @sink ( type = log , prefix = LOGGER ) define stream PowerSurgeAlertStream ( deviceType string, power int ) ; @info ( name = power-filter ) from DevicePowerStream [ deviceType == dryer and power = 600 ] select deviceType, power insert into PowerSurgeAlertStream ; Container: Env: Name: RECEIVER_URL Value: http://0.0.0.0:8080/checkPower Name: BASIC_AUTH_ENABLED Value: false Image: siddhiio/siddhi-runner-ubuntu:5.1.0-alpha Status: Nodes: nil Ready: 1 /1 Status: Running Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal DeploymentCreated 11m siddhiprocess-controller power-surge-app-0 deployment created successfully Normal ServiceCreated 11m siddhiprocess-controller power-surge-app-0 service created successfully","title":"View Siddhi process configs"},{"location":"docs/siddhi-as-a-kubernetes-microservice/#view-siddhi-process-logs","text":"To view the Siddhi process logs, first get the Siddhi process pods using the kubectl get pods command as follows. 1 2 3 4 5 $ kubectl get pods NAME READY STATUS RESTARTS AGE power-surge-app-0-646c4f9dd5-rxzkq 1 /1 Running 0 4m siddhi-operator-6698d8f69d-6rfb6 1 /1 Running 0 4m Then to retrieve the Siddhi process logs, run kubectl logs pod name command. Here pod name should be replaced with the name of the pod that starts with the relevant SiddhiProcess's name. A sample output logs are of this command is as follows. 1 2 3 4 5 6 7 $ kubectl logs power-surge-app-0-646c4f9dd5-rxzkq ... [ 2019 -07-12 07 :12:48,925 ] INFO { org.wso2.transport.http.netty.contractimpl.listener.ServerConnectorBootstrap $HttpServerConnector } - HTTP ( S ) Interface starting on host 0 .0.0.0 and port 9443 [ 2019 -07-12 07 :12:48,927 ] INFO { org.wso2.transport.http.netty.contractimpl.listener.ServerConnectorBootstrap $HttpServerConnector } - HTTP ( S ) Interface starting on host 0 .0.0.0 and port 9090 [ 2019 -07-12 07 :12:48,941 ] INFO { org.wso2.carbon.kernel.internal.CarbonStartupHandler } - Siddhi Runner Distribution started in 6 .853 sec [ 2019 -07-12 07 :17:22,219 ] INFO { io.siddhi.core.stream.output.sink.LogSink } - LOGGER : Event { timestamp = 1562915842182 , data =[ dryer, 60000 ] , isExpired = false }","title":"View Siddhi process logs"},{"location":"docs/siddhi-as-a-kubernetes-microservice/#deploy-and-run-siddhi-app-using-config-maps","text":"Siddhi operator allows you to deploy Siddhi app configurations via config maps instead of just adding them inline. Through this, you can also run multiple Siddhi Apps in a single SiddhiProcess. This can be done by passing the config maps containing Siddhi app files to the SiddhiProcess's apps configuration as follows. 1 2 3 apps : - configMap : power-surge-cm1 - configMap : power-surge-cm2 Sample on deploying and running Siddhi Apps via config maps Here we will be creating a very simple Siddhi stream processing application that receives power consumption from several devices in a house. If the power consumption of dryer exceeds the consumption limit of 6000W then that Siddhi app sends an alert from printing a log. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 @App:name( PowerSurgeDetection ) @App:description( App consume events from HTTP as a JSON message of { deviceType : dryer , power : 6000 } format and inserts the events into DevicePowerStream, and alerts the user if the power level is greater than or equal to 600 by printing a message in the log. ) /* Input: deviceType string and powerConsuption int(Watt) Output: Alert user from printing a log, if there is a power surge in the dryer. In other words, notify when power is greater than or equal to 600W. */ @source( type= http , receiver.url= ${RECEIVER_URL} , basic.auth.enabled= false , @map(type= json ) ) define stream DevicePowerStream(deviceType string, power int); @sink(type= log , prefix= LOGGER ) define stream PowerSurgeAlertStream(deviceType string, power int); @info(name= power-filter ) from DevicePowerStream[deviceType == dryer and power = 600] select deviceType, power insert into PowerSurgeAlertStream; Siddhi Tooling You can also use the powerful Siddhi Editor to implement and test steam processing applications. Save the above Siddhi App file as PowerSurgeDetection.siddhi , and use this file to create a Kubernetes config map with the name power-surge-cm . This can be achieved by running the following command. 1 kubectl create configmap power-surge-cm --from-file = absolute-file-path /PowerSurgeDetection.siddhi The created config map can be added to SiddhiProcess YAML under the apps entry as follows. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 apiVersion : siddhi.io/v1alpha2 kind : SiddhiProcess metadata : name : power-surge-app spec : apps : - configMap : power-surge-cm container : env : - name : RECEIVER_URL value : http://0.0.0.0:8080/checkPower image : siddhiio/siddhi-runner-ubuntu:5.1.0-alpha Save the YAML file as power-surge-app.yaml , and use the following command to deploy the SiddhiProcess. 1 kubectl create -f absolute-yaml-file-path /power-surge-app.yaml Using a config, created from a directory containing multiple Siddhi files SiddhiProcess's apps.configMap configuration also supports a config map that is created from a directory containing multiple Siddhi files. Use kubectl create configmap siddhi-apps --from-file= DIRECTORY_PATH command to create a config map from a directory. Invoke Siddhi Applications To invoke the Siddhi App, first obtain the external IP of the ingress load balancer using kubectl get ingress command as follows. 1 2 3 $ kubectl get ingress NAME HOSTS ADDRESS PORTS AGE siddhi siddhi 10 .0.2.15 80 2m Then, add the host siddhi and related external IP ( ADDRESS ) to the /etc/hosts file in your machine. Minikube For Minikube, you have to use Minikube IP as the external IP. Hence, run minikube ip command to get the IP of the Minikube cluster. Use the following CURL command to send events to power-surge-app deployed in Kubernetes. 1 2 3 4 5 6 7 8 9 10 curl -X POST \\ http://siddhi/power-surge-app-0/8080/checkPower \\ -H Accept: */* \\ -H Content-Type: application/json \\ -H Host: siddhi \\ -H cache-control: no-cache \\ -d { deviceType : dryer , power : 60000 } View Siddhi Process Logs Since the output of power-surge-app is logged, you can see the output by monitoring the associated pod's logs. To find the power-surge-app pod use the kubectl get pods command. This will list down all the deployed pods. 1 2 3 4 5 $ kubectl get pods NAME READY STATUS RESTARTS AGE power-surge-app-0-646c4f9dd5-tns7l 1 /1 Running 0 2m siddhi-operator-6698d8f69d-6rfb6 1 /1 Running 0 8m Here, the pod starting with the SiddhiProcess name (in this case power-surge-app- ) is the pod we need to monitor. To view the logs, run the kubectl logs pod name command. This will show all the Siddhi process logs, along with the filtered output events as given below. 1 2 3 4 5 6 7 $ kubectl logs power-surge-app-0-646c4f9dd5-tns7l ... [ 2019 -07-12 07 :50:32,861 ] INFO { org.wso2.carbon.kernel.internal.CarbonStartupHandler } - Siddhi Runner Distribution started in 8 .048 sec [ 2019 -07-12 07 :50:32,864 ] INFO { org.wso2.transport.http.netty.contractimpl.listener.ServerConnectorBootstrap $HttpServerConnector } - HTTP ( S ) Interface starting on host 0 .0.0.0 and port 9443 [ 2019 -07-12 07 :50:32,866 ] INFO { org.wso2.transport.http.netty.contractimpl.listener.ServerConnectorBootstrap $HttpServerConnector } - HTTP ( S ) Interface starting on host 0 .0.0.0 and port 9090 [ 2019 -07-12 07 :51:42,488 ] INFO { io.siddhi.core.stream.output.sink.LogSink } - LOGGER : Event { timestamp = 1562917902484 , data =[ dryer, 60000 ] , isExpired = false }","title":"Deploy and run Siddhi App using config maps"},{"location":"docs/siddhi-as-a-kubernetes-microservice/#deploy-siddhi-apps-without-ingress-creation","text":"By default, Siddhi operator creates an NGINX ingress and exposes your HTTP/HTTPS through that ingress. If you need to disable automatic ingress creation, you have to change the autoIngressCreation value in the Siddhi siddhi-operator-config config map to false or null as below. 1 2 3 4 5 6 7 8 9 10 # This config map used to parse configurations to the Siddhi operator. apiVersion : v1 kind : ConfigMap metadata : name : siddhi-operator-config data : siddhiHome : /home/siddhi_user/siddhi-runner/ siddhiProfile : runner siddhiImage : siddhiio/siddhi-runner-alpine:5.1.0-alpha autoIngressCreation : false","title":"Deploy Siddhi Apps without Ingress creation"},{"location":"docs/siddhi-as-a-kubernetes-microservice/#deploy-and-run-siddhi-app-with-https","text":"Configuring TLS will allow Siddhi ingress NGINX to expose HTTPS endpoints of your Siddhi Apps. To do so, create a Kubernetes secret( siddhi-tls ) and add that to the TLS configuration in siddhi-operator-config config map as given below. 1 ingressTLS : siddhi-tls Sample on deploying and running Siddhi App with HTTPS First, you need to create a certificate using the following commands. For more details about the certificate creation refers this . 1 openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout siddhi.key -out siddhi.crt -subj /CN=siddhi/O=siddhi After that, create a kubernetes secret called siddhi-tls , which we intended to add to the TLS configurations using the following command. 1 kubectl create secret tls siddhi-tls --key siddhi.key --cert siddhi.crt The created secret then need to be added to the siddhi-operator-config config map as follow. 1 2 3 4 5 6 7 8 9 10 apiVersion : v1 kind : ConfigMap metadata : name : siddhi-operator-config data : siddhiHome : /home/siddhi_user/siddhi-runner/ siddhiProfile : runner siddhiImage : siddhiio/siddhi-runner-alpine:5.1.0-alpha autoIngressCreation : true ingressTLS : siddhi-tls When this is done Siddhi operator will now enable TLS support via the NGINX ingress, and you will be able to access all the HTTPS endpoints. Invoke Siddhi Applications You can use now send the events to following HTTPS endpoint. 1 https://siddhi/power-surge-app-0/8080/checkPower Further, you can use the following CURL command to send a request to the deployed Siddhi applications via HTTPS. 1 2 3 4 5 6 7 8 9 10 curl --cacert siddhi.crt -X POST \\ https://siddhi/power-surge-app-0/8080/checkPower \\ -H Accept: */* \\ -H Content-Type: application/json \\ -H Host: siddhi \\ -H cache-control: no-cache \\ -d { deviceType : dryer , power : 60000 } View Siddhi Process Logs The output logs show the event that you sent using the previous CURL command. 1 2 3 4 5 6 7 8 9 10 11 $ kubectl get pods NAME READY STATUS RESTARTS AGE power-surge-app-0-646c4f9dd5-kk5md 1/1 Running 0 2m siddhi-operator-6698d8f69d-6rfb6 1/1 Running 0 10m $ kubectl logs monitor-app-667c97c898-rrtfs ... [2019-07-12 09:06:15,173] INFO {org.wso2.transport.http.netty.contractimpl.listener.ServerConnectorBootstrap$HttpServerConnector} - HTTP(S) Interface starting on host 0.0.0.0 and port 9443 [2019-07-12 09:06:15,184] INFO {org.wso2.transport.http.netty.contractimpl.listener.ServerConnectorBootstrap$HttpServerConnector} - HTTP(S) Interface starting on host 0.0.0.0 and port 9090 [2019-07-12 09:06:15,187] INFO {org.wso2.carbon.kernel.internal.CarbonStartupHandler} - Siddhi Runner Distribution started in 10.819 sec [2019-07-12 09:07:50,098] INFO {io.siddhi.core.stream.output.sink.LogSink} - LOGGER : Event{timestamp=1562922470093, data=[dryer, 60000], isExpired=false}","title":"Deploy and run Siddhi App with HTTPS"},{"location":"docs/siddhi-as-a-kubernetes-microservice/#deploy-and-run-siddhi-app-in-distributed-mode","text":"Siddhi apps can be in two different types. Stateless Siddhi apps Stateful Siddhi apps The deployment of the stateful Siddhi apps follows distributed architecture to ensure high availability. The fully distributed scenario of Siddhi deployments handle using Siddhi distributed annotations . Without Messaging System With Messaging System Without Distributed Annotations Case 1 : The given Siddhi app will be deployed in a stateless mode in a single kubernetes deployment. Case 2 : If given Siddhi app contains stateful queries then the Siddhi app divided into two partial Siddhi apps (passthrough and process) and deployed in two kubernetes deployments. Use the configured messaging system to communicate between two apps. With Distributed Annotations Case 3 : WIP(Work In Progress) Case 4 : WIP(Work In Progress) The previously described Siddhi app deployments fall under this Case 1 category. The following sample will cover the Siddhi app deployments which fall under Case 2. Sample on deploying and running Siddhi App with a Messaging System The Siddhi operator currently supports NATS as the messaging system. Therefore it is prerequisite to deploying NATS operator and NATS streaming operator in your kubernetes cluster before you install the Siddhi app. Refer this documentation to install NATS operator and NATS streaming operator. Install the Siddhi operator . Create a persistence volume in your cluster. Now we need a NATS cluster and NATS streaming cluster to run the Siddhi app deployment. For this, there are two cases handled by the operator. User can create NATS cluster and NATS streaming cluster as described in this documentation . Specify cluster details in the YAML file like following. 1 2 3 4 5 6 messagingSystem : type : nats config : bootstrapServers : - nats://example-nats:4222 clusterId : example-stan If the user only specifies messaging system as NATS like below then Siddhi operator will automatically create NATS cluster( siddhi-nats ) and NATS streaming cluster( siddhi-stan ), and connect two partial apps. 1 2 messagingSystem : type : nats Before installing a Siddhi app you have to check that all prerequisites(Siddhi-operator, nats-operator, and nats-streaming-operator) up and running perfectly like below. 1 2 3 4 5 6 $ kubectl get deployments NAME READY UP-TO-DATE AVAILABLE AGE nats-operator 1 /1 1 1 5m nats-streaming-operator 1 /1 1 1 5m siddhi-operator 1 /1 1 1 5m Now you need to specify a YAML file like below to create stateful Siddhi app deployment. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 apiVersion : siddhi.io/v1alpha2 kind : SiddhiProcess metadata : name : power-consume-app spec : apps : - script : | @App:name( PowerConsumptionSurgeDetection ) @App:description( App consumes events from HTTP as a JSON message of { deviceType : dryer , power : 6000 } format and inserts the events into DevicePowerStream, and alerts the user if the power consumption in 1 minute is greater than or equal to 10000W by printing a message in the log for every 30 seconds. ) /* Input: deviceType string and powerConsuption int(Joules) Output: Alert user from printing a log, if there is a power surge in the dryer within 1 minute period. Notify the user in every 30 seconds when total power consumption is greater than or equal to 10000W in 1 minute time period. */ @source( type= http , receiver.url= ${RECEIVER_URL} , basic.auth.enabled= false , @map(type= json ) ) define stream DevicePowerStream(deviceType string, power int); @sink(type= log , prefix= LOGGER ) define stream PowerSurgeAlertStream(deviceType string, powerConsumed long); @info(name= power-consumption-window ) from DevicePowerStream#window.time(1 min) select deviceType, sum(power) as powerConsumed group by deviceType having powerConsumed 10000 output every 30 sec insert into PowerSurgeAlertStream; container : env : - name : RECEIVER_URL value : http://0.0.0.0:8080/checkPower - name : BASIC_AUTH_ENABLED value : false image : siddhiio/siddhi-runner-ubuntu:5.1.0 messagingSystem : type : nats persistentVolumeClaim : accessModes : - ReadWriteOnce resources : requests : storage : 1Gi storageClassName : standard volumeMode : Filesystem runner : | state.persistence: enabled: true intervalInMin: 1 revisionsToKeep: 2 persistenceStore: io.siddhi.distribution.core.persistence.FileSystemPersistenceStore config: location: siddhi-app-persistence Save this YAML as power-consume-app.yaml as use kubectl to deploy the app. 1 kubectl apply -f power-consume-app.yaml This kubectl execution in the Siddhi operator will do the following tasks. Create a NATS cluster and streaming cluster since the user did not specify it. Parse the given Siddhi app and create two partial Siddhi apps(passthrough and process). Then deploy both apps in separate deployments to distribute I/O time. Check health of the Siddhi runner and make deployments up and running. Create a service for passthrough app. Create an ingress rule that maps to passthrough service. After a successful deployment, your kubernetes cluster should have these artifacts. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 $ kubectl get SiddhiProcesses NAME STATUS READY AGE power-consume-app Running 2 /2 5m $ kubectl get deployments NAME READY UP-TO-DATE AVAILABLE AGE nats-operator 1 /1 1 1 10m nats-streaming-operator 1 /1 1 1 10m power-consume-app-0 1 /1 1 1 5m power-consume-app-1 1 /1 1 1 5m siddhi-operator 1 /1 1 1 10m $ kubectl get service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE kubernetes ClusterIP 10 .96.0.1 none 443 /TCP 2d7h power-consume-app-0 ClusterIP 10 .105.67.227 none 8080 /TCP 5m siddhi-nats ClusterIP 10 .100.205.21 none 4222 /TCP 10m siddhi-nats-mgmt ClusterIP None none 6222 /TCP,8222/TCP,7777/TCP 10m siddhi-operator ClusterIP 10 .103.229.109 none 8383 /TCP 10m $ kubectl get ingress NAME HOSTS ADDRESS PORTS AGE siddhi siddhi 10 .0.2.15 80 10m $ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE siddhi-pv 1Gi RWO Recycle Bound default/power-consume-app-1-pvc standard 10m $ kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE power-consume-app-1-pvc Bound siddhi-pv 1Gi RWO standard 5m Here power-consume-app-0 is the passthrough deployment and power-consume-app-1 is the process deployment. Now you can send an HTTP request to the passthrough app. 1 2 3 4 5 6 7 8 9 curl -X POST \\ http://siddhi/power-consume-app-0/8080/checkPower \\ -H Accept: */* \\ -H Content-Type: application/json \\ -H Host: siddhi \\ -d { deviceType : dryer , power : 60000 } The process app logs will show that event. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 $ kubectl get pods NAME READY STATUS RESTARTS AGE nats-operator-dd7f4945f-x4vf8 1 /1 Running 0 10m nats-streaming-operator-6fbb6695ff-9rmlx 1 /1 Running 0 10m power-consume-app-0-7486b87979-6tccx 1 /1 Running 0 5m power-consume-app-1-588996fcfb-prncj 1 /1 Running 0 5m siddhi-nats-1 1 /1 Running 0 5m siddhi-operator-6698d8f69d-w2kvj 1 /1 Running 0 10m siddhi-stan-1 1 /1 Running 1 5m $ kubectl logs power-consume-app-1-588996fcfb-prncj JAVA_HOME environment variable is set to /opt/java/openjdk CARBON_HOME environment variable is set to /home/siddhi_user/siddhi-runner RUNTIME_HOME environment variable is set to /home/siddhi_user/siddhi-runner/wso2/runner Picked up JAVA_TOOL_OPTIONS: -XX:+UnlockExperimentalVMOptions -XX:+UseCGroupMemoryLimitForHeap [ 2019 -07-12 14 :09:16,648 ] INFO { org.wso2.carbon.launcher.extensions.OSGiLibBundleDeployerUtils updateOSGiLib } - Successfully updated the OSGi bundle information of Carbon Runtime: runner ... [ 2019 -07-12 14 :12:04,969 ] INFO { io.siddhi.core.stream.output.sink.LogSink } - LOGGER : Event { timestamp = 1562940716559 , data =[ dryer, 60000 ] , isExpired = false }","title":"Deploy and run Siddhi App in Distributed Mode"},{"location":"docs/siddhi-as-a-local-microservice/","text":"Siddhi 5.1 as a Local Microservice This section provides information on running Siddhi Apps on Bare Metal or VM. Siddhi Microservice can run one or more Siddhi Applications with required system configurations. Here, the Siddhi application ( .siddhi file) contains stream processing logic and the necessary system configurations can be passed via the Siddhi configuration .yaml file. Steps to Run Siddhi Local Microservice is as follows. Download the latest Siddhi Runner distribution Unzip the siddhi-runner-x.x.x.zip Start SiddhiApps with the runner config by executing the following commands from the distribution directory Linux/Mac : ./bin/runner.sh -Dapps= siddhi-file -Dconfig= config-yaml-file Windows : bin\\runner.bat -Dapps= siddhi-file -Dconfig= config-yaml-file Running Multiple SiddhiApps in one runner. To run multiple SiddhiApps in one runtime, have all SiddhiApps in a directory and pass its location through -Dapps parameter as follows, -Dapps= siddhi-apps-directory Always use absolute path for SiddhiApps and runner configs. Providing absolute path of SiddhiApp file, or directory in -Dapps parameter, and when providing the Siddhi runner config yaml on -Dconfig parameter while starting Siddhi runner. Siddhi Tooling You can also use the powerful Siddhi Editor to implement and test steam processing applications. Configuring Siddhi To configure databases, extensions, authentication, periodic state persistence, and statistics for Siddhi as Local Microservice refer Siddhi Config Guide . Samples Running Siddhi App Following SiddhiApp collects events via HTTP and logs the number of events arrived during last 15 seconds. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 @ App : name ( CountOverTime ) @ App : description ( Receive events via HTTP, and logs the number of events received during last 15 seconds ) @ source ( type = http , receiver . url = http://0.0.0.0:8006/production , @ map ( type = json )) define stream ProductionStream ( name string , amount double ); @ sink ( type = log ) define stream TotalCountStream ( totalCount long ); -- Count the incoming events @ info ( name = query1 ) from ProductionStream # window . time ( 15 sec ) select count () as totalCount insert into TotalCountStream ; Copy the above SiddhiApp, and create the SiddhiApp file CountOverTime.siddhi . Run the SiddhiApp by executing following commands from the distribution directory Linux/Mac : ./bin/runner.sh -Dapps= absolute-siddhi-file-path /CountOverTime.siddhi Windows : bin\\runner.bat -Dapps= absolute-siddhi-file-path \\CountOverTime.siddhi Test the SiddhiApp by calling the HTTP endpoint using curl or Postman as follows Publish events with curl command: Publish few json to the http endpoint as follows, curl -X POST http://localhost:8006/production \\ --header \"Content-Type:application/json\" \\ -d '{\"event\":{\"name\":\"Cake\",\"amount\":20.12}}' Publish events with Postman: Install 'Postman' application from Chrome web store Launch the application Make a 'Post' request to 'http://localhost:8006/production' endpoint. Set the Content-Type to 'application/json' and set the request body in json format as follows, { \"event\": { \"name\": \"Cake\", \"amount\": 20.12 } } Runner logs the total count on the console. Note, how the count increments with every event sent. [2019-04-11 13:36:03,517] INFO {io.siddhi.core.stream.output.sink.LogSink} - CountOverTime : TotalCountStream : Event{timestamp=1554969963512, data=[1], isExpired=false} [2019-04-11 13:36:10,267] INFO {io.siddhi.core.stream.output.sink.LogSink} - CountOverTime : TotalCountStream : Event{timestamp=1554969970267, data=[2], isExpired=false} [2019-04-11 13:36:41,694] INFO {io.siddhi.core.stream.output.sink.LogSink} - CountOverTime : TotalCountStream : Event{timestamp=1554970001694, data=[1], isExpired=false} Running with runner config When running SiddhiApps users can optionally provide a config yaml to Siddhi runner to manage configurations such as state persistence, databases connections and secure vault. Following SiddhiApp collects events via HTTP and store them in H2 Database. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 @ App : name ( ConsumeAndStore ) @ App : description ( Consume events from HTTP and write to TEST_DB ) @ source ( type = http , receiver . url = http://0.0.0.0:8006/production , @ map ( type = json )) define stream ProductionStream ( name string , amount double ); @ store ( type = rdbms , datasource = TEST_DB ) define table ProductionTable ( name string , amount double ); -- Store all events to the table @ info ( name = query1 ) from ProductionStream insert into ProductionTable ; The runner config can by configured with the relevant datasource information and passed when starting the runner 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 wso2.datasources : dataSources : - name : TEST_DB description : The datasource used for testing definition : type : RDBMS configuration : jdbcUrl : jdbc:h2:${sys:carbon.home}/wso2/${sys:wso2.runtime}/database/TEST_DB;DB_CLOSE_ON_EXIT=FALSE;LOCK_TIMEOUT=60000 username : admin password : admin driverClassName : org.h2.Driver maxPoolSize : 10 idleTimeout : 60000 connectionTestQuery : SELECT 1 validationTimeout : 30000 isAutoCommit : false Copy the above SiddhiApp, & config yaml, and create corresponding the SiddhiApp file ConsumeAndStore.siddhi and TestDb.yaml files. Run the SiddhiApp by executing following commands from the distribution directory Linux/Mac : ./bin/runner.sh -Dapps= absolute-siddhi-file-path /ConsumeAndStore.siddhi \\ -Dconfig= absolute-config-yaml-path /TestDb.yaml Windows : bin\\runner.sh -Dapps= absolute-siddhi-file-path \\ConsumeAndStore.siddhi ^ -Dconfig= absolute-config-yaml-path \\TestDb.yaml Test the SiddhiApp by calling the HTTP endpoint using curl or Postman as follows Publish events with curl command: Publish few json to the http endpoint as follows, curl -X POST http://localhost:8006/production \\ --header \"Content-Type:application/json\" \\ -d '{\"event\":{\"name\":\"Cake\",\"amount\":20.12}}' Publish events with Postman: Install 'Postman' application from Chrome web store Launch the application Make a 'Post' request to 'http://localhost:8006/production' endpoint. Set the Content-Type to 'application/json' and set the request body in json format as follows, { \"event\": { \"name\": \"Cake\", \"amount\": 20.12 } } Query Siddhi Store APIs to retrieve 10 records from the table. Query stored events with curl command: Publish few json to the http endpoint as follows, curl -X POST https://localhost:9443/stores/query \\ -H \"content-type: application/json\" \\ -u \"admin:admin\" \\ -d '{\"appName\" : \"ConsumeAndStore\", \"query\" : \"from ProductionTable select * limit 10;\" }' -k Query stored events with Postman: Install 'Postman' application from Chrome web store Launch the application Make a 'Post' request to 'https://localhost:9443/stores/query' endpoint. Set the Content-Type to 'application/json' and set the request body in json format as follows, { \"appName\" : \"ConsumeAndStore\", \"query\" : \"from ProductionTable select * limit 10;\" } The results of the query will be as follows, { \"records\":[ [\"Cake\",20.12] ] } Running with environmental/system variables Templating SiddhiApps allows users to provide environment/system variables to siddhiApps at runtime. This can help users to migrate SiddhiApps from one environment to another (E.g from dev, test and to prod). Following templated SiddhiApp collects events via HTTP, filters them based on amount greater than a given threshold value, and only sends the filtered events via email. Here the THRESHOLD value, and TO_EMAIL are templated in the TemplatedFilterAndEmail.siddhi SiddhiApp. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 @ App : name ( TemplatedFilterAndEmail ) @ App : description ( Consumes events from HTTP, filters them based on amount greater than a templated threshold value, and sends filtered events via email. ) @ source ( type = http , receiver . url = http://0.0.0.0:8006/production , @ map ( type = json )) define stream ProductionStream ( name string , amount double ); @ sink ( ref = email-sink , subject = High {{name}} production! , to = ${TO_EMAIL} , content . type = text/html , @ map ( type = text , @ payload ( Hi, br/ br/ High production of b {{name}}, /b with amount b {{amount}} /b identified. br/ br/ For more information please contact production department. br/ br/ Thank you ))) define stream FilteredProductionStream ( name string , amount double ); -- Filters the events based on threshold @ info ( name = query1 ) from ProductionStream [ amount ${ THRESHOLD } ] insert into FilteredProductionStream ; The runner config is configured with a gmail account to send email messages in EmailConfig.yaml by templating sending EMAIL_ADDRESS , EMAIL_USERNAME and EMAIL_PASSWORD . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 siddhi : refs : - ref : name : email-sink type : email properties : port : 465 host : smtp.gmail.com ssl.enable : true auth : true ssl.enable : true # User your gmail configurations here address : ${EMAIL_ADDRESS} #E.g. test@gmail.com username : ${EMAIL_USERNAME} #E.g. test password : ${EMAIL_PASSWORD} #E.g. password Copy the above SiddhiApp, & config yaml, and create corresponding the SiddhiApp file TemplatedFilterAndEmail.siddhi and EmailConfig.yaml files. Set environment variables by running following in the termial Siddhi is about to run: export THRESHOLD=20 export TO_EMAIL= to email address export EMAIL_ADDRESS= gmail address export EMAIL_USERNAME= gmail username export EMAIL_PASSWORD= gmail password Or they can also be passed as system variables by adding -DTHRESHOLD=20 -DTO_EMAIL= to email address -DEMAIL_ADDRESS= gmail address -DEMAIL_USERNAME= gmail username -DEMAIL_PASSWORD= gmail password to the end of the runner startup script. Run the SiddhiApp by executing following commands from the distribution directory Linux/Mac : ./bin/runner.sh -Dapps= absolute-file-path /TemplatedFilterAndEmail.siddhi \\ -Dconfig= absolute-config-yaml-path /EmailConfig.yaml Windows : bin\\runner.bat -Dapps= absolute-file-path \\TemplatedFilterAndEmail.siddhi ^ -Dconfig= absolute-config-yaml-path \\EmailConfig.yaml Test the SiddhiApp by calling the HTTP endpoint using curl or Postman as follows Publish events with curl command: Publish few json to the http endpoint as follows, curl -X POST http://localhost:8006/production \\ --header \"Content-Type:application/json\" \\ -d '{\"event\":{\"name\":\"Cake\",\"amount\":2000.0}}' Publish events with Postman: Install 'Postman' application from Chrome web store Launch the application Make a 'Post' request to 'http://localhost:8006/production' endpoint. Set the Content-Type to 'application/json' and set the request body in json format as follows, { \"event\": { \"name\": \"Cake\", \"amount\": 2000.0 } } Check the to.email for the published email message, which will look as follows, Subject : High Cake production! Hi, High production of Cake, with amount 2000.0 identified. For more information please contact production department. Thank you","title":"Siddhi Local Microservice"},{"location":"docs/siddhi-as-a-local-microservice/#siddhi-51-as-a-local-microservice","text":"This section provides information on running Siddhi Apps on Bare Metal or VM. Siddhi Microservice can run one or more Siddhi Applications with required system configurations. Here, the Siddhi application ( .siddhi file) contains stream processing logic and the necessary system configurations can be passed via the Siddhi configuration .yaml file. Steps to Run Siddhi Local Microservice is as follows. Download the latest Siddhi Runner distribution Unzip the siddhi-runner-x.x.x.zip Start SiddhiApps with the runner config by executing the following commands from the distribution directory Linux/Mac : ./bin/runner.sh -Dapps= siddhi-file -Dconfig= config-yaml-file Windows : bin\\runner.bat -Dapps= siddhi-file -Dconfig= config-yaml-file Running Multiple SiddhiApps in one runner. To run multiple SiddhiApps in one runtime, have all SiddhiApps in a directory and pass its location through -Dapps parameter as follows, -Dapps= siddhi-apps-directory Always use absolute path for SiddhiApps and runner configs. Providing absolute path of SiddhiApp file, or directory in -Dapps parameter, and when providing the Siddhi runner config yaml on -Dconfig parameter while starting Siddhi runner. Siddhi Tooling You can also use the powerful Siddhi Editor to implement and test steam processing applications. Configuring Siddhi To configure databases, extensions, authentication, periodic state persistence, and statistics for Siddhi as Local Microservice refer Siddhi Config Guide .","title":"Siddhi 5.1 as a Local Microservice"},{"location":"docs/siddhi-as-a-local-microservice/#samples","text":"","title":"Samples"},{"location":"docs/siddhi-as-a-local-microservice/#running-siddhi-app","text":"Following SiddhiApp collects events via HTTP and logs the number of events arrived during last 15 seconds. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 @ App : name ( CountOverTime ) @ App : description ( Receive events via HTTP, and logs the number of events received during last 15 seconds ) @ source ( type = http , receiver . url = http://0.0.0.0:8006/production , @ map ( type = json )) define stream ProductionStream ( name string , amount double ); @ sink ( type = log ) define stream TotalCountStream ( totalCount long ); -- Count the incoming events @ info ( name = query1 ) from ProductionStream # window . time ( 15 sec ) select count () as totalCount insert into TotalCountStream ; Copy the above SiddhiApp, and create the SiddhiApp file CountOverTime.siddhi . Run the SiddhiApp by executing following commands from the distribution directory Linux/Mac : ./bin/runner.sh -Dapps= absolute-siddhi-file-path /CountOverTime.siddhi Windows : bin\\runner.bat -Dapps= absolute-siddhi-file-path \\CountOverTime.siddhi Test the SiddhiApp by calling the HTTP endpoint using curl or Postman as follows Publish events with curl command: Publish few json to the http endpoint as follows, curl -X POST http://localhost:8006/production \\ --header \"Content-Type:application/json\" \\ -d '{\"event\":{\"name\":\"Cake\",\"amount\":20.12}}' Publish events with Postman: Install 'Postman' application from Chrome web store Launch the application Make a 'Post' request to 'http://localhost:8006/production' endpoint. Set the Content-Type to 'application/json' and set the request body in json format as follows, { \"event\": { \"name\": \"Cake\", \"amount\": 20.12 } } Runner logs the total count on the console. Note, how the count increments with every event sent. [2019-04-11 13:36:03,517] INFO {io.siddhi.core.stream.output.sink.LogSink} - CountOverTime : TotalCountStream : Event{timestamp=1554969963512, data=[1], isExpired=false} [2019-04-11 13:36:10,267] INFO {io.siddhi.core.stream.output.sink.LogSink} - CountOverTime : TotalCountStream : Event{timestamp=1554969970267, data=[2], isExpired=false} [2019-04-11 13:36:41,694] INFO {io.siddhi.core.stream.output.sink.LogSink} - CountOverTime : TotalCountStream : Event{timestamp=1554970001694, data=[1], isExpired=false}","title":"Running Siddhi App"},{"location":"docs/siddhi-as-a-local-microservice/#running-with-runner-config","text":"When running SiddhiApps users can optionally provide a config yaml to Siddhi runner to manage configurations such as state persistence, databases connections and secure vault. Following SiddhiApp collects events via HTTP and store them in H2 Database. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 @ App : name ( ConsumeAndStore ) @ App : description ( Consume events from HTTP and write to TEST_DB ) @ source ( type = http , receiver . url = http://0.0.0.0:8006/production , @ map ( type = json )) define stream ProductionStream ( name string , amount double ); @ store ( type = rdbms , datasource = TEST_DB ) define table ProductionTable ( name string , amount double ); -- Store all events to the table @ info ( name = query1 ) from ProductionStream insert into ProductionTable ; The runner config can by configured with the relevant datasource information and passed when starting the runner 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 wso2.datasources : dataSources : - name : TEST_DB description : The datasource used for testing definition : type : RDBMS configuration : jdbcUrl : jdbc:h2:${sys:carbon.home}/wso2/${sys:wso2.runtime}/database/TEST_DB;DB_CLOSE_ON_EXIT=FALSE;LOCK_TIMEOUT=60000 username : admin password : admin driverClassName : org.h2.Driver maxPoolSize : 10 idleTimeout : 60000 connectionTestQuery : SELECT 1 validationTimeout : 30000 isAutoCommit : false Copy the above SiddhiApp, & config yaml, and create corresponding the SiddhiApp file ConsumeAndStore.siddhi and TestDb.yaml files. Run the SiddhiApp by executing following commands from the distribution directory Linux/Mac : ./bin/runner.sh -Dapps= absolute-siddhi-file-path /ConsumeAndStore.siddhi \\ -Dconfig= absolute-config-yaml-path /TestDb.yaml Windows : bin\\runner.sh -Dapps= absolute-siddhi-file-path \\ConsumeAndStore.siddhi ^ -Dconfig= absolute-config-yaml-path \\TestDb.yaml Test the SiddhiApp by calling the HTTP endpoint using curl or Postman as follows Publish events with curl command: Publish few json to the http endpoint as follows, curl -X POST http://localhost:8006/production \\ --header \"Content-Type:application/json\" \\ -d '{\"event\":{\"name\":\"Cake\",\"amount\":20.12}}' Publish events with Postman: Install 'Postman' application from Chrome web store Launch the application Make a 'Post' request to 'http://localhost:8006/production' endpoint. Set the Content-Type to 'application/json' and set the request body in json format as follows, { \"event\": { \"name\": \"Cake\", \"amount\": 20.12 } } Query Siddhi Store APIs to retrieve 10 records from the table. Query stored events with curl command: Publish few json to the http endpoint as follows, curl -X POST https://localhost:9443/stores/query \\ -H \"content-type: application/json\" \\ -u \"admin:admin\" \\ -d '{\"appName\" : \"ConsumeAndStore\", \"query\" : \"from ProductionTable select * limit 10;\" }' -k Query stored events with Postman: Install 'Postman' application from Chrome web store Launch the application Make a 'Post' request to 'https://localhost:9443/stores/query' endpoint. Set the Content-Type to 'application/json' and set the request body in json format as follows, { \"appName\" : \"ConsumeAndStore\", \"query\" : \"from ProductionTable select * limit 10;\" } The results of the query will be as follows, { \"records\":[ [\"Cake\",20.12] ] }","title":"Running with runner config"},{"location":"docs/siddhi-as-a-local-microservice/#running-with-environmentalsystem-variables","text":"Templating SiddhiApps allows users to provide environment/system variables to siddhiApps at runtime. This can help users to migrate SiddhiApps from one environment to another (E.g from dev, test and to prod). Following templated SiddhiApp collects events via HTTP, filters them based on amount greater than a given threshold value, and only sends the filtered events via email. Here the THRESHOLD value, and TO_EMAIL are templated in the TemplatedFilterAndEmail.siddhi SiddhiApp. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 @ App : name ( TemplatedFilterAndEmail ) @ App : description ( Consumes events from HTTP, filters them based on amount greater than a templated threshold value, and sends filtered events via email. ) @ source ( type = http , receiver . url = http://0.0.0.0:8006/production , @ map ( type = json )) define stream ProductionStream ( name string , amount double ); @ sink ( ref = email-sink , subject = High {{name}} production! , to = ${TO_EMAIL} , content . type = text/html , @ map ( type = text , @ payload ( Hi, br/ br/ High production of b {{name}}, /b with amount b {{amount}} /b identified. br/ br/ For more information please contact production department. br/ br/ Thank you ))) define stream FilteredProductionStream ( name string , amount double ); -- Filters the events based on threshold @ info ( name = query1 ) from ProductionStream [ amount ${ THRESHOLD } ] insert into FilteredProductionStream ; The runner config is configured with a gmail account to send email messages in EmailConfig.yaml by templating sending EMAIL_ADDRESS , EMAIL_USERNAME and EMAIL_PASSWORD . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 siddhi : refs : - ref : name : email-sink type : email properties : port : 465 host : smtp.gmail.com ssl.enable : true auth : true ssl.enable : true # User your gmail configurations here address : ${EMAIL_ADDRESS} #E.g. test@gmail.com username : ${EMAIL_USERNAME} #E.g. test password : ${EMAIL_PASSWORD} #E.g. password Copy the above SiddhiApp, & config yaml, and create corresponding the SiddhiApp file TemplatedFilterAndEmail.siddhi and EmailConfig.yaml files. Set environment variables by running following in the termial Siddhi is about to run: export THRESHOLD=20 export TO_EMAIL= to email address export EMAIL_ADDRESS= gmail address export EMAIL_USERNAME= gmail username export EMAIL_PASSWORD= gmail password Or they can also be passed as system variables by adding -DTHRESHOLD=20 -DTO_EMAIL= to email address -DEMAIL_ADDRESS= gmail address -DEMAIL_USERNAME= gmail username -DEMAIL_PASSWORD= gmail password to the end of the runner startup script. Run the SiddhiApp by executing following commands from the distribution directory Linux/Mac : ./bin/runner.sh -Dapps= absolute-file-path /TemplatedFilterAndEmail.siddhi \\ -Dconfig= absolute-config-yaml-path /EmailConfig.yaml Windows : bin\\runner.bat -Dapps= absolute-file-path \\TemplatedFilterAndEmail.siddhi ^ -Dconfig= absolute-config-yaml-path \\EmailConfig.yaml Test the SiddhiApp by calling the HTTP endpoint using curl or Postman as follows Publish events with curl command: Publish few json to the http endpoint as follows, curl -X POST http://localhost:8006/production \\ --header \"Content-Type:application/json\" \\ -d '{\"event\":{\"name\":\"Cake\",\"amount\":2000.0}}' Publish events with Postman: Install 'Postman' application from Chrome web store Launch the application Make a 'Post' request to 'http://localhost:8006/production' endpoint. Set the Content-Type to 'application/json' and set the request body in json format as follows, { \"event\": { \"name\": \"Cake\", \"amount\": 2000.0 } } Check the to.email for the published email message, which will look as follows, Subject : High Cake production! Hi, High production of Cake, with amount 2000.0 identified. For more information please contact production department. Thank you","title":"Running with environmental/system variables"},{"location":"docs/tooling/","text":"Siddhi 5.1 Tooling Siddhi Editor Siddhi provides tooling that supports following features to develop and test stream processing applications: Text Query Editor with syntax highlighting and advanced auto completion support. Event Simulator and Debugger to test Siddhi Applications. Graphical Query Editor with drag and drop query building support. Graphical Query Editor Text Query Editor","title":"Tooling"},{"location":"docs/tooling/#siddhi-51-tooling","text":"","title":"Siddhi 5.1 Tooling"},{"location":"docs/tooling/#siddhi-editor","text":"Siddhi provides tooling that supports following features to develop and test stream processing applications: Text Query Editor with syntax highlighting and advanced auto completion support. Event Simulator and Debugger to test Siddhi Applications. Graphical Query Editor with drag and drop query building support. Graphical Query Editor Text Query Editor","title":"Siddhi Editor"},{"location":"docs/api/5.1.0/","text":"API Docs - v5.1.0 Core and (Aggregate Function) Returns the results of AND operation for all the events. Syntax 1 BOOL and( BOOL arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be AND operation. BOOL No No Examples EXAMPLE 1 1 2 3 from cscStream#window.lengthBatch(10) select and(isFraud) as isFraudTransaction insert into alertStream; This will returns the result for AND operation of isFraud values as a boolean value for event chunk expiry by window length batch. avg (Aggregate Function) Calculates the average for all the events. Syntax 1 DOUBLE avg( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that need to be averaged. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 1 2 3 from fooStream#window.timeBatch select avg(temp) as avgTemp insert into barStream; avg(temp) returns the average temp value for all the events based on their arrival and expiry. count (Aggregate Function) Returns the count of all the events. Syntax 1 LONG count() Examples EXAMPLE 1 1 2 3 from fooStream#window.timeBatch(10 sec) select count() as count insert into barStream; This will return the count of all the events for time batch in 10 seconds. distinctCount (Aggregate Function) This returns the count of distinct occurrences for a given arg. Syntax 1 LONG distinctCount( INT|LONG|DOUBLE|FLOAT|STRING arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The object for which the number of distinct occurences needs to be counted. INT LONG DOUBLE FLOAT STRING No No Examples EXAMPLE 1 1 2 3 from fooStream select distinctcount(pageID) as count insert into barStream; distinctcount(pageID) for the following output returns '3' when the available values are as follows. \"WEB_PAGE_1\" \"WEB_PAGE_1\" \"WEB_PAGE_2\" \"WEB_PAGE_3\" \"WEB_PAGE_1\" \"WEB_PAGE_2\" The three distinct occurences identified are 'WEB_PAGE_1', 'WEB_PAGE_2', and 'WEB_PAGE_3'. max (Aggregate Function) Returns the maximum value for all the events. Syntax 1 INT|LONG|DOUBLE|FLOAT max( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be compared to find the maximum value. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 1 2 3 from fooStream#window.timeBatch(10 sec) select max(temp) as maxTemp insert into barStream; max(temp) returns the maximum temp value recorded for all the events based on their arrival and expiry. maxForever (Aggregate Function) This is the attribute aggregator to store the maximum value for a given attribute throughout the lifetime of the query regardless of any windows in-front. Syntax 1 INT|LONG|DOUBLE|FLOAT maxForever( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be compared to find the maximum value. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 1 2 3 from inputStream select maxForever(temp) as max insert into outputStream; maxForever(temp) returns the maximum temp value recorded for all the events throughout the lifetime of the query. min (Aggregate Function) Returns the minimum value for all the events. Syntax 1 INT|LONG|DOUBLE|FLOAT min( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be compared to find the minimum value. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 1 2 3 from inputStream select min(temp) as minTemp insert into outputStream; min(temp) returns the minimum temp value recorded for all the events based on their arrival and expiry. minForever (Aggregate Function) This is the attribute aggregator to store the minimum value for a given attribute throughout the lifetime of the query regardless of any windows in-front. Syntax 1 INT|LONG|DOUBLE|FLOAT minForever( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be compared to find the minimum value. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 1 2 3 from inputStream select minForever(temp) as max insert into outputStream; minForever(temp) returns the minimum temp value recorded for all the events throughoutthe lifetime of the query. or (Aggregate Function) Returns the results of OR operation for all the events. Syntax 1 BOOL or( BOOL arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be OR operation. BOOL No No Examples EXAMPLE 1 1 2 3 from cscStream#window.lengthBatch(10) select or(isFraud) as isFraudTransaction insert into alertStream; This will returns the result for OR operation of isFraud values as a boolean value for event chunk expiry by window length batch. stdDev (Aggregate Function) Returns the calculated standard deviation for all the events. Syntax 1 DOUBLE stdDev( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that should be used to calculate the standard deviation. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 1 2 3 from inputStream select stddev(temp) as stdTemp insert into outputStream; stddev(temp) returns the calculated standard deviation of temp for all the events based on their arrival and expiry. sum (Aggregate Function) Returns the sum for all the events. Syntax 1 LONG|DOUBLE sum( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be summed. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 1 2 3 from inputStream select sum(volume) as sumOfVolume insert into outputStream; This will returns the sum of volume values as a long value for each event arrival and expiry. unionSet (Aggregate Function) Union multiple sets. This attribute aggregator maintains a union of sets. The given input set is put into the union set and the union set is returned. Syntax 1 OBJECT unionSet( OBJECT set) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic set The java.util.Set object that needs to be added into the union set. OBJECT No No Examples EXAMPLE 1 1 2 3 4 5 6 7 from stockStream select createSet(symbol) as initialSet insert into initStream from initStream#window.timeBatch(10 sec) select unionSet(initialSet) as distinctSymbols insert into distinctStockStream; distinctStockStream will return the set object which contains the distinct set of stock symbols received during a sliding window of 10 seconds. UUID (Function) Generates a UUID (Universally Unique Identifier). Syntax 1 STRING UUID() Examples EXAMPLE 1 1 2 3 from TempStream select convert(roomNo, string ) as roomNo, temp, UUID() as messageID insert into RoomTempStream; This will converts a room number to string, introducing a message ID to each event asUUID() returns a34eec40-32c2-44fe-8075-7f4fde2e2dd8 from TempStream select convert(roomNo, 'string') as roomNo, temp, UUID() as messageID insert into RoomTempStream; cast (Function) Converts the first parameter according to the cast.to parameter. Incompatible arguments cause Class Cast exceptions if further processed. This function is used with map extension that returns attributes of the object type. You can use this function to cast the object to an accurate and concrete type. Syntax 1 INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT cast( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT to.be.caster, STRING cast.to) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic to.be.caster This specifies the attribute to be casted. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No cast.to A string constant parameter expressing the cast to type using one of the following strings values: int, long, float, double, string, bool. STRING No No Examples EXAMPLE 1 1 2 3 from fooStream select symbol as name, cast(temp, double ) as temp insert into barStream; This will cast the fooStream temp field value into 'double' format. coalesce (Function) Returns the value of the first input parameter that is not null, and all input parameters have to be on the same type. Syntax 1 INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT coalesce( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT args) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic args This function accepts one or more parameters. They can belong to any one of the available types. All the specified parameters should be of the same type. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 1 2 3 from fooStream select coalesce( 123 , null, 789 ) as value insert into barStream; This will returns first null value 123. EXAMPLE 2 1 2 3 from fooStream select coalesce(null, 76, 567) as value insert into barStream; This will returns first null value 76. EXAMPLE 3 1 2 3 from fooStream select coalesce(null, null, null) as value insert into barStream; This will returns null as there are no notnull values. convert (Function) Converts the first input parameter according to the convertedTo parameter. Syntax 1 INT|LONG|DOUBLE|FLOAT|STRING|BOOL convert( INT|LONG|DOUBLE|FLOAT|STRING|BOOL to.be.converted, STRING converted.to) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic to.be.converted This specifies the value to be converted. INT LONG DOUBLE FLOAT STRING BOOL No No converted.to A string constant parameter to which type the attribute need to be converted using one of the following strings values: 'int', 'long', 'float', 'double', 'string', 'bool'. STRING No No Examples EXAMPLE 1 1 2 3 from fooStream select convert(temp, double ) as temp insert into barStream; This will convert fooStream temp value into 'double'. EXAMPLE 2 1 2 3 from fooStream select convert(temp, int ) as temp insert into barStream; This will convert fooStream temp value into 'int' (value = \"convert(45.9, 'int') returns 46\"). createSet (Function) Includes the given input parameter in a java.util.HashSet and returns the set. Syntax 1 OBJECT createSet( INT|LONG|DOUBLE|FLOAT|STRING|BOOL input) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input The input that needs to be added into the set. INT LONG DOUBLE FLOAT STRING BOOL No No Examples EXAMPLE 1 1 2 3 from stockStream select createSet(symbol) as initialSet insert into initStream; For every incoming stockStream event, the initStream stream will produce a set object having only one element: the symbol in the incoming stockStream. currentTimeMillis (Function) Returns the current timestamp of siddhi application in milliseconds. Syntax 1 LONG currentTimeMillis() Examples EXAMPLE 1 1 2 3 from fooStream select symbol as name, currentTimeMillis() as eventTimestamp insert into barStream; This will extract current siddhi application timestamp. default (Function) Checks if the 'attribute' parameter is null and if so returns the value of the 'default' parameter Syntax 1 INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT default( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT attribute, INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT default) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic attribute The attribute that could be null. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No default The default value that will be used when 'attribute' parameter is null INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 1 2 3 from TempStream select default(temp, 0.0) as temp, roomNum insert into StandardTempStream; This will replace TempStream's temp attribute with default value if the temp is null. eventTimestamp (Function) Returns the timestamp of the processed event. Syntax 1 LONG eventTimestamp() Examples EXAMPLE 1 1 2 3 from fooStream select symbol as name, eventTimestamp() as eventTimestamp insert into barStream; This will extract current events timestamp. ifThenElse (Function) Evaluates the 'condition' parameter and returns value of the 'if.expression' parameter if the condition is true, or returns value of the 'else.expression' parameter if the condition is false. Here both 'if.expression' and 'else.expression' should be of the same type. Syntax 1 INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT ifThenElse( BOOL condition, INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT if.expression, INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT else.expression) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic condition This specifies the if then else condition value. BOOL No No if.expression This specifies the value to be returned if the value of the condition parameter is true. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No else.expression This specifies the value to be returned if the value of the condition parameter is false. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 1 2 3 4 @info(name = query1 ) from sensorEventStream select sensorValue, ifThenElse(sensorValue 35, High , Low ) as status insert into outputStream; This will returns High if sensorValue = 50. EXAMPLE 2 1 2 3 4 @info(name = query1 ) from sensorEventStream select sensorValue, ifThenElse(voltage 5, 0, 1) as status insert into outputStream; This will returns 1 if voltage= 12. EXAMPLE 3 1 2 3 4 @info(name = query1 ) from userEventStream select userName, ifThenElse(password == admin , true, false) as passwordState insert into outputStream; This will returns passwordState as true if password = admin. instanceOfBoolean (Function) Checks whether the parameter is an instance of Boolean or not. Syntax 1 BOOL instanceOfBoolean( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 1 2 3 from fooStream select instanceOfBoolean(switchState) as state insert into barStream; This will return true if the value of switchState is true. EXAMPLE 2 1 2 3 from fooStream select instanceOfBoolean(value) as state insert into barStream; if the value = 32 then this will returns false as the value is not an instance of the boolean. instanceOfDouble (Function) Checks whether the parameter is an instance of Double or not. Syntax 1 BOOL instanceOfDouble( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 1 2 3 from fooStream select instanceOfDouble(value) as state insert into barStream; This will return true if the value field format is double ex : 56.45. EXAMPLE 2 1 2 3 from fooStream select instanceOfDouble(switchState) as state insert into barStream; if the switchState = true then this will returns false as the value is not an instance of the double. instanceOfFloat (Function) Checks whether the parameter is an instance of Float or not. Syntax 1 BOOL instanceOfFloat( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 1 2 3 from fooStream select instanceOfFloat(value) as state insert into barStream; This will return true if the value field format is float ex : 56.45f. EXAMPLE 2 1 2 3 from fooStream select instanceOfFloat(switchState) as state insert into barStream; if the switchState = true then this will returns false as the value is an instance of the boolean not a float. instanceOfInteger (Function) Checks whether the parameter is an instance of Integer or not. Syntax 1 BOOL instanceOfInteger( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 1 2 3 from fooStream select instanceOfInteger(value) as state insert into barStream; This will return true if the value field format is integer. EXAMPLE 2 1 2 3 from fooStream select instanceOfInteger(switchState) as state insert into barStream; if the switchState = true then this will returns false as the value is an instance of the boolean not a long. instanceOfLong (Function) Checks whether the parameter is an instance of Long or not. Syntax 1 BOOL instanceOfLong( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 1 2 3 from fooStream select instanceOfLong(value) as state insert into barStream; This will return true if the value field format is long ex : 56456l. EXAMPLE 2 1 2 3 from fooStream select instanceOfLong(switchState) as state insert into barStream; if the switchState = true then this will returns false as the value is an instance of the boolean not a long. instanceOfString (Function) Checks whether the parameter is an instance of String or not. Syntax 1 BOOL instanceOfString( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 1 2 3 from fooStream select instanceOfString(value) as state insert into barStream; This will return true if the value field format is string ex : 'test'. EXAMPLE 2 1 2 3 from fooStream select instanceOfString(switchState) as state insert into barStream; if the switchState = true then this will returns false as the value is an instance of the boolean not a string. maximum (Function) Returns the maximum value of the input parameters. Syntax 1 INT|LONG|DOUBLE|FLOAT maximum( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg This function accepts one or more parameters. They can belong to any one of the available types. All the specified parameters should be of the same type. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 1 2 3 @info(name = query1 ) from inputStream select maximum(price1, price2, price3) as max insert into outputStream; This will returns the maximum value of the input parameters price1, price2, price3. minimum (Function) Returns the minimum value of the input parameters. Syntax 1 INT|LONG|DOUBLE|FLOAT minimum( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg This function accepts one or more parameters. They can belong to any one of the available types. All the specified parameters should be of the same type. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 1 2 3 @info(name = query1 ) from inputStream select maximum(price1, price2, price3) as max insert into outputStream; This will returns the minimum value of the input parameters price1, price2, price3. sizeOfSet (Function) Returns the size of an object of type java.util.Set. Syntax 1 INT sizeOfSet( OBJECT set) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic set The set object. This parameter should be of type java.util.Set. A set object may be created by the 'set' attribute aggregator in Siddhi. OBJECT No No Examples EXAMPLE 1 1 2 3 4 5 6 7 8 9 10 11 from stockStream select initSet(symbol) as initialSet insert into initStream; ;from initStream#window.timeBatch(10 sec) select union(initialSet) as distinctSymbols insert into distinctStockStream; from distinctStockStream select sizeOfSet(distinctSymbols) sizeOfSymbolSet insert into sizeStream; The sizeStream stream will output the number of distinct stock symbols received during a sliding window of 10 seconds. pol2Cart (Stream Function) The pol2Cart function calculating the cartesian coordinates x & y for the given theta, rho coordinates and adding them as new attributes to the existing events. Syntax 1 pol2Cart( DOUBLE theta, DOUBLE rho, DOUBLE z) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic theta The theta value of the coordinates. DOUBLE No No rho The rho value of the coordinates. DOUBLE No No z z value of the cartesian coordinates. If z value is not given, drop the third parameter of the output. DOUBLE Yes No Examples EXAMPLE 1 1 2 3 from PolarStream#pol2Cart(theta, rho) select x, y insert into outputStream ; This will return cartesian coordinates (4.99953024681082, 0.06853693328228748) for theta: 0.7854 and rho: 5. EXAMPLE 2 1 2 3 from PolarStream#pol2Cart(theta, rho, 3.4) select x, y, z insert into outputStream ; This will return cartesian coordinates (4.99953024681082, 0.06853693328228748, 3.4)for theta: 0.7854 and rho: 5 and z: 3.4. log (Stream Processor) The logger logs the message on the given priority with or without processed event. Syntax 1 log( STRING priority, STRING log.message, BOOL is.event.logged) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic priority The priority/type of this log message (INFO, DEBUG, WARN, FATAL, ERROR, OFF, TRACE). INFO STRING Yes No log.message This message will be logged. STRING No No is.event.logged To log the processed event. true BOOL Yes No Examples EXAMPLE 1 1 2 3 from fooStream#log( INFO , Sample Event : , true) select * insert into barStream; This will log as INFO with the message \"Sample Event :\" + fooStream:events. EXAMPLE 2 1 2 3 from fooStream#log( Sample Event : , true) select * insert into barStream; This will logs with default log level as INFO. EXAMPLE 3 1 2 3 from fooStream#log( Sample Event : , fasle) select * insert into barStream; This will only log message. EXAMPLE 4 1 2 3 from fooStream#log(true) select * insert into barStream; This will only log fooStream:events. EXAMPLE 5 1 2 3 from fooStream#log( Sample Event : ) select * insert into barStream; This will log message and fooStream:events. batch (Window) A window that holds an incoming events batch. When a new set of events arrives, the previously arrived old events will be expired. Batch window can be used to aggregate events that comes in batches. If it has the parameter length specified, then batch window process the batch as several chunks. Syntax 1 batch( INT window.length) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.length The length of a chunk If length value was not given it assign 0 as length and process the whole batch as once INT Yes No Examples EXAMPLE 1 1 2 3 4 5 6 define stream consumerItemStream (itemId string, price float) from consumerItemStream#window.batch() select price, str:groupConcat(itemId) as itemIds group by price insert into outputStream; This will output comma separated items IDs that have the same price for each incoming batch of events. cron (Window) This window outputs the arriving events as and when they arrive, and resets (expires) the window periodically based on the given cron expression. Syntax 1 cron( STRING cron.expression) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic cron.expression The cron expression that resets the window. STRING No No Examples EXAMPLE 1 1 2 3 4 5 6 define stream InputEventStream (symbol string, price float, volume int); @info(name = query1 ) from InputEventStream#cron( */5 * * * * ? ) select symbol, sum(price) as totalPrice insert into OutputStream; This let the totalPrice to gradually increase and resets to zero as a batch every 5 seconds. EXAMPLE 2 1 2 3 4 5 6 7 8 9 10 11 define stream StockEventStream (symbol string, price float, volume int) define window StockEventWindow (symbol string, price float, volume int) cron( */5 * * * * ? ); @info(name = query0 ) from StockEventStream insert into StockEventWindow; @info(name = query1 ) from StockEventWindow select symbol, sum(price) as totalPrice insert into OutputStream ; The defined window will let the totalPrice to gradually increase and resets to zero as a batch every 5 seconds. delay (Window) A delay window holds events for a specific time period that is regarded as a delay period before processing them. Syntax 1 delay( INT|LONG|TIME window.delay) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.delay The time period (specified in sec, min, ms) for which the window should delay the events. INT LONG TIME No No Examples EXAMPLE 1 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 define window delayWindow(symbol string, volume int) delay(1 hour); define stream PurchaseStream(symbol string, volume int); define stream DeliveryStream(symbol string); define stream OutputStream(symbol string); @info(name= query1 ) from PurchaseStream select symbol, volume insert into delayWindow; @info(name= query2 ) from delayWindow join DeliveryStream on delayWindow.symbol == DeliveryStream.symbol select delayWindow.symbol insert into OutputStream; In this example, purchase events that arrive in the 'PurchaseStream' stream are directed to a delay window. At any given time, this delay window holds purchase events that have arrived within the last hour. These purchase events in the window are matched by the 'symbol' attribute, with delivery events that arrive in the 'DeliveryStream' stream. This monitors whether the delivery of products is done with a minimum delay of one hour after the purchase. externalTime (Window) A sliding time window based on external time. It holds events that arrived during the last windowTime period from the external timestamp, and gets updated on every monotonically increasing timestamp. Syntax 1 externalTime( LONG timestamp, INT|LONG|TIME window.time) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic timestamp The time which the window determines as current time and will act upon. The value of this parameter should be monotonically increasing. LONG No No window.time The sliding time period for which the window should hold events. INT LONG TIME No No Examples EXAMPLE 1 1 2 3 4 5 6 7 8 9 10 define window cseEventWindow (symbol string, price float, volume int) externalTime(eventTime, 20 sec) output expired events; @info(name = query0 ) from cseEventStream insert into cseEventWindow; @info(name = query1 ) from cseEventWindow select symbol, sum(price) as price insert expired events into outputStream ; processing events arrived within the last 20 seconds from the eventTime and output expired events. externalTimeBatch (Window) A batch (tumbling) time window based on external time, that holds events arrived during windowTime periods, and gets updated for every windowTime. Syntax 1 externalTimeBatch( LONG timestamp, INT|LONG|TIME window.time, INT|LONG|TIME start.time, INT|LONG|TIME timeout) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic timestamp The time which the window determines as current time and will act upon. The value of this parameter should be monotonically increasing. LONG No No window.time The batch time period for which the window should hold events. INT LONG TIME No No start.time User defined start time. This could either be a constant (of type int, long or time) or an attribute of the corresponding stream (of type long). If an attribute is provided, initial value of attribute would be considered as startTime. Timestamp of first event INT LONG TIME Yes No timeout Time to wait for arrival of new event, before flushing and giving output for events belonging to a specific batch. System waits till an event from next batch arrives to flush current batch INT LONG TIME Yes No Examples EXAMPLE 1 1 2 3 4 5 6 7 8 define window cseEventWindow (symbol string, price float, volume int) externalTimeBatch(eventTime, 1 sec) output expired events; @info(name = query0 ) from cseEventStream insert into cseEventWindow; @info(name = query1 ) from cseEventWindow select symbol, sum(price) as price insert expired events into outputStream ; This will processing events that arrive every 1 seconds from the eventTime. EXAMPLE 2 1 define window cseEventWindow (symbol string, price float, volume int) externalTimeBatch(eventTime, 20 sec, 0) output expired events; This will processing events that arrive every 1 seconds from the eventTime. Starts on 0 th millisecond of an hour. EXAMPLE 3 1 define window cseEventWindow (symbol string, price float, volume int) externalTimeBatch(eventTime, 2 sec, eventTimestamp, 100) output expired events; This will processing events that arrive every 2 seconds from the eventTim. Considers the first event's eventTimestamp value as startTime. Waits 100 milliseconds for the arrival of a new event before flushing current batch. frequent (Window) This window returns the latest events with the most frequently occurred value for a given attribute(s). Frequency calculation for this window processor is based on Misra-Gries counting algorithm. Syntax 1 frequent( INT event.count, STRING attribute) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic event.count The number of most frequent events to be emitted to the stream. INT No No attribute The attributes to group the events. If no attributes are given, the concatenation of all the attributes of the event is considered. The concatenation of all the attributes of the event is considered. STRING Yes No Examples EXAMPLE 1 1 2 3 4 @info(name = query1 ) from purchase[price = 30]#window.frequent(2) select cardNo, price insert all events into PotentialFraud; This will returns the 2 most frequent events. EXAMPLE 2 1 2 3 4 @info(name = query1 ) from purchase[price = 30]#window.frequent(2, cardNo) select cardNo, price insert all events into PotentialFraud; This will returns the 2 latest events with the most frequently appeared card numbers. length (Window) A sliding length window that holds the last 'window.length' events at a given time, and gets updated for each arrival and expiry. Syntax 1 length( INT window.length) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.length The number of events that should be included in a sliding length window. INT No No Examples EXAMPLE 1 1 2 3 4 5 6 7 8 9 10 define window StockEventWindow (symbol string, price float, volume int) length(10) output all events; @info(name = query0 ) from StockEventStream insert into StockEventWindow; @info(name = query1 ) from StockEventWindow select symbol, sum(price) as price insert all events into outputStream ; This will process last 10 events in a sliding manner. lengthBatch (Window) A batch (tumbling) length window that holds and process a number of events as specified in the window.length. Syntax 1 lengthBatch( INT window.length, BOOL stream.current.event) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.length The number of events the window should tumble. INT No No stream.current.event Let the window stream the current events out as and when they arrive to the window while expiring them in batches. false BOOL Yes No Examples EXAMPLE 1 1 2 3 4 5 6 define stream InputEventStream (symbol string, price float, volume int); @info(name = query1 ) from InputEventStream#lengthBatch(10) select symbol, sum(price) as price insert into OutputStream; This collect and process 10 events as a batch and output them. EXAMPLE 2 1 2 3 4 5 6 define stream InputEventStream (symbol string, price float, volume int); @info(name = query1 ) from InputEventStream#lengthBatch(10, true) select symbol, sum(price) as sumPrice insert into OutputStream; This window sends the arriving events directly to the output letting the sumPrice to increase gradually, after every 10 events it clears the window as a batch and resets the sumPrice to zero. EXAMPLE 3 1 2 3 4 5 6 7 8 9 10 11 define stream InputEventStream (symbol string, price float, volume int); define window StockEventWindow (symbol string, price float, volume int) lengthBatch(10) output all events; @info(name = query0 ) from InputEventStream insert into StockEventWindow; @info(name = query1 ) from StockEventWindow select symbol, sum(price) as price insert all events into OutputStream ; This uses an defined window to process 10 events as a batch and output all events. lossyFrequent (Window) This window identifies and returns all the events of which the current frequency exceeds the value specified for the supportThreshold parameter. Syntax 1 lossyFrequent( DOUBLE support.threshold, DOUBLE error.bound, STRING attribute) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic support.threshold The support threshold value. DOUBLE No No error.bound The error bound value. DOUBLE No No attribute The attributes to group the events. If no attributes are given, the concatenation of all the attributes of the event is considered. The concatenation of all the attributes of the event is considered. STRING Yes No Examples EXAMPLE 1 1 2 3 4 5 6 7 8 9 define stream purchase (cardNo string, price float); define window purchaseWindow (cardNo string, price float) lossyFrequent(0.1, 0.01); @info(name = query0 ) from purchase[price = 30] insert into purchaseWindow; @info(name = query1 ) from purchaseWindow select cardNo, price insert all events into PotentialFraud; lossyFrequent(0.1, 0.01) returns all the events of which the current frequency exceeds 0.1, with an error bound of 0.01. EXAMPLE 2 1 2 3 4 5 6 7 8 9 define stream purchase (cardNo string, price float); define window purchaseWindow (cardNo string, price float) lossyFrequent(0.3, 0.05, cardNo); @info(name = query0 ) from purchase[price = 30] insert into purchaseWindow; @info(name = query1 ) from purchaseWindow select cardNo, price insert all events into PotentialFraud; lossyFrequent(0.3, 0.05, cardNo) returns all the events of which the cardNo attributes frequency exceeds 0.3, with an error bound of 0.05. session (Window) This is a session window that holds events that belong to a specific session. The events that belong to a specific session are identified by a grouping attribute (i.e., a session key). A session gap period is specified to determine the time period after which the session is considered to be expired. A new event that arrives with a specific value for the session key is matched with the session window with the same session key. There can be out of order and late arrival of events, these events can arrive after the session is expired, to include those events to the matching session key specify a latency time period that is less than the session gap period.To have aggregate functions with session windows, the events need to be grouped by the session key via a 'group by' clause. Syntax 1 session( INT|LONG|TIME window.session, STRING window.key, INT|LONG|TIME window.allowedlatency) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.session The time period for which the session considered is valid. This is specified in seconds, minutes, or milliseconds (i.e., 'min', 'sec', or 'ms'. INT LONG TIME No No window.key The grouping attribute for events. default-key STRING Yes No window.allowedlatency This specifies the time period for which the session window is valid after the expiration of the session. The time period specified here should be less than the session time gap (which is specified via the 'window.session' parameter). 0 INT LONG TIME Yes No Examples EXAMPLE 1 1 2 3 4 5 6 define stream PurchaseEventStream (user string, item_number int, price float, quantity int); @info(name= query0) from PurchaseEventStream#window.session(5 sec, user, 2 sec) select * insert all events into OutputStream; This query processes events that arrive at the PurchaseEvent input stream. The 'user' attribute is the session key, and the session gap is 5 seconds. '2 sec' is specified as the allowed latency. Therefore, events with the matching user name that arrive 2 seconds after the expiration of the session are also considered when performing aggregations for the session identified by the given user name. sort (Window) This window holds a batch of events that equal the number specified as the windowLength and sorts them in the given order. Syntax 1 sort( INT window.length, STRING attribute, STRING order) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.length The size of the window length. INT No No attribute The attribute that should be checked for the order. The concatenation of all the attributes of the event is considered. STRING Yes No order The order define as \"asc\" or \"desc\". asc STRING Yes No Examples EXAMPLE 1 1 2 3 4 5 6 7 8 9 define stream cseEventStream (symbol string, price float, volume long); define window cseEventWindow (symbol string, price float, volume long) sort(2,volume, asc ); @info(name = query0 ) from cseEventStream insert into cseEventWindow; @info(name = query1 ) from cseEventWindow select volume insert all events into outputStream ; sort(5, price, 'asc') keeps the events sorted by price in the ascending order. Therefore, at any given time, the window contains the 5 lowest prices. time (Window) A sliding time window that holds events that arrived during the last windowTime period at a given time, and gets updated for each event arrival and expiry. Syntax 1 time( INT|LONG|TIME window.time) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.time The sliding time period for which the window should hold events. INT LONG TIME No No Examples EXAMPLE 1 1 2 3 4 5 6 7 8 define window cseEventWindow (symbol string, price float, volume int) time(20) output all events; @info(name = query0 ) from cseEventStream insert into cseEventWindow; @info(name = query1 ) from cseEventWindow select symbol, sum(price) as price insert all events into outputStream ; This will processing events that arrived within the last 20 milliseconds. timeBatch (Window) A batch (tumbling) time window that holds and process events that arrive during 'window.time' period as a batch. Syntax 1 timeBatch( INT|LONG|TIME window.time, INT start.time, BOOL stream.current.event) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.time The batch time period in which the window process the events. INT LONG TIME No No start.time This specifies an offset in milliseconds in order to start the window at a time different to the standard time. Timestamp of first event INT Yes No stream.current.event Let the window stream the current events out as and when they arrive to the window while expiring them in batches. false BOOL Yes No Examples EXAMPLE 1 1 2 3 4 5 6 define stream InputEventStream (symbol string, price float, volume int); @info(name = query1 ) from InputEventStream#timeBatch(20 sec) select symbol, sum(price) as price insert into OutputStream; This collect and process incoming events as a batch every 20 seconds and output them. EXAMPLE 2 1 2 3 4 5 6 define stream InputEventStream (symbol string, price float, volume int); @info(name = query1 ) from InputEventStream#timeBatch(20 sec, true) select symbol, sum(price) as sumPrice insert into OutputStream; This window sends the arriving events directly to the output letting the sumPrice to increase gradually and on every 20 second interval it clears the window as a batch resetting the sumPrice to zero. EXAMPLE 3 1 2 3 4 5 6 7 8 9 10 11 define stream InputEventStream (symbol string, price float, volume int); define window StockEventWindow (symbol string, price float, volume int) timeBatch(20 sec) output all events; @info(name = query0 ) from InputEventStream insert into StockEventWindow; @info(name = query1 ) from StockEventWindow select symbol, sum(price) as price insert all events into OutputStream ; This uses an defined window to process events arrived every 20 seconds as a batch and output all events. timeLength (Window) A sliding time window that, at a given time holds the last window.length events that arrived during last window.time period, and gets updated for every event arrival and expiry. Syntax 1 timeLength( INT|LONG|TIME window.time, INT window.length) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.time The sliding time period for which the window should hold events. INT LONG TIME No No window.length The number of events that should be be included in a sliding length window.. INT No No Examples EXAMPLE 1 1 2 3 4 5 6 7 8 define stream cseEventStream (symbol string, price float, volume int); define window cseEventWindow (symbol string, price float, volume int) timeLength(2 sec, 10); @info(name = query0 ) from cseEventStream insert into cseEventWindow; @info(name = query1 ) from cseEventWindow select symbol, price, volume insert all events into outputStream; window.timeLength(2 sec, 10) holds the last 10 events that arrived during last 2 seconds and gets updated for every event arrival and expiry. Sink inMemory (Sink) In-memory transport that can communicate with other in-memory transports within the same JVM, itis assumed that the publisher and subscriber of a topic uses same event schema (stream definition). Syntax 1 @sink(type= inMemory , topic= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic topic Event will be delivered to allthe subscribers of the same topic STRING No No Examples EXAMPLE 1 1 2 @sink(type= inMemory , @map(type= passThrough )) define stream BarStream (symbol string, price float, volume long) In this example BarStream uses inMemory transport which emit the Siddhi events internally without using external transport and transformation. log (Sink) This is a sink that can be used as a logger. This will log the output events in the output stream with user specified priority and a prefix Syntax 1 @sink(type= log , priority= STRING , prefix= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic priority This will set the logger priority i.e log level. Accepted values are INFO, DEBUG, WARN, FATAL, ERROR, OFF, TRACE INFO STRING Yes No prefix This will be the prefix to the output message. If the output stream has event [2,4] and the prefix is given as \"Hello\" then the log will show \"Hello : [2,4]\" default prefix will be : STRING Yes No Examples EXAMPLE 1 1 2 @sink(type= log , prefix= My Log , priority= DEBUG ) define stream BarStream (symbol string, price float, volume long) In this example BarStream uses log sink and the prefix is given as My Log. Also the priority is set to DEBUG. EXAMPLE 2 1 2 @sink(type= log , priority= DEBUG ) define stream BarStream (symbol string, price float, volume long) In this example BarStream uses log sink and the priority is set to DEBUG. User has not specified prefix so the default prefix will be in the form Siddhi App Name : Stream Name EXAMPLE 3 1 2 @sink(type= log , prefix= My Log ) define stream BarStream (symbol string, price float, volume long) In this example BarStream uses log sink and the prefix is given as My Log. User has not given a priority so it will be set to default INFO. EXAMPLE 4 1 2 @sink(type= log ) define stream BarStream (symbol string, price float, volume long) In this example BarStream uses log sink. The user has not given prefix or priority so they will be set to their default values. Sinkmapper passThrough (Sink Mapper) Pass-through mapper passed events (Event[]) through without any mapping or modifications. Syntax 1 @sink(..., @map(type= passThrough ) Examples EXAMPLE 1 1 2 @sink(type= inMemory , @map(type= passThrough )) define stream BarStream (symbol string, price float, volume long); In the following example BarStream uses passThrough outputmapper which emit Siddhi event directly without any transformation into sink. Source inMemory (Source) In-memory source that can communicate with other in-memory sinks within the same JVM, it is assumed that the publisher and subscriber of a topic uses same event schema (stream definition). Syntax 1 @source(type= inMemory , topic= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic topic Subscribes to sent on the given topic. STRING No No Examples EXAMPLE 1 1 2 @source(type= inMemory , @map(type= passThrough )) define stream BarStream (symbol string, price float, volume long) In this example BarStream uses inMemory transport which passes the received event internally without using external transport. Sourcemapper passThrough (Source Mapper) Pass-through mapper passed events (Event[]) through without any mapping or modifications. Syntax 1 @source(..., @map(type= passThrough ) Examples EXAMPLE 1 1 2 @source(type= tcp , @map(type= passThrough )) define stream BarStream (symbol string, price float, volume long); In this example BarStream uses passThrough inputmapper which passes the received Siddhi event directly without any transformation into source.","title":"5.1.0"},{"location":"docs/api/5.1.0/#api-docs-v510","text":"","title":"API Docs - v5.1.0"},{"location":"docs/api/5.1.0/#core","text":"","title":"Core"},{"location":"docs/api/5.1.0/#and-aggregate-function","text":"Returns the results of AND operation for all the events. Syntax 1 BOOL and( BOOL arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be AND operation. BOOL No No Examples EXAMPLE 1 1 2 3 from cscStream#window.lengthBatch(10) select and(isFraud) as isFraudTransaction insert into alertStream; This will returns the result for AND operation of isFraud values as a boolean value for event chunk expiry by window length batch.","title":"and (Aggregate Function)"},{"location":"docs/api/5.1.0/#avg-aggregate-function","text":"Calculates the average for all the events. Syntax 1 DOUBLE avg( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that need to be averaged. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 1 2 3 from fooStream#window.timeBatch select avg(temp) as avgTemp insert into barStream; avg(temp) returns the average temp value for all the events based on their arrival and expiry.","title":"avg (Aggregate Function)"},{"location":"docs/api/5.1.0/#count-aggregate-function","text":"Returns the count of all the events. Syntax 1 LONG count() Examples EXAMPLE 1 1 2 3 from fooStream#window.timeBatch(10 sec) select count() as count insert into barStream; This will return the count of all the events for time batch in 10 seconds.","title":"count (Aggregate Function)"},{"location":"docs/api/5.1.0/#distinctcount-aggregate-function","text":"This returns the count of distinct occurrences for a given arg. Syntax 1 LONG distinctCount( INT|LONG|DOUBLE|FLOAT|STRING arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The object for which the number of distinct occurences needs to be counted. INT LONG DOUBLE FLOAT STRING No No Examples EXAMPLE 1 1 2 3 from fooStream select distinctcount(pageID) as count insert into barStream; distinctcount(pageID) for the following output returns '3' when the available values are as follows. \"WEB_PAGE_1\" \"WEB_PAGE_1\" \"WEB_PAGE_2\" \"WEB_PAGE_3\" \"WEB_PAGE_1\" \"WEB_PAGE_2\" The three distinct occurences identified are 'WEB_PAGE_1', 'WEB_PAGE_2', and 'WEB_PAGE_3'.","title":"distinctCount (Aggregate Function)"},{"location":"docs/api/5.1.0/#max-aggregate-function","text":"Returns the maximum value for all the events. Syntax 1 INT|LONG|DOUBLE|FLOAT max( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be compared to find the maximum value. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 1 2 3 from fooStream#window.timeBatch(10 sec) select max(temp) as maxTemp insert into barStream; max(temp) returns the maximum temp value recorded for all the events based on their arrival and expiry.","title":"max (Aggregate Function)"},{"location":"docs/api/5.1.0/#maxforever-aggregate-function","text":"This is the attribute aggregator to store the maximum value for a given attribute throughout the lifetime of the query regardless of any windows in-front. Syntax 1 INT|LONG|DOUBLE|FLOAT maxForever( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be compared to find the maximum value. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 1 2 3 from inputStream select maxForever(temp) as max insert into outputStream; maxForever(temp) returns the maximum temp value recorded for all the events throughout the lifetime of the query.","title":"maxForever (Aggregate Function)"},{"location":"docs/api/5.1.0/#min-aggregate-function","text":"Returns the minimum value for all the events. Syntax 1 INT|LONG|DOUBLE|FLOAT min( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be compared to find the minimum value. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 1 2 3 from inputStream select min(temp) as minTemp insert into outputStream; min(temp) returns the minimum temp value recorded for all the events based on their arrival and expiry.","title":"min (Aggregate Function)"},{"location":"docs/api/5.1.0/#minforever-aggregate-function","text":"This is the attribute aggregator to store the minimum value for a given attribute throughout the lifetime of the query regardless of any windows in-front. Syntax 1 INT|LONG|DOUBLE|FLOAT minForever( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be compared to find the minimum value. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 1 2 3 from inputStream select minForever(temp) as max insert into outputStream; minForever(temp) returns the minimum temp value recorded for all the events throughoutthe lifetime of the query.","title":"minForever (Aggregate Function)"},{"location":"docs/api/5.1.0/#or-aggregate-function","text":"Returns the results of OR operation for all the events. Syntax 1 BOOL or( BOOL arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be OR operation. BOOL No No Examples EXAMPLE 1 1 2 3 from cscStream#window.lengthBatch(10) select or(isFraud) as isFraudTransaction insert into alertStream; This will returns the result for OR operation of isFraud values as a boolean value for event chunk expiry by window length batch.","title":"or (Aggregate Function)"},{"location":"docs/api/5.1.0/#stddev-aggregate-function","text":"Returns the calculated standard deviation for all the events. Syntax 1 DOUBLE stdDev( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that should be used to calculate the standard deviation. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 1 2 3 from inputStream select stddev(temp) as stdTemp insert into outputStream; stddev(temp) returns the calculated standard deviation of temp for all the events based on their arrival and expiry.","title":"stdDev (Aggregate Function)"},{"location":"docs/api/5.1.0/#sum-aggregate-function","text":"Returns the sum for all the events. Syntax 1 LONG|DOUBLE sum( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be summed. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 1 2 3 from inputStream select sum(volume) as sumOfVolume insert into outputStream; This will returns the sum of volume values as a long value for each event arrival and expiry.","title":"sum (Aggregate Function)"},{"location":"docs/api/5.1.0/#unionset-aggregate-function","text":"Union multiple sets. This attribute aggregator maintains a union of sets. The given input set is put into the union set and the union set is returned. Syntax 1 OBJECT unionSet( OBJECT set) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic set The java.util.Set object that needs to be added into the union set. OBJECT No No Examples EXAMPLE 1 1 2 3 4 5 6 7 from stockStream select createSet(symbol) as initialSet insert into initStream from initStream#window.timeBatch(10 sec) select unionSet(initialSet) as distinctSymbols insert into distinctStockStream; distinctStockStream will return the set object which contains the distinct set of stock symbols received during a sliding window of 10 seconds.","title":"unionSet (Aggregate Function)"},{"location":"docs/api/5.1.0/#uuid-function","text":"Generates a UUID (Universally Unique Identifier). Syntax 1 STRING UUID() Examples EXAMPLE 1 1 2 3 from TempStream select convert(roomNo, string ) as roomNo, temp, UUID() as messageID insert into RoomTempStream; This will converts a room number to string, introducing a message ID to each event asUUID() returns a34eec40-32c2-44fe-8075-7f4fde2e2dd8 from TempStream select convert(roomNo, 'string') as roomNo, temp, UUID() as messageID insert into RoomTempStream;","title":"UUID (Function)"},{"location":"docs/api/5.1.0/#cast-function","text":"Converts the first parameter according to the cast.to parameter. Incompatible arguments cause Class Cast exceptions if further processed. This function is used with map extension that returns attributes of the object type. You can use this function to cast the object to an accurate and concrete type. Syntax 1 INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT cast( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT to.be.caster, STRING cast.to) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic to.be.caster This specifies the attribute to be casted. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No cast.to A string constant parameter expressing the cast to type using one of the following strings values: int, long, float, double, string, bool. STRING No No Examples EXAMPLE 1 1 2 3 from fooStream select symbol as name, cast(temp, double ) as temp insert into barStream; This will cast the fooStream temp field value into 'double' format.","title":"cast (Function)"},{"location":"docs/api/5.1.0/#coalesce-function","text":"Returns the value of the first input parameter that is not null, and all input parameters have to be on the same type. Syntax 1 INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT coalesce( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT args) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic args This function accepts one or more parameters. They can belong to any one of the available types. All the specified parameters should be of the same type. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 1 2 3 from fooStream select coalesce( 123 , null, 789 ) as value insert into barStream; This will returns first null value 123. EXAMPLE 2 1 2 3 from fooStream select coalesce(null, 76, 567) as value insert into barStream; This will returns first null value 76. EXAMPLE 3 1 2 3 from fooStream select coalesce(null, null, null) as value insert into barStream; This will returns null as there are no notnull values.","title":"coalesce (Function)"},{"location":"docs/api/5.1.0/#convert-function","text":"Converts the first input parameter according to the convertedTo parameter. Syntax 1 INT|LONG|DOUBLE|FLOAT|STRING|BOOL convert( INT|LONG|DOUBLE|FLOAT|STRING|BOOL to.be.converted, STRING converted.to) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic to.be.converted This specifies the value to be converted. INT LONG DOUBLE FLOAT STRING BOOL No No converted.to A string constant parameter to which type the attribute need to be converted using one of the following strings values: 'int', 'long', 'float', 'double', 'string', 'bool'. STRING No No Examples EXAMPLE 1 1 2 3 from fooStream select convert(temp, double ) as temp insert into barStream; This will convert fooStream temp value into 'double'. EXAMPLE 2 1 2 3 from fooStream select convert(temp, int ) as temp insert into barStream; This will convert fooStream temp value into 'int' (value = \"convert(45.9, 'int') returns 46\").","title":"convert (Function)"},{"location":"docs/api/5.1.0/#createset-function","text":"Includes the given input parameter in a java.util.HashSet and returns the set. Syntax 1 OBJECT createSet( INT|LONG|DOUBLE|FLOAT|STRING|BOOL input) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input The input that needs to be added into the set. INT LONG DOUBLE FLOAT STRING BOOL No No Examples EXAMPLE 1 1 2 3 from stockStream select createSet(symbol) as initialSet insert into initStream; For every incoming stockStream event, the initStream stream will produce a set object having only one element: the symbol in the incoming stockStream.","title":"createSet (Function)"},{"location":"docs/api/5.1.0/#currenttimemillis-function","text":"Returns the current timestamp of siddhi application in milliseconds. Syntax 1 LONG currentTimeMillis() Examples EXAMPLE 1 1 2 3 from fooStream select symbol as name, currentTimeMillis() as eventTimestamp insert into barStream; This will extract current siddhi application timestamp.","title":"currentTimeMillis (Function)"},{"location":"docs/api/5.1.0/#default-function","text":"Checks if the 'attribute' parameter is null and if so returns the value of the 'default' parameter Syntax 1 INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT default( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT attribute, INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT default) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic attribute The attribute that could be null. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No default The default value that will be used when 'attribute' parameter is null INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 1 2 3 from TempStream select default(temp, 0.0) as temp, roomNum insert into StandardTempStream; This will replace TempStream's temp attribute with default value if the temp is null.","title":"default (Function)"},{"location":"docs/api/5.1.0/#eventtimestamp-function","text":"Returns the timestamp of the processed event. Syntax 1 LONG eventTimestamp() Examples EXAMPLE 1 1 2 3 from fooStream select symbol as name, eventTimestamp() as eventTimestamp insert into barStream; This will extract current events timestamp.","title":"eventTimestamp (Function)"},{"location":"docs/api/5.1.0/#ifthenelse-function","text":"Evaluates the 'condition' parameter and returns value of the 'if.expression' parameter if the condition is true, or returns value of the 'else.expression' parameter if the condition is false. Here both 'if.expression' and 'else.expression' should be of the same type. Syntax 1 INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT ifThenElse( BOOL condition, INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT if.expression, INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT else.expression) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic condition This specifies the if then else condition value. BOOL No No if.expression This specifies the value to be returned if the value of the condition parameter is true. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No else.expression This specifies the value to be returned if the value of the condition parameter is false. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 1 2 3 4 @info(name = query1 ) from sensorEventStream select sensorValue, ifThenElse(sensorValue 35, High , Low ) as status insert into outputStream; This will returns High if sensorValue = 50. EXAMPLE 2 1 2 3 4 @info(name = query1 ) from sensorEventStream select sensorValue, ifThenElse(voltage 5, 0, 1) as status insert into outputStream; This will returns 1 if voltage= 12. EXAMPLE 3 1 2 3 4 @info(name = query1 ) from userEventStream select userName, ifThenElse(password == admin , true, false) as passwordState insert into outputStream; This will returns passwordState as true if password = admin.","title":"ifThenElse (Function)"},{"location":"docs/api/5.1.0/#instanceofboolean-function","text":"Checks whether the parameter is an instance of Boolean or not. Syntax 1 BOOL instanceOfBoolean( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 1 2 3 from fooStream select instanceOfBoolean(switchState) as state insert into barStream; This will return true if the value of switchState is true. EXAMPLE 2 1 2 3 from fooStream select instanceOfBoolean(value) as state insert into barStream; if the value = 32 then this will returns false as the value is not an instance of the boolean.","title":"instanceOfBoolean (Function)"},{"location":"docs/api/5.1.0/#instanceofdouble-function","text":"Checks whether the parameter is an instance of Double or not. Syntax 1 BOOL instanceOfDouble( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 1 2 3 from fooStream select instanceOfDouble(value) as state insert into barStream; This will return true if the value field format is double ex : 56.45. EXAMPLE 2 1 2 3 from fooStream select instanceOfDouble(switchState) as state insert into barStream; if the switchState = true then this will returns false as the value is not an instance of the double.","title":"instanceOfDouble (Function)"},{"location":"docs/api/5.1.0/#instanceoffloat-function","text":"Checks whether the parameter is an instance of Float or not. Syntax 1 BOOL instanceOfFloat( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 1 2 3 from fooStream select instanceOfFloat(value) as state insert into barStream; This will return true if the value field format is float ex : 56.45f. EXAMPLE 2 1 2 3 from fooStream select instanceOfFloat(switchState) as state insert into barStream; if the switchState = true then this will returns false as the value is an instance of the boolean not a float.","title":"instanceOfFloat (Function)"},{"location":"docs/api/5.1.0/#instanceofinteger-function","text":"Checks whether the parameter is an instance of Integer or not. Syntax 1 BOOL instanceOfInteger( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 1 2 3 from fooStream select instanceOfInteger(value) as state insert into barStream; This will return true if the value field format is integer. EXAMPLE 2 1 2 3 from fooStream select instanceOfInteger(switchState) as state insert into barStream; if the switchState = true then this will returns false as the value is an instance of the boolean not a long.","title":"instanceOfInteger (Function)"},{"location":"docs/api/5.1.0/#instanceoflong-function","text":"Checks whether the parameter is an instance of Long or not. Syntax 1 BOOL instanceOfLong( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 1 2 3 from fooStream select instanceOfLong(value) as state insert into barStream; This will return true if the value field format is long ex : 56456l. EXAMPLE 2 1 2 3 from fooStream select instanceOfLong(switchState) as state insert into barStream; if the switchState = true then this will returns false as the value is an instance of the boolean not a long.","title":"instanceOfLong (Function)"},{"location":"docs/api/5.1.0/#instanceofstring-function","text":"Checks whether the parameter is an instance of String or not. Syntax 1 BOOL instanceOfString( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 1 2 3 from fooStream select instanceOfString(value) as state insert into barStream; This will return true if the value field format is string ex : 'test'. EXAMPLE 2 1 2 3 from fooStream select instanceOfString(switchState) as state insert into barStream; if the switchState = true then this will returns false as the value is an instance of the boolean not a string.","title":"instanceOfString (Function)"},{"location":"docs/api/5.1.0/#maximum-function","text":"Returns the maximum value of the input parameters. Syntax 1 INT|LONG|DOUBLE|FLOAT maximum( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg This function accepts one or more parameters. They can belong to any one of the available types. All the specified parameters should be of the same type. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 1 2 3 @info(name = query1 ) from inputStream select maximum(price1, price2, price3) as max insert into outputStream; This will returns the maximum value of the input parameters price1, price2, price3.","title":"maximum (Function)"},{"location":"docs/api/5.1.0/#minimum-function","text":"Returns the minimum value of the input parameters. Syntax 1 INT|LONG|DOUBLE|FLOAT minimum( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg This function accepts one or more parameters. They can belong to any one of the available types. All the specified parameters should be of the same type. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 1 2 3 @info(name = query1 ) from inputStream select maximum(price1, price2, price3) as max insert into outputStream; This will returns the minimum value of the input parameters price1, price2, price3.","title":"minimum (Function)"},{"location":"docs/api/5.1.0/#sizeofset-function","text":"Returns the size of an object of type java.util.Set. Syntax 1 INT sizeOfSet( OBJECT set) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic set The set object. This parameter should be of type java.util.Set. A set object may be created by the 'set' attribute aggregator in Siddhi. OBJECT No No Examples EXAMPLE 1 1 2 3 4 5 6 7 8 9 10 11 from stockStream select initSet(symbol) as initialSet insert into initStream; ;from initStream#window.timeBatch(10 sec) select union(initialSet) as distinctSymbols insert into distinctStockStream; from distinctStockStream select sizeOfSet(distinctSymbols) sizeOfSymbolSet insert into sizeStream; The sizeStream stream will output the number of distinct stock symbols received during a sliding window of 10 seconds.","title":"sizeOfSet (Function)"},{"location":"docs/api/5.1.0/#pol2cart-stream-function","text":"The pol2Cart function calculating the cartesian coordinates x & y for the given theta, rho coordinates and adding them as new attributes to the existing events. Syntax 1 pol2Cart( DOUBLE theta, DOUBLE rho, DOUBLE z) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic theta The theta value of the coordinates. DOUBLE No No rho The rho value of the coordinates. DOUBLE No No z z value of the cartesian coordinates. If z value is not given, drop the third parameter of the output. DOUBLE Yes No Examples EXAMPLE 1 1 2 3 from PolarStream#pol2Cart(theta, rho) select x, y insert into outputStream ; This will return cartesian coordinates (4.99953024681082, 0.06853693328228748) for theta: 0.7854 and rho: 5. EXAMPLE 2 1 2 3 from PolarStream#pol2Cart(theta, rho, 3.4) select x, y, z insert into outputStream ; This will return cartesian coordinates (4.99953024681082, 0.06853693328228748, 3.4)for theta: 0.7854 and rho: 5 and z: 3.4.","title":"pol2Cart (Stream Function)"},{"location":"docs/api/5.1.0/#log-stream-processor","text":"The logger logs the message on the given priority with or without processed event. Syntax 1 log( STRING priority, STRING log.message, BOOL is.event.logged) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic priority The priority/type of this log message (INFO, DEBUG, WARN, FATAL, ERROR, OFF, TRACE). INFO STRING Yes No log.message This message will be logged. STRING No No is.event.logged To log the processed event. true BOOL Yes No Examples EXAMPLE 1 1 2 3 from fooStream#log( INFO , Sample Event : , true) select * insert into barStream; This will log as INFO with the message \"Sample Event :\" + fooStream:events. EXAMPLE 2 1 2 3 from fooStream#log( Sample Event : , true) select * insert into barStream; This will logs with default log level as INFO. EXAMPLE 3 1 2 3 from fooStream#log( Sample Event : , fasle) select * insert into barStream; This will only log message. EXAMPLE 4 1 2 3 from fooStream#log(true) select * insert into barStream; This will only log fooStream:events. EXAMPLE 5 1 2 3 from fooStream#log( Sample Event : ) select * insert into barStream; This will log message and fooStream:events.","title":"log (Stream Processor)"},{"location":"docs/api/5.1.0/#batch-window","text":"A window that holds an incoming events batch. When a new set of events arrives, the previously arrived old events will be expired. Batch window can be used to aggregate events that comes in batches. If it has the parameter length specified, then batch window process the batch as several chunks. Syntax 1 batch( INT window.length) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.length The length of a chunk If length value was not given it assign 0 as length and process the whole batch as once INT Yes No Examples EXAMPLE 1 1 2 3 4 5 6 define stream consumerItemStream (itemId string, price float) from consumerItemStream#window.batch() select price, str:groupConcat(itemId) as itemIds group by price insert into outputStream; This will output comma separated items IDs that have the same price for each incoming batch of events.","title":"batch (Window)"},{"location":"docs/api/5.1.0/#cron-window","text":"This window outputs the arriving events as and when they arrive, and resets (expires) the window periodically based on the given cron expression. Syntax 1 cron( STRING cron.expression) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic cron.expression The cron expression that resets the window. STRING No No Examples EXAMPLE 1 1 2 3 4 5 6 define stream InputEventStream (symbol string, price float, volume int); @info(name = query1 ) from InputEventStream#cron( */5 * * * * ? ) select symbol, sum(price) as totalPrice insert into OutputStream; This let the totalPrice to gradually increase and resets to zero as a batch every 5 seconds. EXAMPLE 2 1 2 3 4 5 6 7 8 9 10 11 define stream StockEventStream (symbol string, price float, volume int) define window StockEventWindow (symbol string, price float, volume int) cron( */5 * * * * ? ); @info(name = query0 ) from StockEventStream insert into StockEventWindow; @info(name = query1 ) from StockEventWindow select symbol, sum(price) as totalPrice insert into OutputStream ; The defined window will let the totalPrice to gradually increase and resets to zero as a batch every 5 seconds.","title":"cron (Window)"},{"location":"docs/api/5.1.0/#delay-window","text":"A delay window holds events for a specific time period that is regarded as a delay period before processing them. Syntax 1 delay( INT|LONG|TIME window.delay) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.delay The time period (specified in sec, min, ms) for which the window should delay the events. INT LONG TIME No No Examples EXAMPLE 1 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 define window delayWindow(symbol string, volume int) delay(1 hour); define stream PurchaseStream(symbol string, volume int); define stream DeliveryStream(symbol string); define stream OutputStream(symbol string); @info(name= query1 ) from PurchaseStream select symbol, volume insert into delayWindow; @info(name= query2 ) from delayWindow join DeliveryStream on delayWindow.symbol == DeliveryStream.symbol select delayWindow.symbol insert into OutputStream; In this example, purchase events that arrive in the 'PurchaseStream' stream are directed to a delay window. At any given time, this delay window holds purchase events that have arrived within the last hour. These purchase events in the window are matched by the 'symbol' attribute, with delivery events that arrive in the 'DeliveryStream' stream. This monitors whether the delivery of products is done with a minimum delay of one hour after the purchase.","title":"delay (Window)"},{"location":"docs/api/5.1.0/#externaltime-window","text":"A sliding time window based on external time. It holds events that arrived during the last windowTime period from the external timestamp, and gets updated on every monotonically increasing timestamp. Syntax 1 externalTime( LONG timestamp, INT|LONG|TIME window.time) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic timestamp The time which the window determines as current time and will act upon. The value of this parameter should be monotonically increasing. LONG No No window.time The sliding time period for which the window should hold events. INT LONG TIME No No Examples EXAMPLE 1 1 2 3 4 5 6 7 8 9 10 define window cseEventWindow (symbol string, price float, volume int) externalTime(eventTime, 20 sec) output expired events; @info(name = query0 ) from cseEventStream insert into cseEventWindow; @info(name = query1 ) from cseEventWindow select symbol, sum(price) as price insert expired events into outputStream ; processing events arrived within the last 20 seconds from the eventTime and output expired events.","title":"externalTime (Window)"},{"location":"docs/api/5.1.0/#externaltimebatch-window","text":"A batch (tumbling) time window based on external time, that holds events arrived during windowTime periods, and gets updated for every windowTime. Syntax 1 externalTimeBatch( LONG timestamp, INT|LONG|TIME window.time, INT|LONG|TIME start.time, INT|LONG|TIME timeout) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic timestamp The time which the window determines as current time and will act upon. The value of this parameter should be monotonically increasing. LONG No No window.time The batch time period for which the window should hold events. INT LONG TIME No No start.time User defined start time. This could either be a constant (of type int, long or time) or an attribute of the corresponding stream (of type long). If an attribute is provided, initial value of attribute would be considered as startTime. Timestamp of first event INT LONG TIME Yes No timeout Time to wait for arrival of new event, before flushing and giving output for events belonging to a specific batch. System waits till an event from next batch arrives to flush current batch INT LONG TIME Yes No Examples EXAMPLE 1 1 2 3 4 5 6 7 8 define window cseEventWindow (symbol string, price float, volume int) externalTimeBatch(eventTime, 1 sec) output expired events; @info(name = query0 ) from cseEventStream insert into cseEventWindow; @info(name = query1 ) from cseEventWindow select symbol, sum(price) as price insert expired events into outputStream ; This will processing events that arrive every 1 seconds from the eventTime. EXAMPLE 2 1 define window cseEventWindow (symbol string, price float, volume int) externalTimeBatch(eventTime, 20 sec, 0) output expired events; This will processing events that arrive every 1 seconds from the eventTime. Starts on 0 th millisecond of an hour. EXAMPLE 3 1 define window cseEventWindow (symbol string, price float, volume int) externalTimeBatch(eventTime, 2 sec, eventTimestamp, 100) output expired events; This will processing events that arrive every 2 seconds from the eventTim. Considers the first event's eventTimestamp value as startTime. Waits 100 milliseconds for the arrival of a new event before flushing current batch.","title":"externalTimeBatch (Window)"},{"location":"docs/api/5.1.0/#frequent-window","text":"This window returns the latest events with the most frequently occurred value for a given attribute(s). Frequency calculation for this window processor is based on Misra-Gries counting algorithm. Syntax 1 frequent( INT event.count, STRING attribute) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic event.count The number of most frequent events to be emitted to the stream. INT No No attribute The attributes to group the events. If no attributes are given, the concatenation of all the attributes of the event is considered. The concatenation of all the attributes of the event is considered. STRING Yes No Examples EXAMPLE 1 1 2 3 4 @info(name = query1 ) from purchase[price = 30]#window.frequent(2) select cardNo, price insert all events into PotentialFraud; This will returns the 2 most frequent events. EXAMPLE 2 1 2 3 4 @info(name = query1 ) from purchase[price = 30]#window.frequent(2, cardNo) select cardNo, price insert all events into PotentialFraud; This will returns the 2 latest events with the most frequently appeared card numbers.","title":"frequent (Window)"},{"location":"docs/api/5.1.0/#length-window","text":"A sliding length window that holds the last 'window.length' events at a given time, and gets updated for each arrival and expiry. Syntax 1 length( INT window.length) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.length The number of events that should be included in a sliding length window. INT No No Examples EXAMPLE 1 1 2 3 4 5 6 7 8 9 10 define window StockEventWindow (symbol string, price float, volume int) length(10) output all events; @info(name = query0 ) from StockEventStream insert into StockEventWindow; @info(name = query1 ) from StockEventWindow select symbol, sum(price) as price insert all events into outputStream ; This will process last 10 events in a sliding manner.","title":"length (Window)"},{"location":"docs/api/5.1.0/#lengthbatch-window","text":"A batch (tumbling) length window that holds and process a number of events as specified in the window.length. Syntax 1 lengthBatch( INT window.length, BOOL stream.current.event) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.length The number of events the window should tumble. INT No No stream.current.event Let the window stream the current events out as and when they arrive to the window while expiring them in batches. false BOOL Yes No Examples EXAMPLE 1 1 2 3 4 5 6 define stream InputEventStream (symbol string, price float, volume int); @info(name = query1 ) from InputEventStream#lengthBatch(10) select symbol, sum(price) as price insert into OutputStream; This collect and process 10 events as a batch and output them. EXAMPLE 2 1 2 3 4 5 6 define stream InputEventStream (symbol string, price float, volume int); @info(name = query1 ) from InputEventStream#lengthBatch(10, true) select symbol, sum(price) as sumPrice insert into OutputStream; This window sends the arriving events directly to the output letting the sumPrice to increase gradually, after every 10 events it clears the window as a batch and resets the sumPrice to zero. EXAMPLE 3 1 2 3 4 5 6 7 8 9 10 11 define stream InputEventStream (symbol string, price float, volume int); define window StockEventWindow (symbol string, price float, volume int) lengthBatch(10) output all events; @info(name = query0 ) from InputEventStream insert into StockEventWindow; @info(name = query1 ) from StockEventWindow select symbol, sum(price) as price insert all events into OutputStream ; This uses an defined window to process 10 events as a batch and output all events.","title":"lengthBatch (Window)"},{"location":"docs/api/5.1.0/#lossyfrequent-window","text":"This window identifies and returns all the events of which the current frequency exceeds the value specified for the supportThreshold parameter. Syntax 1 lossyFrequent( DOUBLE support.threshold, DOUBLE error.bound, STRING attribute) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic support.threshold The support threshold value. DOUBLE No No error.bound The error bound value. DOUBLE No No attribute The attributes to group the events. If no attributes are given, the concatenation of all the attributes of the event is considered. The concatenation of all the attributes of the event is considered. STRING Yes No Examples EXAMPLE 1 1 2 3 4 5 6 7 8 9 define stream purchase (cardNo string, price float); define window purchaseWindow (cardNo string, price float) lossyFrequent(0.1, 0.01); @info(name = query0 ) from purchase[price = 30] insert into purchaseWindow; @info(name = query1 ) from purchaseWindow select cardNo, price insert all events into PotentialFraud; lossyFrequent(0.1, 0.01) returns all the events of which the current frequency exceeds 0.1, with an error bound of 0.01. EXAMPLE 2 1 2 3 4 5 6 7 8 9 define stream purchase (cardNo string, price float); define window purchaseWindow (cardNo string, price float) lossyFrequent(0.3, 0.05, cardNo); @info(name = query0 ) from purchase[price = 30] insert into purchaseWindow; @info(name = query1 ) from purchaseWindow select cardNo, price insert all events into PotentialFraud; lossyFrequent(0.3, 0.05, cardNo) returns all the events of which the cardNo attributes frequency exceeds 0.3, with an error bound of 0.05.","title":"lossyFrequent (Window)"},{"location":"docs/api/5.1.0/#session-window","text":"This is a session window that holds events that belong to a specific session. The events that belong to a specific session are identified by a grouping attribute (i.e., a session key). A session gap period is specified to determine the time period after which the session is considered to be expired. A new event that arrives with a specific value for the session key is matched with the session window with the same session key. There can be out of order and late arrival of events, these events can arrive after the session is expired, to include those events to the matching session key specify a latency time period that is less than the session gap period.To have aggregate functions with session windows, the events need to be grouped by the session key via a 'group by' clause. Syntax 1 session( INT|LONG|TIME window.session, STRING window.key, INT|LONG|TIME window.allowedlatency) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.session The time period for which the session considered is valid. This is specified in seconds, minutes, or milliseconds (i.e., 'min', 'sec', or 'ms'. INT LONG TIME No No window.key The grouping attribute for events. default-key STRING Yes No window.allowedlatency This specifies the time period for which the session window is valid after the expiration of the session. The time period specified here should be less than the session time gap (which is specified via the 'window.session' parameter). 0 INT LONG TIME Yes No Examples EXAMPLE 1 1 2 3 4 5 6 define stream PurchaseEventStream (user string, item_number int, price float, quantity int); @info(name= query0) from PurchaseEventStream#window.session(5 sec, user, 2 sec) select * insert all events into OutputStream; This query processes events that arrive at the PurchaseEvent input stream. The 'user' attribute is the session key, and the session gap is 5 seconds. '2 sec' is specified as the allowed latency. Therefore, events with the matching user name that arrive 2 seconds after the expiration of the session are also considered when performing aggregations for the session identified by the given user name.","title":"session (Window)"},{"location":"docs/api/5.1.0/#sort-window","text":"This window holds a batch of events that equal the number specified as the windowLength and sorts them in the given order. Syntax 1 sort( INT window.length, STRING attribute, STRING order) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.length The size of the window length. INT No No attribute The attribute that should be checked for the order. The concatenation of all the attributes of the event is considered. STRING Yes No order The order define as \"asc\" or \"desc\". asc STRING Yes No Examples EXAMPLE 1 1 2 3 4 5 6 7 8 9 define stream cseEventStream (symbol string, price float, volume long); define window cseEventWindow (symbol string, price float, volume long) sort(2,volume, asc ); @info(name = query0 ) from cseEventStream insert into cseEventWindow; @info(name = query1 ) from cseEventWindow select volume insert all events into outputStream ; sort(5, price, 'asc') keeps the events sorted by price in the ascending order. Therefore, at any given time, the window contains the 5 lowest prices.","title":"sort (Window)"},{"location":"docs/api/5.1.0/#time-window","text":"A sliding time window that holds events that arrived during the last windowTime period at a given time, and gets updated for each event arrival and expiry. Syntax 1 time( INT|LONG|TIME window.time) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.time The sliding time period for which the window should hold events. INT LONG TIME No No Examples EXAMPLE 1 1 2 3 4 5 6 7 8 define window cseEventWindow (symbol string, price float, volume int) time(20) output all events; @info(name = query0 ) from cseEventStream insert into cseEventWindow; @info(name = query1 ) from cseEventWindow select symbol, sum(price) as price insert all events into outputStream ; This will processing events that arrived within the last 20 milliseconds.","title":"time (Window)"},{"location":"docs/api/5.1.0/#timebatch-window","text":"A batch (tumbling) time window that holds and process events that arrive during 'window.time' period as a batch. Syntax 1 timeBatch( INT|LONG|TIME window.time, INT start.time, BOOL stream.current.event) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.time The batch time period in which the window process the events. INT LONG TIME No No start.time This specifies an offset in milliseconds in order to start the window at a time different to the standard time. Timestamp of first event INT Yes No stream.current.event Let the window stream the current events out as and when they arrive to the window while expiring them in batches. false BOOL Yes No Examples EXAMPLE 1 1 2 3 4 5 6 define stream InputEventStream (symbol string, price float, volume int); @info(name = query1 ) from InputEventStream#timeBatch(20 sec) select symbol, sum(price) as price insert into OutputStream; This collect and process incoming events as a batch every 20 seconds and output them. EXAMPLE 2 1 2 3 4 5 6 define stream InputEventStream (symbol string, price float, volume int); @info(name = query1 ) from InputEventStream#timeBatch(20 sec, true) select symbol, sum(price) as sumPrice insert into OutputStream; This window sends the arriving events directly to the output letting the sumPrice to increase gradually and on every 20 second interval it clears the window as a batch resetting the sumPrice to zero. EXAMPLE 3 1 2 3 4 5 6 7 8 9 10 11 define stream InputEventStream (symbol string, price float, volume int); define window StockEventWindow (symbol string, price float, volume int) timeBatch(20 sec) output all events; @info(name = query0 ) from InputEventStream insert into StockEventWindow; @info(name = query1 ) from StockEventWindow select symbol, sum(price) as price insert all events into OutputStream ; This uses an defined window to process events arrived every 20 seconds as a batch and output all events.","title":"timeBatch (Window)"},{"location":"docs/api/5.1.0/#timelength-window","text":"A sliding time window that, at a given time holds the last window.length events that arrived during last window.time period, and gets updated for every event arrival and expiry. Syntax 1 timeLength( INT|LONG|TIME window.time, INT window.length) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.time The sliding time period for which the window should hold events. INT LONG TIME No No window.length The number of events that should be be included in a sliding length window.. INT No No Examples EXAMPLE 1 1 2 3 4 5 6 7 8 define stream cseEventStream (symbol string, price float, volume int); define window cseEventWindow (symbol string, price float, volume int) timeLength(2 sec, 10); @info(name = query0 ) from cseEventStream insert into cseEventWindow; @info(name = query1 ) from cseEventWindow select symbol, price, volume insert all events into outputStream; window.timeLength(2 sec, 10) holds the last 10 events that arrived during last 2 seconds and gets updated for every event arrival and expiry.","title":"timeLength (Window)"},{"location":"docs/api/5.1.0/#sink","text":"","title":"Sink"},{"location":"docs/api/5.1.0/#inmemory-sink","text":"In-memory transport that can communicate with other in-memory transports within the same JVM, itis assumed that the publisher and subscriber of a topic uses same event schema (stream definition). Syntax 1 @sink(type= inMemory , topic= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic topic Event will be delivered to allthe subscribers of the same topic STRING No No Examples EXAMPLE 1 1 2 @sink(type= inMemory , @map(type= passThrough )) define stream BarStream (symbol string, price float, volume long) In this example BarStream uses inMemory transport which emit the Siddhi events internally without using external transport and transformation.","title":"inMemory (Sink)"},{"location":"docs/api/5.1.0/#log-sink","text":"This is a sink that can be used as a logger. This will log the output events in the output stream with user specified priority and a prefix Syntax 1 @sink(type= log , priority= STRING , prefix= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic priority This will set the logger priority i.e log level. Accepted values are INFO, DEBUG, WARN, FATAL, ERROR, OFF, TRACE INFO STRING Yes No prefix This will be the prefix to the output message. If the output stream has event [2,4] and the prefix is given as \"Hello\" then the log will show \"Hello : [2,4]\" default prefix will be : STRING Yes No Examples EXAMPLE 1 1 2 @sink(type= log , prefix= My Log , priority= DEBUG ) define stream BarStream (symbol string, price float, volume long) In this example BarStream uses log sink and the prefix is given as My Log. Also the priority is set to DEBUG. EXAMPLE 2 1 2 @sink(type= log , priority= DEBUG ) define stream BarStream (symbol string, price float, volume long) In this example BarStream uses log sink and the priority is set to DEBUG. User has not specified prefix so the default prefix will be in the form Siddhi App Name : Stream Name EXAMPLE 3 1 2 @sink(type= log , prefix= My Log ) define stream BarStream (symbol string, price float, volume long) In this example BarStream uses log sink and the prefix is given as My Log. User has not given a priority so it will be set to default INFO. EXAMPLE 4 1 2 @sink(type= log ) define stream BarStream (symbol string, price float, volume long) In this example BarStream uses log sink. The user has not given prefix or priority so they will be set to their default values.","title":"log (Sink)"},{"location":"docs/api/5.1.0/#sinkmapper","text":"","title":"Sinkmapper"},{"location":"docs/api/5.1.0/#passthrough-sink-mapper","text":"Pass-through mapper passed events (Event[]) through without any mapping or modifications. Syntax 1 @sink(..., @map(type= passThrough ) Examples EXAMPLE 1 1 2 @sink(type= inMemory , @map(type= passThrough )) define stream BarStream (symbol string, price float, volume long); In the following example BarStream uses passThrough outputmapper which emit Siddhi event directly without any transformation into sink.","title":"passThrough (Sink Mapper)"},{"location":"docs/api/5.1.0/#source","text":"","title":"Source"},{"location":"docs/api/5.1.0/#inmemory-source","text":"In-memory source that can communicate with other in-memory sinks within the same JVM, it is assumed that the publisher and subscriber of a topic uses same event schema (stream definition). Syntax 1 @source(type= inMemory , topic= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic topic Subscribes to sent on the given topic. STRING No No Examples EXAMPLE 1 1 2 @source(type= inMemory , @map(type= passThrough )) define stream BarStream (symbol string, price float, volume long) In this example BarStream uses inMemory transport which passes the received event internally without using external transport.","title":"inMemory (Source)"},{"location":"docs/api/5.1.0/#sourcemapper","text":"","title":"Sourcemapper"},{"location":"docs/api/5.1.0/#passthrough-source-mapper","text":"Pass-through mapper passed events (Event[]) through without any mapping or modifications. Syntax 1 @source(..., @map(type= passThrough ) Examples EXAMPLE 1 1 2 @source(type= tcp , @map(type= passThrough )) define stream BarStream (symbol string, price float, volume long); In this example BarStream uses passThrough inputmapper which passes the received Siddhi event directly without any transformation into source.","title":"passThrough (Source Mapper)"},{"location":"docs/api/latest/","text":"API Docs - v5.1.0 Core and (Aggregate Function) Returns the results of AND operation for all the events. Syntax 1 BOOL and( BOOL arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be AND operation. BOOL No No Examples EXAMPLE 1 1 2 3 from cscStream#window.lengthBatch(10) select and(isFraud) as isFraudTransaction insert into alertStream; This will returns the result for AND operation of isFraud values as a boolean value for event chunk expiry by window length batch. avg (Aggregate Function) Calculates the average for all the events. Syntax 1 DOUBLE avg( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that need to be averaged. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 1 2 3 from fooStream#window.timeBatch select avg(temp) as avgTemp insert into barStream; avg(temp) returns the average temp value for all the events based on their arrival and expiry. count (Aggregate Function) Returns the count of all the events. Syntax 1 LONG count() Examples EXAMPLE 1 1 2 3 from fooStream#window.timeBatch(10 sec) select count() as count insert into barStream; This will return the count of all the events for time batch in 10 seconds. distinctCount (Aggregate Function) This returns the count of distinct occurrences for a given arg. Syntax 1 LONG distinctCount( INT|LONG|DOUBLE|FLOAT|STRING arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The object for which the number of distinct occurences needs to be counted. INT LONG DOUBLE FLOAT STRING No No Examples EXAMPLE 1 1 2 3 from fooStream select distinctcount(pageID) as count insert into barStream; distinctcount(pageID) for the following output returns '3' when the available values are as follows. \"WEB_PAGE_1\" \"WEB_PAGE_1\" \"WEB_PAGE_2\" \"WEB_PAGE_3\" \"WEB_PAGE_1\" \"WEB_PAGE_2\" The three distinct occurences identified are 'WEB_PAGE_1', 'WEB_PAGE_2', and 'WEB_PAGE_3'. max (Aggregate Function) Returns the maximum value for all the events. Syntax 1 INT|LONG|DOUBLE|FLOAT max( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be compared to find the maximum value. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 1 2 3 from fooStream#window.timeBatch(10 sec) select max(temp) as maxTemp insert into barStream; max(temp) returns the maximum temp value recorded for all the events based on their arrival and expiry. maxForever (Aggregate Function) This is the attribute aggregator to store the maximum value for a given attribute throughout the lifetime of the query regardless of any windows in-front. Syntax 1 INT|LONG|DOUBLE|FLOAT maxForever( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be compared to find the maximum value. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 1 2 3 from inputStream select maxForever(temp) as max insert into outputStream; maxForever(temp) returns the maximum temp value recorded for all the events throughout the lifetime of the query. min (Aggregate Function) Returns the minimum value for all the events. Syntax 1 INT|LONG|DOUBLE|FLOAT min( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be compared to find the minimum value. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 1 2 3 from inputStream select min(temp) as minTemp insert into outputStream; min(temp) returns the minimum temp value recorded for all the events based on their arrival and expiry. minForever (Aggregate Function) This is the attribute aggregator to store the minimum value for a given attribute throughout the lifetime of the query regardless of any windows in-front. Syntax 1 INT|LONG|DOUBLE|FLOAT minForever( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be compared to find the minimum value. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 1 2 3 from inputStream select minForever(temp) as max insert into outputStream; minForever(temp) returns the minimum temp value recorded for all the events throughoutthe lifetime of the query. or (Aggregate Function) Returns the results of OR operation for all the events. Syntax 1 BOOL or( BOOL arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be OR operation. BOOL No No Examples EXAMPLE 1 1 2 3 from cscStream#window.lengthBatch(10) select or(isFraud) as isFraudTransaction insert into alertStream; This will returns the result for OR operation of isFraud values as a boolean value for event chunk expiry by window length batch. stdDev (Aggregate Function) Returns the calculated standard deviation for all the events. Syntax 1 DOUBLE stdDev( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that should be used to calculate the standard deviation. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 1 2 3 from inputStream select stddev(temp) as stdTemp insert into outputStream; stddev(temp) returns the calculated standard deviation of temp for all the events based on their arrival and expiry. sum (Aggregate Function) Returns the sum for all the events. Syntax 1 LONG|DOUBLE sum( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be summed. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 1 2 3 from inputStream select sum(volume) as sumOfVolume insert into outputStream; This will returns the sum of volume values as a long value for each event arrival and expiry. unionSet (Aggregate Function) Union multiple sets. This attribute aggregator maintains a union of sets. The given input set is put into the union set and the union set is returned. Syntax 1 OBJECT unionSet( OBJECT set) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic set The java.util.Set object that needs to be added into the union set. OBJECT No No Examples EXAMPLE 1 1 2 3 4 5 6 7 from stockStream select createSet(symbol) as initialSet insert into initStream from initStream#window.timeBatch(10 sec) select unionSet(initialSet) as distinctSymbols insert into distinctStockStream; distinctStockStream will return the set object which contains the distinct set of stock symbols received during a sliding window of 10 seconds. UUID (Function) Generates a UUID (Universally Unique Identifier). Syntax 1 STRING UUID() Examples EXAMPLE 1 1 2 3 from TempStream select convert(roomNo, string ) as roomNo, temp, UUID() as messageID insert into RoomTempStream; This will converts a room number to string, introducing a message ID to each event asUUID() returns a34eec40-32c2-44fe-8075-7f4fde2e2dd8 from TempStream select convert(roomNo, 'string') as roomNo, temp, UUID() as messageID insert into RoomTempStream; cast (Function) Converts the first parameter according to the cast.to parameter. Incompatible arguments cause Class Cast exceptions if further processed. This function is used with map extension that returns attributes of the object type. You can use this function to cast the object to an accurate and concrete type. Syntax 1 INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT cast( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT to.be.caster, STRING cast.to) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic to.be.caster This specifies the attribute to be casted. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No cast.to A string constant parameter expressing the cast to type using one of the following strings values: int, long, float, double, string, bool. STRING No No Examples EXAMPLE 1 1 2 3 from fooStream select symbol as name, cast(temp, double ) as temp insert into barStream; This will cast the fooStream temp field value into 'double' format. coalesce (Function) Returns the value of the first input parameter that is not null, and all input parameters have to be on the same type. Syntax 1 INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT coalesce( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT args) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic args This function accepts one or more parameters. They can belong to any one of the available types. All the specified parameters should be of the same type. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 1 2 3 from fooStream select coalesce( 123 , null, 789 ) as value insert into barStream; This will returns first null value 123. EXAMPLE 2 1 2 3 from fooStream select coalesce(null, 76, 567) as value insert into barStream; This will returns first null value 76. EXAMPLE 3 1 2 3 from fooStream select coalesce(null, null, null) as value insert into barStream; This will returns null as there are no notnull values. convert (Function) Converts the first input parameter according to the convertedTo parameter. Syntax 1 INT|LONG|DOUBLE|FLOAT|STRING|BOOL convert( INT|LONG|DOUBLE|FLOAT|STRING|BOOL to.be.converted, STRING converted.to) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic to.be.converted This specifies the value to be converted. INT LONG DOUBLE FLOAT STRING BOOL No No converted.to A string constant parameter to which type the attribute need to be converted using one of the following strings values: 'int', 'long', 'float', 'double', 'string', 'bool'. STRING No No Examples EXAMPLE 1 1 2 3 from fooStream select convert(temp, double ) as temp insert into barStream; This will convert fooStream temp value into 'double'. EXAMPLE 2 1 2 3 from fooStream select convert(temp, int ) as temp insert into barStream; This will convert fooStream temp value into 'int' (value = \"convert(45.9, 'int') returns 46\"). createSet (Function) Includes the given input parameter in a java.util.HashSet and returns the set. Syntax 1 OBJECT createSet( INT|LONG|DOUBLE|FLOAT|STRING|BOOL input) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input The input that needs to be added into the set. INT LONG DOUBLE FLOAT STRING BOOL No No Examples EXAMPLE 1 1 2 3 from stockStream select createSet(symbol) as initialSet insert into initStream; For every incoming stockStream event, the initStream stream will produce a set object having only one element: the symbol in the incoming stockStream. currentTimeMillis (Function) Returns the current timestamp of siddhi application in milliseconds. Syntax 1 LONG currentTimeMillis() Examples EXAMPLE 1 1 2 3 from fooStream select symbol as name, currentTimeMillis() as eventTimestamp insert into barStream; This will extract current siddhi application timestamp. default (Function) Checks if the 'attribute' parameter is null and if so returns the value of the 'default' parameter Syntax 1 INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT default( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT attribute, INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT default) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic attribute The attribute that could be null. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No default The default value that will be used when 'attribute' parameter is null INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 1 2 3 from TempStream select default(temp, 0.0) as temp, roomNum insert into StandardTempStream; This will replace TempStream's temp attribute with default value if the temp is null. eventTimestamp (Function) Returns the timestamp of the processed event. Syntax 1 LONG eventTimestamp() Examples EXAMPLE 1 1 2 3 from fooStream select symbol as name, eventTimestamp() as eventTimestamp insert into barStream; This will extract current events timestamp. ifThenElse (Function) Evaluates the 'condition' parameter and returns value of the 'if.expression' parameter if the condition is true, or returns value of the 'else.expression' parameter if the condition is false. Here both 'if.expression' and 'else.expression' should be of the same type. Syntax 1 INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT ifThenElse( BOOL condition, INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT if.expression, INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT else.expression) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic condition This specifies the if then else condition value. BOOL No No if.expression This specifies the value to be returned if the value of the condition parameter is true. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No else.expression This specifies the value to be returned if the value of the condition parameter is false. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 1 2 3 4 @info(name = query1 ) from sensorEventStream select sensorValue, ifThenElse(sensorValue 35, High , Low ) as status insert into outputStream; This will returns High if sensorValue = 50. EXAMPLE 2 1 2 3 4 @info(name = query1 ) from sensorEventStream select sensorValue, ifThenElse(voltage 5, 0, 1) as status insert into outputStream; This will returns 1 if voltage= 12. EXAMPLE 3 1 2 3 4 @info(name = query1 ) from userEventStream select userName, ifThenElse(password == admin , true, false) as passwordState insert into outputStream; This will returns passwordState as true if password = admin. instanceOfBoolean (Function) Checks whether the parameter is an instance of Boolean or not. Syntax 1 BOOL instanceOfBoolean( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 1 2 3 from fooStream select instanceOfBoolean(switchState) as state insert into barStream; This will return true if the value of switchState is true. EXAMPLE 2 1 2 3 from fooStream select instanceOfBoolean(value) as state insert into barStream; if the value = 32 then this will returns false as the value is not an instance of the boolean. instanceOfDouble (Function) Checks whether the parameter is an instance of Double or not. Syntax 1 BOOL instanceOfDouble( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 1 2 3 from fooStream select instanceOfDouble(value) as state insert into barStream; This will return true if the value field format is double ex : 56.45. EXAMPLE 2 1 2 3 from fooStream select instanceOfDouble(switchState) as state insert into barStream; if the switchState = true then this will returns false as the value is not an instance of the double. instanceOfFloat (Function) Checks whether the parameter is an instance of Float or not. Syntax 1 BOOL instanceOfFloat( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 1 2 3 from fooStream select instanceOfFloat(value) as state insert into barStream; This will return true if the value field format is float ex : 56.45f. EXAMPLE 2 1 2 3 from fooStream select instanceOfFloat(switchState) as state insert into barStream; if the switchState = true then this will returns false as the value is an instance of the boolean not a float. instanceOfInteger (Function) Checks whether the parameter is an instance of Integer or not. Syntax 1 BOOL instanceOfInteger( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 1 2 3 from fooStream select instanceOfInteger(value) as state insert into barStream; This will return true if the value field format is integer. EXAMPLE 2 1 2 3 from fooStream select instanceOfInteger(switchState) as state insert into barStream; if the switchState = true then this will returns false as the value is an instance of the boolean not a long. instanceOfLong (Function) Checks whether the parameter is an instance of Long or not. Syntax 1 BOOL instanceOfLong( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 1 2 3 from fooStream select instanceOfLong(value) as state insert into barStream; This will return true if the value field format is long ex : 56456l. EXAMPLE 2 1 2 3 from fooStream select instanceOfLong(switchState) as state insert into barStream; if the switchState = true then this will returns false as the value is an instance of the boolean not a long. instanceOfString (Function) Checks whether the parameter is an instance of String or not. Syntax 1 BOOL instanceOfString( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 1 2 3 from fooStream select instanceOfString(value) as state insert into barStream; This will return true if the value field format is string ex : 'test'. EXAMPLE 2 1 2 3 from fooStream select instanceOfString(switchState) as state insert into barStream; if the switchState = true then this will returns false as the value is an instance of the boolean not a string. maximum (Function) Returns the maximum value of the input parameters. Syntax 1 INT|LONG|DOUBLE|FLOAT maximum( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg This function accepts one or more parameters. They can belong to any one of the available types. All the specified parameters should be of the same type. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 1 2 3 @info(name = query1 ) from inputStream select maximum(price1, price2, price3) as max insert into outputStream; This will returns the maximum value of the input parameters price1, price2, price3. minimum (Function) Returns the minimum value of the input parameters. Syntax 1 INT|LONG|DOUBLE|FLOAT minimum( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg This function accepts one or more parameters. They can belong to any one of the available types. All the specified parameters should be of the same type. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 1 2 3 @info(name = query1 ) from inputStream select maximum(price1, price2, price3) as max insert into outputStream; This will returns the minimum value of the input parameters price1, price2, price3. sizeOfSet (Function) Returns the size of an object of type java.util.Set. Syntax 1 INT sizeOfSet( OBJECT set) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic set The set object. This parameter should be of type java.util.Set. A set object may be created by the 'set' attribute aggregator in Siddhi. OBJECT No No Examples EXAMPLE 1 1 2 3 4 5 6 7 8 9 10 11 from stockStream select initSet(symbol) as initialSet insert into initStream; ;from initStream#window.timeBatch(10 sec) select union(initialSet) as distinctSymbols insert into distinctStockStream; from distinctStockStream select sizeOfSet(distinctSymbols) sizeOfSymbolSet insert into sizeStream; The sizeStream stream will output the number of distinct stock symbols received during a sliding window of 10 seconds. pol2Cart (Stream Function) The pol2Cart function calculating the cartesian coordinates x & y for the given theta, rho coordinates and adding them as new attributes to the existing events. Syntax 1 pol2Cart( DOUBLE theta, DOUBLE rho, DOUBLE z) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic theta The theta value of the coordinates. DOUBLE No No rho The rho value of the coordinates. DOUBLE No No z z value of the cartesian coordinates. If z value is not given, drop the third parameter of the output. DOUBLE Yes No Examples EXAMPLE 1 1 2 3 from PolarStream#pol2Cart(theta, rho) select x, y insert into outputStream ; This will return cartesian coordinates (4.99953024681082, 0.06853693328228748) for theta: 0.7854 and rho: 5. EXAMPLE 2 1 2 3 from PolarStream#pol2Cart(theta, rho, 3.4) select x, y, z insert into outputStream ; This will return cartesian coordinates (4.99953024681082, 0.06853693328228748, 3.4)for theta: 0.7854 and rho: 5 and z: 3.4. log (Stream Processor) The logger logs the message on the given priority with or without processed event. Syntax 1 log( STRING priority, STRING log.message, BOOL is.event.logged) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic priority The priority/type of this log message (INFO, DEBUG, WARN, FATAL, ERROR, OFF, TRACE). INFO STRING Yes No log.message This message will be logged. STRING No No is.event.logged To log the processed event. true BOOL Yes No Examples EXAMPLE 1 1 2 3 from fooStream#log( INFO , Sample Event : , true) select * insert into barStream; This will log as INFO with the message \"Sample Event :\" + fooStream:events. EXAMPLE 2 1 2 3 from fooStream#log( Sample Event : , true) select * insert into barStream; This will logs with default log level as INFO. EXAMPLE 3 1 2 3 from fooStream#log( Sample Event : , fasle) select * insert into barStream; This will only log message. EXAMPLE 4 1 2 3 from fooStream#log(true) select * insert into barStream; This will only log fooStream:events. EXAMPLE 5 1 2 3 from fooStream#log( Sample Event : ) select * insert into barStream; This will log message and fooStream:events. batch (Window) A window that holds an incoming events batch. When a new set of events arrives, the previously arrived old events will be expired. Batch window can be used to aggregate events that comes in batches. If it has the parameter length specified, then batch window process the batch as several chunks. Syntax 1 batch( INT window.length) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.length The length of a chunk If length value was not given it assign 0 as length and process the whole batch as once INT Yes No Examples EXAMPLE 1 1 2 3 4 5 6 define stream consumerItemStream (itemId string, price float) from consumerItemStream#window.batch() select price, str:groupConcat(itemId) as itemIds group by price insert into outputStream; This will output comma separated items IDs that have the same price for each incoming batch of events. cron (Window) This window outputs the arriving events as and when they arrive, and resets (expires) the window periodically based on the given cron expression. Syntax 1 cron( STRING cron.expression) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic cron.expression The cron expression that resets the window. STRING No No Examples EXAMPLE 1 1 2 3 4 5 6 define stream InputEventStream (symbol string, price float, volume int); @info(name = query1 ) from InputEventStream#cron( */5 * * * * ? ) select symbol, sum(price) as totalPrice insert into OutputStream; This let the totalPrice to gradually increase and resets to zero as a batch every 5 seconds. EXAMPLE 2 1 2 3 4 5 6 7 8 9 10 11 define stream StockEventStream (symbol string, price float, volume int) define window StockEventWindow (symbol string, price float, volume int) cron( */5 * * * * ? ); @info(name = query0 ) from StockEventStream insert into StockEventWindow; @info(name = query1 ) from StockEventWindow select symbol, sum(price) as totalPrice insert into OutputStream ; The defined window will let the totalPrice to gradually increase and resets to zero as a batch every 5 seconds. delay (Window) A delay window holds events for a specific time period that is regarded as a delay period before processing them. Syntax 1 delay( INT|LONG|TIME window.delay) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.delay The time period (specified in sec, min, ms) for which the window should delay the events. INT LONG TIME No No Examples EXAMPLE 1 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 define window delayWindow(symbol string, volume int) delay(1 hour); define stream PurchaseStream(symbol string, volume int); define stream DeliveryStream(symbol string); define stream OutputStream(symbol string); @info(name= query1 ) from PurchaseStream select symbol, volume insert into delayWindow; @info(name= query2 ) from delayWindow join DeliveryStream on delayWindow.symbol == DeliveryStream.symbol select delayWindow.symbol insert into OutputStream; In this example, purchase events that arrive in the 'PurchaseStream' stream are directed to a delay window. At any given time, this delay window holds purchase events that have arrived within the last hour. These purchase events in the window are matched by the 'symbol' attribute, with delivery events that arrive in the 'DeliveryStream' stream. This monitors whether the delivery of products is done with a minimum delay of one hour after the purchase. externalTime (Window) A sliding time window based on external time. It holds events that arrived during the last windowTime period from the external timestamp, and gets updated on every monotonically increasing timestamp. Syntax 1 externalTime( LONG timestamp, INT|LONG|TIME window.time) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic timestamp The time which the window determines as current time and will act upon. The value of this parameter should be monotonically increasing. LONG No No window.time The sliding time period for which the window should hold events. INT LONG TIME No No Examples EXAMPLE 1 1 2 3 4 5 6 7 8 9 10 define window cseEventWindow (symbol string, price float, volume int) externalTime(eventTime, 20 sec) output expired events; @info(name = query0 ) from cseEventStream insert into cseEventWindow; @info(name = query1 ) from cseEventWindow select symbol, sum(price) as price insert expired events into outputStream ; processing events arrived within the last 20 seconds from the eventTime and output expired events. externalTimeBatch (Window) A batch (tumbling) time window based on external time, that holds events arrived during windowTime periods, and gets updated for every windowTime. Syntax 1 externalTimeBatch( LONG timestamp, INT|LONG|TIME window.time, INT|LONG|TIME start.time, INT|LONG|TIME timeout) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic timestamp The time which the window determines as current time and will act upon. The value of this parameter should be monotonically increasing. LONG No No window.time The batch time period for which the window should hold events. INT LONG TIME No No start.time User defined start time. This could either be a constant (of type int, long or time) or an attribute of the corresponding stream (of type long). If an attribute is provided, initial value of attribute would be considered as startTime. Timestamp of first event INT LONG TIME Yes No timeout Time to wait for arrival of new event, before flushing and giving output for events belonging to a specific batch. System waits till an event from next batch arrives to flush current batch INT LONG TIME Yes No Examples EXAMPLE 1 1 2 3 4 5 6 7 8 define window cseEventWindow (symbol string, price float, volume int) externalTimeBatch(eventTime, 1 sec) output expired events; @info(name = query0 ) from cseEventStream insert into cseEventWindow; @info(name = query1 ) from cseEventWindow select symbol, sum(price) as price insert expired events into outputStream ; This will processing events that arrive every 1 seconds from the eventTime. EXAMPLE 2 1 define window cseEventWindow (symbol string, price float, volume int) externalTimeBatch(eventTime, 20 sec, 0) output expired events; This will processing events that arrive every 1 seconds from the eventTime. Starts on 0 th millisecond of an hour. EXAMPLE 3 1 define window cseEventWindow (symbol string, price float, volume int) externalTimeBatch(eventTime, 2 sec, eventTimestamp, 100) output expired events; This will processing events that arrive every 2 seconds from the eventTim. Considers the first event's eventTimestamp value as startTime. Waits 100 milliseconds for the arrival of a new event before flushing current batch. frequent (Window) This window returns the latest events with the most frequently occurred value for a given attribute(s). Frequency calculation for this window processor is based on Misra-Gries counting algorithm. Syntax 1 frequent( INT event.count, STRING attribute) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic event.count The number of most frequent events to be emitted to the stream. INT No No attribute The attributes to group the events. If no attributes are given, the concatenation of all the attributes of the event is considered. The concatenation of all the attributes of the event is considered. STRING Yes No Examples EXAMPLE 1 1 2 3 4 @info(name = query1 ) from purchase[price = 30]#window.frequent(2) select cardNo, price insert all events into PotentialFraud; This will returns the 2 most frequent events. EXAMPLE 2 1 2 3 4 @info(name = query1 ) from purchase[price = 30]#window.frequent(2, cardNo) select cardNo, price insert all events into PotentialFraud; This will returns the 2 latest events with the most frequently appeared card numbers. length (Window) A sliding length window that holds the last 'window.length' events at a given time, and gets updated for each arrival and expiry. Syntax 1 length( INT window.length) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.length The number of events that should be included in a sliding length window. INT No No Examples EXAMPLE 1 1 2 3 4 5 6 7 8 9 10 define window StockEventWindow (symbol string, price float, volume int) length(10) output all events; @info(name = query0 ) from StockEventStream insert into StockEventWindow; @info(name = query1 ) from StockEventWindow select symbol, sum(price) as price insert all events into outputStream ; This will process last 10 events in a sliding manner. lengthBatch (Window) A batch (tumbling) length window that holds and process a number of events as specified in the window.length. Syntax 1 lengthBatch( INT window.length, BOOL stream.current.event) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.length The number of events the window should tumble. INT No No stream.current.event Let the window stream the current events out as and when they arrive to the window while expiring them in batches. false BOOL Yes No Examples EXAMPLE 1 1 2 3 4 5 6 define stream InputEventStream (symbol string, price float, volume int); @info(name = query1 ) from InputEventStream#lengthBatch(10) select symbol, sum(price) as price insert into OutputStream; This collect and process 10 events as a batch and output them. EXAMPLE 2 1 2 3 4 5 6 define stream InputEventStream (symbol string, price float, volume int); @info(name = query1 ) from InputEventStream#lengthBatch(10, true) select symbol, sum(price) as sumPrice insert into OutputStream; This window sends the arriving events directly to the output letting the sumPrice to increase gradually, after every 10 events it clears the window as a batch and resets the sumPrice to zero. EXAMPLE 3 1 2 3 4 5 6 7 8 9 10 11 define stream InputEventStream (symbol string, price float, volume int); define window StockEventWindow (symbol string, price float, volume int) lengthBatch(10) output all events; @info(name = query0 ) from InputEventStream insert into StockEventWindow; @info(name = query1 ) from StockEventWindow select symbol, sum(price) as price insert all events into OutputStream ; This uses an defined window to process 10 events as a batch and output all events. lossyFrequent (Window) This window identifies and returns all the events of which the current frequency exceeds the value specified for the supportThreshold parameter. Syntax 1 lossyFrequent( DOUBLE support.threshold, DOUBLE error.bound, STRING attribute) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic support.threshold The support threshold value. DOUBLE No No error.bound The error bound value. DOUBLE No No attribute The attributes to group the events. If no attributes are given, the concatenation of all the attributes of the event is considered. The concatenation of all the attributes of the event is considered. STRING Yes No Examples EXAMPLE 1 1 2 3 4 5 6 7 8 9 define stream purchase (cardNo string, price float); define window purchaseWindow (cardNo string, price float) lossyFrequent(0.1, 0.01); @info(name = query0 ) from purchase[price = 30] insert into purchaseWindow; @info(name = query1 ) from purchaseWindow select cardNo, price insert all events into PotentialFraud; lossyFrequent(0.1, 0.01) returns all the events of which the current frequency exceeds 0.1, with an error bound of 0.01. EXAMPLE 2 1 2 3 4 5 6 7 8 9 define stream purchase (cardNo string, price float); define window purchaseWindow (cardNo string, price float) lossyFrequent(0.3, 0.05, cardNo); @info(name = query0 ) from purchase[price = 30] insert into purchaseWindow; @info(name = query1 ) from purchaseWindow select cardNo, price insert all events into PotentialFraud; lossyFrequent(0.3, 0.05, cardNo) returns all the events of which the cardNo attributes frequency exceeds 0.3, with an error bound of 0.05. session (Window) This is a session window that holds events that belong to a specific session. The events that belong to a specific session are identified by a grouping attribute (i.e., a session key). A session gap period is specified to determine the time period after which the session is considered to be expired. A new event that arrives with a specific value for the session key is matched with the session window with the same session key. There can be out of order and late arrival of events, these events can arrive after the session is expired, to include those events to the matching session key specify a latency time period that is less than the session gap period.To have aggregate functions with session windows, the events need to be grouped by the session key via a 'group by' clause. Syntax 1 session( INT|LONG|TIME window.session, STRING window.key, INT|LONG|TIME window.allowedlatency) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.session The time period for which the session considered is valid. This is specified in seconds, minutes, or milliseconds (i.e., 'min', 'sec', or 'ms'. INT LONG TIME No No window.key The grouping attribute for events. default-key STRING Yes No window.allowedlatency This specifies the time period for which the session window is valid after the expiration of the session. The time period specified here should be less than the session time gap (which is specified via the 'window.session' parameter). 0 INT LONG TIME Yes No Examples EXAMPLE 1 1 2 3 4 5 6 define stream PurchaseEventStream (user string, item_number int, price float, quantity int); @info(name= query0) from PurchaseEventStream#window.session(5 sec, user, 2 sec) select * insert all events into OutputStream; This query processes events that arrive at the PurchaseEvent input stream. The 'user' attribute is the session key, and the session gap is 5 seconds. '2 sec' is specified as the allowed latency. Therefore, events with the matching user name that arrive 2 seconds after the expiration of the session are also considered when performing aggregations for the session identified by the given user name. sort (Window) This window holds a batch of events that equal the number specified as the windowLength and sorts them in the given order. Syntax 1 sort( INT window.length, STRING attribute, STRING order) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.length The size of the window length. INT No No attribute The attribute that should be checked for the order. The concatenation of all the attributes of the event is considered. STRING Yes No order The order define as \"asc\" or \"desc\". asc STRING Yes No Examples EXAMPLE 1 1 2 3 4 5 6 7 8 9 define stream cseEventStream (symbol string, price float, volume long); define window cseEventWindow (symbol string, price float, volume long) sort(2,volume, asc ); @info(name = query0 ) from cseEventStream insert into cseEventWindow; @info(name = query1 ) from cseEventWindow select volume insert all events into outputStream ; sort(5, price, 'asc') keeps the events sorted by price in the ascending order. Therefore, at any given time, the window contains the 5 lowest prices. time (Window) A sliding time window that holds events that arrived during the last windowTime period at a given time, and gets updated for each event arrival and expiry. Syntax 1 time( INT|LONG|TIME window.time) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.time The sliding time period for which the window should hold events. INT LONG TIME No No Examples EXAMPLE 1 1 2 3 4 5 6 7 8 define window cseEventWindow (symbol string, price float, volume int) time(20) output all events; @info(name = query0 ) from cseEventStream insert into cseEventWindow; @info(name = query1 ) from cseEventWindow select symbol, sum(price) as price insert all events into outputStream ; This will processing events that arrived within the last 20 milliseconds. timeBatch (Window) A batch (tumbling) time window that holds and process events that arrive during 'window.time' period as a batch. Syntax 1 timeBatch( INT|LONG|TIME window.time, INT start.time, BOOL stream.current.event) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.time The batch time period in which the window process the events. INT LONG TIME No No start.time This specifies an offset in milliseconds in order to start the window at a time different to the standard time. Timestamp of first event INT Yes No stream.current.event Let the window stream the current events out as and when they arrive to the window while expiring them in batches. false BOOL Yes No Examples EXAMPLE 1 1 2 3 4 5 6 define stream InputEventStream (symbol string, price float, volume int); @info(name = query1 ) from InputEventStream#timeBatch(20 sec) select symbol, sum(price) as price insert into OutputStream; This collect and process incoming events as a batch every 20 seconds and output them. EXAMPLE 2 1 2 3 4 5 6 define stream InputEventStream (symbol string, price float, volume int); @info(name = query1 ) from InputEventStream#timeBatch(20 sec, true) select symbol, sum(price) as sumPrice insert into OutputStream; This window sends the arriving events directly to the output letting the sumPrice to increase gradually and on every 20 second interval it clears the window as a batch resetting the sumPrice to zero. EXAMPLE 3 1 2 3 4 5 6 7 8 9 10 11 define stream InputEventStream (symbol string, price float, volume int); define window StockEventWindow (symbol string, price float, volume int) timeBatch(20 sec) output all events; @info(name = query0 ) from InputEventStream insert into StockEventWindow; @info(name = query1 ) from StockEventWindow select symbol, sum(price) as price insert all events into OutputStream ; This uses an defined window to process events arrived every 20 seconds as a batch and output all events. timeLength (Window) A sliding time window that, at a given time holds the last window.length events that arrived during last window.time period, and gets updated for every event arrival and expiry. Syntax 1 timeLength( INT|LONG|TIME window.time, INT window.length) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.time The sliding time period for which the window should hold events. INT LONG TIME No No window.length The number of events that should be be included in a sliding length window.. INT No No Examples EXAMPLE 1 1 2 3 4 5 6 7 8 define stream cseEventStream (symbol string, price float, volume int); define window cseEventWindow (symbol string, price float, volume int) timeLength(2 sec, 10); @info(name = query0 ) from cseEventStream insert into cseEventWindow; @info(name = query1 ) from cseEventWindow select symbol, price, volume insert all events into outputStream; window.timeLength(2 sec, 10) holds the last 10 events that arrived during last 2 seconds and gets updated for every event arrival and expiry. Sink inMemory (Sink) In-memory transport that can communicate with other in-memory transports within the same JVM, itis assumed that the publisher and subscriber of a topic uses same event schema (stream definition). Syntax 1 @sink(type= inMemory , topic= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic topic Event will be delivered to allthe subscribers of the same topic STRING No No Examples EXAMPLE 1 1 2 @sink(type= inMemory , @map(type= passThrough )) define stream BarStream (symbol string, price float, volume long) In this example BarStream uses inMemory transport which emit the Siddhi events internally without using external transport and transformation. log (Sink) This is a sink that can be used as a logger. This will log the output events in the output stream with user specified priority and a prefix Syntax 1 @sink(type= log , priority= STRING , prefix= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic priority This will set the logger priority i.e log level. Accepted values are INFO, DEBUG, WARN, FATAL, ERROR, OFF, TRACE INFO STRING Yes No prefix This will be the prefix to the output message. If the output stream has event [2,4] and the prefix is given as \"Hello\" then the log will show \"Hello : [2,4]\" default prefix will be : STRING Yes No Examples EXAMPLE 1 1 2 @sink(type= log , prefix= My Log , priority= DEBUG ) define stream BarStream (symbol string, price float, volume long) In this example BarStream uses log sink and the prefix is given as My Log. Also the priority is set to DEBUG. EXAMPLE 2 1 2 @sink(type= log , priority= DEBUG ) define stream BarStream (symbol string, price float, volume long) In this example BarStream uses log sink and the priority is set to DEBUG. User has not specified prefix so the default prefix will be in the form Siddhi App Name : Stream Name EXAMPLE 3 1 2 @sink(type= log , prefix= My Log ) define stream BarStream (symbol string, price float, volume long) In this example BarStream uses log sink and the prefix is given as My Log. User has not given a priority so it will be set to default INFO. EXAMPLE 4 1 2 @sink(type= log ) define stream BarStream (symbol string, price float, volume long) In this example BarStream uses log sink. The user has not given prefix or priority so they will be set to their default values. Sinkmapper passThrough (Sink Mapper) Pass-through mapper passed events (Event[]) through without any mapping or modifications. Syntax 1 @sink(..., @map(type= passThrough ) Examples EXAMPLE 1 1 2 @sink(type= inMemory , @map(type= passThrough )) define stream BarStream (symbol string, price float, volume long); In the following example BarStream uses passThrough outputmapper which emit Siddhi event directly without any transformation into sink. Source inMemory (Source) In-memory source that can communicate with other in-memory sinks within the same JVM, it is assumed that the publisher and subscriber of a topic uses same event schema (stream definition). Syntax 1 @source(type= inMemory , topic= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic topic Subscribes to sent on the given topic. STRING No No Examples EXAMPLE 1 1 2 @source(type= inMemory , @map(type= passThrough )) define stream BarStream (symbol string, price float, volume long) In this example BarStream uses inMemory transport which passes the received event internally without using external transport. Sourcemapper passThrough (Source Mapper) Pass-through mapper passed events (Event[]) through without any mapping or modifications. Syntax 1 @source(..., @map(type= passThrough ) Examples EXAMPLE 1 1 2 @source(type= tcp , @map(type= passThrough )) define stream BarStream (symbol string, price float, volume long); In this example BarStream uses passThrough inputmapper which passes the received Siddhi event directly without any transformation into source.","title":"latest"},{"location":"docs/api/latest/#api-docs-v510","text":"","title":"API Docs - v5.1.0"},{"location":"docs/api/latest/#core","text":"","title":"Core"},{"location":"docs/api/latest/#and-aggregate-function","text":"Returns the results of AND operation for all the events. Syntax 1 BOOL and( BOOL arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be AND operation. BOOL No No Examples EXAMPLE 1 1 2 3 from cscStream#window.lengthBatch(10) select and(isFraud) as isFraudTransaction insert into alertStream; This will returns the result for AND operation of isFraud values as a boolean value for event chunk expiry by window length batch.","title":"and (Aggregate Function)"},{"location":"docs/api/latest/#avg-aggregate-function","text":"Calculates the average for all the events. Syntax 1 DOUBLE avg( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that need to be averaged. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 1 2 3 from fooStream#window.timeBatch select avg(temp) as avgTemp insert into barStream; avg(temp) returns the average temp value for all the events based on their arrival and expiry.","title":"avg (Aggregate Function)"},{"location":"docs/api/latest/#count-aggregate-function","text":"Returns the count of all the events. Syntax 1 LONG count() Examples EXAMPLE 1 1 2 3 from fooStream#window.timeBatch(10 sec) select count() as count insert into barStream; This will return the count of all the events for time batch in 10 seconds.","title":"count (Aggregate Function)"},{"location":"docs/api/latest/#distinctcount-aggregate-function","text":"This returns the count of distinct occurrences for a given arg. Syntax 1 LONG distinctCount( INT|LONG|DOUBLE|FLOAT|STRING arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The object for which the number of distinct occurences needs to be counted. INT LONG DOUBLE FLOAT STRING No No Examples EXAMPLE 1 1 2 3 from fooStream select distinctcount(pageID) as count insert into barStream; distinctcount(pageID) for the following output returns '3' when the available values are as follows. \"WEB_PAGE_1\" \"WEB_PAGE_1\" \"WEB_PAGE_2\" \"WEB_PAGE_3\" \"WEB_PAGE_1\" \"WEB_PAGE_2\" The three distinct occurences identified are 'WEB_PAGE_1', 'WEB_PAGE_2', and 'WEB_PAGE_3'.","title":"distinctCount (Aggregate Function)"},{"location":"docs/api/latest/#max-aggregate-function","text":"Returns the maximum value for all the events. Syntax 1 INT|LONG|DOUBLE|FLOAT max( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be compared to find the maximum value. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 1 2 3 from fooStream#window.timeBatch(10 sec) select max(temp) as maxTemp insert into barStream; max(temp) returns the maximum temp value recorded for all the events based on their arrival and expiry.","title":"max (Aggregate Function)"},{"location":"docs/api/latest/#maxforever-aggregate-function","text":"This is the attribute aggregator to store the maximum value for a given attribute throughout the lifetime of the query regardless of any windows in-front. Syntax 1 INT|LONG|DOUBLE|FLOAT maxForever( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be compared to find the maximum value. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 1 2 3 from inputStream select maxForever(temp) as max insert into outputStream; maxForever(temp) returns the maximum temp value recorded for all the events throughout the lifetime of the query.","title":"maxForever (Aggregate Function)"},{"location":"docs/api/latest/#min-aggregate-function","text":"Returns the minimum value for all the events. Syntax 1 INT|LONG|DOUBLE|FLOAT min( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be compared to find the minimum value. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 1 2 3 from inputStream select min(temp) as minTemp insert into outputStream; min(temp) returns the minimum temp value recorded for all the events based on their arrival and expiry.","title":"min (Aggregate Function)"},{"location":"docs/api/latest/#minforever-aggregate-function","text":"This is the attribute aggregator to store the minimum value for a given attribute throughout the lifetime of the query regardless of any windows in-front. Syntax 1 INT|LONG|DOUBLE|FLOAT minForever( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be compared to find the minimum value. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 1 2 3 from inputStream select minForever(temp) as max insert into outputStream; minForever(temp) returns the minimum temp value recorded for all the events throughoutthe lifetime of the query.","title":"minForever (Aggregate Function)"},{"location":"docs/api/latest/#or-aggregate-function","text":"Returns the results of OR operation for all the events. Syntax 1 BOOL or( BOOL arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be OR operation. BOOL No No Examples EXAMPLE 1 1 2 3 from cscStream#window.lengthBatch(10) select or(isFraud) as isFraudTransaction insert into alertStream; This will returns the result for OR operation of isFraud values as a boolean value for event chunk expiry by window length batch.","title":"or (Aggregate Function)"},{"location":"docs/api/latest/#stddev-aggregate-function","text":"Returns the calculated standard deviation for all the events. Syntax 1 DOUBLE stdDev( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that should be used to calculate the standard deviation. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 1 2 3 from inputStream select stddev(temp) as stdTemp insert into outputStream; stddev(temp) returns the calculated standard deviation of temp for all the events based on their arrival and expiry.","title":"stdDev (Aggregate Function)"},{"location":"docs/api/latest/#sum-aggregate-function","text":"Returns the sum for all the events. Syntax 1 LONG|DOUBLE sum( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be summed. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 1 2 3 from inputStream select sum(volume) as sumOfVolume insert into outputStream; This will returns the sum of volume values as a long value for each event arrival and expiry.","title":"sum (Aggregate Function)"},{"location":"docs/api/latest/#unionset-aggregate-function","text":"Union multiple sets. This attribute aggregator maintains a union of sets. The given input set is put into the union set and the union set is returned. Syntax 1 OBJECT unionSet( OBJECT set) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic set The java.util.Set object that needs to be added into the union set. OBJECT No No Examples EXAMPLE 1 1 2 3 4 5 6 7 from stockStream select createSet(symbol) as initialSet insert into initStream from initStream#window.timeBatch(10 sec) select unionSet(initialSet) as distinctSymbols insert into distinctStockStream; distinctStockStream will return the set object which contains the distinct set of stock symbols received during a sliding window of 10 seconds.","title":"unionSet (Aggregate Function)"},{"location":"docs/api/latest/#uuid-function","text":"Generates a UUID (Universally Unique Identifier). Syntax 1 STRING UUID() Examples EXAMPLE 1 1 2 3 from TempStream select convert(roomNo, string ) as roomNo, temp, UUID() as messageID insert into RoomTempStream; This will converts a room number to string, introducing a message ID to each event asUUID() returns a34eec40-32c2-44fe-8075-7f4fde2e2dd8 from TempStream select convert(roomNo, 'string') as roomNo, temp, UUID() as messageID insert into RoomTempStream;","title":"UUID (Function)"},{"location":"docs/api/latest/#cast-function","text":"Converts the first parameter according to the cast.to parameter. Incompatible arguments cause Class Cast exceptions if further processed. This function is used with map extension that returns attributes of the object type. You can use this function to cast the object to an accurate and concrete type. Syntax 1 INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT cast( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT to.be.caster, STRING cast.to) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic to.be.caster This specifies the attribute to be casted. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No cast.to A string constant parameter expressing the cast to type using one of the following strings values: int, long, float, double, string, bool. STRING No No Examples EXAMPLE 1 1 2 3 from fooStream select symbol as name, cast(temp, double ) as temp insert into barStream; This will cast the fooStream temp field value into 'double' format.","title":"cast (Function)"},{"location":"docs/api/latest/#coalesce-function","text":"Returns the value of the first input parameter that is not null, and all input parameters have to be on the same type. Syntax 1 INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT coalesce( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT args) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic args This function accepts one or more parameters. They can belong to any one of the available types. All the specified parameters should be of the same type. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 1 2 3 from fooStream select coalesce( 123 , null, 789 ) as value insert into barStream; This will returns first null value 123. EXAMPLE 2 1 2 3 from fooStream select coalesce(null, 76, 567) as value insert into barStream; This will returns first null value 76. EXAMPLE 3 1 2 3 from fooStream select coalesce(null, null, null) as value insert into barStream; This will returns null as there are no notnull values.","title":"coalesce (Function)"},{"location":"docs/api/latest/#convert-function","text":"Converts the first input parameter according to the convertedTo parameter. Syntax 1 INT|LONG|DOUBLE|FLOAT|STRING|BOOL convert( INT|LONG|DOUBLE|FLOAT|STRING|BOOL to.be.converted, STRING converted.to) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic to.be.converted This specifies the value to be converted. INT LONG DOUBLE FLOAT STRING BOOL No No converted.to A string constant parameter to which type the attribute need to be converted using one of the following strings values: 'int', 'long', 'float', 'double', 'string', 'bool'. STRING No No Examples EXAMPLE 1 1 2 3 from fooStream select convert(temp, double ) as temp insert into barStream; This will convert fooStream temp value into 'double'. EXAMPLE 2 1 2 3 from fooStream select convert(temp, int ) as temp insert into barStream; This will convert fooStream temp value into 'int' (value = \"convert(45.9, 'int') returns 46\").","title":"convert (Function)"},{"location":"docs/api/latest/#createset-function","text":"Includes the given input parameter in a java.util.HashSet and returns the set. Syntax 1 OBJECT createSet( INT|LONG|DOUBLE|FLOAT|STRING|BOOL input) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input The input that needs to be added into the set. INT LONG DOUBLE FLOAT STRING BOOL No No Examples EXAMPLE 1 1 2 3 from stockStream select createSet(symbol) as initialSet insert into initStream; For every incoming stockStream event, the initStream stream will produce a set object having only one element: the symbol in the incoming stockStream.","title":"createSet (Function)"},{"location":"docs/api/latest/#currenttimemillis-function","text":"Returns the current timestamp of siddhi application in milliseconds. Syntax 1 LONG currentTimeMillis() Examples EXAMPLE 1 1 2 3 from fooStream select symbol as name, currentTimeMillis() as eventTimestamp insert into barStream; This will extract current siddhi application timestamp.","title":"currentTimeMillis (Function)"},{"location":"docs/api/latest/#default-function","text":"Checks if the 'attribute' parameter is null and if so returns the value of the 'default' parameter Syntax 1 INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT default( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT attribute, INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT default) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic attribute The attribute that could be null. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No default The default value that will be used when 'attribute' parameter is null INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 1 2 3 from TempStream select default(temp, 0.0) as temp, roomNum insert into StandardTempStream; This will replace TempStream's temp attribute with default value if the temp is null.","title":"default (Function)"},{"location":"docs/api/latest/#eventtimestamp-function","text":"Returns the timestamp of the processed event. Syntax 1 LONG eventTimestamp() Examples EXAMPLE 1 1 2 3 from fooStream select symbol as name, eventTimestamp() as eventTimestamp insert into barStream; This will extract current events timestamp.","title":"eventTimestamp (Function)"},{"location":"docs/api/latest/#ifthenelse-function","text":"Evaluates the 'condition' parameter and returns value of the 'if.expression' parameter if the condition is true, or returns value of the 'else.expression' parameter if the condition is false. Here both 'if.expression' and 'else.expression' should be of the same type. Syntax 1 INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT ifThenElse( BOOL condition, INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT if.expression, INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT else.expression) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic condition This specifies the if then else condition value. BOOL No No if.expression This specifies the value to be returned if the value of the condition parameter is true. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No else.expression This specifies the value to be returned if the value of the condition parameter is false. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 1 2 3 4 @info(name = query1 ) from sensorEventStream select sensorValue, ifThenElse(sensorValue 35, High , Low ) as status insert into outputStream; This will returns High if sensorValue = 50. EXAMPLE 2 1 2 3 4 @info(name = query1 ) from sensorEventStream select sensorValue, ifThenElse(voltage 5, 0, 1) as status insert into outputStream; This will returns 1 if voltage= 12. EXAMPLE 3 1 2 3 4 @info(name = query1 ) from userEventStream select userName, ifThenElse(password == admin , true, false) as passwordState insert into outputStream; This will returns passwordState as true if password = admin.","title":"ifThenElse (Function)"},{"location":"docs/api/latest/#instanceofboolean-function","text":"Checks whether the parameter is an instance of Boolean or not. Syntax 1 BOOL instanceOfBoolean( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 1 2 3 from fooStream select instanceOfBoolean(switchState) as state insert into barStream; This will return true if the value of switchState is true. EXAMPLE 2 1 2 3 from fooStream select instanceOfBoolean(value) as state insert into barStream; if the value = 32 then this will returns false as the value is not an instance of the boolean.","title":"instanceOfBoolean (Function)"},{"location":"docs/api/latest/#instanceofdouble-function","text":"Checks whether the parameter is an instance of Double or not. Syntax 1 BOOL instanceOfDouble( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 1 2 3 from fooStream select instanceOfDouble(value) as state insert into barStream; This will return true if the value field format is double ex : 56.45. EXAMPLE 2 1 2 3 from fooStream select instanceOfDouble(switchState) as state insert into barStream; if the switchState = true then this will returns false as the value is not an instance of the double.","title":"instanceOfDouble (Function)"},{"location":"docs/api/latest/#instanceoffloat-function","text":"Checks whether the parameter is an instance of Float or not. Syntax 1 BOOL instanceOfFloat( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 1 2 3 from fooStream select instanceOfFloat(value) as state insert into barStream; This will return true if the value field format is float ex : 56.45f. EXAMPLE 2 1 2 3 from fooStream select instanceOfFloat(switchState) as state insert into barStream; if the switchState = true then this will returns false as the value is an instance of the boolean not a float.","title":"instanceOfFloat (Function)"},{"location":"docs/api/latest/#instanceofinteger-function","text":"Checks whether the parameter is an instance of Integer or not. Syntax 1 BOOL instanceOfInteger( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 1 2 3 from fooStream select instanceOfInteger(value) as state insert into barStream; This will return true if the value field format is integer. EXAMPLE 2 1 2 3 from fooStream select instanceOfInteger(switchState) as state insert into barStream; if the switchState = true then this will returns false as the value is an instance of the boolean not a long.","title":"instanceOfInteger (Function)"},{"location":"docs/api/latest/#instanceoflong-function","text":"Checks whether the parameter is an instance of Long or not. Syntax 1 BOOL instanceOfLong( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 1 2 3 from fooStream select instanceOfLong(value) as state insert into barStream; This will return true if the value field format is long ex : 56456l. EXAMPLE 2 1 2 3 from fooStream select instanceOfLong(switchState) as state insert into barStream; if the switchState = true then this will returns false as the value is an instance of the boolean not a long.","title":"instanceOfLong (Function)"},{"location":"docs/api/latest/#instanceofstring-function","text":"Checks whether the parameter is an instance of String or not. Syntax 1 BOOL instanceOfString( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 1 2 3 from fooStream select instanceOfString(value) as state insert into barStream; This will return true if the value field format is string ex : 'test'. EXAMPLE 2 1 2 3 from fooStream select instanceOfString(switchState) as state insert into barStream; if the switchState = true then this will returns false as the value is an instance of the boolean not a string.","title":"instanceOfString (Function)"},{"location":"docs/api/latest/#maximum-function","text":"Returns the maximum value of the input parameters. Syntax 1 INT|LONG|DOUBLE|FLOAT maximum( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg This function accepts one or more parameters. They can belong to any one of the available types. All the specified parameters should be of the same type. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 1 2 3 @info(name = query1 ) from inputStream select maximum(price1, price2, price3) as max insert into outputStream; This will returns the maximum value of the input parameters price1, price2, price3.","title":"maximum (Function)"},{"location":"docs/api/latest/#minimum-function","text":"Returns the minimum value of the input parameters. Syntax 1 INT|LONG|DOUBLE|FLOAT minimum( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg This function accepts one or more parameters. They can belong to any one of the available types. All the specified parameters should be of the same type. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 1 2 3 @info(name = query1 ) from inputStream select maximum(price1, price2, price3) as max insert into outputStream; This will returns the minimum value of the input parameters price1, price2, price3.","title":"minimum (Function)"},{"location":"docs/api/latest/#sizeofset-function","text":"Returns the size of an object of type java.util.Set. Syntax 1 INT sizeOfSet( OBJECT set) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic set The set object. This parameter should be of type java.util.Set. A set object may be created by the 'set' attribute aggregator in Siddhi. OBJECT No No Examples EXAMPLE 1 1 2 3 4 5 6 7 8 9 10 11 from stockStream select initSet(symbol) as initialSet insert into initStream; ;from initStream#window.timeBatch(10 sec) select union(initialSet) as distinctSymbols insert into distinctStockStream; from distinctStockStream select sizeOfSet(distinctSymbols) sizeOfSymbolSet insert into sizeStream; The sizeStream stream will output the number of distinct stock symbols received during a sliding window of 10 seconds.","title":"sizeOfSet (Function)"},{"location":"docs/api/latest/#pol2cart-stream-function","text":"The pol2Cart function calculating the cartesian coordinates x & y for the given theta, rho coordinates and adding them as new attributes to the existing events. Syntax 1 pol2Cart( DOUBLE theta, DOUBLE rho, DOUBLE z) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic theta The theta value of the coordinates. DOUBLE No No rho The rho value of the coordinates. DOUBLE No No z z value of the cartesian coordinates. If z value is not given, drop the third parameter of the output. DOUBLE Yes No Examples EXAMPLE 1 1 2 3 from PolarStream#pol2Cart(theta, rho) select x, y insert into outputStream ; This will return cartesian coordinates (4.99953024681082, 0.06853693328228748) for theta: 0.7854 and rho: 5. EXAMPLE 2 1 2 3 from PolarStream#pol2Cart(theta, rho, 3.4) select x, y, z insert into outputStream ; This will return cartesian coordinates (4.99953024681082, 0.06853693328228748, 3.4)for theta: 0.7854 and rho: 5 and z: 3.4.","title":"pol2Cart (Stream Function)"},{"location":"docs/api/latest/#log-stream-processor","text":"The logger logs the message on the given priority with or without processed event. Syntax 1 log( STRING priority, STRING log.message, BOOL is.event.logged) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic priority The priority/type of this log message (INFO, DEBUG, WARN, FATAL, ERROR, OFF, TRACE). INFO STRING Yes No log.message This message will be logged. STRING No No is.event.logged To log the processed event. true BOOL Yes No Examples EXAMPLE 1 1 2 3 from fooStream#log( INFO , Sample Event : , true) select * insert into barStream; This will log as INFO with the message \"Sample Event :\" + fooStream:events. EXAMPLE 2 1 2 3 from fooStream#log( Sample Event : , true) select * insert into barStream; This will logs with default log level as INFO. EXAMPLE 3 1 2 3 from fooStream#log( Sample Event : , fasle) select * insert into barStream; This will only log message. EXAMPLE 4 1 2 3 from fooStream#log(true) select * insert into barStream; This will only log fooStream:events. EXAMPLE 5 1 2 3 from fooStream#log( Sample Event : ) select * insert into barStream; This will log message and fooStream:events.","title":"log (Stream Processor)"},{"location":"docs/api/latest/#batch-window","text":"A window that holds an incoming events batch. When a new set of events arrives, the previously arrived old events will be expired. Batch window can be used to aggregate events that comes in batches. If it has the parameter length specified, then batch window process the batch as several chunks. Syntax 1 batch( INT window.length) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.length The length of a chunk If length value was not given it assign 0 as length and process the whole batch as once INT Yes No Examples EXAMPLE 1 1 2 3 4 5 6 define stream consumerItemStream (itemId string, price float) from consumerItemStream#window.batch() select price, str:groupConcat(itemId) as itemIds group by price insert into outputStream; This will output comma separated items IDs that have the same price for each incoming batch of events.","title":"batch (Window)"},{"location":"docs/api/latest/#cron-window","text":"This window outputs the arriving events as and when they arrive, and resets (expires) the window periodically based on the given cron expression. Syntax 1 cron( STRING cron.expression) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic cron.expression The cron expression that resets the window. STRING No No Examples EXAMPLE 1 1 2 3 4 5 6 define stream InputEventStream (symbol string, price float, volume int); @info(name = query1 ) from InputEventStream#cron( */5 * * * * ? ) select symbol, sum(price) as totalPrice insert into OutputStream; This let the totalPrice to gradually increase and resets to zero as a batch every 5 seconds. EXAMPLE 2 1 2 3 4 5 6 7 8 9 10 11 define stream StockEventStream (symbol string, price float, volume int) define window StockEventWindow (symbol string, price float, volume int) cron( */5 * * * * ? ); @info(name = query0 ) from StockEventStream insert into StockEventWindow; @info(name = query1 ) from StockEventWindow select symbol, sum(price) as totalPrice insert into OutputStream ; The defined window will let the totalPrice to gradually increase and resets to zero as a batch every 5 seconds.","title":"cron (Window)"},{"location":"docs/api/latest/#delay-window","text":"A delay window holds events for a specific time period that is regarded as a delay period before processing them. Syntax 1 delay( INT|LONG|TIME window.delay) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.delay The time period (specified in sec, min, ms) for which the window should delay the events. INT LONG TIME No No Examples EXAMPLE 1 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 define window delayWindow(symbol string, volume int) delay(1 hour); define stream PurchaseStream(symbol string, volume int); define stream DeliveryStream(symbol string); define stream OutputStream(symbol string); @info(name= query1 ) from PurchaseStream select symbol, volume insert into delayWindow; @info(name= query2 ) from delayWindow join DeliveryStream on delayWindow.symbol == DeliveryStream.symbol select delayWindow.symbol insert into OutputStream; In this example, purchase events that arrive in the 'PurchaseStream' stream are directed to a delay window. At any given time, this delay window holds purchase events that have arrived within the last hour. These purchase events in the window are matched by the 'symbol' attribute, with delivery events that arrive in the 'DeliveryStream' stream. This monitors whether the delivery of products is done with a minimum delay of one hour after the purchase.","title":"delay (Window)"},{"location":"docs/api/latest/#externaltime-window","text":"A sliding time window based on external time. It holds events that arrived during the last windowTime period from the external timestamp, and gets updated on every monotonically increasing timestamp. Syntax 1 externalTime( LONG timestamp, INT|LONG|TIME window.time) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic timestamp The time which the window determines as current time and will act upon. The value of this parameter should be monotonically increasing. LONG No No window.time The sliding time period for which the window should hold events. INT LONG TIME No No Examples EXAMPLE 1 1 2 3 4 5 6 7 8 9 10 define window cseEventWindow (symbol string, price float, volume int) externalTime(eventTime, 20 sec) output expired events; @info(name = query0 ) from cseEventStream insert into cseEventWindow; @info(name = query1 ) from cseEventWindow select symbol, sum(price) as price insert expired events into outputStream ; processing events arrived within the last 20 seconds from the eventTime and output expired events.","title":"externalTime (Window)"},{"location":"docs/api/latest/#externaltimebatch-window","text":"A batch (tumbling) time window based on external time, that holds events arrived during windowTime periods, and gets updated for every windowTime. Syntax 1 externalTimeBatch( LONG timestamp, INT|LONG|TIME window.time, INT|LONG|TIME start.time, INT|LONG|TIME timeout) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic timestamp The time which the window determines as current time and will act upon. The value of this parameter should be monotonically increasing. LONG No No window.time The batch time period for which the window should hold events. INT LONG TIME No No start.time User defined start time. This could either be a constant (of type int, long or time) or an attribute of the corresponding stream (of type long). If an attribute is provided, initial value of attribute would be considered as startTime. Timestamp of first event INT LONG TIME Yes No timeout Time to wait for arrival of new event, before flushing and giving output for events belonging to a specific batch. System waits till an event from next batch arrives to flush current batch INT LONG TIME Yes No Examples EXAMPLE 1 1 2 3 4 5 6 7 8 define window cseEventWindow (symbol string, price float, volume int) externalTimeBatch(eventTime, 1 sec) output expired events; @info(name = query0 ) from cseEventStream insert into cseEventWindow; @info(name = query1 ) from cseEventWindow select symbol, sum(price) as price insert expired events into outputStream ; This will processing events that arrive every 1 seconds from the eventTime. EXAMPLE 2 1 define window cseEventWindow (symbol string, price float, volume int) externalTimeBatch(eventTime, 20 sec, 0) output expired events; This will processing events that arrive every 1 seconds from the eventTime. Starts on 0 th millisecond of an hour. EXAMPLE 3 1 define window cseEventWindow (symbol string, price float, volume int) externalTimeBatch(eventTime, 2 sec, eventTimestamp, 100) output expired events; This will processing events that arrive every 2 seconds from the eventTim. Considers the first event's eventTimestamp value as startTime. Waits 100 milliseconds for the arrival of a new event before flushing current batch.","title":"externalTimeBatch (Window)"},{"location":"docs/api/latest/#frequent-window","text":"This window returns the latest events with the most frequently occurred value for a given attribute(s). Frequency calculation for this window processor is based on Misra-Gries counting algorithm. Syntax 1 frequent( INT event.count, STRING attribute) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic event.count The number of most frequent events to be emitted to the stream. INT No No attribute The attributes to group the events. If no attributes are given, the concatenation of all the attributes of the event is considered. The concatenation of all the attributes of the event is considered. STRING Yes No Examples EXAMPLE 1 1 2 3 4 @info(name = query1 ) from purchase[price = 30]#window.frequent(2) select cardNo, price insert all events into PotentialFraud; This will returns the 2 most frequent events. EXAMPLE 2 1 2 3 4 @info(name = query1 ) from purchase[price = 30]#window.frequent(2, cardNo) select cardNo, price insert all events into PotentialFraud; This will returns the 2 latest events with the most frequently appeared card numbers.","title":"frequent (Window)"},{"location":"docs/api/latest/#length-window","text":"A sliding length window that holds the last 'window.length' events at a given time, and gets updated for each arrival and expiry. Syntax 1 length( INT window.length) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.length The number of events that should be included in a sliding length window. INT No No Examples EXAMPLE 1 1 2 3 4 5 6 7 8 9 10 define window StockEventWindow (symbol string, price float, volume int) length(10) output all events; @info(name = query0 ) from StockEventStream insert into StockEventWindow; @info(name = query1 ) from StockEventWindow select symbol, sum(price) as price insert all events into outputStream ; This will process last 10 events in a sliding manner.","title":"length (Window)"},{"location":"docs/api/latest/#lengthbatch-window","text":"A batch (tumbling) length window that holds and process a number of events as specified in the window.length. Syntax 1 lengthBatch( INT window.length, BOOL stream.current.event) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.length The number of events the window should tumble. INT No No stream.current.event Let the window stream the current events out as and when they arrive to the window while expiring them in batches. false BOOL Yes No Examples EXAMPLE 1 1 2 3 4 5 6 define stream InputEventStream (symbol string, price float, volume int); @info(name = query1 ) from InputEventStream#lengthBatch(10) select symbol, sum(price) as price insert into OutputStream; This collect and process 10 events as a batch and output them. EXAMPLE 2 1 2 3 4 5 6 define stream InputEventStream (symbol string, price float, volume int); @info(name = query1 ) from InputEventStream#lengthBatch(10, true) select symbol, sum(price) as sumPrice insert into OutputStream; This window sends the arriving events directly to the output letting the sumPrice to increase gradually, after every 10 events it clears the window as a batch and resets the sumPrice to zero. EXAMPLE 3 1 2 3 4 5 6 7 8 9 10 11 define stream InputEventStream (symbol string, price float, volume int); define window StockEventWindow (symbol string, price float, volume int) lengthBatch(10) output all events; @info(name = query0 ) from InputEventStream insert into StockEventWindow; @info(name = query1 ) from StockEventWindow select symbol, sum(price) as price insert all events into OutputStream ; This uses an defined window to process 10 events as a batch and output all events.","title":"lengthBatch (Window)"},{"location":"docs/api/latest/#lossyfrequent-window","text":"This window identifies and returns all the events of which the current frequency exceeds the value specified for the supportThreshold parameter. Syntax 1 lossyFrequent( DOUBLE support.threshold, DOUBLE error.bound, STRING attribute) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic support.threshold The support threshold value. DOUBLE No No error.bound The error bound value. DOUBLE No No attribute The attributes to group the events. If no attributes are given, the concatenation of all the attributes of the event is considered. The concatenation of all the attributes of the event is considered. STRING Yes No Examples EXAMPLE 1 1 2 3 4 5 6 7 8 9 define stream purchase (cardNo string, price float); define window purchaseWindow (cardNo string, price float) lossyFrequent(0.1, 0.01); @info(name = query0 ) from purchase[price = 30] insert into purchaseWindow; @info(name = query1 ) from purchaseWindow select cardNo, price insert all events into PotentialFraud; lossyFrequent(0.1, 0.01) returns all the events of which the current frequency exceeds 0.1, with an error bound of 0.01. EXAMPLE 2 1 2 3 4 5 6 7 8 9 define stream purchase (cardNo string, price float); define window purchaseWindow (cardNo string, price float) lossyFrequent(0.3, 0.05, cardNo); @info(name = query0 ) from purchase[price = 30] insert into purchaseWindow; @info(name = query1 ) from purchaseWindow select cardNo, price insert all events into PotentialFraud; lossyFrequent(0.3, 0.05, cardNo) returns all the events of which the cardNo attributes frequency exceeds 0.3, with an error bound of 0.05.","title":"lossyFrequent (Window)"},{"location":"docs/api/latest/#session-window","text":"This is a session window that holds events that belong to a specific session. The events that belong to a specific session are identified by a grouping attribute (i.e., a session key). A session gap period is specified to determine the time period after which the session is considered to be expired. A new event that arrives with a specific value for the session key is matched with the session window with the same session key. There can be out of order and late arrival of events, these events can arrive after the session is expired, to include those events to the matching session key specify a latency time period that is less than the session gap period.To have aggregate functions with session windows, the events need to be grouped by the session key via a 'group by' clause. Syntax 1 session( INT|LONG|TIME window.session, STRING window.key, INT|LONG|TIME window.allowedlatency) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.session The time period for which the session considered is valid. This is specified in seconds, minutes, or milliseconds (i.e., 'min', 'sec', or 'ms'. INT LONG TIME No No window.key The grouping attribute for events. default-key STRING Yes No window.allowedlatency This specifies the time period for which the session window is valid after the expiration of the session. The time period specified here should be less than the session time gap (which is specified via the 'window.session' parameter). 0 INT LONG TIME Yes No Examples EXAMPLE 1 1 2 3 4 5 6 define stream PurchaseEventStream (user string, item_number int, price float, quantity int); @info(name= query0) from PurchaseEventStream#window.session(5 sec, user, 2 sec) select * insert all events into OutputStream; This query processes events that arrive at the PurchaseEvent input stream. The 'user' attribute is the session key, and the session gap is 5 seconds. '2 sec' is specified as the allowed latency. Therefore, events with the matching user name that arrive 2 seconds after the expiration of the session are also considered when performing aggregations for the session identified by the given user name.","title":"session (Window)"},{"location":"docs/api/latest/#sort-window","text":"This window holds a batch of events that equal the number specified as the windowLength and sorts them in the given order. Syntax 1 sort( INT window.length, STRING attribute, STRING order) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.length The size of the window length. INT No No attribute The attribute that should be checked for the order. The concatenation of all the attributes of the event is considered. STRING Yes No order The order define as \"asc\" or \"desc\". asc STRING Yes No Examples EXAMPLE 1 1 2 3 4 5 6 7 8 9 define stream cseEventStream (symbol string, price float, volume long); define window cseEventWindow (symbol string, price float, volume long) sort(2,volume, asc ); @info(name = query0 ) from cseEventStream insert into cseEventWindow; @info(name = query1 ) from cseEventWindow select volume insert all events into outputStream ; sort(5, price, 'asc') keeps the events sorted by price in the ascending order. Therefore, at any given time, the window contains the 5 lowest prices.","title":"sort (Window)"},{"location":"docs/api/latest/#time-window","text":"A sliding time window that holds events that arrived during the last windowTime period at a given time, and gets updated for each event arrival and expiry. Syntax 1 time( INT|LONG|TIME window.time) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.time The sliding time period for which the window should hold events. INT LONG TIME No No Examples EXAMPLE 1 1 2 3 4 5 6 7 8 define window cseEventWindow (symbol string, price float, volume int) time(20) output all events; @info(name = query0 ) from cseEventStream insert into cseEventWindow; @info(name = query1 ) from cseEventWindow select symbol, sum(price) as price insert all events into outputStream ; This will processing events that arrived within the last 20 milliseconds.","title":"time (Window)"},{"location":"docs/api/latest/#timebatch-window","text":"A batch (tumbling) time window that holds and process events that arrive during 'window.time' period as a batch. Syntax 1 timeBatch( INT|LONG|TIME window.time, INT start.time, BOOL stream.current.event) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.time The batch time period in which the window process the events. INT LONG TIME No No start.time This specifies an offset in milliseconds in order to start the window at a time different to the standard time. Timestamp of first event INT Yes No stream.current.event Let the window stream the current events out as and when they arrive to the window while expiring them in batches. false BOOL Yes No Examples EXAMPLE 1 1 2 3 4 5 6 define stream InputEventStream (symbol string, price float, volume int); @info(name = query1 ) from InputEventStream#timeBatch(20 sec) select symbol, sum(price) as price insert into OutputStream; This collect and process incoming events as a batch every 20 seconds and output them. EXAMPLE 2 1 2 3 4 5 6 define stream InputEventStream (symbol string, price float, volume int); @info(name = query1 ) from InputEventStream#timeBatch(20 sec, true) select symbol, sum(price) as sumPrice insert into OutputStream; This window sends the arriving events directly to the output letting the sumPrice to increase gradually and on every 20 second interval it clears the window as a batch resetting the sumPrice to zero. EXAMPLE 3 1 2 3 4 5 6 7 8 9 10 11 define stream InputEventStream (symbol string, price float, volume int); define window StockEventWindow (symbol string, price float, volume int) timeBatch(20 sec) output all events; @info(name = query0 ) from InputEventStream insert into StockEventWindow; @info(name = query1 ) from StockEventWindow select symbol, sum(price) as price insert all events into OutputStream ; This uses an defined window to process events arrived every 20 seconds as a batch and output all events.","title":"timeBatch (Window)"},{"location":"docs/api/latest/#timelength-window","text":"A sliding time window that, at a given time holds the last window.length events that arrived during last window.time period, and gets updated for every event arrival and expiry. Syntax 1 timeLength( INT|LONG|TIME window.time, INT window.length) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.time The sliding time period for which the window should hold events. INT LONG TIME No No window.length The number of events that should be be included in a sliding length window.. INT No No Examples EXAMPLE 1 1 2 3 4 5 6 7 8 define stream cseEventStream (symbol string, price float, volume int); define window cseEventWindow (symbol string, price float, volume int) timeLength(2 sec, 10); @info(name = query0 ) from cseEventStream insert into cseEventWindow; @info(name = query1 ) from cseEventWindow select symbol, price, volume insert all events into outputStream; window.timeLength(2 sec, 10) holds the last 10 events that arrived during last 2 seconds and gets updated for every event arrival and expiry.","title":"timeLength (Window)"},{"location":"docs/api/latest/#sink","text":"","title":"Sink"},{"location":"docs/api/latest/#inmemory-sink","text":"In-memory transport that can communicate with other in-memory transports within the same JVM, itis assumed that the publisher and subscriber of a topic uses same event schema (stream definition). Syntax 1 @sink(type= inMemory , topic= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic topic Event will be delivered to allthe subscribers of the same topic STRING No No Examples EXAMPLE 1 1 2 @sink(type= inMemory , @map(type= passThrough )) define stream BarStream (symbol string, price float, volume long) In this example BarStream uses inMemory transport which emit the Siddhi events internally without using external transport and transformation.","title":"inMemory (Sink)"},{"location":"docs/api/latest/#log-sink","text":"This is a sink that can be used as a logger. This will log the output events in the output stream with user specified priority and a prefix Syntax 1 @sink(type= log , priority= STRING , prefix= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic priority This will set the logger priority i.e log level. Accepted values are INFO, DEBUG, WARN, FATAL, ERROR, OFF, TRACE INFO STRING Yes No prefix This will be the prefix to the output message. If the output stream has event [2,4] and the prefix is given as \"Hello\" then the log will show \"Hello : [2,4]\" default prefix will be : STRING Yes No Examples EXAMPLE 1 1 2 @sink(type= log , prefix= My Log , priority= DEBUG ) define stream BarStream (symbol string, price float, volume long) In this example BarStream uses log sink and the prefix is given as My Log. Also the priority is set to DEBUG. EXAMPLE 2 1 2 @sink(type= log , priority= DEBUG ) define stream BarStream (symbol string, price float, volume long) In this example BarStream uses log sink and the priority is set to DEBUG. User has not specified prefix so the default prefix will be in the form Siddhi App Name : Stream Name EXAMPLE 3 1 2 @sink(type= log , prefix= My Log ) define stream BarStream (symbol string, price float, volume long) In this example BarStream uses log sink and the prefix is given as My Log. User has not given a priority so it will be set to default INFO. EXAMPLE 4 1 2 @sink(type= log ) define stream BarStream (symbol string, price float, volume long) In this example BarStream uses log sink. The user has not given prefix or priority so they will be set to their default values.","title":"log (Sink)"},{"location":"docs/api/latest/#sinkmapper","text":"","title":"Sinkmapper"},{"location":"docs/api/latest/#passthrough-sink-mapper","text":"Pass-through mapper passed events (Event[]) through without any mapping or modifications. Syntax 1 @sink(..., @map(type= passThrough ) Examples EXAMPLE 1 1 2 @sink(type= inMemory , @map(type= passThrough )) define stream BarStream (symbol string, price float, volume long); In the following example BarStream uses passThrough outputmapper which emit Siddhi event directly without any transformation into sink.","title":"passThrough (Sink Mapper)"},{"location":"docs/api/latest/#source","text":"","title":"Source"},{"location":"docs/api/latest/#inmemory-source","text":"In-memory source that can communicate with other in-memory sinks within the same JVM, it is assumed that the publisher and subscriber of a topic uses same event schema (stream definition). Syntax 1 @source(type= inMemory , topic= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic topic Subscribes to sent on the given topic. STRING No No Examples EXAMPLE 1 1 2 @source(type= inMemory , @map(type= passThrough )) define stream BarStream (symbol string, price float, volume long) In this example BarStream uses inMemory transport which passes the received event internally without using external transport.","title":"inMemory (Source)"},{"location":"docs/api/latest/#sourcemapper","text":"","title":"Sourcemapper"},{"location":"docs/api/latest/#passthrough-source-mapper","text":"Pass-through mapper passed events (Event[]) through without any mapping or modifications. Syntax 1 @source(..., @map(type= passThrough ) Examples EXAMPLE 1 1 2 @source(type= tcp , @map(type= passThrough )) define stream BarStream (symbol string, price float, volume long); In this example BarStream uses passThrough inputmapper which passes the received Siddhi event directly without any transformation into source.","title":"passThrough (Source Mapper)"},{"location":"docs/guides/alerts-for-thresholds/guide/","text":"Generating Alerts Based on Static and Dynamic Thresholds In this guide, you will understand one of the common requirements of a Stream Processing which is generating alerts based on static and dynamic thresholds. To understand this requirement, let\u2019s consider the throttling use case in API management solutions. Scenario - Throttling for API Requests Throttling has become as one of the unavoidable needs with the evolution of APIs and API management. Throttling is a process that is used to control the usage of APIs by consumers during a given period. The following sections are available in this guide. What you'll build Prerequisites Implementation Testing Deployment Output What you'll build Let's consider a real world use case to implement the throttling requirement. This will help you to understand some Siddhi Stream Processing constructs such as windows, aggregations, source, and etc. Let\u2019s jump into the use case directly. Let's assume that you are an API developer and you have published a few APIs to the API store and there are subscribers who have subscribed to these APIs in different tiers which are categorized based on the number of requests per min/sec. If any subscriber is consuming an API more than the allowed quota within a time frame then that specific user will be throttled until that time frame passes. Also if a subscriber is getting throttled often then the system sends a notification to that user requesting to upgrade the tier. For example, let\u2019s assume that user \u201cJohn\u201d has subscribed to an API with the tier 'Silver'; silver tier allows a user to make 10 API requests per minute. If John, made more than 10 requests within a minute then his subsequent requests get throttled until the end of the minute, and if he has got throttled more than 10 times in an hour, then he will be notified to upgrade his tier via email. Now, let\u2019s understand how this could be implemented in Siddhi engine. Prerequisites Below are the prerequisites that should be considered to implement the above use case. Mandatory Requirements Siddhi tooling VM/Local distribution One of the Siddhi runner distributions VM/Local Runtime Docker Image K8S Operator (commands are given in deployment section) Java 8 or higher Requirements needed to deploy Siddhi in Docker/Kubernetes Docker Minikube or Google Kubernetes Engine(GKE) Cluster or Docker for Mac Implementation When a subscriber made an API call to order-mgt-v1 API it sends an event with the API request information to Siddhi runtime through HTTP transport. Siddhi runtime, keep track of each API request and make decisions to throttle subscribers. Again, once the corresponding time frame passed Siddhi release those throttle users. Throttling decisions are informed to API management solution through an API call. If a subscriber is getting throttled more than 10 times in an hour then sends a notification mail once every 15 minutes to that user requesting to upgrade the tier. Implement Streaming Queries Start the Siddhi tooling runtime and go to the editor UI in http://localhost:9390/editor Follow below steps to start the Siddhi tooling runtime. * Extract the downloaded zip and navigate to /bin. (TOOLING_HOME refers to the extracted folder) * Issue the following command in the command prompt (Windows) / terminal (Linux/Mac) 1 2 For Windows: tooling.bat For Linux/Mac: ./tooling.sh Select File - New option, then you could either use the source view or design view to write/build the Siddhi Application. You can find the Siddhi Application bellow, that implements the requirements mentioned above. Let\u2019s write (develop) the Siddhi Application, as given below. Once the Siddhi app is created, you can use the Event Simulator option in the editor to simulate events to streams and perform developer testing. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 @ App : name ( API-Request-Throttler ) @ App : description ( Enforces throttling on API requests ) -- HTTP endpoint which listens for api request related events @ source ( type = http , receiver . url = http://0.0.0.0:8006/apiRequest , basic . auth . enabled = false , @ map ( type = json )) define stream APIRequestStream ( apiName string , version string , tier string , user string , userEmail string ); -- HTTP sink to publich throttle decisions. For testing purpose, there is a mock logger service provided @ sink ( type = http , publisher . url = http://${LOGGER_SERVICE_HOST}:8080/logger , method = POST , @ map ( type = json )) @ sink ( type = log , @ map ( type = text )) define stream ThrottleOutputStream ( apiName string , version string , user string , tier string , userEmail string , isThrottled bool ); -- Email sink to send alerts @ sink ( type = log , @ map ( type = text )) @ sink ( type = email , username = ${EMAIL_USERNAME} , address = ${SENDER_EMAIL_ADDRESS} , password = ${EMAIL_PASSWORD} , subject = Upgrade API Subscription Tier , to = {{userEmail}} , host = smtp.gmail.com , port = 465 , ssl . enable = true , auth = true , @ map ( type = text , @ payload ( Hi {{user}} You have subscribed to API called {{apiName}}:{{version}} with {{tier}} tier. Based on our records, it seems you are hitting the upper limit of the API requests in a frequent manner. We kindly request you to consider upgrading to next API subscription tier to avoid this in the future. Thanks, API Team ))) define stream UserNotificationStream ( user string , apiName string , version string , tier string , userEmail string , throttledCount long ); @ info ( name = Query to find users who needs to be throttled based on tier `silver` ) from APIRequestStream [ tier == silver ] # window . timeBatch ( 1 min , 0 , true ) select apiName , version , user , tier , userEmail , count () as totalRequestCount group by apiName , version , user having totalRequestCount == 10 or totalRequestCount == 0 insert all events into ThrottledStream ; @ info ( name = Query to find users who needs to be throttled based on tier `gold` ) from APIRequestStream [ tier == gold ] # window . timeBatch ( 1 min , 0 , true ) select apiName , version , user , tier , userEmail , count () as totalRequestCount group by apiName , version , user having totalRequestCount == 100 or totalRequestCount == 0 insert all events into ThrottledStream ; @ info ( name = Query to add a flag for throttled request ) from ThrottledStream select apiName , version , user , tier , userEmail , ifThenElse ( totalRequestCount == 0 , false , true ) as isThrottled insert into ThrottleOutputStream ; @ info ( name = Query to find frequently throttled users - who have throttled more than 10 times in the last hour ) from ThrottleOutputStream [ isThrottled ] # window . time ( 1 hour ) select user , apiName , version , tier , userEmail , count () as throttledCount group by user , apiName , version , tier having throttledCount 10 output first every 15 min insert into UserNotificationStream ; Source view of the Siddhi app. Below is the flow diagram of the above Siddhi App. Testing NOTE: In the provided Siddhi app, there are some environmental variables (EMAIL_PASSWORD, EMAIL_USERNAME, and SENDER_EMAIL_ADDRESS) used which are required to be set to send an email alert based on the Siddhi queries defined. Again, there is a mock service configured to receive the throttle decisions, and its host is configured via LOGGER_SERVICE_HOST environment property. Hence, make sure to set the environmental variables with the proper values in the system EMAIL_USERNAME: Username of the email account which used to send email alerts. (eg: 'siddhi.gke.user') EMAIL_PASSWORD: Password of the email account which used to send email alerts. (eg: 'siddhi123') SENDER_EMAIL_ADDRESS: Email address of the account used to send email alerts. (eg: 'siddhi.gke.user@gmail.com') LOGGER_SERVICE_HOST: IP address of the host where logger service is running. (eg: 'localhost') When you run the Siddhi app in the editor, you will see below logs getting printed in the editor console. You could simply simulate some events directly into the stream and test your Siddhi app in the editor itself. Then, you can also simulate some events through HTTP to test the application. The following sections explain how you can test the Siddhi app via HTTP using cURL. Run Mock Logger service In the provided Siddhi app, there is an HTTP sink configured to push output events to an HTTP endpoint of the API Manager. For simplicity, you will be mocking this service. Please download the mock server jar and run that mock service by executing the following command. 1 java -jar logservice-1.0.0.jar Invoking the Siddhi App As per the Siddhi app that you wrote in the 'Implementation' section, there is an HTTP service running in Siddhi which is listening for events related to API requests. The respective service can be accessed via the URL http://localhost:9090/ThotttleService . As per the app, the API request will get throttled if there are more than 10 requests by the same user, to the same API (for 'silver\u2019 tier). 1 curl -v -X POST -d { event : { apiName : order-mgt-v1 , version : 1.0.0 , tier : silver , user : mohan , userEmail : example@wso2.com }} http://localhost:8006/apiRequest -H Content-Type:application/json If you invoke the above cURL request for more than 10 times within a minute, then Siddhi starts throttling the request, and sends an alert to the API Manager (logservice), while logging the alert as below. 1 2 3 INFO {io.siddhi.core.stream.output.sink.LogSink} - API-Request-Throttler : ThrottleOutputStream : Event{timestamp=1564056341280, data=[order-mgt-v1, 1.0.0, mohan, silver, true], isExpired=false} You can validate that the alert has reached the API Manager (logservice) from its console logs. If a user gets throttled more than 10 times within an hour then Siddhi sends an email to the respective user. Note: The configurations provided in the email sink along with the environment properties will work for Gmail, but if you use other mail servers, please make sure to change the config values accordingly. Deployment Once you are done with the development, export the Siddhi app that you have developed with 'File' - 'Export File' option. You can deploy the Siddhi app using any of the methods listed below. Deploy on VM/ Bare Metal Download the latest Siddhi Runner distribution . Unzip the siddhi-runner-x.x.x.zip . Configure the necessary environmental variables In the above provided Siddhi app, there are some environmental variables (EMAIL_USERNAME, EMAIL_PASSWORD, and SENDER_EMAIL_ADDRESS) which are required to be set to send email alerts based on the Siddhi queries defined. Again, there is a mock service configured to receive the throttle decisions (instructions given below), and its host is configured via LOGGER_SERVICE_HOST environment property. Hence, make sure to set the environmental variables with the proper values in the system (make sure to follow necessary steps based on the underneath operating system). Start Siddhi app with the runner config by executing the following commands from the distribution directory. ``` Linux/Mac : ./bin/runner.sh -Dapps= Windows : bin\\runner.bat -Dapps= 1 2 Eg: If exported siddhi app in Siddhi home directory, ./bin/runner.sh -Dapps=API-Request-Throttler.siddhi ``` Download the mock logging service which is used to demonstrate the capability of Siddhi HTTP sink. Execute the below command to run the mock server. 1 java -jar logservice-1.0.0.jar Invoke the apiRequest service with the following cURL request for more than 10 times within a minute time period. Please make sure to change the userEmail property value to an email address that you could use to test the email alerting purposes. 1 curl -v -X POST -d { event : { apiName : order-mgt-v1 , version : 1.0.0 , tier : silver , user : mohan , userEmail : example@wso2.com }} http://localhost:8006/apiRequest -H Content-Type:application/json You can see the output log in the console. Here, you will be able to see the alert log printed as shown below. At the same time, you could also see the events received to HTTP mock service endpoint (started in step #5) via its log as below. Deploy on Docker Create a folder locally on your host machine (eg: /home/siddhi-apps ) and copy the Siddhi app into it. Pull the latest Siddhi Runner image from [Siddhiio Docker Hub] (https://hub.docker.com/u/siddhiio). 1 docker pull siddhiio/siddhi-runner-alpine:5.1.0-alpha Start SiddhiApp by executing the following docker command. 1 docker run -it -p 8006:8006 -v /home/siddhi-apps:/apps -e EMAIL_PASSWORD=siddhi123 -e EMAIL_USERNAME=siddhi.gke.user -e SENDER_EMAIL_ADDRESS=siddhi.gke.user@gmail.com -e LOGGER_SERVICE_HOST=10.100.0.99 siddhiio/siddhi-runner-alpine:5.1.0-alpha -Dapps=/apps/API-Request-Throttler.siddhi NOTE: In the above provided Siddhi app, there are some environmental variables (EMAIL_PASSWORD, EMAIL_USERNAME, and SENDER_EMAIL_ADDRESS) which are required to be set to send email alerts based on the Siddhi queries defined. Again, there is a mock service configured to receive the throttle decisions (instructions given below), and its host is configured via LOGGER_SERVICE_HOST environment property. Hence, make sure to add proper values for the environmental variables in the above command. Download the mock logging service which is used to demonstrate the capability of Siddhi HTTP sink. Execute the below command to run the mock server. 1 java -jar logservice-1.0.0.jar Invoke the apiRequest service with the following cURL request for more than 10 times within a minute time period. Please make sure to change the userEmail property value to an email address that you could use to test the email alerting purposes. 1 curl -v -X POST -d { event : { apiName : order-mgt-v1 , version : 1.0.0 , tier : silver , user : mohan , userEmail : example@wso2.com }} http://localhost:8006/apiRequest -H Content-Type:application/json Since you have started the docker in interactive mode you can see the output in its console as below. (If it is not started in the interactive mode then you can run docker exec -it docker-container-id sh command, go into the container and check the log file in home/siddhi_user/siddhi-runner/wso2/runner/logs/carbon.log file) At the same time, you could also see the events received to HTTP mock service endpoint (started in step #4) via its log as below. If there are more than 10 requests get throttled within 1 hour then the API invoker will receive an email (as shown in the 'Testing' section). Deploy on Kubernetes Install Siddhi Operator To install the Siddhi Kubernetes operator run the following commands. 1 2 kubectl apply -f https://github.com/siddhi-io/siddhi-operator/releases/download/v0.2.0-alpha/00-prereqs.yaml kubectl apply -f https://github.com/siddhi-io/siddhi-operator/releases/download/v0.2.0-alpha/01-siddhi-operator.yaml You can verify the installation by making sure the following deployments are running in your Kubernetes cluster. Download the mock logging service which is used to demonstrate the capability of Siddhi HTTP sink. Execute the below command to run the mock server. 1 java -jar logservice-1.0.0.jar Siddhi applications can be deployed on Kubernetes using the Siddhi operator. Before deploying the apps you have to define an Ingress , this is because there is an HTTP endpoint in the Siddhi app you have written and you will be sending events to that. To deploy the above created Siddhi app, you have to create a custom resource object YAML file (with the kind as SiddhiProcess) as following 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 apiVersion : siddhi.io/v1alpha2 kind : SiddhiProcess metadata : name : api-throttler-app spec : apps : - script : | @App:name( API-Request-Throttler ) @App:description( Enforcesthrottling to API requests ) -- HTTP endpoint which listens for api request related events @source(type = http , receiver.url = http://0.0.0.0:8006/apiRequest , basic.auth.enabled = false , @map(type = json )) define stream APIRequestStream (apiName string, version string, tier string, user string, userEmail string); -- HTTP sink to publich throttle decisions. For testing purpose, there is a mock logger service provided @sink(type = http , publisher.url = http://${LOGGER_SERVICE_HOST}:8080/logger , method = POST , @map(type = json )) @sink(type = log , @map(type = text )) define stream ThrottleOutputStream (apiName string, version string, user string, tier string, userEmail string, isThrottled bool); -- Email sink to send alerts @sink(type = log , @map(type = text )) @sink(type = email , username = ${EMAIL_USERNAME} , address = ${SENDER_EMAIL_ADDRESS} , password = ${EMAIL_PASSWORD} , subject = Upgrade API Subscription Tier , to = {{userEmail}} , host = smtp.gmail.com , port = 465 , ssl.enable = true , auth = true , @map(type = text , @payload( Hi {{user}} You have subscribed to API called {{apiName}}:{{version}} with {{tier}} tier. Based on our records, it seems you are hitting the upper limit of the API requests in a frequent manner. We kindly request you to consider upgrading to next API subscription tier to avoid this in the future. Thanks, API Team ))) define stream UserNotificationStream (user string, apiName string, version string, tier string, userEmail string, throttledCount long); @info(name = Query to find users who needs to be throttled based on tier `silver` ) from APIRequestStream[tier == silver ]#window.timeBatch(1 min, 0, true) select apiName, version, user, tier, userEmail, count() as totalRequestCount group by apiName, version, user having totalRequestCount == 10 or totalRequestCount == 0 insert all events into ThrottledStream; @info(name = Query to find users who needs to be throttled based on tier `gold` ) from APIRequestStream[tier == gold ]#window.timeBatch(1 min, 0, true) select apiName, version, user, tier, userEmail, count() as totalRequestCount group by apiName, version, user having totalRequestCount == 100 or totalRequestCount == 0 insert all events into ThrottledStream; @info(name = Query to add a flag for throttled request ) from ThrottledStream select apiName, version, user, tier, userEmail, ifThenElse(totalRequestCount == 0, false, true) as isThrottled insert into ThrottleOutputStream; @info(name = Query to find frequently throttled users - who have throttled more than 10 times in the last hour ) from ThrottleOutputStream[isThrottled]#window.time(1 hour) select user, apiName, version, tier, userEmail, count() as throttledCount group by user, apiName, version, tier having throttledCount 10 output first every 15 min insert into UserNotificationStream; container : env : - name : EMAIL_PASSWORD value : siddhi123 - name : EMAIL_USERNAME value : siddhi.gke.user - name : SENDER_EMAIL_ADDRESS value : siddhi.gke.user@gmail.com - name : LOGGER_SERVICE_HOST value : 10.100.0.99 image : siddhiio/siddhi-runner-ubuntu:5.1.0-alpha NOTE: In the above provided Siddhi app, there are some environmental variables (EMAIL_PASSWORD, EMAIL_USERNAME, and SENDER_EMAIL_ADDRESS) which are required to be set to send email alerts based on the Siddhi queries defined. Again, there is a mock service configured to receive the throttle decisions (instructions given below), and its host is configured via LOGGER_SERVICE_HOST environment property. Hence, make sure to add proper values for the environmental variables in the above YAML file (check the env section of the YAML file). Now, let\u2019s create the above resource in the Kubernetes cluster with the following command. 1 kubectl create -f absolute-yaml-file-path /API-Request-Throttler.yaml Once, Siddhi app is successfully deployed. You can verify its health using the following commands Then, add the host siddhi and related external IP (ADDRESS) to the /etc/hosts file in your machine. For Docker for Mac , external IP is 0.0.0.0 . For Minikube, you have to use Minikube IP as the external IP. Hence, run minikube ip command to get the IP of the Minikube cluster. You can find the alert logs in the Siddhi runner log file. To see the Siddhi runner log file, first, invoke below command to get the pods. 1 kubectl get pods Then, find the pod name of the Siddhi app deployed, and invoke below command to view the logs. 1 kubectl logs siddhi-app-pod-name -f Eg: as shown below image, Invoke the apiRequest service with below cURL request for more than 10 times within a minute. Please make sure to change the userEmail property value to an email address that you could use to test the email alerting purposes. 1 curl -v -X POST -d { event : { apiName : order-mgt-v1 , version : 1.0.0 , tier : silver , user : mohan , userEmail : example@wso2.com }} http://siddhi/api-throttler-app-0/8006/apiRequest -H Content-Type:application/json Then, you will be able to see the throttle decisions as console logs (as given below). At the same time, you could also see the events received to HTTP mock service endpoint (started in step #2) via its log as below. If there are more than 10 requests get throttled within 1 hour then the API invoker will receive an email (as shown in the 'Testing' section). Refer here to get more details about running Siddhi on Kubernetes.","title":"Alerts Based on Thresholds"},{"location":"docs/guides/alerts-for-thresholds/guide/#generating-alerts-based-on-static-and-dynamic-thresholds","text":"In this guide, you will understand one of the common requirements of a Stream Processing which is generating alerts based on static and dynamic thresholds. To understand this requirement, let\u2019s consider the throttling use case in API management solutions.","title":"Generating Alerts Based on Static and Dynamic Thresholds"},{"location":"docs/guides/alerts-for-thresholds/guide/#scenario-throttling-for-api-requests","text":"Throttling has become as one of the unavoidable needs with the evolution of APIs and API management. Throttling is a process that is used to control the usage of APIs by consumers during a given period. The following sections are available in this guide. What you'll build Prerequisites Implementation Testing Deployment Output","title":"Scenario - Throttling for API Requests"},{"location":"docs/guides/alerts-for-thresholds/guide/#what-youll-build","text":"Let's consider a real world use case to implement the throttling requirement. This will help you to understand some Siddhi Stream Processing constructs such as windows, aggregations, source, and etc. Let\u2019s jump into the use case directly. Let's assume that you are an API developer and you have published a few APIs to the API store and there are subscribers who have subscribed to these APIs in different tiers which are categorized based on the number of requests per min/sec. If any subscriber is consuming an API more than the allowed quota within a time frame then that specific user will be throttled until that time frame passes. Also if a subscriber is getting throttled often then the system sends a notification to that user requesting to upgrade the tier. For example, let\u2019s assume that user \u201cJohn\u201d has subscribed to an API with the tier 'Silver'; silver tier allows a user to make 10 API requests per minute. If John, made more than 10 requests within a minute then his subsequent requests get throttled until the end of the minute, and if he has got throttled more than 10 times in an hour, then he will be notified to upgrade his tier via email. Now, let\u2019s understand how this could be implemented in Siddhi engine.","title":"What you'll build"},{"location":"docs/guides/alerts-for-thresholds/guide/#prerequisites","text":"Below are the prerequisites that should be considered to implement the above use case.","title":"Prerequisites"},{"location":"docs/guides/alerts-for-thresholds/guide/#mandatory-requirements","text":"Siddhi tooling VM/Local distribution One of the Siddhi runner distributions VM/Local Runtime Docker Image K8S Operator (commands are given in deployment section) Java 8 or higher","title":"Mandatory Requirements"},{"location":"docs/guides/alerts-for-thresholds/guide/#requirements-needed-to-deploy-siddhi-in-dockerkubernetes","text":"Docker Minikube or Google Kubernetes Engine(GKE) Cluster or Docker for Mac","title":"Requirements needed to deploy Siddhi in Docker/Kubernetes"},{"location":"docs/guides/alerts-for-thresholds/guide/#implementation","text":"When a subscriber made an API call to order-mgt-v1 API it sends an event with the API request information to Siddhi runtime through HTTP transport. Siddhi runtime, keep track of each API request and make decisions to throttle subscribers. Again, once the corresponding time frame passed Siddhi release those throttle users. Throttling decisions are informed to API management solution through an API call. If a subscriber is getting throttled more than 10 times in an hour then sends a notification mail once every 15 minutes to that user requesting to upgrade the tier.","title":"Implementation"},{"location":"docs/guides/alerts-for-thresholds/guide/#implement-streaming-queries","text":"Start the Siddhi tooling runtime and go to the editor UI in http://localhost:9390/editor Follow below steps to start the Siddhi tooling runtime. * Extract the downloaded zip and navigate to /bin. (TOOLING_HOME refers to the extracted folder) * Issue the following command in the command prompt (Windows) / terminal (Linux/Mac) 1 2 For Windows: tooling.bat For Linux/Mac: ./tooling.sh Select File - New option, then you could either use the source view or design view to write/build the Siddhi Application. You can find the Siddhi Application bellow, that implements the requirements mentioned above. Let\u2019s write (develop) the Siddhi Application, as given below. Once the Siddhi app is created, you can use the Event Simulator option in the editor to simulate events to streams and perform developer testing. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 @ App : name ( API-Request-Throttler ) @ App : description ( Enforces throttling on API requests ) -- HTTP endpoint which listens for api request related events @ source ( type = http , receiver . url = http://0.0.0.0:8006/apiRequest , basic . auth . enabled = false , @ map ( type = json )) define stream APIRequestStream ( apiName string , version string , tier string , user string , userEmail string ); -- HTTP sink to publich throttle decisions. For testing purpose, there is a mock logger service provided @ sink ( type = http , publisher . url = http://${LOGGER_SERVICE_HOST}:8080/logger , method = POST , @ map ( type = json )) @ sink ( type = log , @ map ( type = text )) define stream ThrottleOutputStream ( apiName string , version string , user string , tier string , userEmail string , isThrottled bool ); -- Email sink to send alerts @ sink ( type = log , @ map ( type = text )) @ sink ( type = email , username = ${EMAIL_USERNAME} , address = ${SENDER_EMAIL_ADDRESS} , password = ${EMAIL_PASSWORD} , subject = Upgrade API Subscription Tier , to = {{userEmail}} , host = smtp.gmail.com , port = 465 , ssl . enable = true , auth = true , @ map ( type = text , @ payload ( Hi {{user}} You have subscribed to API called {{apiName}}:{{version}} with {{tier}} tier. Based on our records, it seems you are hitting the upper limit of the API requests in a frequent manner. We kindly request you to consider upgrading to next API subscription tier to avoid this in the future. Thanks, API Team ))) define stream UserNotificationStream ( user string , apiName string , version string , tier string , userEmail string , throttledCount long ); @ info ( name = Query to find users who needs to be throttled based on tier `silver` ) from APIRequestStream [ tier == silver ] # window . timeBatch ( 1 min , 0 , true ) select apiName , version , user , tier , userEmail , count () as totalRequestCount group by apiName , version , user having totalRequestCount == 10 or totalRequestCount == 0 insert all events into ThrottledStream ; @ info ( name = Query to find users who needs to be throttled based on tier `gold` ) from APIRequestStream [ tier == gold ] # window . timeBatch ( 1 min , 0 , true ) select apiName , version , user , tier , userEmail , count () as totalRequestCount group by apiName , version , user having totalRequestCount == 100 or totalRequestCount == 0 insert all events into ThrottledStream ; @ info ( name = Query to add a flag for throttled request ) from ThrottledStream select apiName , version , user , tier , userEmail , ifThenElse ( totalRequestCount == 0 , false , true ) as isThrottled insert into ThrottleOutputStream ; @ info ( name = Query to find frequently throttled users - who have throttled more than 10 times in the last hour ) from ThrottleOutputStream [ isThrottled ] # window . time ( 1 hour ) select user , apiName , version , tier , userEmail , count () as throttledCount group by user , apiName , version , tier having throttledCount 10 output first every 15 min insert into UserNotificationStream ; Source view of the Siddhi app. Below is the flow diagram of the above Siddhi App.","title":"Implement Streaming Queries"},{"location":"docs/guides/alerts-for-thresholds/guide/#testing","text":"NOTE: In the provided Siddhi app, there are some environmental variables (EMAIL_PASSWORD, EMAIL_USERNAME, and SENDER_EMAIL_ADDRESS) used which are required to be set to send an email alert based on the Siddhi queries defined. Again, there is a mock service configured to receive the throttle decisions, and its host is configured via LOGGER_SERVICE_HOST environment property. Hence, make sure to set the environmental variables with the proper values in the system EMAIL_USERNAME: Username of the email account which used to send email alerts. (eg: 'siddhi.gke.user') EMAIL_PASSWORD: Password of the email account which used to send email alerts. (eg: 'siddhi123') SENDER_EMAIL_ADDRESS: Email address of the account used to send email alerts. (eg: 'siddhi.gke.user@gmail.com') LOGGER_SERVICE_HOST: IP address of the host where logger service is running. (eg: 'localhost') When you run the Siddhi app in the editor, you will see below logs getting printed in the editor console. You could simply simulate some events directly into the stream and test your Siddhi app in the editor itself. Then, you can also simulate some events through HTTP to test the application. The following sections explain how you can test the Siddhi app via HTTP using cURL.","title":"Testing"},{"location":"docs/guides/alerts-for-thresholds/guide/#run-mock-logger-service","text":"In the provided Siddhi app, there is an HTTP sink configured to push output events to an HTTP endpoint of the API Manager. For simplicity, you will be mocking this service. Please download the mock server jar and run that mock service by executing the following command. 1 java -jar logservice-1.0.0.jar","title":"Run Mock Logger service"},{"location":"docs/guides/alerts-for-thresholds/guide/#invoking-the-siddhi-app","text":"As per the Siddhi app that you wrote in the 'Implementation' section, there is an HTTP service running in Siddhi which is listening for events related to API requests. The respective service can be accessed via the URL http://localhost:9090/ThotttleService . As per the app, the API request will get throttled if there are more than 10 requests by the same user, to the same API (for 'silver\u2019 tier). 1 curl -v -X POST -d { event : { apiName : order-mgt-v1 , version : 1.0.0 , tier : silver , user : mohan , userEmail : example@wso2.com }} http://localhost:8006/apiRequest -H Content-Type:application/json If you invoke the above cURL request for more than 10 times within a minute, then Siddhi starts throttling the request, and sends an alert to the API Manager (logservice), while logging the alert as below. 1 2 3 INFO {io.siddhi.core.stream.output.sink.LogSink} - API-Request-Throttler : ThrottleOutputStream : Event{timestamp=1564056341280, data=[order-mgt-v1, 1.0.0, mohan, silver, true], isExpired=false} You can validate that the alert has reached the API Manager (logservice) from its console logs. If a user gets throttled more than 10 times within an hour then Siddhi sends an email to the respective user. Note: The configurations provided in the email sink along with the environment properties will work for Gmail, but if you use other mail servers, please make sure to change the config values accordingly.","title":"Invoking the Siddhi App"},{"location":"docs/guides/alerts-for-thresholds/guide/#deployment","text":"Once you are done with the development, export the Siddhi app that you have developed with 'File' - 'Export File' option. You can deploy the Siddhi app using any of the methods listed below.","title":"Deployment"},{"location":"docs/guides/alerts-for-thresholds/guide/#deploy-on-vm-bare-metal","text":"Download the latest Siddhi Runner distribution . Unzip the siddhi-runner-x.x.x.zip . Configure the necessary environmental variables In the above provided Siddhi app, there are some environmental variables (EMAIL_USERNAME, EMAIL_PASSWORD, and SENDER_EMAIL_ADDRESS) which are required to be set to send email alerts based on the Siddhi queries defined. Again, there is a mock service configured to receive the throttle decisions (instructions given below), and its host is configured via LOGGER_SERVICE_HOST environment property. Hence, make sure to set the environmental variables with the proper values in the system (make sure to follow necessary steps based on the underneath operating system). Start Siddhi app with the runner config by executing the following commands from the distribution directory. ``` Linux/Mac : ./bin/runner.sh -Dapps= Windows : bin\\runner.bat -Dapps= 1 2 Eg: If exported siddhi app in Siddhi home directory, ./bin/runner.sh -Dapps=API-Request-Throttler.siddhi ``` Download the mock logging service which is used to demonstrate the capability of Siddhi HTTP sink. Execute the below command to run the mock server. 1 java -jar logservice-1.0.0.jar Invoke the apiRequest service with the following cURL request for more than 10 times within a minute time period. Please make sure to change the userEmail property value to an email address that you could use to test the email alerting purposes. 1 curl -v -X POST -d { event : { apiName : order-mgt-v1 , version : 1.0.0 , tier : silver , user : mohan , userEmail : example@wso2.com }} http://localhost:8006/apiRequest -H Content-Type:application/json You can see the output log in the console. Here, you will be able to see the alert log printed as shown below. At the same time, you could also see the events received to HTTP mock service endpoint (started in step #5) via its log as below.","title":"Deploy on VM/ Bare Metal"},{"location":"docs/guides/alerts-for-thresholds/guide/#deploy-on-docker","text":"Create a folder locally on your host machine (eg: /home/siddhi-apps ) and copy the Siddhi app into it. Pull the latest Siddhi Runner image from [Siddhiio Docker Hub] (https://hub.docker.com/u/siddhiio). 1 docker pull siddhiio/siddhi-runner-alpine:5.1.0-alpha Start SiddhiApp by executing the following docker command. 1 docker run -it -p 8006:8006 -v /home/siddhi-apps:/apps -e EMAIL_PASSWORD=siddhi123 -e EMAIL_USERNAME=siddhi.gke.user -e SENDER_EMAIL_ADDRESS=siddhi.gke.user@gmail.com -e LOGGER_SERVICE_HOST=10.100.0.99 siddhiio/siddhi-runner-alpine:5.1.0-alpha -Dapps=/apps/API-Request-Throttler.siddhi NOTE: In the above provided Siddhi app, there are some environmental variables (EMAIL_PASSWORD, EMAIL_USERNAME, and SENDER_EMAIL_ADDRESS) which are required to be set to send email alerts based on the Siddhi queries defined. Again, there is a mock service configured to receive the throttle decisions (instructions given below), and its host is configured via LOGGER_SERVICE_HOST environment property. Hence, make sure to add proper values for the environmental variables in the above command. Download the mock logging service which is used to demonstrate the capability of Siddhi HTTP sink. Execute the below command to run the mock server. 1 java -jar logservice-1.0.0.jar Invoke the apiRequest service with the following cURL request for more than 10 times within a minute time period. Please make sure to change the userEmail property value to an email address that you could use to test the email alerting purposes. 1 curl -v -X POST -d { event : { apiName : order-mgt-v1 , version : 1.0.0 , tier : silver , user : mohan , userEmail : example@wso2.com }} http://localhost:8006/apiRequest -H Content-Type:application/json Since you have started the docker in interactive mode you can see the output in its console as below. (If it is not started in the interactive mode then you can run docker exec -it docker-container-id sh command, go into the container and check the log file in home/siddhi_user/siddhi-runner/wso2/runner/logs/carbon.log file) At the same time, you could also see the events received to HTTP mock service endpoint (started in step #4) via its log as below. If there are more than 10 requests get throttled within 1 hour then the API invoker will receive an email (as shown in the 'Testing' section).","title":"Deploy on Docker"},{"location":"docs/guides/alerts-for-thresholds/guide/#deploy-on-kubernetes","text":"Install Siddhi Operator To install the Siddhi Kubernetes operator run the following commands. 1 2 kubectl apply -f https://github.com/siddhi-io/siddhi-operator/releases/download/v0.2.0-alpha/00-prereqs.yaml kubectl apply -f https://github.com/siddhi-io/siddhi-operator/releases/download/v0.2.0-alpha/01-siddhi-operator.yaml You can verify the installation by making sure the following deployments are running in your Kubernetes cluster. Download the mock logging service which is used to demonstrate the capability of Siddhi HTTP sink. Execute the below command to run the mock server. 1 java -jar logservice-1.0.0.jar Siddhi applications can be deployed on Kubernetes using the Siddhi operator. Before deploying the apps you have to define an Ingress , this is because there is an HTTP endpoint in the Siddhi app you have written and you will be sending events to that. To deploy the above created Siddhi app, you have to create a custom resource object YAML file (with the kind as SiddhiProcess) as following 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 apiVersion : siddhi.io/v1alpha2 kind : SiddhiProcess metadata : name : api-throttler-app spec : apps : - script : | @App:name( API-Request-Throttler ) @App:description( Enforcesthrottling to API requests ) -- HTTP endpoint which listens for api request related events @source(type = http , receiver.url = http://0.0.0.0:8006/apiRequest , basic.auth.enabled = false , @map(type = json )) define stream APIRequestStream (apiName string, version string, tier string, user string, userEmail string); -- HTTP sink to publich throttle decisions. For testing purpose, there is a mock logger service provided @sink(type = http , publisher.url = http://${LOGGER_SERVICE_HOST}:8080/logger , method = POST , @map(type = json )) @sink(type = log , @map(type = text )) define stream ThrottleOutputStream (apiName string, version string, user string, tier string, userEmail string, isThrottled bool); -- Email sink to send alerts @sink(type = log , @map(type = text )) @sink(type = email , username = ${EMAIL_USERNAME} , address = ${SENDER_EMAIL_ADDRESS} , password = ${EMAIL_PASSWORD} , subject = Upgrade API Subscription Tier , to = {{userEmail}} , host = smtp.gmail.com , port = 465 , ssl.enable = true , auth = true , @map(type = text , @payload( Hi {{user}} You have subscribed to API called {{apiName}}:{{version}} with {{tier}} tier. Based on our records, it seems you are hitting the upper limit of the API requests in a frequent manner. We kindly request you to consider upgrading to next API subscription tier to avoid this in the future. Thanks, API Team ))) define stream UserNotificationStream (user string, apiName string, version string, tier string, userEmail string, throttledCount long); @info(name = Query to find users who needs to be throttled based on tier `silver` ) from APIRequestStream[tier == silver ]#window.timeBatch(1 min, 0, true) select apiName, version, user, tier, userEmail, count() as totalRequestCount group by apiName, version, user having totalRequestCount == 10 or totalRequestCount == 0 insert all events into ThrottledStream; @info(name = Query to find users who needs to be throttled based on tier `gold` ) from APIRequestStream[tier == gold ]#window.timeBatch(1 min, 0, true) select apiName, version, user, tier, userEmail, count() as totalRequestCount group by apiName, version, user having totalRequestCount == 100 or totalRequestCount == 0 insert all events into ThrottledStream; @info(name = Query to add a flag for throttled request ) from ThrottledStream select apiName, version, user, tier, userEmail, ifThenElse(totalRequestCount == 0, false, true) as isThrottled insert into ThrottleOutputStream; @info(name = Query to find frequently throttled users - who have throttled more than 10 times in the last hour ) from ThrottleOutputStream[isThrottled]#window.time(1 hour) select user, apiName, version, tier, userEmail, count() as throttledCount group by user, apiName, version, tier having throttledCount 10 output first every 15 min insert into UserNotificationStream; container : env : - name : EMAIL_PASSWORD value : siddhi123 - name : EMAIL_USERNAME value : siddhi.gke.user - name : SENDER_EMAIL_ADDRESS value : siddhi.gke.user@gmail.com - name : LOGGER_SERVICE_HOST value : 10.100.0.99 image : siddhiio/siddhi-runner-ubuntu:5.1.0-alpha NOTE: In the above provided Siddhi app, there are some environmental variables (EMAIL_PASSWORD, EMAIL_USERNAME, and SENDER_EMAIL_ADDRESS) which are required to be set to send email alerts based on the Siddhi queries defined. Again, there is a mock service configured to receive the throttle decisions (instructions given below), and its host is configured via LOGGER_SERVICE_HOST environment property. Hence, make sure to add proper values for the environmental variables in the above YAML file (check the env section of the YAML file). Now, let\u2019s create the above resource in the Kubernetes cluster with the following command. 1 kubectl create -f absolute-yaml-file-path /API-Request-Throttler.yaml Once, Siddhi app is successfully deployed. You can verify its health using the following commands Then, add the host siddhi and related external IP (ADDRESS) to the /etc/hosts file in your machine. For Docker for Mac , external IP is 0.0.0.0 . For Minikube, you have to use Minikube IP as the external IP. Hence, run minikube ip command to get the IP of the Minikube cluster. You can find the alert logs in the Siddhi runner log file. To see the Siddhi runner log file, first, invoke below command to get the pods. 1 kubectl get pods Then, find the pod name of the Siddhi app deployed, and invoke below command to view the logs. 1 kubectl logs siddhi-app-pod-name -f Eg: as shown below image, Invoke the apiRequest service with below cURL request for more than 10 times within a minute. Please make sure to change the userEmail property value to an email address that you could use to test the email alerting purposes. 1 curl -v -X POST -d { event : { apiName : order-mgt-v1 , version : 1.0.0 , tier : silver , user : mohan , userEmail : example@wso2.com }} http://siddhi/api-throttler-app-0/8006/apiRequest -H Content-Type:application/json Then, you will be able to see the throttle decisions as console logs (as given below). At the same time, you could also see the events received to HTTP mock service endpoint (started in step #2) via its log as below. If there are more than 10 requests get throttled within 1 hour then the API invoker will receive an email (as shown in the 'Testing' section). Refer here to get more details about running Siddhi on Kubernetes.","title":"Deploy on Kubernetes"},{"location":"docs/guides/fault-tolerance/guide/","text":"Data Preprocessing, Fault Tolerance, and Error Handling In this guide, we are going to understand some interesting topics around streaming data integration; they are data preprocessing, fault tolerance and error handling. To understand these capabilities, we are going to consider a health care use case. Scenario - Processing Health Care Events (Glucose readings from sensors) In this scenario, Glucose reading events are received from sensors that mounted on patients. These events are received to the Stream Processing engine, get preprocessed, unrelated attributes are removed and send them to another processing layer to process if there are any abnormal behavior observed. The following sections are available in this guide. What you'll build Prerequisites Implementation Testing Deployment Output What you'll build By following this guide, you will understand the capabilities of Siddhi streaming engine related to data preprocessing, fault tolerance and error handling. To understand better, let\u2019s consider a real-world use case in the health care sector. Let\u2019s jump into the use case directly. Let\u2019s consider a hospital (or healthcare institution) which primarily provide treatment to diabetic patients. Then, as you are aware it is important to keep close monitoring about the Glucose level of the patients and act accordingly. Then, there is a sensor mounted to each patient to track the Glucose level of the patients. These Glucose reading events are pushed to the central data collection hub, then it pushes those events to Kafka message broker to allow respective interested parties(systems) to consume those events for further processing. These events are published as Avro type messages to Kafka then the Siddhi Stream processing engine consume those events from Kafka message broker, perform some preprocessing, identify abnormal events and push them to an HTTP endpoint. In this complete flow, reliable data processing is a mandatory requirement and cannot lose any events due to failures then there should be proper error handling and fault tolerance features are activated and in place to avoid it. Now, let\u2019s understand how this could be implemented in Siddhi engine. Prerequisites Below are the prerequisites that should be considered to implement the above use case. Mandatory Requirements Siddhi tooling VM/Local distribution One of the Siddhi runner distributions VM/Local Runtime Docker Image K8S Operator (commands are given in deployment section) Kafka Distribution MySQL Database Java 8 or higher Requirements needed to deploy Siddhi in Docker/Kubernetes Docker Minikube or Google Kubernetes Engine(GKE) Cluster or Docker for Mac Implementation Events are consumed by Siddhi engine from Kafka message broker. These events are AVRO type. Siddhi performs preprocessing for received events and checks for abnormal events. If there are any abnormal Glucose reading found then it is forwarded to another processing layer through HTTP. To cater to reliable event messaging, necessary fault tolerance and error handling mechanisms are enabled in Siddhi Stream Processor. Implement Streaming Queries Start the Siddhi tooling runtime and go to the editor UI in http://localhost:9390/editor Follow below steps to start the Siddhi tooling runtime. * Extract the downloaded zip and navigate to /bin. (TOOLING_HOME refers to the extracted folder) * Issue the following command in the command prompt (Windows) / terminal (Linux/Mac) 1 2 For Windows: tooling.bat For Linux/Mac: ./tooling.sh Select File - New option, then you could either use the source view or design view to write/build the Siddhi Application. You can find the Siddhi Application bellow, that implements the requirements mentioned above. Let\u2019s write (develop) the Siddhi Application, as given below. Once the Siddhi app is created, you can use the Event Simulator option in the editor to simulate events to streams and perform developer testing. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 @ App : name ( Glucose-Reading-PreProcessing-App ) @ App : description ( Process Glucose Readings received from patients ) -- Kakfka source which consumes Glucose reading events @ source ( type = kafka , topic . list = glucose-readings , partition . no . list = 0 , threading . option = single.thread , group . id = group , is . binary . message = true , bootstrap . servers = ${KAFKA_BOOTSTRAP_SERVER_URL} , @ map ( type = avro , schema . def = { type : record , name : glucose_reading , namespace : glucose , fields : [{ name : locationRoom , type : string }, { name : locationBed , type : string }, { name : timeStamp , type : string }, { name : sensorID , type : long }, { name : patientGroup , type : string }, { name : patientFirstName , type : string }, { name : patientLastName , type : string }, { name : sensorValue , type : double }, { name : unitOfMeasure , type : string }] } )) define stream GlucoseReadingStream ( locationRoom string , locationBed string , timeStamp string , sensorID long , patientGroup string , patientFirstName string , patientLastName string , sensorValue double , unitOfMeasure string ); -- HTTP sink which publishes abnormal Glucose reading related events. -- If there is any errors occurred when publishing events to HTTP endpoint then respective events are sent to error stream @ OnError ( action = STREAM ) @ sink ( type = http , blocking . io = true , publisher . url = http://localhost:8080/logger , method = POST , @ map ( type = json )) define stream AbnormalGlucoseReadingStream ( timeStampInLong long , locationRoom string , locationBed string , sensorID long , patientGroup string , patientFullName string , sensorReadingValue double , unitOfMeasure string , abnormalReadingCount long ); -- RDBMS event table which stores the failed events when publishing to HTTP endpoint @ Store ( type = rdbms , jdbc . url = ${MYSQL_DB_URL} , username = ${MYSQL_USERNAME} , password = ${MYSQL_PASSWORD} , jdbc . driver . name = com.mysql.jdbc.Driver ) @ PrimaryKey ( locationRoom , locationBed , sensorID , failedEventTime ) define table FailedAbnormalReadingTable ( failedEventTime string , originTime long , locationRoom string , locationBed string , sensorID long , patientFullName string , sensorReadingValue double , rootCause string ); -- Javascript function which converts the Glucose sensor reading to mg/dl define function sensorReadingInMGDL [ JavaScript ] return double { var sensorReading = data [ 0 ]; var metricUnit = data [ 1 ]; if ( metricUnit === MGDL ) { return metricUnit ; } return sensorReading * 18 ; } ; @ info ( name = Glucose-preprocessing ) from GlucoseReadingStream select math : parseLong ( timeStamp ) as timeStampInLong , locationRoom , locationBed , sensorID , patientGroup , str : concat ( patientFirstName , , patientLastName ) as patientFullName , sensorReadingInMGDL ( sensorValue , unitOfMeasure ) as sensorReadingValue , MGDL as unitOfMeasure insert into PreProcessedGlucoseReadingStream ; @ info ( name = Abnormal-Glucose-reading-identifier ) from PreProcessedGlucoseReadingStream [ sensorReadingValue 100 ] # window . time ( 15 min ) select timeStampInLong , locationRoom , locationBed , sensorID , patientGroup , patientFullName , sensorReadingValue , unitOfMeasure , count () as abnormalReadingCount group by locationRoom , locationBed , sensorID having abnormalReadingCount 3 output first every 15 min insert into AbnormalGlucoseReadingStream ; @ info ( name = Error-handler ) from ! AbnormalGlucoseReadingStream # log ( Error Occurred! ) select time : currentTimestamp () as failedEventTime , timeStampInLong as originTime , locationRoom , locationBed , sensorID , patientFullName , sensorReadingValue , convert ( _error , string ) as rootCause insert into FailedAbnormalReadingStream ; @ info ( name = Dump-to-event-store ) from FailedAbnormalReadingStream select * insert into FailedAbnormalReadingTable ; Source view of the Siddhi app. Below is the flow diagram of the above Siddhi App. Testing NOTE: In the provided Siddhi app, there are some environmental variables (MYSQL_DB_URL, MYSQL_USERNAME, MYSQL_PASSWORD, and KAFKA_BOOTSTRAP_SERVER_URL) are used. These values are required to be set to try out the scenario end to end. MYSQL related environmental variables are required to store the events which are failed to publish to the HTTP endpoint. Environmental variable KAFKA_BOOTSTRAP_SERVER_URL is the Kafka endpoint URL where Siddhi listens and consume events from. MYSQL_DB_URL: MySQL database jdbc url to persist failed events. (eg: 'jdbc:mysql://localhost:3306/HCD') MYSQL_USERNAME: Username of the user account to connect MySQL database. (eg: 'root') MYSQL_PASSWORD: Password of the user account to connect MySQL database. (eg: 'root') KAFKA_BOOTSTRAP_SERVER_URL: List of Kafka servers to which the Siddhi Kafka source must listen. (eg: 'localhost:9092') Setup Kafka As a prerequisite, you have to start the Kafka message broker. Please follow better steps. 1. Download the Kafka distribution 2. Unzip the above distribution and go to the \u2018bin\u2019 directory 3. Start the zookeeper by executing below command, 1 zookeeper-server-start.sh config/zookeeper.properties 4. Start the Kafka broker by executing below command, 1 kafka-server-start.sh config/server.properties Refer the Kafka documentation for more details, https://kafka.apache.org/quickstart Then, you have to add necessary client jars (from /libs directory) to Siddhi distribution as given below. * Copy below client libs to /bundles directory * scala-library-2.12.8.jar * zkclient-0.11.jar * zookeeper-3.4.14.jar Copy below client libs to jars directory kafka-clients-2.3.0.jar kafka_2.12-2.3.0.jar metrics-core-2.2.0.jar bundles directory to add OSGI bundles and jars directory to add non-OSGI jars. Setup MySQL Download and Install MySQL database as per the guidelines (https://www.mysql.com/downloads/) Log in to the MySQL server and create a database called \u201cHCD\u201d Download the MySQL client connector jar and add it to jars (if it is non OSGI) or bundles (if it is OSGI bundle) directory of Siddhi distribution Tryout There are multiple options available to test the developed Siddhi App. As mentioned in the previous step you could simply simulate some events directly into the stream and test your queries. But, if you are willing to test the end to end flow (from an input source to sink) then you can start the Siddhi app in the editor itself. In this guide, we are going to run the Siddhi App in the editor itself. Once the server is started, you will see below logs get printed in the editor console. In the above provided Siddhi App, last 15 minutes events are kept in memory for processing (because 15 minutes time window is defined in the query). As mentioned in the use case these events (Glucose reading of the patients) are very sensitive hence durable messaging is a requirement; losing a single event might cause a huge impact. In this situation, if the Siddhi server goes down while processing events then there is a possibility that events kept in the memory get lost. Then, to avoid this you should enable state persistence to Siddhi distribution. State persistence store in-memory state into the file system or database in a time interval. If the Siddhi runtime failed due to some reasons then Siddhi can recover to the state which is stored and continue processing. But, even though state persistence is enabled still there is a possibility that some intermediate events get lost due to runtime failures (or machine failures). For example, let\u2019s assume you have enabled snapshot persistence interval as 5 minutes (means, in-memory state get persisted to file system/DB each 5 minutes) then, if Siddhi runtime failed at 4 th minute after consuming events for last 4 minutes then you will be anyway losing last 4 minutes events even though you could recover to the previous state. To overcome this, we could leverage the \u201cOffset\u201d support provided by Kafka. Kafka is a publish-subscribe architecture that can handle multiple subscribers. To keep track of the consumers and how many events they have consumed Kafka uses something called \u201coffsets\u201d. An offset is a unique identifier which shows the sequential ID of a record within a Kafka topic partition. The offset is controlled by the subscriber. When the subscriber reads a record it advances its offset by one. In the snapshot persistence, Kafka offset value is also get persisted then when the specific snapshot is restored then Siddhi can request events from Kafka with the restored offset then Kafka can send those events of that specific offset. You can configure the snapshot state persistence with the below configuration. Save below configuration as YAML file (for example snapshot.yaml) 1 2 3 4 5 6 7 state.persistence : enabled : true intervalInMin : 1 revisionsToKeep : 2 persistenceStore : io.siddhi.distribution.core.persistence.FileSystemPersistenceStore config : location : siddhi-app-persistence Then start the Siddhi runtime with below command. 1 ./tooling.sh -Dconfig= File-Path /snapshot.yaml Invoking the Siddhi App Now, you can publish events to Kafka message broker using a Kafka publisher. In this case, you can use the sample Kafka publisher implementation given in here When you send more than 3 events (where sensorReading value is greater than 100) to Kafka within 15 minutes then it is identified as the abnormal situation and there will be an alert sent to HTTP endpoint as defined in the above provided Siddhi application. In this scenario, to demonstrate the error handling scenario, Siddhi app is configured to send the events to an unavailable endpoint. In this situation, since Siddhi cannot publish those events to HTTP endpoint then respective error handling flow gets activated as per the configuration (@onError (action=\u2019STREAM\u2019)) and respective failed events are pushed to the error stream as per the configuration (in this case, those failed events are pushed to the stream !AbnormalGlucoseReadingStream). Then, those events are dumped to a database for further reference for respective authorities. In this flow, Siddhi guarantees that there is no events get lost and ensure the durability of the system. You can see, as per the below figure you can find the respective failed events in the database table. Deployment Once you are done with the development, export the Siddhi app that you have developed with 'File' - 'Export File' option. You can deploy the Siddhi app using any of the methods listed below. Deploy on VM/ Bare Metal Prerequisites First, please make sure that necessary prerequisites are met as given the Testing section . Apache Kafka and MySQL are required to try out the use case. Then, as given in Setup Kafka and Setup MySQL section. Download the Kafka distribution and start it up and download the MySQL database and install it. Then create a database called \u201cHCD\u201d in the MySQL database. Siddhi Runtime Configuration Make sure to set the necessary environmental variables as given above. Note: In the above provided Siddhi app, there are some environmental variables (MYSQL_DB_URL, MYSQL_USERNAME, and MYSQL_PASSWORD) are used. These values are required to be set to try out the scenario end to end. MYSQL related environmental variables are required to store the events which are failed to publish to the HTTP endpoint. Environmental variable KAFKA_BOOTSTRAP_SERVER_URL is the Kafka endpoint URL where Siddhi listens and consume events from. Hence, make sure to set the environmental variables with the proper values in the system (make sure to follow necessary steps based on the underneath operating system). Download the latest Siddhi Runner distribution . Unzip the siddhi-runner-x.x.x.zip . You have to copy necessary Kafka and Mysql client jars to Siddhi runner distribution to connect with Kafka and MySQL database. Download the Kafka distribution and copy below Kafka client jars from /lib folder * Copy below client libs to /bundles directory * scala-library-2.12.8.jar * zkclient-0.11.jar * zookeeper-3.4.14.jar * Copy below client libs to jars directory * kafka-clients-2.3.0.jar * kafka_2.12-2.3.0.jar * metrics-core-2.2.0.jar Then, copy the MySQL client connector jar to jars (if it is non OSGI) or bundles (if it is OSGI bundle) directory of Siddhi runner. Please refer this . Start Siddhi app with the runner config by executing the following commands from the distribution directory. ``` Linux/Mac : ./bin/runner.sh -Dapps= -Dconfig= Windows : bin\\runner.bat -Dapps= -Dconfig= 1 2 Eg: If exported siddhi app in Siddhi home directory, ./bin/runner.sh -Dapps=Glucose-Reading-PreProcessing-App.siddhi -Dconfig=snapshot.yaml `` Note: snapshot.yaml` file contains the configuration to enable state snapshot persistence Once server is started, download the sample Kafka Avro event generator from here and execute below command. 1 java -jar kafka-avro-producer-1.0.0-jar-with-dependencies.jar Above event publishes send 4 Avro events to generate an abnormal behavior as defined in the Siddhi application. You can change the kafka endpoint and topic by passing them as java arguments. If not, sample client consider \u201clocalhost:9092\u201d as the kafka bootstrap server endpoint and \u201cglucose-readings\u201d as the topic. You can find the sample client source code in here In this situation, you can find a log gets printed in the Siddhi runner console/log and respective failed event is added to the database with the error cause. Deploy on Docker Prerequisites Apache Kafka and MySQL are the external dependencies for this use case. Hence, you could use the corresponding docker artifacts to test the requirement. First, you can create a docker network for the deployment as shown below 1 docker network create siddhi-tier --driver bridge Then, you can get the MySQL docker image from here and run it with below command. We are going to use mysql version 5.7.27. Start the MySQL docker images with below command, 1 docker run --name mysql-server --network siddhi-tier -e MYSQL_ROOT_PASSWORD=root e1e1680ac726 e1e1680ac726 is the MySQL docker image id in this case Login to the MySQL docker instance and create a database called \u201cHCD\u201d. Then, you can pull the Kafka docker image and deploy it. There is no any official Apache Kafka image is available in docker hub hence you can use the Kafka docker image provided by bitnami in here Launch the Zookeeper server instance with below provided command, 1 docker run -d --name zookeeper-server --network siddhi-tier -e ALLOW_ANONYMOUS_LOGIN=yes bitnami/zookeeper:latest Launch the Kafka server instance with below provide command, 1 docker run -d --name kafka-server --network siddhi-tier -e ALLOW_PLAINTEXT_LISTENER=yes -e KAFKA_CFG_ZOOKEEPER_CONNECT=zookeeper-server:2181 bitnami/kafka:latest Now, you have configured necessary prerequisites that required to run the use case. Siddhi Docker Configuration Since, there are some external client jars (Kafka MySQL) are required for the Siddhi runner. You have to create the docker image accordingly. Below is the sample Docker file created 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 FROM siddhiio/siddhi-runner-base-alpine:5.1.0-alpha MAINTAINER Siddhi IO Docker Maintainers siddhi-dev@googlegroups.com ARG HOST_BUNDLES_DIR=./files/bundles ARG HOST_JARS_DIR=./files/jars ARG JARS=${RUNTIME_SERVER_HOME}/jars ARG BUNDLES=${RUNTIME_SERVER_HOME}/bundles # copy bundles jars to the siddhi-runner distribution COPY --chown=siddhi_user:siddhi_io ${HOST_BUNDLES_DIR}/ ${BUNDLES} COPY --chown=siddhi_user:siddhi_io ${HOST_JARS_DIR}/ ${JARS} # expose ports EXPOSE 9090 9443 9712 9612 7711 7611 7070 7443 RUN bash ${RUNTIME_SERVER_HOME}/bin/install-jars.sh STOPSIGNAL SIGINT ENTRYPOINT [ /home/siddhi_user/siddhi-runner/bin/runner.sh , -- ] Here, you have to create two folders called bundles and jars to add necessary external client dependencies to the docker image. You can refer the official Siddhi documentation reference for this purpose. Once, Dockerfile is created you can create the docker image with below command. 1 docker build -t siddhi_for_kafka . Create a folder locally (eg: /home/siddhi-artifacts) and copy the Siddhi app in to it. Also, create a YAML file (snapshot.yaml) with below configuration to enable state snapshot persistence in the created folder. 1 2 3 4 5 6 7 state.persistence : enabled : true intervalInMin : 1 revisionsToKeep : 2 persistenceStore : io.siddhi.distribution.core.persistence.FileSystemPersistenceStore config : location : /siddhi-app-persistence Then, you can run the Siddhi docker image that you created with necessary external dependencies to work with Kafka and MySQL. 1 docker run --network siddhi-tier -it -v /Users/mohan/scratch/siddhi-artifacts:/artifacts -v /Users/mohan/scratch/local-mount:/siddhi-app-persistence -e MYSQL_DB_URL=jdbc:mysql://mysql-server:3306/HCD -e MYSQL_USERNAME=root -e MYSQL_PASSWORD=root -e KAFKA_BOOTSTRAP_SERVER_URL=kafka-server:9092 siddhi_for_kafka:latest -Dapps=/artifacts/Glucose-Reading-PreProcessing-App.siddhi -Dconfig=/artifacts/snapshot.yaml Note: In the above provided Siddhi app, there are some environmental variables (MYSQL_DB_URL, MYSQL_USERNAME and MYSQL_PASSWORD) are used. These values are required to be set to tryout the scenario end to end. MYSQL related environmental variables are required to store the events which are failed to publish to the HTTP endpoint. Environmental variable KAFKA_BOOTSTRAP_SERVER_URL is the Kafka endpoint url where Siddhi listens and consume events from. You can use the sample Kafka publisher client available in docker hub to simulate required events. Use the below command to use the sample docker Kafka publisher client. 1 docker run --network siddhi-tier -it mohanvive/kafka-event-publisher:latest Then, you could see below log gets printed in the Siddhi runner console and failed events are stored in the database table. Deploy on Kubernetes It is advisable to create a namespace in Kubernetes to follow below steps. 1 kubectl create ns siddhi-kafka-test There are some prerequisites that you should meet to tryout below SiddhiProcess. Such as configure MySQL database and Kafka messaging system in Kubernetes. First, configure the MySQL server within the above created namespace. You can use the official helm chart provided for MySQL. First, install the MySQL helm chart as shown below, 1 helm install --name mysql-db --namespace=siddhi-kafka-test --set mysqlRootPassword=root,mysqlDatabase=HCD stable/mysql Here, you can define the root password to connect to the MYSQL database and also define the database name. BTW, make sure to do helm init if it is not done yet. Then, you can set a port forwarding to the MySQL service which allows you to connect from the Host. 1 kubectl port-forward svc/mysql-db 13306:3306 --namespace=siddhi-kafka-test Then, you can login to the MySQL server from your host machine as shown below. Next, you can configure Kafka messaging system in the Kubernetes (in the above created namespace). You can use the Kafka Helm chart for this purpose- https://github.com/helm/charts/tree/master/incubator/kafka First, install Kafka helm chart as shown below. 1 helm install --name my-kafka incubator/kafka --namespace=siddhi-kafka-test Then, you could find required Kafka broker pods and Zookeeper pods are getting created. Then, you can install Siddhi Operator To install the Siddhi Kubernetes operator run the following commands. 1 2 kubectl apply -f https://github.com/siddhi-io/siddhi-operator/releases/download/v0.2.0-alpha/00-prereqs.yaml --namespace=siddhi-kafka-test kubectl apply -f https://github.com/siddhi-io/siddhi-operator/releases/download/v0.2.0-alpha/01-siddhi-operator.yaml --namespace=siddhi-kafka-test You can verify the installation by making sure the following deployments are running in your Kubernetes cluster. Siddhi applications can be deployed on Kubernetes using the Siddhi operator. To deploy the above created Siddhi app, we have to create custom resource object yaml file (with the kind as SiddhiProcess) as given below 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 ````yaml apiVersion: siddhi.io/v1alpha2 kind: SiddhiProcess metadata: name: glucose-reading-preprocessing-app spec: apps: - script: | @App:name( Glucose-Reading-PreProcessing-App ) @App:description( Process Glucose Readings received from patients ) -- Kakfka source which consumes Glucose reading events @source(type= kafka , topic.list= glucose-readings , partition.no.list= 0 , threading.option= single.thread , group.id= group , is.binary.message= true , bootstrap.servers= ${KAFKA_BOOTSTRAP_SERVER_URL} , @map(type= avro ,schema.def= { type : record , name : glucose_reading , namespace : glucose , fields : [{ name : locationRoom , type : string }, { name : locationBed , type : string }, { name : timeStamp , type : string }, { name : sensorID , type : long }, { name : patientGroup , type : string }, { name : patientFirstName , type : string }, { name : patientLastName , type : string }, { name : sensorValue , type : double }, { name : unitOfMeasure , type : string }] } )) define stream GlucoseReadingStream (locationRoom string, locationBed string, timeStamp string, sensorID long, patientGroup string, patientFirstName string, patientLastName string, sensorValue double, unitOfMeasure string); -- HTTP sink which publishes abnormal Glucose reading related events. -- If there is any errors occurred when publishing events to HTTP endpoint then respective events are sent to error stream @OnError(action= STREAM ) @sink(type = http , blocking.io= true , publisher.url = http://localhost:8080/logger , method = POST , @map(type = json )) define stream AbnormalGlucoseReadingStream (timeStampInLong long, locationRoom string, locationBed string, sensorID long, patientGroup string, patientFullName string, sensorReadingValue double, unitOfMeasure string, abnormalReadingCount long); -- RDBMS event table which stores the failed events when publishing to HTTP endpoint @Store(type= rdbms , jdbc.url= ${MYSQL_DB_URL} , username= ${MYSQL_USERNAME} , password= ${MYSQL_PASSWORD} , jdbc.driver.name= com.mysql.jdbc.Driver ) @PrimaryKey( locationRoom , locationBed , sensorID , failedEventTime ) define table FailedAbnormalReadingTable (failedEventTime string, originTime long, locationRoom string, locationBed string, sensorID long, patientFullName string, sensorReadingValue double, rootCause string); -- Javascript function which converts the Glucose sensor reading to mg/dl define function sensorReadingInMGDL[JavaScript] return double { var sensorReading = data[0]; var metricUnit = data[1]; if(metricUnit === MGDL ) { return metricUnit; } return sensorReading * 18; }; @info(name= Glucose-preprocessing ) from GlucoseReadingStream select math:parseLong(timeStamp) as timeStampInLong, locationRoom, locationBed, sensorID, patientGroup, str:concat(patientFirstName, , patientLastName) as patientFullName, sensorReadingInMGDL(sensorValue, unitOfMeasure) as sensorReadingValue, MGDL as unitOfMeasure insert into PreProcessedGlucoseReadingStream; @info(name= Abnormal-Glucose-reading-identifier ) from PreProcessedGlucoseReadingStream[sensorReadingValue 100]#window.time(15 min) select timeStampInLong, locationRoom, locationBed, sensorID, patientGroup, patientFullName, sensorReadingValue, unitOfMeasure, count() as abnormalReadingCount group by locationRoom, locationBed, sensorID having abnormalReadingCount 3 output first every 15 min insert into AbnormalGlucoseReadingStream; @info(name = Error-handler ) from !AbnormalGlucoseReadingStream#log( Error Occurred! ) select time:currentTimestamp() as failedEventTime, timeStampInLong as originTime, locationRoom, locationBed, sensorID, patientFullName, sensorReadingValue, convert(_error, string ) as rootCause insert into FailedAbnormalReadingStream; @info(name= Dump-to-event-store ) from FailedAbnormalReadingStream select * insert into FailedAbnormalReadingTable; persistentVolumeClaim: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi storageClassName: hostpath volumeMode: Filesystem runner: | state.persistence: enabled: true intervalInMin: 1 revisionsToKeep: 2 persistenceStore: io.siddhi.distribution.core.persistence.FileSystemPersistenceStore config: location: /siddhi-app-persistence container: env: - name: MYSQL_DB_URL value: jdbc:mysql://mysql-db:3306/HCD - name: MYSQL_USERNAME value: root - name: MYSQL_PASSWORD value: root - name: KAFKA_BOOTSTRAP_SERVER_URL value: my-kafka-headless:9092 image: mohanvive/siddhi_for_kafka:latest ```` Note: In the above provided Siddhi app, there are some environmental variables (MYSQL_DB_URL, MYSQL_USERNAME and MYSQL_PASSWORD) are used. These values are required to be set to tryout the scenario end to end. MYSQL related environmental variables are required to store the events which are failed to publish to the HTTP endpoint. Environmental variable KAFKA_BOOTSTRAP_SERVER_URL is the Kafka endpoint url where Siddhi listens and consume events from. Hence, make sure to add proper values for the environmental variables in the above yaml file (check the env section of the yaml file). Here, you can use the docker image that created in the Deploy on Docker section since you need a docker images with required extensions and client jars to test it in Kubernetes. Other than that, Siddhi runtime is configured to enable state snapshot persistence under the runner entry as shown above. Now, let\u2019s create the above resource in the Kubernetes cluster with below command. 1 kubectl --namespace=siddhi-kafka-test create -f absolute-yaml-file-path /glucose-reading-preprocessing-app.yaml 1 Once, siddhi app is successfully deployed. You can verify its health with below Kubernetes commands Now, you can send some events to Kafka messaging server and test the use case. If you are planning to push events to Kafka messaging server from your host machine then you have to enable few external listeners in Kafka. Please check the documentation provided for the Kafka helm chart. For testing purposes, you could use the test-client pod provided by us. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 * Create a pod with below definition. ````yaml apiVersion: v1 kind: Pod metadata: name: testclient namespace: siddhi-kafka-test spec: containers: - name: kafka image: mohanvive/test-client command: - sh - -c - exec tail -f /dev/null ```` * Create a YAML file called test-client.yaml, add above pod definition in it and run below command. ```` kubectl apply -f test-client.yaml ```` Then, go into the above created pod using below command. kubectl exec -it testclient sh --namespace=siddhi-kafka-test Download the sample Kafka client which could publish events related to above use case. 1 wget https://github.com/mohanvive/siddhi-sample-clients/releases/download/v1.0.0/kafka-avro-producer-1.0.0-jar-with-dependencies.jar Then execute below command to push events to Kafka messaging system. 1 java -jar kafka-avro-producer-1.0.0-jar-with-dependencies.jar my-kafka-headless:9092 Then, as defined in the SIddhi application abnormal events get logged since it tries to publish to an unavailable endpoint. !!! info \"Refer here to get more details about running Siddhi on Kubernetes.\"","title":"Fault Tolerance & Error Handling"},{"location":"docs/guides/fault-tolerance/guide/#data-preprocessing-fault-tolerance-and-error-handling","text":"In this guide, we are going to understand some interesting topics around streaming data integration; they are data preprocessing, fault tolerance and error handling. To understand these capabilities, we are going to consider a health care use case.","title":"Data Preprocessing, Fault Tolerance, and Error Handling"},{"location":"docs/guides/fault-tolerance/guide/#scenario-processing-health-care-events-glucose-readings-from-sensors","text":"In this scenario, Glucose reading events are received from sensors that mounted on patients. These events are received to the Stream Processing engine, get preprocessed, unrelated attributes are removed and send them to another processing layer to process if there are any abnormal behavior observed. The following sections are available in this guide. What you'll build Prerequisites Implementation Testing Deployment Output","title":"Scenario - Processing Health Care Events (Glucose readings from sensors)"},{"location":"docs/guides/fault-tolerance/guide/#what-youll-build","text":"By following this guide, you will understand the capabilities of Siddhi streaming engine related to data preprocessing, fault tolerance and error handling. To understand better, let\u2019s consider a real-world use case in the health care sector. Let\u2019s jump into the use case directly. Let\u2019s consider a hospital (or healthcare institution) which primarily provide treatment to diabetic patients. Then, as you are aware it is important to keep close monitoring about the Glucose level of the patients and act accordingly. Then, there is a sensor mounted to each patient to track the Glucose level of the patients. These Glucose reading events are pushed to the central data collection hub, then it pushes those events to Kafka message broker to allow respective interested parties(systems) to consume those events for further processing. These events are published as Avro type messages to Kafka then the Siddhi Stream processing engine consume those events from Kafka message broker, perform some preprocessing, identify abnormal events and push them to an HTTP endpoint. In this complete flow, reliable data processing is a mandatory requirement and cannot lose any events due to failures then there should be proper error handling and fault tolerance features are activated and in place to avoid it. Now, let\u2019s understand how this could be implemented in Siddhi engine.","title":"What you'll build"},{"location":"docs/guides/fault-tolerance/guide/#prerequisites","text":"Below are the prerequisites that should be considered to implement the above use case.","title":"Prerequisites"},{"location":"docs/guides/fault-tolerance/guide/#mandatory-requirements","text":"Siddhi tooling VM/Local distribution One of the Siddhi runner distributions VM/Local Runtime Docker Image K8S Operator (commands are given in deployment section) Kafka Distribution MySQL Database Java 8 or higher","title":"Mandatory Requirements"},{"location":"docs/guides/fault-tolerance/guide/#requirements-needed-to-deploy-siddhi-in-dockerkubernetes","text":"Docker Minikube or Google Kubernetes Engine(GKE) Cluster or Docker for Mac","title":"Requirements needed to deploy Siddhi in Docker/Kubernetes"},{"location":"docs/guides/fault-tolerance/guide/#implementation","text":"Events are consumed by Siddhi engine from Kafka message broker. These events are AVRO type. Siddhi performs preprocessing for received events and checks for abnormal events. If there are any abnormal Glucose reading found then it is forwarded to another processing layer through HTTP. To cater to reliable event messaging, necessary fault tolerance and error handling mechanisms are enabled in Siddhi Stream Processor.","title":"Implementation"},{"location":"docs/guides/fault-tolerance/guide/#implement-streaming-queries","text":"Start the Siddhi tooling runtime and go to the editor UI in http://localhost:9390/editor Follow below steps to start the Siddhi tooling runtime. * Extract the downloaded zip and navigate to /bin. (TOOLING_HOME refers to the extracted folder) * Issue the following command in the command prompt (Windows) / terminal (Linux/Mac) 1 2 For Windows: tooling.bat For Linux/Mac: ./tooling.sh Select File - New option, then you could either use the source view or design view to write/build the Siddhi Application. You can find the Siddhi Application bellow, that implements the requirements mentioned above. Let\u2019s write (develop) the Siddhi Application, as given below. Once the Siddhi app is created, you can use the Event Simulator option in the editor to simulate events to streams and perform developer testing. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 @ App : name ( Glucose-Reading-PreProcessing-App ) @ App : description ( Process Glucose Readings received from patients ) -- Kakfka source which consumes Glucose reading events @ source ( type = kafka , topic . list = glucose-readings , partition . no . list = 0 , threading . option = single.thread , group . id = group , is . binary . message = true , bootstrap . servers = ${KAFKA_BOOTSTRAP_SERVER_URL} , @ map ( type = avro , schema . def = { type : record , name : glucose_reading , namespace : glucose , fields : [{ name : locationRoom , type : string }, { name : locationBed , type : string }, { name : timeStamp , type : string }, { name : sensorID , type : long }, { name : patientGroup , type : string }, { name : patientFirstName , type : string }, { name : patientLastName , type : string }, { name : sensorValue , type : double }, { name : unitOfMeasure , type : string }] } )) define stream GlucoseReadingStream ( locationRoom string , locationBed string , timeStamp string , sensorID long , patientGroup string , patientFirstName string , patientLastName string , sensorValue double , unitOfMeasure string ); -- HTTP sink which publishes abnormal Glucose reading related events. -- If there is any errors occurred when publishing events to HTTP endpoint then respective events are sent to error stream @ OnError ( action = STREAM ) @ sink ( type = http , blocking . io = true , publisher . url = http://localhost:8080/logger , method = POST , @ map ( type = json )) define stream AbnormalGlucoseReadingStream ( timeStampInLong long , locationRoom string , locationBed string , sensorID long , patientGroup string , patientFullName string , sensorReadingValue double , unitOfMeasure string , abnormalReadingCount long ); -- RDBMS event table which stores the failed events when publishing to HTTP endpoint @ Store ( type = rdbms , jdbc . url = ${MYSQL_DB_URL} , username = ${MYSQL_USERNAME} , password = ${MYSQL_PASSWORD} , jdbc . driver . name = com.mysql.jdbc.Driver ) @ PrimaryKey ( locationRoom , locationBed , sensorID , failedEventTime ) define table FailedAbnormalReadingTable ( failedEventTime string , originTime long , locationRoom string , locationBed string , sensorID long , patientFullName string , sensorReadingValue double , rootCause string ); -- Javascript function which converts the Glucose sensor reading to mg/dl define function sensorReadingInMGDL [ JavaScript ] return double { var sensorReading = data [ 0 ]; var metricUnit = data [ 1 ]; if ( metricUnit === MGDL ) { return metricUnit ; } return sensorReading * 18 ; } ; @ info ( name = Glucose-preprocessing ) from GlucoseReadingStream select math : parseLong ( timeStamp ) as timeStampInLong , locationRoom , locationBed , sensorID , patientGroup , str : concat ( patientFirstName , , patientLastName ) as patientFullName , sensorReadingInMGDL ( sensorValue , unitOfMeasure ) as sensorReadingValue , MGDL as unitOfMeasure insert into PreProcessedGlucoseReadingStream ; @ info ( name = Abnormal-Glucose-reading-identifier ) from PreProcessedGlucoseReadingStream [ sensorReadingValue 100 ] # window . time ( 15 min ) select timeStampInLong , locationRoom , locationBed , sensorID , patientGroup , patientFullName , sensorReadingValue , unitOfMeasure , count () as abnormalReadingCount group by locationRoom , locationBed , sensorID having abnormalReadingCount 3 output first every 15 min insert into AbnormalGlucoseReadingStream ; @ info ( name = Error-handler ) from ! AbnormalGlucoseReadingStream # log ( Error Occurred! ) select time : currentTimestamp () as failedEventTime , timeStampInLong as originTime , locationRoom , locationBed , sensorID , patientFullName , sensorReadingValue , convert ( _error , string ) as rootCause insert into FailedAbnormalReadingStream ; @ info ( name = Dump-to-event-store ) from FailedAbnormalReadingStream select * insert into FailedAbnormalReadingTable ; Source view of the Siddhi app. Below is the flow diagram of the above Siddhi App.","title":"Implement Streaming Queries"},{"location":"docs/guides/fault-tolerance/guide/#testing","text":"NOTE: In the provided Siddhi app, there are some environmental variables (MYSQL_DB_URL, MYSQL_USERNAME, MYSQL_PASSWORD, and KAFKA_BOOTSTRAP_SERVER_URL) are used. These values are required to be set to try out the scenario end to end. MYSQL related environmental variables are required to store the events which are failed to publish to the HTTP endpoint. Environmental variable KAFKA_BOOTSTRAP_SERVER_URL is the Kafka endpoint URL where Siddhi listens and consume events from. MYSQL_DB_URL: MySQL database jdbc url to persist failed events. (eg: 'jdbc:mysql://localhost:3306/HCD') MYSQL_USERNAME: Username of the user account to connect MySQL database. (eg: 'root') MYSQL_PASSWORD: Password of the user account to connect MySQL database. (eg: 'root') KAFKA_BOOTSTRAP_SERVER_URL: List of Kafka servers to which the Siddhi Kafka source must listen. (eg: 'localhost:9092')","title":"Testing"},{"location":"docs/guides/fault-tolerance/guide/#setup-kafka","text":"As a prerequisite, you have to start the Kafka message broker. Please follow better steps. 1. Download the Kafka distribution 2. Unzip the above distribution and go to the \u2018bin\u2019 directory 3. Start the zookeeper by executing below command, 1 zookeeper-server-start.sh config/zookeeper.properties 4. Start the Kafka broker by executing below command, 1 kafka-server-start.sh config/server.properties Refer the Kafka documentation for more details, https://kafka.apache.org/quickstart Then, you have to add necessary client jars (from /libs directory) to Siddhi distribution as given below. * Copy below client libs to /bundles directory * scala-library-2.12.8.jar * zkclient-0.11.jar * zookeeper-3.4.14.jar Copy below client libs to jars directory kafka-clients-2.3.0.jar kafka_2.12-2.3.0.jar metrics-core-2.2.0.jar bundles directory to add OSGI bundles and jars directory to add non-OSGI jars.","title":"Setup Kafka"},{"location":"docs/guides/fault-tolerance/guide/#setup-mysql","text":"Download and Install MySQL database as per the guidelines (https://www.mysql.com/downloads/) Log in to the MySQL server and create a database called \u201cHCD\u201d Download the MySQL client connector jar and add it to jars (if it is non OSGI) or bundles (if it is OSGI bundle) directory of Siddhi distribution","title":"Setup MySQL"},{"location":"docs/guides/fault-tolerance/guide/#tryout","text":"There are multiple options available to test the developed Siddhi App. As mentioned in the previous step you could simply simulate some events directly into the stream and test your queries. But, if you are willing to test the end to end flow (from an input source to sink) then you can start the Siddhi app in the editor itself. In this guide, we are going to run the Siddhi App in the editor itself. Once the server is started, you will see below logs get printed in the editor console. In the above provided Siddhi App, last 15 minutes events are kept in memory for processing (because 15 minutes time window is defined in the query). As mentioned in the use case these events (Glucose reading of the patients) are very sensitive hence durable messaging is a requirement; losing a single event might cause a huge impact. In this situation, if the Siddhi server goes down while processing events then there is a possibility that events kept in the memory get lost. Then, to avoid this you should enable state persistence to Siddhi distribution. State persistence store in-memory state into the file system or database in a time interval. If the Siddhi runtime failed due to some reasons then Siddhi can recover to the state which is stored and continue processing. But, even though state persistence is enabled still there is a possibility that some intermediate events get lost due to runtime failures (or machine failures). For example, let\u2019s assume you have enabled snapshot persistence interval as 5 minutes (means, in-memory state get persisted to file system/DB each 5 minutes) then, if Siddhi runtime failed at 4 th minute after consuming events for last 4 minutes then you will be anyway losing last 4 minutes events even though you could recover to the previous state. To overcome this, we could leverage the \u201cOffset\u201d support provided by Kafka. Kafka is a publish-subscribe architecture that can handle multiple subscribers. To keep track of the consumers and how many events they have consumed Kafka uses something called \u201coffsets\u201d. An offset is a unique identifier which shows the sequential ID of a record within a Kafka topic partition. The offset is controlled by the subscriber. When the subscriber reads a record it advances its offset by one. In the snapshot persistence, Kafka offset value is also get persisted then when the specific snapshot is restored then Siddhi can request events from Kafka with the restored offset then Kafka can send those events of that specific offset. You can configure the snapshot state persistence with the below configuration. Save below configuration as YAML file (for example snapshot.yaml) 1 2 3 4 5 6 7 state.persistence : enabled : true intervalInMin : 1 revisionsToKeep : 2 persistenceStore : io.siddhi.distribution.core.persistence.FileSystemPersistenceStore config : location : siddhi-app-persistence Then start the Siddhi runtime with below command. 1 ./tooling.sh -Dconfig= File-Path /snapshot.yaml","title":"Tryout"},{"location":"docs/guides/fault-tolerance/guide/#invoking-the-siddhi-app","text":"Now, you can publish events to Kafka message broker using a Kafka publisher. In this case, you can use the sample Kafka publisher implementation given in here When you send more than 3 events (where sensorReading value is greater than 100) to Kafka within 15 minutes then it is identified as the abnormal situation and there will be an alert sent to HTTP endpoint as defined in the above provided Siddhi application. In this scenario, to demonstrate the error handling scenario, Siddhi app is configured to send the events to an unavailable endpoint. In this situation, since Siddhi cannot publish those events to HTTP endpoint then respective error handling flow gets activated as per the configuration (@onError (action=\u2019STREAM\u2019)) and respective failed events are pushed to the error stream as per the configuration (in this case, those failed events are pushed to the stream !AbnormalGlucoseReadingStream). Then, those events are dumped to a database for further reference for respective authorities. In this flow, Siddhi guarantees that there is no events get lost and ensure the durability of the system. You can see, as per the below figure you can find the respective failed events in the database table.","title":"Invoking the Siddhi App"},{"location":"docs/guides/fault-tolerance/guide/#deployment","text":"Once you are done with the development, export the Siddhi app that you have developed with 'File' - 'Export File' option. You can deploy the Siddhi app using any of the methods listed below.","title":"Deployment"},{"location":"docs/guides/fault-tolerance/guide/#deploy-on-vm-bare-metal","text":"","title":"Deploy on VM/ Bare Metal"},{"location":"docs/guides/fault-tolerance/guide/#prerequisites_1","text":"First, please make sure that necessary prerequisites are met as given the Testing section . Apache Kafka and MySQL are required to try out the use case. Then, as given in Setup Kafka and Setup MySQL section. Download the Kafka distribution and start it up and download the MySQL database and install it. Then create a database called \u201cHCD\u201d in the MySQL database.","title":"Prerequisites"},{"location":"docs/guides/fault-tolerance/guide/#siddhi-runtime-configuration","text":"Make sure to set the necessary environmental variables as given above. Note: In the above provided Siddhi app, there are some environmental variables (MYSQL_DB_URL, MYSQL_USERNAME, and MYSQL_PASSWORD) are used. These values are required to be set to try out the scenario end to end. MYSQL related environmental variables are required to store the events which are failed to publish to the HTTP endpoint. Environmental variable KAFKA_BOOTSTRAP_SERVER_URL is the Kafka endpoint URL where Siddhi listens and consume events from. Hence, make sure to set the environmental variables with the proper values in the system (make sure to follow necessary steps based on the underneath operating system). Download the latest Siddhi Runner distribution . Unzip the siddhi-runner-x.x.x.zip . You have to copy necessary Kafka and Mysql client jars to Siddhi runner distribution to connect with Kafka and MySQL database. Download the Kafka distribution and copy below Kafka client jars from /lib folder * Copy below client libs to /bundles directory * scala-library-2.12.8.jar * zkclient-0.11.jar * zookeeper-3.4.14.jar * Copy below client libs to jars directory * kafka-clients-2.3.0.jar * kafka_2.12-2.3.0.jar * metrics-core-2.2.0.jar Then, copy the MySQL client connector jar to jars (if it is non OSGI) or bundles (if it is OSGI bundle) directory of Siddhi runner. Please refer this . Start Siddhi app with the runner config by executing the following commands from the distribution directory. ``` Linux/Mac : ./bin/runner.sh -Dapps= -Dconfig= Windows : bin\\runner.bat -Dapps= -Dconfig= 1 2 Eg: If exported siddhi app in Siddhi home directory, ./bin/runner.sh -Dapps=Glucose-Reading-PreProcessing-App.siddhi -Dconfig=snapshot.yaml `` Note: snapshot.yaml` file contains the configuration to enable state snapshot persistence Once server is started, download the sample Kafka Avro event generator from here and execute below command. 1 java -jar kafka-avro-producer-1.0.0-jar-with-dependencies.jar Above event publishes send 4 Avro events to generate an abnormal behavior as defined in the Siddhi application. You can change the kafka endpoint and topic by passing them as java arguments. If not, sample client consider \u201clocalhost:9092\u201d as the kafka bootstrap server endpoint and \u201cglucose-readings\u201d as the topic. You can find the sample client source code in here In this situation, you can find a log gets printed in the Siddhi runner console/log and respective failed event is added to the database with the error cause.","title":"Siddhi Runtime Configuration"},{"location":"docs/guides/fault-tolerance/guide/#deploy-on-docker","text":"","title":"Deploy on Docker"},{"location":"docs/guides/fault-tolerance/guide/#prerequisites_2","text":"Apache Kafka and MySQL are the external dependencies for this use case. Hence, you could use the corresponding docker artifacts to test the requirement. First, you can create a docker network for the deployment as shown below 1 docker network create siddhi-tier --driver bridge Then, you can get the MySQL docker image from here and run it with below command. We are going to use mysql version 5.7.27. Start the MySQL docker images with below command, 1 docker run --name mysql-server --network siddhi-tier -e MYSQL_ROOT_PASSWORD=root e1e1680ac726 e1e1680ac726 is the MySQL docker image id in this case Login to the MySQL docker instance and create a database called \u201cHCD\u201d. Then, you can pull the Kafka docker image and deploy it. There is no any official Apache Kafka image is available in docker hub hence you can use the Kafka docker image provided by bitnami in here Launch the Zookeeper server instance with below provided command, 1 docker run -d --name zookeeper-server --network siddhi-tier -e ALLOW_ANONYMOUS_LOGIN=yes bitnami/zookeeper:latest Launch the Kafka server instance with below provide command, 1 docker run -d --name kafka-server --network siddhi-tier -e ALLOW_PLAINTEXT_LISTENER=yes -e KAFKA_CFG_ZOOKEEPER_CONNECT=zookeeper-server:2181 bitnami/kafka:latest Now, you have configured necessary prerequisites that required to run the use case.","title":"Prerequisites"},{"location":"docs/guides/fault-tolerance/guide/#siddhi-docker-configuration","text":"Since, there are some external client jars (Kafka MySQL) are required for the Siddhi runner. You have to create the docker image accordingly. Below is the sample Docker file created 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 FROM siddhiio/siddhi-runner-base-alpine:5.1.0-alpha MAINTAINER Siddhi IO Docker Maintainers siddhi-dev@googlegroups.com ARG HOST_BUNDLES_DIR=./files/bundles ARG HOST_JARS_DIR=./files/jars ARG JARS=${RUNTIME_SERVER_HOME}/jars ARG BUNDLES=${RUNTIME_SERVER_HOME}/bundles # copy bundles jars to the siddhi-runner distribution COPY --chown=siddhi_user:siddhi_io ${HOST_BUNDLES_DIR}/ ${BUNDLES} COPY --chown=siddhi_user:siddhi_io ${HOST_JARS_DIR}/ ${JARS} # expose ports EXPOSE 9090 9443 9712 9612 7711 7611 7070 7443 RUN bash ${RUNTIME_SERVER_HOME}/bin/install-jars.sh STOPSIGNAL SIGINT ENTRYPOINT [ /home/siddhi_user/siddhi-runner/bin/runner.sh , -- ] Here, you have to create two folders called bundles and jars to add necessary external client dependencies to the docker image. You can refer the official Siddhi documentation reference for this purpose. Once, Dockerfile is created you can create the docker image with below command. 1 docker build -t siddhi_for_kafka . Create a folder locally (eg: /home/siddhi-artifacts) and copy the Siddhi app in to it. Also, create a YAML file (snapshot.yaml) with below configuration to enable state snapshot persistence in the created folder. 1 2 3 4 5 6 7 state.persistence : enabled : true intervalInMin : 1 revisionsToKeep : 2 persistenceStore : io.siddhi.distribution.core.persistence.FileSystemPersistenceStore config : location : /siddhi-app-persistence Then, you can run the Siddhi docker image that you created with necessary external dependencies to work with Kafka and MySQL. 1 docker run --network siddhi-tier -it -v /Users/mohan/scratch/siddhi-artifacts:/artifacts -v /Users/mohan/scratch/local-mount:/siddhi-app-persistence -e MYSQL_DB_URL=jdbc:mysql://mysql-server:3306/HCD -e MYSQL_USERNAME=root -e MYSQL_PASSWORD=root -e KAFKA_BOOTSTRAP_SERVER_URL=kafka-server:9092 siddhi_for_kafka:latest -Dapps=/artifacts/Glucose-Reading-PreProcessing-App.siddhi -Dconfig=/artifacts/snapshot.yaml Note: In the above provided Siddhi app, there are some environmental variables (MYSQL_DB_URL, MYSQL_USERNAME and MYSQL_PASSWORD) are used. These values are required to be set to tryout the scenario end to end. MYSQL related environmental variables are required to store the events which are failed to publish to the HTTP endpoint. Environmental variable KAFKA_BOOTSTRAP_SERVER_URL is the Kafka endpoint url where Siddhi listens and consume events from. You can use the sample Kafka publisher client available in docker hub to simulate required events. Use the below command to use the sample docker Kafka publisher client. 1 docker run --network siddhi-tier -it mohanvive/kafka-event-publisher:latest Then, you could see below log gets printed in the Siddhi runner console and failed events are stored in the database table.","title":"Siddhi Docker Configuration"},{"location":"docs/guides/fault-tolerance/guide/#deploy-on-kubernetes","text":"It is advisable to create a namespace in Kubernetes to follow below steps. 1 kubectl create ns siddhi-kafka-test There are some prerequisites that you should meet to tryout below SiddhiProcess. Such as configure MySQL database and Kafka messaging system in Kubernetes. First, configure the MySQL server within the above created namespace. You can use the official helm chart provided for MySQL. First, install the MySQL helm chart as shown below, 1 helm install --name mysql-db --namespace=siddhi-kafka-test --set mysqlRootPassword=root,mysqlDatabase=HCD stable/mysql Here, you can define the root password to connect to the MYSQL database and also define the database name. BTW, make sure to do helm init if it is not done yet. Then, you can set a port forwarding to the MySQL service which allows you to connect from the Host. 1 kubectl port-forward svc/mysql-db 13306:3306 --namespace=siddhi-kafka-test Then, you can login to the MySQL server from your host machine as shown below. Next, you can configure Kafka messaging system in the Kubernetes (in the above created namespace). You can use the Kafka Helm chart for this purpose- https://github.com/helm/charts/tree/master/incubator/kafka First, install Kafka helm chart as shown below. 1 helm install --name my-kafka incubator/kafka --namespace=siddhi-kafka-test Then, you could find required Kafka broker pods and Zookeeper pods are getting created. Then, you can install Siddhi Operator To install the Siddhi Kubernetes operator run the following commands. 1 2 kubectl apply -f https://github.com/siddhi-io/siddhi-operator/releases/download/v0.2.0-alpha/00-prereqs.yaml --namespace=siddhi-kafka-test kubectl apply -f https://github.com/siddhi-io/siddhi-operator/releases/download/v0.2.0-alpha/01-siddhi-operator.yaml --namespace=siddhi-kafka-test You can verify the installation by making sure the following deployments are running in your Kubernetes cluster. Siddhi applications can be deployed on Kubernetes using the Siddhi operator. To deploy the above created Siddhi app, we have to create custom resource object yaml file (with the kind as SiddhiProcess) as given below 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 ````yaml apiVersion: siddhi.io/v1alpha2 kind: SiddhiProcess metadata: name: glucose-reading-preprocessing-app spec: apps: - script: | @App:name( Glucose-Reading-PreProcessing-App ) @App:description( Process Glucose Readings received from patients ) -- Kakfka source which consumes Glucose reading events @source(type= kafka , topic.list= glucose-readings , partition.no.list= 0 , threading.option= single.thread , group.id= group , is.binary.message= true , bootstrap.servers= ${KAFKA_BOOTSTRAP_SERVER_URL} , @map(type= avro ,schema.def= { type : record , name : glucose_reading , namespace : glucose , fields : [{ name : locationRoom , type : string }, { name : locationBed , type : string }, { name : timeStamp , type : string }, { name : sensorID , type : long }, { name : patientGroup , type : string }, { name : patientFirstName , type : string }, { name : patientLastName , type : string }, { name : sensorValue , type : double }, { name : unitOfMeasure , type : string }] } )) define stream GlucoseReadingStream (locationRoom string, locationBed string, timeStamp string, sensorID long, patientGroup string, patientFirstName string, patientLastName string, sensorValue double, unitOfMeasure string); -- HTTP sink which publishes abnormal Glucose reading related events. -- If there is any errors occurred when publishing events to HTTP endpoint then respective events are sent to error stream @OnError(action= STREAM ) @sink(type = http , blocking.io= true , publisher.url = http://localhost:8080/logger , method = POST , @map(type = json )) define stream AbnormalGlucoseReadingStream (timeStampInLong long, locationRoom string, locationBed string, sensorID long, patientGroup string, patientFullName string, sensorReadingValue double, unitOfMeasure string, abnormalReadingCount long); -- RDBMS event table which stores the failed events when publishing to HTTP endpoint @Store(type= rdbms , jdbc.url= ${MYSQL_DB_URL} , username= ${MYSQL_USERNAME} , password= ${MYSQL_PASSWORD} , jdbc.driver.name= com.mysql.jdbc.Driver ) @PrimaryKey( locationRoom , locationBed , sensorID , failedEventTime ) define table FailedAbnormalReadingTable (failedEventTime string, originTime long, locationRoom string, locationBed string, sensorID long, patientFullName string, sensorReadingValue double, rootCause string); -- Javascript function which converts the Glucose sensor reading to mg/dl define function sensorReadingInMGDL[JavaScript] return double { var sensorReading = data[0]; var metricUnit = data[1]; if(metricUnit === MGDL ) { return metricUnit; } return sensorReading * 18; }; @info(name= Glucose-preprocessing ) from GlucoseReadingStream select math:parseLong(timeStamp) as timeStampInLong, locationRoom, locationBed, sensorID, patientGroup, str:concat(patientFirstName, , patientLastName) as patientFullName, sensorReadingInMGDL(sensorValue, unitOfMeasure) as sensorReadingValue, MGDL as unitOfMeasure insert into PreProcessedGlucoseReadingStream; @info(name= Abnormal-Glucose-reading-identifier ) from PreProcessedGlucoseReadingStream[sensorReadingValue 100]#window.time(15 min) select timeStampInLong, locationRoom, locationBed, sensorID, patientGroup, patientFullName, sensorReadingValue, unitOfMeasure, count() as abnormalReadingCount group by locationRoom, locationBed, sensorID having abnormalReadingCount 3 output first every 15 min insert into AbnormalGlucoseReadingStream; @info(name = Error-handler ) from !AbnormalGlucoseReadingStream#log( Error Occurred! ) select time:currentTimestamp() as failedEventTime, timeStampInLong as originTime, locationRoom, locationBed, sensorID, patientFullName, sensorReadingValue, convert(_error, string ) as rootCause insert into FailedAbnormalReadingStream; @info(name= Dump-to-event-store ) from FailedAbnormalReadingStream select * insert into FailedAbnormalReadingTable; persistentVolumeClaim: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi storageClassName: hostpath volumeMode: Filesystem runner: | state.persistence: enabled: true intervalInMin: 1 revisionsToKeep: 2 persistenceStore: io.siddhi.distribution.core.persistence.FileSystemPersistenceStore config: location: /siddhi-app-persistence container: env: - name: MYSQL_DB_URL value: jdbc:mysql://mysql-db:3306/HCD - name: MYSQL_USERNAME value: root - name: MYSQL_PASSWORD value: root - name: KAFKA_BOOTSTRAP_SERVER_URL value: my-kafka-headless:9092 image: mohanvive/siddhi_for_kafka:latest ```` Note: In the above provided Siddhi app, there are some environmental variables (MYSQL_DB_URL, MYSQL_USERNAME and MYSQL_PASSWORD) are used. These values are required to be set to tryout the scenario end to end. MYSQL related environmental variables are required to store the events which are failed to publish to the HTTP endpoint. Environmental variable KAFKA_BOOTSTRAP_SERVER_URL is the Kafka endpoint url where Siddhi listens and consume events from. Hence, make sure to add proper values for the environmental variables in the above yaml file (check the env section of the yaml file). Here, you can use the docker image that created in the Deploy on Docker section since you need a docker images with required extensions and client jars to test it in Kubernetes. Other than that, Siddhi runtime is configured to enable state snapshot persistence under the runner entry as shown above. Now, let\u2019s create the above resource in the Kubernetes cluster with below command. 1 kubectl --namespace=siddhi-kafka-test create -f absolute-yaml-file-path /glucose-reading-preprocessing-app.yaml 1 Once, siddhi app is successfully deployed. You can verify its health with below Kubernetes commands Now, you can send some events to Kafka messaging server and test the use case. If you are planning to push events to Kafka messaging server from your host machine then you have to enable few external listeners in Kafka. Please check the documentation provided for the Kafka helm chart. For testing purposes, you could use the test-client pod provided by us. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 * Create a pod with below definition. ````yaml apiVersion: v1 kind: Pod metadata: name: testclient namespace: siddhi-kafka-test spec: containers: - name: kafka image: mohanvive/test-client command: - sh - -c - exec tail -f /dev/null ```` * Create a YAML file called test-client.yaml, add above pod definition in it and run below command. ```` kubectl apply -f test-client.yaml ```` Then, go into the above created pod using below command. kubectl exec -it testclient sh --namespace=siddhi-kafka-test Download the sample Kafka client which could publish events related to above use case. 1 wget https://github.com/mohanvive/siddhi-sample-clients/releases/download/v1.0.0/kafka-avro-producer-1.0.0-jar-with-dependencies.jar Then execute below command to push events to Kafka messaging system. 1 java -jar kafka-avro-producer-1.0.0-jar-with-dependencies.jar my-kafka-headless:9092 Then, as defined in the SIddhi application abnormal events get logged since it tries to publish to an unavailable endpoint. !!! info \"Refer here to get more details about running Siddhi on Kubernetes.\"","title":"Deploy on Kubernetes"},{"location":"docs/guides/patterns-and-trends/guide/","text":"Analyze Event Occurrence Patterns and Trends Over Time In this guide, we are going to discuss a unique and appealing feature of a complex event processing system which is Patterns and Trends . Patterns and Trends are highly utilized in various business domains for the day to day business activities and growth. To understand these capabilities, we are going to consider a Taxi service use case. Scenario - Optimize Rider Requests in a Taxi Service Company Taxi service is one of the emerging businesses in metro cities. There are a lot of Taxi service companies such as UBER, LYFT, OLA, GRAB, etc.. are in the market. Due to the number of competitors in the market passengers have the freedom to select their preferred Taxi service based on cost, waiting time, etc.. As a passenger, the main requirement is to find a Taxi within a short time (less waiting time). Then, it is important to understand the rider requests and effectively use the available drivers/riders. In this, identifying the trend of passenger request will help to get more passengers and increase the business overall. The following sections are available in this guide. What you'll build Prerequisites Implementation Testing Deployment Output What you'll build You will be implementing a scenario to identify the increasing trend of rider requests over time and direct required riders to that specific geographical area to increase the chance of getting more rides. You will be using Siddhi streaming engine and related stream processing capabilities to achieve the requirement. Let\u2019s jump into the use case directly. Let\u2019s consider a Taxi service company called myTaxi . myTaxi is one of the startup Taxi service companies in the city and they have launched very recently. As per the analysis, they have found that they are a lot of ride cancellations happened over the last few months because the waiting time for the taxi is high. Even Though, myTaxi has enough riders they are not around the expected area where there is a sudden peak for rider requests. Then, they have decided to integrate a Stream Processing system to analyze the patterns and trends in real-time and act accordingly. In this solution, passengers use the myTaxi mobile application to book Taxi and those events are received to the myTaxi request processing system, it sends those events to Siddhi Stream Processor through TCP endpoint. Siddhi process those incoming events to identify predefined trends and patterns. Once a specific trend/patterns are identified then Siddhi sends those required trend/pattern specific attributes for further processing. In this case, that information is stored in a database. Now, let\u2019s understand how this could be implemented in Siddhi engine. Prerequisites Below are the prerequisites that should be considered to implement the above use case. Mandatory Requirements Siddhi tooling VM/Local distribution One of the Siddhi runner distributions VM/Local Runtime Docker Image K8S Operator (commands are given in deployment section) MySQL Database Java 8 or higher Requirements needed to deploy Siddhi in Docker/Kubernetes Docker Minikube or Google Kubernetes Engine(GKE) Cluster or Docker for Mac Implementation Implement Streaming Queries Start the Siddhi tooling runtime and go to the editor UI in http://localhost:9390/editor Follow below steps to start the Siddhi tooling runtime. * Extract the downloaded zip and navigate to /bin. (TOOLING_HOME refers to the extracted folder) * Issue the following command in the command prompt (Windows) / terminal (Linux/Mac) 1 2 For Windows: tooling.bat For Linux/Mac: ./tooling.sh Select File - New option, then you could either use the source view or design view to write/build the Siddhi Application. You can find the Siddhi Application bellow, that implements the requirements mentioned above. Let\u2019s write (develop) the Siddhi Application, as given below. Once the Siddhi app is created, you can use the Event Simulator option in the editor to simulate events to streams and perform developer testing. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 @ App : name ( Taxi-Rider-Requests-Processing-App ) @ App : description ( Siddhi application that processes Taxi Rider request events ) -- TCP source which accepts Taxi rider requests @ source ( type = tcp , context = taxiRideRequests , @ map ( type = binary )) define stream TaxiRideEventStream ( id long , time string , passengerId string , passengerName string , pickUpAddress string , pickUpZone string , dropOutAddress string , routingDetails string , expectedFare double , status string , passengerGrade string , additionalNote string ); -- For testing purposes, offer messages are logged in console. -- This could be further extended to send as sms to the premium users @ sink ( type = log , @ map ( type = text , @ payload ( Hi {{passengerName}} Unfortunately, you couldn t travel with us Today. We apologise for the high waiting time. As a token of apology please accept {{offerAmount}} USD off from your next ride. Truly, MyTaxi Team ))) define stream InstantOfferAlertStream ( passengerName string , pickUpZone string , offerAmount double ); -- RDBMS event table which stores events related to the requirement of need more riders @ store ( type = rdbms , jdbc . url = ${MYSQL_DB_URL} , username = ${MYSQL_USERNAME} , password = ${MYSQL_PASSWORD} , jdbc . driver . name = com.mysql.jdbc.Driver ) define table NeedMoreRidersTable ( systemTime string , zone string ); -- Email sink which send email alerts to the manager of MyTaxi @ sink ( type = email , username = ${EMAIL_USERNAME} , address = ${SENDER_EMAIL_ADDRESS} , password = ${EMAIL_PASSWORD} , subject = [Need Immediate Attention] High Waiting Time , to = ${MANAGER_EMAIL_ADDRESS} , host = smtp.gmail.com , port = 465 , ssl . enable = true , auth = true , @ map ( type = text , @ payload ( Hi, There is an increasing trend of ride cancellations in the {{zone}} area due to high waiting time. Increasing trend is detected at {{systemTime}}. Please take immediate action to sort this out. Thanks... ))) define stream AttentionRequiredCancellationStream ( systemTime string , zone string , lastNoOfCancellations long ); @ info ( name = Ride-cancellation-identifier ) from every e1 = TaxiRideEventStream [ status == Assigned ] - e2 = TaxiRideEventStream [ e1 . passengerId == passengerId and status == Cancelled and additionalNote == WT is High ] within 30 seconds select e2 . passengerName , e1 . passengerGrade , e1 . pickUpZone , e1 . expectedFare insert into TaxiRideCancelStream ; @ info ( name = Offer-for-premium-users ) from TaxiRideCancelStream [ passengerGrade == Premium ] select passengerName , pickUpZone , math : floor ( expectedFare ) / 2 . 0 as offerAmount insert into InstantOfferAlertStream ; @ info ( name = Frequently-ride-cancellation-identifier ) from TaxiRideCancelStream # window . timeBatch ( 1 min ) select pickUpZone , count () as totalCancellations , time : currentTimestamp () as systemTime group by pickUpZone having totalCancellations 3 insert into NeedMoreRidersStream ; @ info ( name = Dump-needMoreRider-events ) from NeedMoreRidersStream select systemTime , pickUpZone as zone insert into NeedMoreRidersTable ; @ info ( name = Ride-cancellation-increasing-trend-identifer ) partition with ( pickUpZone of NeedMoreRidersStream ) begin from e1 = NeedMoreRidersStream , e2 = NeedMoreRidersStream [ totalCancellations e1 . totalCancellations ], e3 = NeedMoreRidersStream [ totalCancellations e2 . totalCancellations ] select e3 . systemTime , e1 . pickUpZone as zone , e3 . totalCancellations as lastNoOfCancellations insert into AttentionRequiredCancellationStream ; end ; Source view of the Siddhi app. Below is the flow diagram of the above Siddhi App. Testing NOTE: In the above provided Siddhi app, there are some environmental variables (MYSQL_DB_URL, MYSQL_USERNAME, and MYSQL_PASSWORD) are used. These values are required to be set to try out the scenario end to end. MYSQL related environmental variables are required to store the events of stream NeedMoreRidersStream . Environmental variables EMAIL_PASSWORD, EMAIL_USERNAME, SENDER_EMAIL_ADDRESS and MANAGER_EMAIL_ADDRESS are used to send an email alert when there is an increasing trend of cancellation on specific area. MYSQL_DB_URL: MySQL database jdbc url to persist failed events. (eg: 'jdbc:mysql://localhost:3306/MyTaxi') MYSQL_USERNAME: Username of the user account to connect MySQL database. (eg: 'root') MYSQL_PASSWORD: Password of the user account to connect MySQL database. (eg: 'root') EMAIL_USERNAME: Username of the email account which used to send email alerts. (eg: 'siddhi.gke.user') EMAIL_PASSWORD: Password of the email account which used to send email alerts. (eg: 'siddhi123') SENDER_EMAIL_ADDRESS: Email address of the account used to send email alerts. (eg: 'siddhi.gke.user@gmail.com') MANAGER_EMAIL_ADDRESS: Destination Email address where escalation mails are sent. (eg: 'manager@mytaxi.com') Setup MySQL Download and Install MySQL database as per the guidelines (https://www.mysql.com/downloads/) Log in to the MySQL server and create a database called \u201cMyTaxi\u201d Download the MySQL client connector jar and add it to jars (if it is non OSGI) or bundles (if it is OSGI bundle) directory of Siddhi distribution Tryout There are multiple options available to test the developed Siddhi App. As mentioned in the previous step you could simply simulate some events directly into the stream and test your queries. But, if you are willing to test the end to end flow (from an input source to sink) then you can start the Siddhi app in the editor itself. In this guide, we are going to run the Siddhi App in the editor itself. Once the server is started, you will see below logs get printed in the editor console. As written in the above Siddhi application, taxi ride requests are accepted by the TCP endpoint of the Siddhi Stream Processor; those events are pushed to a stream called taxiRideEventStream . The first query is written to identify the pattern of rider cancellation after rider request within 30 seconds due to high waiting time. If such a pattern is identified then system will the user grade and grant some offers for subsequent rides for premium users. Parallelly, Stream Processor keep tracking the number of ride cancellations for each minute and if it found a situation of more than 3 ride cancellation then the system will identify that area/zone and send that details to next processing system to take necessary action. In the above query, such events are pushed to a database table. There is another query which continuously listens for the total number of cancellations for each minute and looking for increasing trend of ride cancellations and notifies accordingly. In this situation, if the Streaming system sends an email alert to the manager of the MyTaxi for his/her further consideration. Invoking the Siddhi App To try out the above use case, you have to send a set of events in a certain order to match with the query conditions. There is a sample TCP publisher could publish events in such an order. Hence, you could use the sample publisher given in here . Then you can execute below command to run the TCP client. 1 java -jar tcp-producer-1.0.0-jar-with-dependencies.jar It will take nearly 3 minutes to publish events which required to test all the flows in the given Siddhi app. When you are publishing events to Siddhi Stream processor, you could see the logs that get printed in Siddhi Stream processor side as well. There are related to instant offer alerts. Once, TCP publisher completes publishing events then you could check the email alert which is generated. Deployment Once you are done with the development, export the Siddhi app that you have developed with 'File' - 'Export File' option. You can deploy the Siddhi app using any of the methods listed below. Deploy on VM/ Bare Metal Prerequisites First, please make sure that necessary prerequisites are met as given the Testing section . MySQL is required to try out the use case. Then, as given in the Setup MySQL section. Download the MySQL database and install it. Then create a database called \u201cMyTaxi\u201d in the MySQL database. Siddhi Runtime Configuration Make sure to set the necessary environmental variables as given above. Note: In the above provided Siddhi app, there are some environmental variables (MYSQL_DB_URL, MYSQL_USERNAME, and MYSQL_PASSWORD) are used. These values are required to be set to try out the scenario end to end. MYSQL related environmental variables are required to store the events of stream NeedMoreRidersStream . Environmental variables EMAIL_PASSWORD, EMAIL_USERNAME, SENDER_EMAIL_ADDRESS and MANAGER_EMAIL_ADDRESS are used to send an email alert when there is an increasing trend of cancellation on specific area. Hence, make sure to set the environmental variables with the proper values in the system (make sure to follow necessary steps based on the underneath operating system). Download the latest Siddhi Runner distribution . Unzip the siddhi-runner-x.x.x.zip . You have to copy necessary Mysql client jar to Siddhi runner distribution to connect with MySQL database. Copy the MySQL client connector jar to jars (if it is non OSGI) or bundles (if it is OSGI bundle) directory of Siddhi runner. Please refer this for information. Start Siddhi app with the runner config by executing the following commands from the distribution directory. ``` Linux/Mac : ./bin/runner.sh -Dapps= Windows : bin\\runner.bat -Dapps= 1 2 Eg: If exported siddhi app in Siddhi home directory, ./bin/runner.sh -Dapps=Taxi-Rider-Requests-Processing-App.siddhi ``` Once server is started, download the sample TCP event generator from here and execute below command. 1 java -jar tcp-producer-1.0.0-jar-with-dependencies.jar tcp://localhost:9892/taxiRideRequests Above event publishes send binary events through TCP to the TCP endpoint defined in the Siddhi application. You can change the TCP endpoint url by passing them as java arguments. If not, sample client consider tcp://localhost:9892/taxiRideRequests as the TCP endpoint url. You can find the sample client source code in here In this situation, you can find the logs printed in the Siddhi runner console/log, events related to NeedMoreRidersStream are stored in the database table and escalation email is sent to the manager when there is an increasing trend found in the cancellations. Deploy on Docker Prerequisites MySQL is an external dependency for this use case. Hence, you could use the corresponding MySQL docker artifact to test the requirement. First, you can create a docker network for the deployment as shown below 1 docker network create siddhi-tier --driver bridge Then, you can get the MySQL docker image from here and run it with below command. We are going to use mysql version 5.7.27. Start the MySQL docker images with below command, 1 docker run --name mysql-server --network siddhi-tier -e MYSQL_ROOT_PASSWORD=root e1e1680ac726 e1e1680ac726 is the MySQL docker image id in this case Login to the MySQL docker instance and create a database called \u201cMyTaxi\u201d. Now, you have configured necessary prerequisites that required to run the use case. Siddhi Docker Configuration Since, MySQL client jar is required for the Siddhi runner; you have to create the docker image accordingly. Below is the sample Docker file created 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 FROM siddhiio/siddhi-runner-base-alpine:5.1.0-alpha MAINTAINER Siddhi IO Docker Maintainers siddhi-dev@googlegroups.com ARG HOST_BUNDLES_DIR=./files/bundles ARG HOST_JARS_DIR=./files/jars ARG JARS=${RUNTIME_SERVER_HOME}/jars ARG BUNDLES=${RUNTIME_SERVER_HOME}/bundles # copy bundles jars to the siddhi-runner distribution COPY --chown=siddhi_user:siddhi_io ${HOST_JARS_DIR}/ ${JARS} # expose ports EXPOSE 9090 9443 9712 9612 7711 7611 7070 7443 RUN bash ${RUNTIME_SERVER_HOME}/bin/install-jars.sh STOPSIGNAL SIGINT ENTRYPOINT [ /home/siddhi_user/siddhi-runner/bin/runner.sh , -- ] Here, you have to create a folder called jars to add necessary external client dependencies to the docker image. You can refer the official Siddhi documentation reference for this purpose. Once, Dockerfile is created you can create the docker image with below command. 1 docker build -t siddhi_mysql . Then, you can run the Siddhi docker image that you created with necessary external dependencies to work with MySQL. 1 docker run --network siddhi-tier -it -p 9892:9892 -v /Users/mohan/siddhi-apps/:/siddhi-apps -e MYSQL_DB_URL=jdbc:mysql://mysql-server:3306/MyTaxi -e MYSQL_USERNAME=root -e MYSQL_PASSWORD=root -e EMAIL_USERNAME=siddhi.gke.user -e EMAIL_PASSWORD=siddhi123 -e SENDER_EMAIL_ADDRESS=siddhi.gke.user@gmail.com -e MANAGER_EMAIL_ADDRESS=mohan@wso2.com siddhi_mysql:latest -Dapps=/siddhi-apps/Taxi-Rider-Requests-Processing-App.siddhi Note: In the above provided Siddhi app, there are some environmental variables (MYSQL_DB_URL, MYSQL_USERNAME, and MYSQL_PASSWORD) are used. These values are required to be set to try out the scenario end to end. MYSQL related environmental variables are required to store the events of stream NeedMoreRidersStream . Environmental variables EMAIL_PASSWORD, EMAIL_USERNAME, SENDER_EMAIL_ADDRESS and MANAGER_EMAIL_ADDRESS are used to send an email alert when there is an increasing trend of cancellation on specific area. You can use the sample TCP event publisher to simulate required events. Use the below command to publish events to TCP endpoint. 1 java -jar tcp-producer-1.0.0-jar-with-dependencies.jar tcp://localhost:9892/taxiRideRequests Then, you could see below logs get printed in the Siddhi runner console/log, events related to NeedMoreRidersStream are stored in the database table and escalation email is sent to the manager when there is an increasing trend found in the cancellations. Deploy on Kubernetes It is advisable to create a namespace in Kubernetes to follow below steps. 1 kubectl create ns siddhi-mysql-test There is a prerequisite that you should meet to tryout below SiddhiProcess; configuring MySQL database server within the above created namespace. You can use the official helm chart provided for MySQL. First, install the MySQL helm chart as shown below, 1 helm install --name mysql-db --namespace=siddhi-mysql-test --set mysqlRootPassword=root,mysqlDatabase=MyTaxi stable/mysql Here, you can define the root password to connect to the MYSQL database and also define the database name. BTW, make sure to do helm init if it is not done yet. Then, you can set a port forwarding to the MySQL service which allows you to connect from the Host. 1 kubectl port-forward svc/mysql-db 13306:3306 --namespace=siddhi-mysql-test Then, you can login to the MySQL server from your host machine as shown below. Then, you can install Siddhi Operator To install the Siddhi Kubernetes operator run the following commands. 1 2 kubectl apply -f https://github.com/siddhi-io/siddhi-operator/releases/download/v0.2.0-alpha/00-prereqs.yaml --namespace=siddhi-mysql-test kubectl apply -f https://github.com/siddhi-io/siddhi-operator/releases/download/v0.2.0-alpha/01-siddhi-operator.yaml --namespace=siddhi-mysql-test You can verify the installation by making sure the following deployments are running in your Kubernetes cluster. Siddhi applications can be deployed on Kubernetes using the Siddhi operator. To deploy the above created Siddhi app, we have to create custom resource object yaml file (with the kind as SiddhiProcess) as given below 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 ````yaml apiVersion: siddhi.io/v1alpha2 kind: SiddhiProcess metadata: name: taxi-rider-requests-processing-app spec: apps: - script: | @App:name( Taxi-Rider-Requests-Processing-App ) @App:description( Siddhi application that processes Taxi Rider request events ) -- TCP source which accepts Taxi rider requests @source(type= tcp , context= taxiRideRequests , @map(type= binary )) define stream TaxiRideEventStream(id long, time string, passengerId string, passengerName string, pickUpAddress string, pickUpZone string, dropOutAddress string, routingDetails string, expectedFare double, status string, passengerGrade string, additionalNote string); -- For testing purposes, offer messages are logged in console. -- This could be further extended to send as sms to the premium users @sink(type= log , @map(type = text , @payload( Hi {{passengerName}} Unfortunately, you couldn t travel with us Today. We apologise for the high waiting time. As a token of apology please accept {{offerAmount}} USD off from your next ride. Truly, MyTaxi Team ))) define stream InstantOfferAlertStream(passengerName string, pickUpZone string, offerAmount double); -- RDBMS event table which stores events related to the requirement of need more riders @store(type= rdbms , jdbc.url= ${MYSQL_DB_URL} , username= ${MYSQL_USERNAME} , password= ${MYSQL_PASSWORD} , jdbc.driver.name= com.mysql.jdbc.Driver ) define table NeedMoreRidersTable (systemTime string, zone string); -- Email sink which send email alerts to the manager of MyTaxi @sink(type = email , username = ${EMAIL_USERNAME} , address = ${SENDER_EMAIL_ADDRESS} , password = ${EMAIL_PASSWORD} , subject = [Need Immediate Attention] High Waiting Time , to = ${MANAGER_EMAIL_ADDRESS} , host = smtp.gmail.com , port = 465 , ssl.enable = true , auth = true , @map(type = text , @payload( Hi, There is an increasing trend of ride cancellations in the {{zone}} area due to high waiting time. Increasing trend is detected at {{systemTime}}. Please take immediate action to sort this out. Thanks... ))) define stream AttentionRequiredCancellationStream (systemTime string, zone string, lastNoOfCancellations long); @info(name= Ride-cancellation-identifier ) from every e1=TaxiRideEventStream[status == Assigned ] - e2=TaxiRideEventStream[e1.passengerId == passengerId and status == Cancelled and additionalNote == WT is High ] within 30 seconds select e2.passengerName, e1.passengerGrade, e1.pickUpZone, e1.expectedFare insert into TaxiRideCancelStream; @info(name= Offer-for-premium-users ) from TaxiRideCancelStream[passengerGrade == Premium ] select passengerName, pickUpZone, math:floor(expectedFare) / 2.0 as offerAmount insert into InstantOfferAlertStream; @info(name= Frequently-ride-cancellation-identifier ) from TaxiRideCancelStream#window.timeBatch(1 min) select pickUpZone, count() as totalCancellations, time:currentTimestamp() as systemTime group by pickUpZone having totalCancellations 3 insert into NeedMoreRidersStream; @info(name= Dump-needMoreRider-events ) from NeedMoreRidersStream select systemTime, pickUpZone as zone insert into NeedMoreRidersTable; @info(name= Ride-cancellation-increasing-trend-identifer ) partition with (pickUpZone of NeedMoreRidersStream) begin from e1 = NeedMoreRidersStream, e2 = NeedMoreRidersStream[totalCancellations e1.totalCancellations], e3 = NeedMoreRidersStream[totalCancellations e2.totalCancellations] select e3.systemTime, e1.pickUpZone as zone, e3.totalCancellations as lastNoOfCancellations insert into AttentionRequiredCancellationStream; end; persistentVolumeClaim: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi storageClassName: hostpath volumeMode: Filesystem container: env: - name: MYSQL_DB_URL value: jdbc:mysql://mysql-db:3306/MyTaxi - name: MYSQL_USERNAME value: root - name: MYSQL_PASSWORD value: root - name: EMAIL_PASSWORD value: siddhi123 - name: EMAIL_USERNAME value: siddhi.gke.user - name: SENDER_EMAIL_ADDRESS value: siddhi.gke.user@gmail.com - name: MANAGER_EMAIL_ADDRESS value: mohan@wso2.com image: mohanvive/siddhi_mysql:latest ```` Note: In the above provided Siddhi app, there are some environmental variables (MYSQL_DB_URL, MYSQL_USERNAME, and MYSQL_PASSWORD) are used. These values are required to be set to try out the scenario end to end. MYSQL related environmental variables are required to store the events of stream NeedMoreRidersStream . Environmental variables EMAIL_PASSWORD, EMAIL_USERNAME, SENDER_EMAIL_ADDRESS and MANAGER_EMAIL_ADDRESS are used to send an email alert when there is an increasing trend of cancellation on specific area. Hence, make sure to add proper values for the environmental variables in the above yaml file (check the \u2018env\u2019 section of the yaml file). Here, you can use the docker image that created in the section since you need a docker images with required extensions and client jars to test it in Kubernetes. Now, let\u2019s create the above resource in the Kubernetes cluster with below command. 1 kubectl --namespace=siddhi-mysql-test create -f absolute-yaml-file-path /taxi-rider-requests-processing-app.yaml 1 Once, siddhi app is successfully deployed. You can verify its health with below Kubernetes commands You can find the alert logs in the siddhi runner log file. To see the Siddhi runner log file, you can invoke below command. 1 kubectl get pods Then, find the pod name of the siddhi app deployed. Then invoke below command, 1 kubectl logs siddhi-app-pod-name -f Then, you can set a port forwarding to the Siddhi TCP endpoint which allows you to connect from the Host with below command. 1 2 3 4 5 ```` kubectl port-forward svc/taxi-rider-requests-processing-app-0 9892:9892 --namespace=siddhi-mysql-test ```` ![tco_port_forwarding](images/tcp-port-forwarding.png Kubernetes TCP Port Forwarding ) Then execute below command to send TCP events to Siddhi Stream Processor. 1 2 3 ```` java -jar tcp-producer-1.0.0-jar-with-dependencies.jar tcp://0.0.0.0:9892/taxiRideRequests ```` Then, you could see below logs get printed in the Siddhi runner console/log, events related to NeedMoreRidersStream are stored in the database table and escalation email is sent to the manager when there is an increasing trend found in the cancellations. 1 2 3 4 5 ![k8s_instant_offer_in_logs](images/k8s-instant-offer-in-logs.png Instant Offers to Premium Users ) ![k8s_need_more_riders_table](images/k8s-need-more-riders-table.png Need More Riders Data in Database ) ![k8s_escalation_email_alert](images/k8s-escalation-email-alert.png Escalation Mail to Manager )","title":"Patterns & Trends Over Time"},{"location":"docs/guides/patterns-and-trends/guide/#analyze-event-occurrence-patterns-and-trends-over-time","text":"In this guide, we are going to discuss a unique and appealing feature of a complex event processing system which is Patterns and Trends . Patterns and Trends are highly utilized in various business domains for the day to day business activities and growth. To understand these capabilities, we are going to consider a Taxi service use case.","title":"Analyze Event Occurrence Patterns and Trends Over Time"},{"location":"docs/guides/patterns-and-trends/guide/#scenario-optimize-rider-requests-in-a-taxi-service-company","text":"Taxi service is one of the emerging businesses in metro cities. There are a lot of Taxi service companies such as UBER, LYFT, OLA, GRAB, etc.. are in the market. Due to the number of competitors in the market passengers have the freedom to select their preferred Taxi service based on cost, waiting time, etc.. As a passenger, the main requirement is to find a Taxi within a short time (less waiting time). Then, it is important to understand the rider requests and effectively use the available drivers/riders. In this, identifying the trend of passenger request will help to get more passengers and increase the business overall. The following sections are available in this guide. What you'll build Prerequisites Implementation Testing Deployment Output","title":"Scenario - Optimize Rider Requests in a Taxi Service Company"},{"location":"docs/guides/patterns-and-trends/guide/#what-youll-build","text":"You will be implementing a scenario to identify the increasing trend of rider requests over time and direct required riders to that specific geographical area to increase the chance of getting more rides. You will be using Siddhi streaming engine and related stream processing capabilities to achieve the requirement. Let\u2019s jump into the use case directly. Let\u2019s consider a Taxi service company called myTaxi . myTaxi is one of the startup Taxi service companies in the city and they have launched very recently. As per the analysis, they have found that they are a lot of ride cancellations happened over the last few months because the waiting time for the taxi is high. Even Though, myTaxi has enough riders they are not around the expected area where there is a sudden peak for rider requests. Then, they have decided to integrate a Stream Processing system to analyze the patterns and trends in real-time and act accordingly. In this solution, passengers use the myTaxi mobile application to book Taxi and those events are received to the myTaxi request processing system, it sends those events to Siddhi Stream Processor through TCP endpoint. Siddhi process those incoming events to identify predefined trends and patterns. Once a specific trend/patterns are identified then Siddhi sends those required trend/pattern specific attributes for further processing. In this case, that information is stored in a database. Now, let\u2019s understand how this could be implemented in Siddhi engine.","title":"What you'll build"},{"location":"docs/guides/patterns-and-trends/guide/#prerequisites","text":"Below are the prerequisites that should be considered to implement the above use case.","title":"Prerequisites"},{"location":"docs/guides/patterns-and-trends/guide/#mandatory-requirements","text":"Siddhi tooling VM/Local distribution One of the Siddhi runner distributions VM/Local Runtime Docker Image K8S Operator (commands are given in deployment section) MySQL Database Java 8 or higher","title":"Mandatory Requirements"},{"location":"docs/guides/patterns-and-trends/guide/#requirements-needed-to-deploy-siddhi-in-dockerkubernetes","text":"Docker Minikube or Google Kubernetes Engine(GKE) Cluster or Docker for Mac","title":"Requirements needed to deploy Siddhi in Docker/Kubernetes"},{"location":"docs/guides/patterns-and-trends/guide/#implementation","text":"","title":"Implementation"},{"location":"docs/guides/patterns-and-trends/guide/#implement-streaming-queries","text":"Start the Siddhi tooling runtime and go to the editor UI in http://localhost:9390/editor Follow below steps to start the Siddhi tooling runtime. * Extract the downloaded zip and navigate to /bin. (TOOLING_HOME refers to the extracted folder) * Issue the following command in the command prompt (Windows) / terminal (Linux/Mac) 1 2 For Windows: tooling.bat For Linux/Mac: ./tooling.sh Select File - New option, then you could either use the source view or design view to write/build the Siddhi Application. You can find the Siddhi Application bellow, that implements the requirements mentioned above. Let\u2019s write (develop) the Siddhi Application, as given below. Once the Siddhi app is created, you can use the Event Simulator option in the editor to simulate events to streams and perform developer testing. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 @ App : name ( Taxi-Rider-Requests-Processing-App ) @ App : description ( Siddhi application that processes Taxi Rider request events ) -- TCP source which accepts Taxi rider requests @ source ( type = tcp , context = taxiRideRequests , @ map ( type = binary )) define stream TaxiRideEventStream ( id long , time string , passengerId string , passengerName string , pickUpAddress string , pickUpZone string , dropOutAddress string , routingDetails string , expectedFare double , status string , passengerGrade string , additionalNote string ); -- For testing purposes, offer messages are logged in console. -- This could be further extended to send as sms to the premium users @ sink ( type = log , @ map ( type = text , @ payload ( Hi {{passengerName}} Unfortunately, you couldn t travel with us Today. We apologise for the high waiting time. As a token of apology please accept {{offerAmount}} USD off from your next ride. Truly, MyTaxi Team ))) define stream InstantOfferAlertStream ( passengerName string , pickUpZone string , offerAmount double ); -- RDBMS event table which stores events related to the requirement of need more riders @ store ( type = rdbms , jdbc . url = ${MYSQL_DB_URL} , username = ${MYSQL_USERNAME} , password = ${MYSQL_PASSWORD} , jdbc . driver . name = com.mysql.jdbc.Driver ) define table NeedMoreRidersTable ( systemTime string , zone string ); -- Email sink which send email alerts to the manager of MyTaxi @ sink ( type = email , username = ${EMAIL_USERNAME} , address = ${SENDER_EMAIL_ADDRESS} , password = ${EMAIL_PASSWORD} , subject = [Need Immediate Attention] High Waiting Time , to = ${MANAGER_EMAIL_ADDRESS} , host = smtp.gmail.com , port = 465 , ssl . enable = true , auth = true , @ map ( type = text , @ payload ( Hi, There is an increasing trend of ride cancellations in the {{zone}} area due to high waiting time. Increasing trend is detected at {{systemTime}}. Please take immediate action to sort this out. Thanks... ))) define stream AttentionRequiredCancellationStream ( systemTime string , zone string , lastNoOfCancellations long ); @ info ( name = Ride-cancellation-identifier ) from every e1 = TaxiRideEventStream [ status == Assigned ] - e2 = TaxiRideEventStream [ e1 . passengerId == passengerId and status == Cancelled and additionalNote == WT is High ] within 30 seconds select e2 . passengerName , e1 . passengerGrade , e1 . pickUpZone , e1 . expectedFare insert into TaxiRideCancelStream ; @ info ( name = Offer-for-premium-users ) from TaxiRideCancelStream [ passengerGrade == Premium ] select passengerName , pickUpZone , math : floor ( expectedFare ) / 2 . 0 as offerAmount insert into InstantOfferAlertStream ; @ info ( name = Frequently-ride-cancellation-identifier ) from TaxiRideCancelStream # window . timeBatch ( 1 min ) select pickUpZone , count () as totalCancellations , time : currentTimestamp () as systemTime group by pickUpZone having totalCancellations 3 insert into NeedMoreRidersStream ; @ info ( name = Dump-needMoreRider-events ) from NeedMoreRidersStream select systemTime , pickUpZone as zone insert into NeedMoreRidersTable ; @ info ( name = Ride-cancellation-increasing-trend-identifer ) partition with ( pickUpZone of NeedMoreRidersStream ) begin from e1 = NeedMoreRidersStream , e2 = NeedMoreRidersStream [ totalCancellations e1 . totalCancellations ], e3 = NeedMoreRidersStream [ totalCancellations e2 . totalCancellations ] select e3 . systemTime , e1 . pickUpZone as zone , e3 . totalCancellations as lastNoOfCancellations insert into AttentionRequiredCancellationStream ; end ; Source view of the Siddhi app. Below is the flow diagram of the above Siddhi App.","title":"Implement Streaming Queries"},{"location":"docs/guides/patterns-and-trends/guide/#testing","text":"NOTE: In the above provided Siddhi app, there are some environmental variables (MYSQL_DB_URL, MYSQL_USERNAME, and MYSQL_PASSWORD) are used. These values are required to be set to try out the scenario end to end. MYSQL related environmental variables are required to store the events of stream NeedMoreRidersStream . Environmental variables EMAIL_PASSWORD, EMAIL_USERNAME, SENDER_EMAIL_ADDRESS and MANAGER_EMAIL_ADDRESS are used to send an email alert when there is an increasing trend of cancellation on specific area. MYSQL_DB_URL: MySQL database jdbc url to persist failed events. (eg: 'jdbc:mysql://localhost:3306/MyTaxi') MYSQL_USERNAME: Username of the user account to connect MySQL database. (eg: 'root') MYSQL_PASSWORD: Password of the user account to connect MySQL database. (eg: 'root') EMAIL_USERNAME: Username of the email account which used to send email alerts. (eg: 'siddhi.gke.user') EMAIL_PASSWORD: Password of the email account which used to send email alerts. (eg: 'siddhi123') SENDER_EMAIL_ADDRESS: Email address of the account used to send email alerts. (eg: 'siddhi.gke.user@gmail.com') MANAGER_EMAIL_ADDRESS: Destination Email address where escalation mails are sent. (eg: 'manager@mytaxi.com')","title":"Testing"},{"location":"docs/guides/patterns-and-trends/guide/#setup-mysql","text":"Download and Install MySQL database as per the guidelines (https://www.mysql.com/downloads/) Log in to the MySQL server and create a database called \u201cMyTaxi\u201d Download the MySQL client connector jar and add it to jars (if it is non OSGI) or bundles (if it is OSGI bundle) directory of Siddhi distribution","title":"Setup MySQL"},{"location":"docs/guides/patterns-and-trends/guide/#tryout","text":"There are multiple options available to test the developed Siddhi App. As mentioned in the previous step you could simply simulate some events directly into the stream and test your queries. But, if you are willing to test the end to end flow (from an input source to sink) then you can start the Siddhi app in the editor itself. In this guide, we are going to run the Siddhi App in the editor itself. Once the server is started, you will see below logs get printed in the editor console. As written in the above Siddhi application, taxi ride requests are accepted by the TCP endpoint of the Siddhi Stream Processor; those events are pushed to a stream called taxiRideEventStream . The first query is written to identify the pattern of rider cancellation after rider request within 30 seconds due to high waiting time. If such a pattern is identified then system will the user grade and grant some offers for subsequent rides for premium users. Parallelly, Stream Processor keep tracking the number of ride cancellations for each minute and if it found a situation of more than 3 ride cancellation then the system will identify that area/zone and send that details to next processing system to take necessary action. In the above query, such events are pushed to a database table. There is another query which continuously listens for the total number of cancellations for each minute and looking for increasing trend of ride cancellations and notifies accordingly. In this situation, if the Streaming system sends an email alert to the manager of the MyTaxi for his/her further consideration.","title":"Tryout"},{"location":"docs/guides/patterns-and-trends/guide/#invoking-the-siddhi-app","text":"To try out the above use case, you have to send a set of events in a certain order to match with the query conditions. There is a sample TCP publisher could publish events in such an order. Hence, you could use the sample publisher given in here . Then you can execute below command to run the TCP client. 1 java -jar tcp-producer-1.0.0-jar-with-dependencies.jar It will take nearly 3 minutes to publish events which required to test all the flows in the given Siddhi app. When you are publishing events to Siddhi Stream processor, you could see the logs that get printed in Siddhi Stream processor side as well. There are related to instant offer alerts. Once, TCP publisher completes publishing events then you could check the email alert which is generated.","title":"Invoking the Siddhi App"},{"location":"docs/guides/patterns-and-trends/guide/#deployment","text":"Once you are done with the development, export the Siddhi app that you have developed with 'File' - 'Export File' option. You can deploy the Siddhi app using any of the methods listed below.","title":"Deployment"},{"location":"docs/guides/patterns-and-trends/guide/#deploy-on-vm-bare-metal","text":"","title":"Deploy on VM/ Bare Metal"},{"location":"docs/guides/patterns-and-trends/guide/#prerequisites_1","text":"First, please make sure that necessary prerequisites are met as given the Testing section . MySQL is required to try out the use case. Then, as given in the Setup MySQL section. Download the MySQL database and install it. Then create a database called \u201cMyTaxi\u201d in the MySQL database.","title":"Prerequisites"},{"location":"docs/guides/patterns-and-trends/guide/#siddhi-runtime-configuration","text":"Make sure to set the necessary environmental variables as given above. Note: In the above provided Siddhi app, there are some environmental variables (MYSQL_DB_URL, MYSQL_USERNAME, and MYSQL_PASSWORD) are used. These values are required to be set to try out the scenario end to end. MYSQL related environmental variables are required to store the events of stream NeedMoreRidersStream . Environmental variables EMAIL_PASSWORD, EMAIL_USERNAME, SENDER_EMAIL_ADDRESS and MANAGER_EMAIL_ADDRESS are used to send an email alert when there is an increasing trend of cancellation on specific area. Hence, make sure to set the environmental variables with the proper values in the system (make sure to follow necessary steps based on the underneath operating system). Download the latest Siddhi Runner distribution . Unzip the siddhi-runner-x.x.x.zip . You have to copy necessary Mysql client jar to Siddhi runner distribution to connect with MySQL database. Copy the MySQL client connector jar to jars (if it is non OSGI) or bundles (if it is OSGI bundle) directory of Siddhi runner. Please refer this for information. Start Siddhi app with the runner config by executing the following commands from the distribution directory. ``` Linux/Mac : ./bin/runner.sh -Dapps= Windows : bin\\runner.bat -Dapps= 1 2 Eg: If exported siddhi app in Siddhi home directory, ./bin/runner.sh -Dapps=Taxi-Rider-Requests-Processing-App.siddhi ``` Once server is started, download the sample TCP event generator from here and execute below command. 1 java -jar tcp-producer-1.0.0-jar-with-dependencies.jar tcp://localhost:9892/taxiRideRequests Above event publishes send binary events through TCP to the TCP endpoint defined in the Siddhi application. You can change the TCP endpoint url by passing them as java arguments. If not, sample client consider tcp://localhost:9892/taxiRideRequests as the TCP endpoint url. You can find the sample client source code in here In this situation, you can find the logs printed in the Siddhi runner console/log, events related to NeedMoreRidersStream are stored in the database table and escalation email is sent to the manager when there is an increasing trend found in the cancellations.","title":"Siddhi Runtime Configuration"},{"location":"docs/guides/patterns-and-trends/guide/#deploy-on-docker","text":"","title":"Deploy on Docker"},{"location":"docs/guides/patterns-and-trends/guide/#prerequisites_2","text":"MySQL is an external dependency for this use case. Hence, you could use the corresponding MySQL docker artifact to test the requirement. First, you can create a docker network for the deployment as shown below 1 docker network create siddhi-tier --driver bridge Then, you can get the MySQL docker image from here and run it with below command. We are going to use mysql version 5.7.27. Start the MySQL docker images with below command, 1 docker run --name mysql-server --network siddhi-tier -e MYSQL_ROOT_PASSWORD=root e1e1680ac726 e1e1680ac726 is the MySQL docker image id in this case Login to the MySQL docker instance and create a database called \u201cMyTaxi\u201d. Now, you have configured necessary prerequisites that required to run the use case.","title":"Prerequisites"},{"location":"docs/guides/patterns-and-trends/guide/#siddhi-docker-configuration","text":"Since, MySQL client jar is required for the Siddhi runner; you have to create the docker image accordingly. Below is the sample Docker file created 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 FROM siddhiio/siddhi-runner-base-alpine:5.1.0-alpha MAINTAINER Siddhi IO Docker Maintainers siddhi-dev@googlegroups.com ARG HOST_BUNDLES_DIR=./files/bundles ARG HOST_JARS_DIR=./files/jars ARG JARS=${RUNTIME_SERVER_HOME}/jars ARG BUNDLES=${RUNTIME_SERVER_HOME}/bundles # copy bundles jars to the siddhi-runner distribution COPY --chown=siddhi_user:siddhi_io ${HOST_JARS_DIR}/ ${JARS} # expose ports EXPOSE 9090 9443 9712 9612 7711 7611 7070 7443 RUN bash ${RUNTIME_SERVER_HOME}/bin/install-jars.sh STOPSIGNAL SIGINT ENTRYPOINT [ /home/siddhi_user/siddhi-runner/bin/runner.sh , -- ] Here, you have to create a folder called jars to add necessary external client dependencies to the docker image. You can refer the official Siddhi documentation reference for this purpose. Once, Dockerfile is created you can create the docker image with below command. 1 docker build -t siddhi_mysql . Then, you can run the Siddhi docker image that you created with necessary external dependencies to work with MySQL. 1 docker run --network siddhi-tier -it -p 9892:9892 -v /Users/mohan/siddhi-apps/:/siddhi-apps -e MYSQL_DB_URL=jdbc:mysql://mysql-server:3306/MyTaxi -e MYSQL_USERNAME=root -e MYSQL_PASSWORD=root -e EMAIL_USERNAME=siddhi.gke.user -e EMAIL_PASSWORD=siddhi123 -e SENDER_EMAIL_ADDRESS=siddhi.gke.user@gmail.com -e MANAGER_EMAIL_ADDRESS=mohan@wso2.com siddhi_mysql:latest -Dapps=/siddhi-apps/Taxi-Rider-Requests-Processing-App.siddhi Note: In the above provided Siddhi app, there are some environmental variables (MYSQL_DB_URL, MYSQL_USERNAME, and MYSQL_PASSWORD) are used. These values are required to be set to try out the scenario end to end. MYSQL related environmental variables are required to store the events of stream NeedMoreRidersStream . Environmental variables EMAIL_PASSWORD, EMAIL_USERNAME, SENDER_EMAIL_ADDRESS and MANAGER_EMAIL_ADDRESS are used to send an email alert when there is an increasing trend of cancellation on specific area. You can use the sample TCP event publisher to simulate required events. Use the below command to publish events to TCP endpoint. 1 java -jar tcp-producer-1.0.0-jar-with-dependencies.jar tcp://localhost:9892/taxiRideRequests Then, you could see below logs get printed in the Siddhi runner console/log, events related to NeedMoreRidersStream are stored in the database table and escalation email is sent to the manager when there is an increasing trend found in the cancellations.","title":"Siddhi Docker Configuration"},{"location":"docs/guides/patterns-and-trends/guide/#deploy-on-kubernetes","text":"It is advisable to create a namespace in Kubernetes to follow below steps. 1 kubectl create ns siddhi-mysql-test There is a prerequisite that you should meet to tryout below SiddhiProcess; configuring MySQL database server within the above created namespace. You can use the official helm chart provided for MySQL. First, install the MySQL helm chart as shown below, 1 helm install --name mysql-db --namespace=siddhi-mysql-test --set mysqlRootPassword=root,mysqlDatabase=MyTaxi stable/mysql Here, you can define the root password to connect to the MYSQL database and also define the database name. BTW, make sure to do helm init if it is not done yet. Then, you can set a port forwarding to the MySQL service which allows you to connect from the Host. 1 kubectl port-forward svc/mysql-db 13306:3306 --namespace=siddhi-mysql-test Then, you can login to the MySQL server from your host machine as shown below. Then, you can install Siddhi Operator To install the Siddhi Kubernetes operator run the following commands. 1 2 kubectl apply -f https://github.com/siddhi-io/siddhi-operator/releases/download/v0.2.0-alpha/00-prereqs.yaml --namespace=siddhi-mysql-test kubectl apply -f https://github.com/siddhi-io/siddhi-operator/releases/download/v0.2.0-alpha/01-siddhi-operator.yaml --namespace=siddhi-mysql-test You can verify the installation by making sure the following deployments are running in your Kubernetes cluster. Siddhi applications can be deployed on Kubernetes using the Siddhi operator. To deploy the above created Siddhi app, we have to create custom resource object yaml file (with the kind as SiddhiProcess) as given below 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 ````yaml apiVersion: siddhi.io/v1alpha2 kind: SiddhiProcess metadata: name: taxi-rider-requests-processing-app spec: apps: - script: | @App:name( Taxi-Rider-Requests-Processing-App ) @App:description( Siddhi application that processes Taxi Rider request events ) -- TCP source which accepts Taxi rider requests @source(type= tcp , context= taxiRideRequests , @map(type= binary )) define stream TaxiRideEventStream(id long, time string, passengerId string, passengerName string, pickUpAddress string, pickUpZone string, dropOutAddress string, routingDetails string, expectedFare double, status string, passengerGrade string, additionalNote string); -- For testing purposes, offer messages are logged in console. -- This could be further extended to send as sms to the premium users @sink(type= log , @map(type = text , @payload( Hi {{passengerName}} Unfortunately, you couldn t travel with us Today. We apologise for the high waiting time. As a token of apology please accept {{offerAmount}} USD off from your next ride. Truly, MyTaxi Team ))) define stream InstantOfferAlertStream(passengerName string, pickUpZone string, offerAmount double); -- RDBMS event table which stores events related to the requirement of need more riders @store(type= rdbms , jdbc.url= ${MYSQL_DB_URL} , username= ${MYSQL_USERNAME} , password= ${MYSQL_PASSWORD} , jdbc.driver.name= com.mysql.jdbc.Driver ) define table NeedMoreRidersTable (systemTime string, zone string); -- Email sink which send email alerts to the manager of MyTaxi @sink(type = email , username = ${EMAIL_USERNAME} , address = ${SENDER_EMAIL_ADDRESS} , password = ${EMAIL_PASSWORD} , subject = [Need Immediate Attention] High Waiting Time , to = ${MANAGER_EMAIL_ADDRESS} , host = smtp.gmail.com , port = 465 , ssl.enable = true , auth = true , @map(type = text , @payload( Hi, There is an increasing trend of ride cancellations in the {{zone}} area due to high waiting time. Increasing trend is detected at {{systemTime}}. Please take immediate action to sort this out. Thanks... ))) define stream AttentionRequiredCancellationStream (systemTime string, zone string, lastNoOfCancellations long); @info(name= Ride-cancellation-identifier ) from every e1=TaxiRideEventStream[status == Assigned ] - e2=TaxiRideEventStream[e1.passengerId == passengerId and status == Cancelled and additionalNote == WT is High ] within 30 seconds select e2.passengerName, e1.passengerGrade, e1.pickUpZone, e1.expectedFare insert into TaxiRideCancelStream; @info(name= Offer-for-premium-users ) from TaxiRideCancelStream[passengerGrade == Premium ] select passengerName, pickUpZone, math:floor(expectedFare) / 2.0 as offerAmount insert into InstantOfferAlertStream; @info(name= Frequently-ride-cancellation-identifier ) from TaxiRideCancelStream#window.timeBatch(1 min) select pickUpZone, count() as totalCancellations, time:currentTimestamp() as systemTime group by pickUpZone having totalCancellations 3 insert into NeedMoreRidersStream; @info(name= Dump-needMoreRider-events ) from NeedMoreRidersStream select systemTime, pickUpZone as zone insert into NeedMoreRidersTable; @info(name= Ride-cancellation-increasing-trend-identifer ) partition with (pickUpZone of NeedMoreRidersStream) begin from e1 = NeedMoreRidersStream, e2 = NeedMoreRidersStream[totalCancellations e1.totalCancellations], e3 = NeedMoreRidersStream[totalCancellations e2.totalCancellations] select e3.systemTime, e1.pickUpZone as zone, e3.totalCancellations as lastNoOfCancellations insert into AttentionRequiredCancellationStream; end; persistentVolumeClaim: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi storageClassName: hostpath volumeMode: Filesystem container: env: - name: MYSQL_DB_URL value: jdbc:mysql://mysql-db:3306/MyTaxi - name: MYSQL_USERNAME value: root - name: MYSQL_PASSWORD value: root - name: EMAIL_PASSWORD value: siddhi123 - name: EMAIL_USERNAME value: siddhi.gke.user - name: SENDER_EMAIL_ADDRESS value: siddhi.gke.user@gmail.com - name: MANAGER_EMAIL_ADDRESS value: mohan@wso2.com image: mohanvive/siddhi_mysql:latest ```` Note: In the above provided Siddhi app, there are some environmental variables (MYSQL_DB_URL, MYSQL_USERNAME, and MYSQL_PASSWORD) are used. These values are required to be set to try out the scenario end to end. MYSQL related environmental variables are required to store the events of stream NeedMoreRidersStream . Environmental variables EMAIL_PASSWORD, EMAIL_USERNAME, SENDER_EMAIL_ADDRESS and MANAGER_EMAIL_ADDRESS are used to send an email alert when there is an increasing trend of cancellation on specific area. Hence, make sure to add proper values for the environmental variables in the above yaml file (check the \u2018env\u2019 section of the yaml file). Here, you can use the docker image that created in the section since you need a docker images with required extensions and client jars to test it in Kubernetes. Now, let\u2019s create the above resource in the Kubernetes cluster with below command. 1 kubectl --namespace=siddhi-mysql-test create -f absolute-yaml-file-path /taxi-rider-requests-processing-app.yaml 1 Once, siddhi app is successfully deployed. You can verify its health with below Kubernetes commands You can find the alert logs in the siddhi runner log file. To see the Siddhi runner log file, you can invoke below command. 1 kubectl get pods Then, find the pod name of the siddhi app deployed. Then invoke below command, 1 kubectl logs siddhi-app-pod-name -f Then, you can set a port forwarding to the Siddhi TCP endpoint which allows you to connect from the Host with below command. 1 2 3 4 5 ```` kubectl port-forward svc/taxi-rider-requests-processing-app-0 9892:9892 --namespace=siddhi-mysql-test ```` ![tco_port_forwarding](images/tcp-port-forwarding.png Kubernetes TCP Port Forwarding ) Then execute below command to send TCP events to Siddhi Stream Processor. 1 2 3 ```` java -jar tcp-producer-1.0.0-jar-with-dependencies.jar tcp://0.0.0.0:9892/taxiRideRequests ```` Then, you could see below logs get printed in the Siddhi runner console/log, events related to NeedMoreRidersStream are stored in the database table and escalation email is sent to the manager when there is an increasing trend found in the cancellations. 1 2 3 4 5 ![k8s_instant_offer_in_logs](images/k8s-instant-offer-in-logs.png Instant Offers to Premium Users ) ![k8s_need_more_riders_table](images/k8s-need-more-riders-table.png Need More Riders Data in Database ) ![k8s_escalation_email_alert](images/k8s-escalation-email-alert.png Escalation Mail to Manager )","title":"Deploy on Kubernetes"},{"location":"docs/quick-start/","text":"Siddhi 5.1 Quick Start Guide Siddhi is a cloud native Streaming and Complex Event Processing engine that understands Streaming SQL queries in order to capture events from diverse data sources, process them, detect complex conditions, and publish output to various endpoints in real time. Siddhi is used by many companies including Uber, eBay, PayPal (via Apache Eagle), here Uber processed more than 20 billion events per day using Siddhi for their fraud analytics use cases. Siddhi is also used in various analytics and integration platforms such as Apache Eagle as a policy enforcement engine, WSO2 API Manager as analytics and throttling engine, WSO2 Identity Server as an adaptive authorization engine. This quick start guide contains the following six sections: Domain of Siddhi Overview of Siddhi architecture Using Siddhi for the first time Writing first Siddhi Application Testing Siddhi Application A bit of Stream Processing 1. Domain of Siddhi Siddhi is an event driven system where all the data it consumes, processes and sends are modeled as events. Therefore, Siddhi can play a vital part in any event-driven architecture. As Siddhi works with events, first let's understand what an event is through an example. If we consider transactions carried out via an ATM as a data stream, one withdrawal from it can be considered as an event . This event contains data such as amount, time, account number, etc. Many such transactions form a stream. Siddhi provides following functionalities, Streaming Data Analytics Forrester defines Streaming Analytics as: Software that provides analytical operators to orchestrate data flow , calculate analytics , and detect patterns on event data from multiple, disparate live data sources to allow developers to build applications that sense, think, and act in real time . Complex Event Processing (CEP) Gartner\u2019s IT Glossary defines CEP as follows: \"CEP is a kind of computing in which incoming data about events is distilled into more useful, higher level \u201ccomplex\u201d event data that provides insight into what is happening.\" \" CEP is event-driven because the computation is triggered by the receipt of event data. CEP is used for highly demanding, continuous-intelligence applications that enhance situation awareness and support real-time decisions.\" Streaming Data Integration Streaming data integration is a way of integrating several systems by processing, correlating, and analyzing the data in memory, while continuously moving data in real-time from one system to another. Alerts Notifications The system to continuously monitor event streams, and send alerts and notifications, based on defined KPIs and other analytics. Adaptive Decision Making A way to dynamically making real-time decisions based on predefined rules, the current state of the connected systems, and machine learning techniques. Basically, Siddhi receives data event-by-event and processes them in real-time to produce meaningful information. Using the above Siddhi can be used to solve may use-cases as follows: Fraud Analytics Monitoring System Integration Anomaly Detection Sentiment Analysis Processing Customer Behavior .. etc 2. Overview of Siddhi architecture As indicated above, Siddhi can: Accept event inputs from many different types of sources. Process them to transform, enrich, and generate insights. Publish them to multiple types of sinks. To use Siddhi, you need to write the processing logic as a Siddhi Application in the Siddhi Streaming SQL language which is discussed in the section 4 . Here a Siddhi Application is a script file that contains business logic for a scenario. When the Siddhi application is started, it: Consumes data one-by-one as events. Pipe the events to queries through various streams for processing. Generates new events based on the processing done at the queries. Finally, Sends newly generated events through output to streams. 3. Using Siddhi for the first time In this section, we will be using the Siddhi tooling distribution\u200a\u2014\u200aa server version of Siddhi that has a sophisticated web based editor with a GUI (referred to as \u201cSiddhi Editor\u201d ) where you can write Siddhi Apps and simulate events to test your scenario. Step 1 \u200a\u2014\u200aInstall Oracle Java SE Development Kit (JDK) version 1.8. Step 2 \u200a\u2014\u200a Set the JAVA_HOME environment variable. Step 3 \u200a\u2014\u200aDownload the latest tooling distribution from here . Step 4 \u200a\u2014\u200aExtract the downloaded zip and navigate to TOOLING_HOME /bin . ( TOOLING_HOME refers to the extracted folder) Step 5 \u200a\u2014\u200aIssue the following command in the command prompt (Windows) / terminal (Linux/Mac) 1 2 For Windows: tooling.bat For Linux/Mac: ./tooling.sh After successfully starting the Siddhi Editor, the terminal should look like as shown below: After starting the Siddhi Editor, access the Editor GUI by visiting the following link in your browser (Google Chrome is the Recommended). 1 http://localhost:9390/editor This takes you to the Siddhi Editor landing page. 4. Writing first Siddhi Application Siddhi Streaming SQL is a rich, compact, easy-to-use SQL-like language. As the first Siddhi Application, let's learn how to find the total of values from the incoming events and output the current running total value for each event. Siddhi has lot of in-built functions and extensions available for complex analysis, and you can find more information about the Siddhi grammar and its functions from the Siddhi Query Guide . Let's consider sample scenario where we are loading cargo boxes into a ship . Here, we need to keep track of the total weight of the cargo added, and the weight of each loaded cargo box is considered an event . We can write a Siddhi Application for the above scenario using the following 4 parts . Part 1\u200a\u2014\u200aGiving our Siddhi application a suitable name. This allows us to uniquely identity a Siddhi Application. In this example, let's name our application as \u201cHelloWorldApp\u201d 1 @App:name( HelloWorldApp ) Part 2\u200a\u2014\u200aDefining the input stream. The stream needs to have a name and a schema defining the data that each incoming event should contain. The event data attributes are expressed as name and type pairs. We can also attach a \"source\" to the created stream, so that we can consume events from outside and send them to the stream. ( Source is the Siddhi way to consume streams from external systems ). For this scenario we will use an http source to consume Cargo Events. When added the http source will spin up a HTTP endpoint and keep on listening for messages. To learn more about sources, refer source ) In this scenario: The name of the input stream\u200a\u2014\u200a \u201cCargoStream\u201d This contains only one data attribute: The name of the data in each event\u200a\u2014\u200a \u201cweight\u201d Type of the data \u201cweight\u201d \u200a\u2014\u200aint Type of source - HTTP HTTP endpoint address - http://0.0.0.0:8006/cargo Accepted input data format - JSON 1 2 @source(type = http , receiver.url = http://0.0.0.0:8006/cargo , @map(type = json )) define stream CargoStream (weight int); Part 3 - Defining the output stream. This has the same info as the input \u201cCargoStream\u201d stream\u200adefinition with an additional totalWeight attribute containing the total weight calculated so far. In addition we also need to add a log \"sink\" to log the OutputStream so that we can observe the output produced by the stream. ( Sink is the Siddhi way to publish streams to external systems ). This particular log type sink simply logs the stream events. To learn more about sinks, refer sink ) 1 2 @sink(type= log , prefix= LOGGER ) define stream OutputStream(weight int, totalWeight long); Part 4\u200a\u2014\u200aWriting the Siddhi query. As part of the query we need to specify the following: A name for the query\u200a\u2014\u200a \u201cHelloWorldQuery\u201d The input stream from which the query consumes events \u2014\u200a \u201cCargoStream\u201d How the output to be calculated - by calculating the sum of the *weight**s The data outputted to the output stream\u200a\u2014\u200a \u201cweight\u201d , \u201ctotalWeight\u201d The output stream to which the event should be outputted\u200a\u2014\u200a \u201cOutputStream\u201d 1 2 3 4 @info(name= HelloWorldQuery ) from CargoStream select weight, sum(weight) as totalWeight insert into OutputStream; This query will calculate the sum of weights from the start of the Siddhi application. For more complex use cases refer Siddhi Query Guild ) Final Siddhi application in the editor will look like following. You can copy the final Siddhi app from below. 1 2 3 4 5 6 7 8 9 10 11 12 @App:name( HelloWorldApp ) @source(type = http , receiver.url = http://0.0.0.0:8006/cargo , @map(type = json )) define stream CargoStream (weight int); @sink(type= log , prefix= LOGGER ) define stream OutputStream(weight int, totalWeight long); @info(name= HelloWorldQuery ) from CargoStream select weight, sum(weight) as totalWeight insert into OutputStream; 5. Testing Siddhi Application In this section first we will test the logical accuracy of Siddhi query using in-built functions of Siddhi Editor. In a later section we will invoke the HTTP endpoint and perform an end to end test. The Siddhi Editor has in-built support to simulate events. You can do it via the \u201cEvent Simulator\u201d panel at the left of the Siddhi Editor. Before running the event simulation, you should save your HelloWorldApp by browsing to File menu - and clicking Save . To simulate events, click Event Simulator and configure Single Simulation as shown below. Step 1\u200a\u2014\u200aConfigurations: Siddhi App Name\u200a\u2014\u200a \u201cHelloWorldApp\u201d Stream Name\u200a\u2014\u200a \u201cCargoStream\u201d Timestamp\u200a\u2014\u200a(Leave it blank) weight\u200a\u2014\u200a2 (or some integer) Step 2\u200a\u2014\u200aClick \u201cRun\u201d mode and then click \u201cStart and Send\u201d . This starts the Siddhi Application and send the event. If the Siddhi application is successfully started, the following message is printed in the Stream Processor Studio console: 1 HelloWorldApp.siddhi Started Successfully! Step 3\u200a\u2014\u200aClick \u201cSend\u201d and observe the terminal . This will send a new event for each click. You can see a logs containing outputData=[2, 2] and outputData=[2, 4] , etc. You can change the value of the weight and send it to see how the sum of the weight is updated. Bravo! You have successfully completed building and testing your first Siddhi Application! 6. A bit of Stream Processing This section will improve our Siddhi app to demonstrates how to carry out temporal window processing with Siddhi. Up to this point, we are calculating the sum of weights from the start of the Siddhi app, and now let's improve it to consider only the last three events for the calculation. For this scenario, let's imagine that when we are loading cargo boxes into the ship and we need to keep track of the average weight of the last three loaded boxes so that we can balance the weight across the ship. For this purpose, let's try to find the average weight of last three boxes of each event. For window processing, we need to modify our query as follows: 1 2 3 4 @info(name= HelloWorldQuery ) from CargoStream#window.length(3) select weight, sum(weight) as totalWeight, avg(weight) as averageWeight insert into OutputStream; from CargoStream#window.length(3) - Specifies that we need to consider the last three events in a sliding manner. avg(weight) as averageWeight - Specifies calculating the average of events stored in the window and producing the results as \"averageWeight\" (Note: Similarly the sum also calculates the totalWeight based on the last three events). We also need to modify the \"OutputStream\" definition to accommodate the new \"averageWeight\" . 1 define stream OutputStream(weight int, totalWeight long, averageWeight double); The updated Siddhi Application is given below: 1 2 3 4 5 6 7 8 9 10 11 12 @App:name( HelloWorldApp ) @source(type = http , receiver.url = http://0.0.0.0:8006/cargo ,@map(type = json )) define stream CargoStream (weight int); @sink(type= log , prefix= LOGGER ) define stream OutputStream(weight int, totalWeight long, averageWeight double); @info(name= HelloWorldQuery ) from CargoStream#window.length(3) select weight, sum(weight) as totalWeight, avg(weight) as averageWeight insert into OutputStream; Now you can send events using the Event Simulator and observe the log to see the sum and average of the weights based on the last three cargo events. In the earlier scenario when the window is not used, the system only stored the running sum in its memory, and it did not store any events. But for length based window processing the system will retain the events that fall into the window to perform aggregation operations such as average, maximum, etc. In this case when the 4 th event arrives, the first event in the window is removed ensuring the memory usage does not grow beyond a specific limit. Note: some window types in Siddhi are even more optimized to perform the operations with minimal or no event retention. 7. Running Siddhi Application as a Docker microservice In this step we will run above developed Siddhi application as a microservice utilizing Docker. For other available options please refer here . Here we will use siddhi-runner docker distribution. Follow the below steps to obtain the docker. Install docker in your machine and start the daemon ( https://docs.docker.com/install/ ). Pull the latest siddhi-runner image by executing below command. 1 docker pull siddhiio/siddhi-runner-alpine:latest * Navigate to Siddhi Editor and choose File - Export File for download above Siddhi application as a file. * Move downloaded Siddhi file( HelloWorldApp.siddhi ) to a desired location (e.g. /home/me/siddhi-apps ) * Execute below command to start the Siddhi Application as a microservice. 1 2 docker run -it -p 8006:8006 -v /home/me/siddhi-apps:/apps siddhiio/siddhi-runner-alpine -Dapps=/apps/HelloWorldApp.siddhi Note: Make sure to update the /home/me/siddhi-apps with the folder path you have stored the HelloWorldApp.siddhi app. * Once container is started use below curl command to send events into \"CargoStream\" 1 2 3 curl -X POST http://localhost:8006/cargo \\ --header Content-Type:application/json \\ -d { event :{ weight :2}} * You will be able to observe outputs via logs as shown below. 1 2 [2019-04-24 08:54:51,755] INFO {io.siddhi.core.stream.output.sink.LogSink} - LOGGER : Event{timestamp=1556096091751, data=[2, 2, 2.0], isExpired=false} [2019-04-24 08:56:25,307] INFO {io.siddhi.core.stream.output.sink.LogSink} - LOGGER : Event{timestamp=1556096185307, data=[2, 4, 2.0], isExpired=false} To learn more about the Siddhi functionality, see Siddhi Documentation . If you have questions please post them on Stackoverflow with \"Siddhi\" tag.","title":"Quick Start"},{"location":"docs/quick-start/#siddhi-51-quick-start-guide","text":"Siddhi is a cloud native Streaming and Complex Event Processing engine that understands Streaming SQL queries in order to capture events from diverse data sources, process them, detect complex conditions, and publish output to various endpoints in real time. Siddhi is used by many companies including Uber, eBay, PayPal (via Apache Eagle), here Uber processed more than 20 billion events per day using Siddhi for their fraud analytics use cases. Siddhi is also used in various analytics and integration platforms such as Apache Eagle as a policy enforcement engine, WSO2 API Manager as analytics and throttling engine, WSO2 Identity Server as an adaptive authorization engine. This quick start guide contains the following six sections: Domain of Siddhi Overview of Siddhi architecture Using Siddhi for the first time Writing first Siddhi Application Testing Siddhi Application A bit of Stream Processing","title":"Siddhi 5.1 Quick Start Guide"},{"location":"docs/quick-start/#1-domain-of-siddhi","text":"Siddhi is an event driven system where all the data it consumes, processes and sends are modeled as events. Therefore, Siddhi can play a vital part in any event-driven architecture. As Siddhi works with events, first let's understand what an event is through an example. If we consider transactions carried out via an ATM as a data stream, one withdrawal from it can be considered as an event . This event contains data such as amount, time, account number, etc. Many such transactions form a stream. Siddhi provides following functionalities, Streaming Data Analytics Forrester defines Streaming Analytics as: Software that provides analytical operators to orchestrate data flow , calculate analytics , and detect patterns on event data from multiple, disparate live data sources to allow developers to build applications that sense, think, and act in real time . Complex Event Processing (CEP) Gartner\u2019s IT Glossary defines CEP as follows: \"CEP is a kind of computing in which incoming data about events is distilled into more useful, higher level \u201ccomplex\u201d event data that provides insight into what is happening.\" \" CEP is event-driven because the computation is triggered by the receipt of event data. CEP is used for highly demanding, continuous-intelligence applications that enhance situation awareness and support real-time decisions.\" Streaming Data Integration Streaming data integration is a way of integrating several systems by processing, correlating, and analyzing the data in memory, while continuously moving data in real-time from one system to another. Alerts Notifications The system to continuously monitor event streams, and send alerts and notifications, based on defined KPIs and other analytics. Adaptive Decision Making A way to dynamically making real-time decisions based on predefined rules, the current state of the connected systems, and machine learning techniques. Basically, Siddhi receives data event-by-event and processes them in real-time to produce meaningful information. Using the above Siddhi can be used to solve may use-cases as follows: Fraud Analytics Monitoring System Integration Anomaly Detection Sentiment Analysis Processing Customer Behavior .. etc","title":"1. Domain of Siddhi"},{"location":"docs/quick-start/#2-overview-of-siddhi-architecture","text":"As indicated above, Siddhi can: Accept event inputs from many different types of sources. Process them to transform, enrich, and generate insights. Publish them to multiple types of sinks. To use Siddhi, you need to write the processing logic as a Siddhi Application in the Siddhi Streaming SQL language which is discussed in the section 4 . Here a Siddhi Application is a script file that contains business logic for a scenario. When the Siddhi application is started, it: Consumes data one-by-one as events. Pipe the events to queries through various streams for processing. Generates new events based on the processing done at the queries. Finally, Sends newly generated events through output to streams.","title":"2. Overview of Siddhi architecture"},{"location":"docs/quick-start/#3-using-siddhi-for-the-first-time","text":"In this section, we will be using the Siddhi tooling distribution\u200a\u2014\u200aa server version of Siddhi that has a sophisticated web based editor with a GUI (referred to as \u201cSiddhi Editor\u201d ) where you can write Siddhi Apps and simulate events to test your scenario. Step 1 \u200a\u2014\u200aInstall Oracle Java SE Development Kit (JDK) version 1.8. Step 2 \u200a\u2014\u200a Set the JAVA_HOME environment variable. Step 3 \u200a\u2014\u200aDownload the latest tooling distribution from here . Step 4 \u200a\u2014\u200aExtract the downloaded zip and navigate to TOOLING_HOME /bin . ( TOOLING_HOME refers to the extracted folder) Step 5 \u200a\u2014\u200aIssue the following command in the command prompt (Windows) / terminal (Linux/Mac) 1 2 For Windows: tooling.bat For Linux/Mac: ./tooling.sh After successfully starting the Siddhi Editor, the terminal should look like as shown below: After starting the Siddhi Editor, access the Editor GUI by visiting the following link in your browser (Google Chrome is the Recommended). 1 http://localhost:9390/editor This takes you to the Siddhi Editor landing page.","title":"3. Using Siddhi for the first time"},{"location":"docs/quick-start/#4-writing-first-siddhi-application","text":"Siddhi Streaming SQL is a rich, compact, easy-to-use SQL-like language. As the first Siddhi Application, let's learn how to find the total of values from the incoming events and output the current running total value for each event. Siddhi has lot of in-built functions and extensions available for complex analysis, and you can find more information about the Siddhi grammar and its functions from the Siddhi Query Guide . Let's consider sample scenario where we are loading cargo boxes into a ship . Here, we need to keep track of the total weight of the cargo added, and the weight of each loaded cargo box is considered an event . We can write a Siddhi Application for the above scenario using the following 4 parts . Part 1\u200a\u2014\u200aGiving our Siddhi application a suitable name. This allows us to uniquely identity a Siddhi Application. In this example, let's name our application as \u201cHelloWorldApp\u201d 1 @App:name( HelloWorldApp ) Part 2\u200a\u2014\u200aDefining the input stream. The stream needs to have a name and a schema defining the data that each incoming event should contain. The event data attributes are expressed as name and type pairs. We can also attach a \"source\" to the created stream, so that we can consume events from outside and send them to the stream. ( Source is the Siddhi way to consume streams from external systems ). For this scenario we will use an http source to consume Cargo Events. When added the http source will spin up a HTTP endpoint and keep on listening for messages. To learn more about sources, refer source ) In this scenario: The name of the input stream\u200a\u2014\u200a \u201cCargoStream\u201d This contains only one data attribute: The name of the data in each event\u200a\u2014\u200a \u201cweight\u201d Type of the data \u201cweight\u201d \u200a\u2014\u200aint Type of source - HTTP HTTP endpoint address - http://0.0.0.0:8006/cargo Accepted input data format - JSON 1 2 @source(type = http , receiver.url = http://0.0.0.0:8006/cargo , @map(type = json )) define stream CargoStream (weight int); Part 3 - Defining the output stream. This has the same info as the input \u201cCargoStream\u201d stream\u200adefinition with an additional totalWeight attribute containing the total weight calculated so far. In addition we also need to add a log \"sink\" to log the OutputStream so that we can observe the output produced by the stream. ( Sink is the Siddhi way to publish streams to external systems ). This particular log type sink simply logs the stream events. To learn more about sinks, refer sink ) 1 2 @sink(type= log , prefix= LOGGER ) define stream OutputStream(weight int, totalWeight long); Part 4\u200a\u2014\u200aWriting the Siddhi query. As part of the query we need to specify the following: A name for the query\u200a\u2014\u200a \u201cHelloWorldQuery\u201d The input stream from which the query consumes events \u2014\u200a \u201cCargoStream\u201d How the output to be calculated - by calculating the sum of the *weight**s The data outputted to the output stream\u200a\u2014\u200a \u201cweight\u201d , \u201ctotalWeight\u201d The output stream to which the event should be outputted\u200a\u2014\u200a \u201cOutputStream\u201d 1 2 3 4 @info(name= HelloWorldQuery ) from CargoStream select weight, sum(weight) as totalWeight insert into OutputStream; This query will calculate the sum of weights from the start of the Siddhi application. For more complex use cases refer Siddhi Query Guild ) Final Siddhi application in the editor will look like following. You can copy the final Siddhi app from below. 1 2 3 4 5 6 7 8 9 10 11 12 @App:name( HelloWorldApp ) @source(type = http , receiver.url = http://0.0.0.0:8006/cargo , @map(type = json )) define stream CargoStream (weight int); @sink(type= log , prefix= LOGGER ) define stream OutputStream(weight int, totalWeight long); @info(name= HelloWorldQuery ) from CargoStream select weight, sum(weight) as totalWeight insert into OutputStream;","title":"4. Writing first Siddhi Application"},{"location":"docs/quick-start/#5-testing-siddhi-application","text":"In this section first we will test the logical accuracy of Siddhi query using in-built functions of Siddhi Editor. In a later section we will invoke the HTTP endpoint and perform an end to end test. The Siddhi Editor has in-built support to simulate events. You can do it via the \u201cEvent Simulator\u201d panel at the left of the Siddhi Editor. Before running the event simulation, you should save your HelloWorldApp by browsing to File menu - and clicking Save . To simulate events, click Event Simulator and configure Single Simulation as shown below. Step 1\u200a\u2014\u200aConfigurations: Siddhi App Name\u200a\u2014\u200a \u201cHelloWorldApp\u201d Stream Name\u200a\u2014\u200a \u201cCargoStream\u201d Timestamp\u200a\u2014\u200a(Leave it blank) weight\u200a\u2014\u200a2 (or some integer) Step 2\u200a\u2014\u200aClick \u201cRun\u201d mode and then click \u201cStart and Send\u201d . This starts the Siddhi Application and send the event. If the Siddhi application is successfully started, the following message is printed in the Stream Processor Studio console: 1 HelloWorldApp.siddhi Started Successfully! Step 3\u200a\u2014\u200aClick \u201cSend\u201d and observe the terminal . This will send a new event for each click. You can see a logs containing outputData=[2, 2] and outputData=[2, 4] , etc. You can change the value of the weight and send it to see how the sum of the weight is updated. Bravo! You have successfully completed building and testing your first Siddhi Application!","title":"5. Testing Siddhi Application"},{"location":"docs/quick-start/#6-a-bit-of-stream-processing","text":"This section will improve our Siddhi app to demonstrates how to carry out temporal window processing with Siddhi. Up to this point, we are calculating the sum of weights from the start of the Siddhi app, and now let's improve it to consider only the last three events for the calculation. For this scenario, let's imagine that when we are loading cargo boxes into the ship and we need to keep track of the average weight of the last three loaded boxes so that we can balance the weight across the ship. For this purpose, let's try to find the average weight of last three boxes of each event. For window processing, we need to modify our query as follows: 1 2 3 4 @info(name= HelloWorldQuery ) from CargoStream#window.length(3) select weight, sum(weight) as totalWeight, avg(weight) as averageWeight insert into OutputStream; from CargoStream#window.length(3) - Specifies that we need to consider the last three events in a sliding manner. avg(weight) as averageWeight - Specifies calculating the average of events stored in the window and producing the results as \"averageWeight\" (Note: Similarly the sum also calculates the totalWeight based on the last three events). We also need to modify the \"OutputStream\" definition to accommodate the new \"averageWeight\" . 1 define stream OutputStream(weight int, totalWeight long, averageWeight double); The updated Siddhi Application is given below: 1 2 3 4 5 6 7 8 9 10 11 12 @App:name( HelloWorldApp ) @source(type = http , receiver.url = http://0.0.0.0:8006/cargo ,@map(type = json )) define stream CargoStream (weight int); @sink(type= log , prefix= LOGGER ) define stream OutputStream(weight int, totalWeight long, averageWeight double); @info(name= HelloWorldQuery ) from CargoStream#window.length(3) select weight, sum(weight) as totalWeight, avg(weight) as averageWeight insert into OutputStream; Now you can send events using the Event Simulator and observe the log to see the sum and average of the weights based on the last three cargo events. In the earlier scenario when the window is not used, the system only stored the running sum in its memory, and it did not store any events. But for length based window processing the system will retain the events that fall into the window to perform aggregation operations such as average, maximum, etc. In this case when the 4 th event arrives, the first event in the window is removed ensuring the memory usage does not grow beyond a specific limit. Note: some window types in Siddhi are even more optimized to perform the operations with minimal or no event retention.","title":"6. A bit of Stream Processing"},{"location":"docs/quick-start/#7-running-siddhi-application-as-a-docker-microservice","text":"In this step we will run above developed Siddhi application as a microservice utilizing Docker. For other available options please refer here . Here we will use siddhi-runner docker distribution. Follow the below steps to obtain the docker. Install docker in your machine and start the daemon ( https://docs.docker.com/install/ ). Pull the latest siddhi-runner image by executing below command. 1 docker pull siddhiio/siddhi-runner-alpine:latest * Navigate to Siddhi Editor and choose File - Export File for download above Siddhi application as a file. * Move downloaded Siddhi file( HelloWorldApp.siddhi ) to a desired location (e.g. /home/me/siddhi-apps ) * Execute below command to start the Siddhi Application as a microservice. 1 2 docker run -it -p 8006:8006 -v /home/me/siddhi-apps:/apps siddhiio/siddhi-runner-alpine -Dapps=/apps/HelloWorldApp.siddhi Note: Make sure to update the /home/me/siddhi-apps with the folder path you have stored the HelloWorldApp.siddhi app. * Once container is started use below curl command to send events into \"CargoStream\" 1 2 3 curl -X POST http://localhost:8006/cargo \\ --header Content-Type:application/json \\ -d { event :{ weight :2}} * You will be able to observe outputs via logs as shown below. 1 2 [2019-04-24 08:54:51,755] INFO {io.siddhi.core.stream.output.sink.LogSink} - LOGGER : Event{timestamp=1556096091751, data=[2, 2, 2.0], isExpired=false} [2019-04-24 08:56:25,307] INFO {io.siddhi.core.stream.output.sink.LogSink} - LOGGER : Event{timestamp=1556096185307, data=[2, 4, 2.0], isExpired=false} To learn more about the Siddhi functionality, see Siddhi Documentation . If you have questions please post them on Stackoverflow with \"Siddhi\" tag.","title":"7. Running Siddhi Application as a Docker microservice"}]}