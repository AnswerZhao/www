{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"download/","text":"Siddhi 5.1.0 Download Siddhi Select the appropriate Siddhi distribution for your use case. Siddhi Distribution Daily Build Siddhi Tooling & Siddhi Runner 5.1.1 Based on Siddhi Core 5.1.8 Siddhi Tooling md5 SHA-1 asc source code Siddhi Runner md5 SHA-1 asc source code 5.1.0 Based on Siddhi Core 5.1.7 Siddhi Tooling md5 SHA-1 asc source code Siddhi Runner md5 SHA-1 asc source code Refer the user guide to use Siddhi as a Local Microservice . Siddhi Docker Daily Build Based on latest Siddhi distribution and Siddhi Core Siddhi Tooling docker pull siddhiio/siddhi-tooling:latest-dev Siddhi Runner - Alpine docker pull siddhiio/siddhi-runner-alpine:latest-dev Siddhi Runner - Ubuntu docker pull siddhiio/siddhi-runner-ubuntu:latest-dev 5.1.1 Based on Siddhi distribution 5.1.1 and Siddhi Core 5.1.8 Siddhi Tooling docker pull siddhiio/siddhi-tooling:5.1.1 Siddhi Runner - Alpine docker pull siddhiio/siddhi-runner-alpine:5.1.1 Siddhi Runner - Ubuntu docker pull siddhiio/siddhi-runner-ubuntu:5.1.1 5.1.0 Based on Siddhi distribution 5.1.0 and Siddhi Core 5.1.7 Siddhi Tooling docker pull siddhiio/siddhi-tooling:5.1.0 Siddhi Runner - Alpine docker pull siddhiio/siddhi-runner-alpine:5.1.0 Siddhi Runner - Ubuntu docker pull siddhiio/siddhi-runner-ubuntu:5.1.0 Refer the user guide to use Siddhi as a Docker Microservice . Siddhi Kubernetes Daily Build Siddhi Operator 0.2.1 Based on Siddhi distribution 5.1.1 and Siddhi Core 5.1.8 . Siddhi Operator Prerequisites Siddhi Operator 0.2.0 Based on Siddhi distribution 5.1.0 and Siddhi Core 5.1.7 . Siddhi Operator Prerequisites Siddhi Operator Refer the user guide to use Siddhi as Kubernetes Microservice . PySiddhi 5.1.0 Supported Platforms: MacOS, Linux Instruction for installation Refer the guide to use PySiddhi . PySiddhi 5.1.0 release can be integrated with below Siddhi SDK releases. You can select the SDK version based on the Siddhi distribution release. Siddhi SDK 5.1.0 which is based on Siddhi 5.1.0 release (Siddhi Core 5.1.7 ). Siddhi SDK 5.1.1 which is based on Siddhi 5.1.1 release (Siddhi Core 5.1.8 ). Siddhi Libs 5.1.x Siddhi Core Siddhi Query API Siddhi Query Compiler Siddhi Annotation Refer the user guide to use Siddhi as a Java library . For other Siddhi Versions refer the Download Archives .","title":"Download"},{"location":"download/#siddhi-510-download-siddhi","text":"Select the appropriate Siddhi distribution for your use case.","title":"Siddhi 5.1.0 Download Siddhi"},{"location":"download/#siddhi-distribution","text":"","title":"Siddhi Distribution"},{"location":"download/#daily-build","text":"Siddhi Tooling & Siddhi Runner","title":"Daily Build"},{"location":"download/#511","text":"Based on Siddhi Core 5.1.8 Siddhi Tooling md5 SHA-1 asc source code Siddhi Runner md5 SHA-1 asc source code","title":"5.1.1"},{"location":"download/#510","text":"Based on Siddhi Core 5.1.7 Siddhi Tooling md5 SHA-1 asc source code Siddhi Runner md5 SHA-1 asc source code Refer the user guide to use Siddhi as a Local Microservice .","title":"5.1.0"},{"location":"download/#siddhi-docker","text":"","title":"Siddhi Docker"},{"location":"download/#daily-build_1","text":"Based on latest Siddhi distribution and Siddhi Core Siddhi Tooling docker pull siddhiio/siddhi-tooling:latest-dev Siddhi Runner - Alpine docker pull siddhiio/siddhi-runner-alpine:latest-dev Siddhi Runner - Ubuntu docker pull siddhiio/siddhi-runner-ubuntu:latest-dev","title":"Daily Build"},{"location":"download/#511_1","text":"Based on Siddhi distribution 5.1.1 and Siddhi Core 5.1.8 Siddhi Tooling docker pull siddhiio/siddhi-tooling:5.1.1 Siddhi Runner - Alpine docker pull siddhiio/siddhi-runner-alpine:5.1.1 Siddhi Runner - Ubuntu docker pull siddhiio/siddhi-runner-ubuntu:5.1.1","title":"5.1.1"},{"location":"download/#510_1","text":"Based on Siddhi distribution 5.1.0 and Siddhi Core 5.1.7 Siddhi Tooling docker pull siddhiio/siddhi-tooling:5.1.0 Siddhi Runner - Alpine docker pull siddhiio/siddhi-runner-alpine:5.1.0 Siddhi Runner - Ubuntu docker pull siddhiio/siddhi-runner-ubuntu:5.1.0 Refer the user guide to use Siddhi as a Docker Microservice .","title":"5.1.0"},{"location":"download/#siddhi-kubernetes","text":"","title":"Siddhi Kubernetes"},{"location":"download/#daily-build_2","text":"Siddhi Operator","title":"Daily Build"},{"location":"download/#021","text":"Based on Siddhi distribution 5.1.1 and Siddhi Core 5.1.8 . Siddhi Operator Prerequisites Siddhi Operator","title":"0.2.1"},{"location":"download/#020","text":"Based on Siddhi distribution 5.1.0 and Siddhi Core 5.1.7 . Siddhi Operator Prerequisites Siddhi Operator Refer the user guide to use Siddhi as Kubernetes Microservice .","title":"0.2.0"},{"location":"download/#pysiddhi","text":"","title":"PySiddhi"},{"location":"download/#510_2","text":"Supported Platforms: MacOS, Linux Instruction for installation Refer the guide to use PySiddhi . PySiddhi 5.1.0 release can be integrated with below Siddhi SDK releases. You can select the SDK version based on the Siddhi distribution release. Siddhi SDK 5.1.0 which is based on Siddhi 5.1.0 release (Siddhi Core 5.1.7 ). Siddhi SDK 5.1.1 which is based on Siddhi 5.1.1 release (Siddhi Core 5.1.8 ).","title":"5.1.0"},{"location":"download/#siddhi-libs","text":"","title":"Siddhi Libs"},{"location":"download/#51x","text":"Siddhi Core Siddhi Query API Siddhi Query Compiler Siddhi Annotation Refer the user guide to use Siddhi as a Java library . For other Siddhi Versions refer the Download Archives .","title":"5.1.x"},{"location":"release-notes/","text":"Release Notes Siddhi Distribution Releases Siddhi Distribution 5.1.1 Features Improvements Provide snapshot persistence support for Amazon S3 buckets. (#657) Support partial app creation for in-memory bridged apps. (#649) Bug Fixes Add proper names to zip files exported through Docker/K8S export wizard. (#613) Use file separator instead of / in Docker and K8s export files. (#614) Fix retry mechanism deploying a successful app again and again. (#615) Fix parsing Siddhi apps with zero Siddhi query elements. (#643) Avoid removing sink/sources while converting in-mem transports. (#650) Remove editor file system based hot deployment. (#651) Add triple quotes for highlighting rules. (#664) Exception occurs in the Siddhi-Editor while sending events through event simulator. (#668) ) Fix simulator listing siddhi apps with template variables as FAULTY. (#678) Use user-given topic for inMemory bridges during parsing. (#675) Refresh explorer simulator after file import. (#681) Autofocus fails in search bar of the Import Sample option in editor. (#682) Suppress log if the feed simulation is deployed with no relevant siddhi app in the workspace. (#684) Highlighted Change This release onwards, debug feature is removed from Siddhi editor runtime. (#669) Complete Changes Please find the complete changes here Please find more details about the release here Please find the details of the corresponding docker release here Siddhi Distribution 5.1.0 Highlights Siddhi 5.1.0 distribution release contains the features and bug fixes done in Siddhi core 5.1.x releases. Other than that, it provides a lot of new features and improvements that help for the cloud native stream processing deployments. Please find more details in below. Features Improvements Caching support to retain on-demand queries provided by users through editor. (#499) Add alerts for the docker push process. (#507) Improve logging and default selections in K8/Docker export. (#516) Add validations to the export docker config step. (#526) Improve auto completion in editor to include latest keywords. (#528) Support relative path for -Dapps and -Dconfig parameters. (#543) Make templated Siddhi apps collapsable. (#592) Add alert dialog to see the console if artifacts are pushed to Docker. (#598) Docker image push support and set values for templated variables in Siddhi apps (#485) Improvements to perform list operations with events. Check here Add styling for template option in Docker/Kubernetes export dialog (#442) Add side panel to the editor which allows the user to set values for templated variables in Siddhi apps (#446) Deprecate existing query API and introduce on-demand queries (#481) Add preview support to receive/publish gRpc events. (#396) Skip empty jar/bundle selections in Docker/Kubernetes export. (#367) UI/UX improvements in k8/Docker export feature. (#373) , (#369) Improve startup scripts carbon.sh and carbon.bat to support JDK 11. (#394) Add support for offset in siddhi parser (#291) Add overload param support for source view editor (#310) Improve design view to show the connection between *-call-request and *-call-response IOs. (#310) Feature to support downloading docker and Kubernetes artifacts from Tooling UI (#349) Bug Fixes Fix samples not loading in welcome page. (#491) Remove unnecessary console logs. (#494) Fix multiple template value showing issue. (#495) Docker build fails due to in-proper . handling of user inputs. (#496) Avoid syntax validation while app is running and validate when stopped. (#501) More Info on Bug Fixes Refactor gRPC related samples. (#502) Fix UI issues in Docker/Kubernets export. (#504) Add proper error handling to Docker/Kubernets export backend services. (#503) Maintain templated Siddhi app in runtime without overriding with the populated app. (#505) Fix console reconnection issue in editor. (#506) Show only the started siddhi apps in on-demand query dialog box. (#522) Avoid passing empty values for unset variables in Docker/Kubernetes export. (#525) Prettify error messages in editor console. (#531) Sample test client failures in windows environment. (#534) Log error trace (if any) while trying to start Siddhi app from editor. (#542) Jar to Bundle conversion failure in Windows environment. (#554) Maintain a single Siddhi manager and reuse it across components. (#556) Faulty Siddhi app not recovers even after the valid changes. (#559) Siddhi app cannot be saved is the app name annotation is not given. (#563) Docker push is not working in Widnows environment. (#565) CSS issue when deleting elements in editor design view. (#571) Simulator controls are not get disabled when app becomes faulty. (#574) Case sensitivity issue in fault stream annotations. (#578) Fix K8s unexpected character issue and set default messaging. (#583) Issue in docker export with 'Could not acquire image ID or digest following build'. (#587) Duplicate ports being added to docker file and readme. (#589) Docker export does not list all the Siddhi apps. (#591) Avoid parser creating multiple passthrough queries. (#593) Add validation in docker export to select either download or push. (#597) Duplicate stream definitions being added to Siddhi topology. (#599) Bug fixes related to Docker/Kubernetes artifacts export features. (#372) Change export file names and replace init script from the Dockerfile. (#383) Remove unnecessary and unprotected API from runner distribution. (#389) Bug: Switching from design view to source view after changing any element causes the conversion of triple-quotes into single ones in the avro scheme definition. (#353) Fix for snakeyaml dependency issue. (#310) Fix design view toggle button position (#243) Complete Changes Please find the complete changes here Please find more details about the release here Please find the details of the corresponding docker release here Siddhi Core Libraries Releases Siddhi Core 5.1.8 Features Improvements Support mapping transport properties to any data type. (#1560) Bug Fixes NPE being thrown intermittently while removing bundles in shutdown process. (#1541) Avoid triggers to start before other elements. (#1543) SiddhiQL.g4 grammar file not building with antlr. (#1550) Event chunk breaks for scatter-gather data processing with joins. (#1559) Complete Changes Please find the complete changes here Please find more details about the release here Siddhi Core 5.1.7 Bug Fixes Fix aggregation definition error logs. (#1538) Complete Changes Please find the complete changes here Please find more details about the release here Siddhi Core 5.1.6 Highlights Bug fixes related to Siddhi sink retry implementation, logging and aggregations. Most importantly, it contains a bug fix for the Siddhi extension loading issue in slow environments. Features Improvements Improve error with context when updating environment variable (#1523) Bug Fixes Fix reconnection logic when publish() always throw connection unavailable exception. (#1525) Fix Siddhi extension loading issue in slow environments. (#1529) Fix the inconsistent behaviour aggregation optimization. (#1533) Complete Changes Please find the complete changes here Please find more details about the release here Siddhi Core 5.1.5 Highlights Bug fixes covering various execution parts of Siddhi; mainly it contains fixes related to in-memory event table, error handling, extension loading, and event synchronization. Features Improvements Improve logs for duplicate extension additions (#1521) Code refactoring changes to rename store query to On-Demand query (#1506) Bug Fixes Fix for NPE when using stream name to refer to attributes in aggregation join queries. (#1503) Fix update or insert operation in InMemoryTable for EventChunks. (#1497) , (#1512) Fix for extension loading issue in certain OS environments (slow environments) (#1507) Bug fixes related to error handling in Triggers (#1515) Stop running on-demand queries if the Siddhi app has shut down (#1515) Fix input handler being silent when siddhi app is not running (Throw error when input handler used without staring the Siddhi App runtime) (#1518) Fix synchronization issues BaseIncrementalValueStore class (#1520) Complete Changes Please find the complete changes here Please find more details about the release here Siddhi Core 5.1.4 Highlights Improvements related to @index annotation usage in stores and some dependency upgrades. Features Improvements Change the behavior of in-memory tables to support multiple '@index' annotations. (#1491) Bug Fixes Fix NPE when count() AttributeFunction is used (#1485) Complete Changes Please find the complete changes here Please find more details about the release here Siddhi Core 5.1.3 Highlights Improvements done for use cases such as throttling, continuous testing integration and error handling. Features Improvements Introduce RESET processing mode to preserve memory optimization. (#1444) Add support YAML Config Manager for easy setting of system properties in SiddhiManager through a YAML file (#1446) Support to create a Sandbox SiddhiAppRuntime for testing purposes (#1451) Improve convert function to provide message cause for Throwable objects (#1463) Support a way to retrieve the sink options and type at sink mapper. (#1473) Support error handling (log/wait/fault-stream) when event sinks publish data asynchronously. (#1473) Bug Fixes Fixes to TimeBatchWindow to process events in a streaming manner, when it's enabled to send current events in streaming mode. This makes sure all having conditions are matched against the output, whereby allowing users to effectively implement throttling use cases with alert suppression. (#1441) Complete Changes Please find the complete changes here Please find more details about the release here Siddhi Core 5.1.2 Highlights There is an improvement done for Template Builder by removing Java Message Format dependency since it is causing some inconsistencies with performing custom mapping for float, double and long values. Due to this fix, there might be some differences (corrected proper output) in the output that you get for custom output mapping with Text, XML, JSON, and CSV. ( #1431 ) There is a behavioral change introduced with the improvements done with ( #1421 ). When counting patterns are used such as e1=StockStream 2:8 and when they are referred without indexes such as e1.price it collects the price values from all the events in the counting pattern e1 and produces it as a list. Since the list is not native to Siddhi the attribute will have the object as its type. In older Siddhi version, it will output the last matching event\u2019s attribute value. Features Improvements SiddhiManager permits user-defined data to be propagated throughout the stack ( #1406 ) API to check whether the Siddhi App is stateful or not ( #1413 ) Support outputting the events collected in counting-pattern as a list ( #1421 ) Support API docs having multiline code segments ( #1430 ) Improve TemplateBuilder remove Java MessageFormat dependency ( #1431 ) Support pattern \u2018every\u2019 clause containing multiple state elements with within condition ( #1435 ) Bug Fixes Siddhi Error Handlers not getting engaged ( #1419 ) Incremental persistence to work on Windows Environment ( 9c37b0d8fc8ce271551d4106bb20231334846f59 ) Complete Changes Please find the complete changes here Please find more details about the release here Siddhi Core 5.1.1 Features Improvements Siddhi store join query optimizations ( #1382 ) Bug Fixes Log Rolling when aggregation query runs when Database is down ( #1380 ) Fix to avoid API changes introduced for Siddhi store implementation in Siddhi 5.1.0 ( #1388 ) Counting pattern issue with \u2018every\u2019 ( #1392 ) Complete Changes Please find the complete changes here Please find more details about the release here Siddhi Core 5.1.0 Features Improvements Minor improvements related to error messages used for the no param case when paramOverload annotation is in place. ( #1375 ) Complete Changes Please find the complete changes here Please find more details about the release here Siddhi K8s Operator Releases Siddhi Operator 0.2.1 Bug Fixes Enable users to view deployed partial Siddhi apps directly using SiddhiProcess (#101) Enable users to view the READY status when pods become available (#80) Add initial pending state and intermediate updating state to the SiddhiProcess custom resource object (#90) Please find more details about the release here Siddhi Operator 0.2.0 Siddhi team is excited to announce the Siddhi Operator Release 0.2.0. Please find the major improvements and features introduced in this release. Compatibility Support There are specification changes in Siddhi Process Custom Resource Definition. You have to use siddhi.io/v1alpha2 custom resources with this release. Features Improvements SiddhiProcess Spec Changes from 0.1.1 Aggregate previous apps and query specs to a single spec called apps . apps: - configMap: app - script: |- @App:name(\"MonitorApp\") @App:description(\"Description of the plan\") @sink(type='log', prefix='LOGGER') @source( type='http', receiver.url='http://0.0.0.0:8080/example', basic.auth.enabled='false', @map(type='json') ) define stream DevicePowerStream (type string, deviceID string, power int); @sink(type='log', prefix='LOGGER') define stream MonitorDevicesPowerStream(sumPower long); @info(name='monitored-filter') from DevicePowerStream#window.time(100 min) select sum(power) as sumPower insert all events into MonitorDevicesPowerStream; Replace previous pod spec with the container spec. container: env: - name: RECEIVER_URL value: \"http://0.0.0.0:8080/example\" - name: BASIC_AUTH_ENABLED value: \"false\" - name: NATS_URL value: \"nats://siddhi-nats:4222\" - name: NATS_DEST value: siddhi - name: NATS_CLUSTER_ID value: siddhi-stan image: \"siddhiio/siddhi-runner-ubuntu:latest\" The imagePullSecret under pod spec which was in previous releases move to the upper level in the YAML. (i.e Directly under the spec of CR ) Remove previous tls spec. Now you can configure ingress TLS secret using the siddhi-operator-config config map. Change YAML naming convention to the Camel case. messagingSystem: type: nats config: bootstrapServers: - \"nats://nats-siddhi:4222\" streamingClusterId: stan-siddhi persistentVolumeClaim: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi storageClassName: standard volumeMode: Filesystem Enable the version controlling to the SiddhiProcesses. (#57) , (#66) NGINX ingress 0.22.0+ support. Adding readiness and liveness probes to the Siddhi runner. (#46) Change previous static Siddhi parser to a dynamic one which embedded with the Siddhi runner. (#71) Bug Fixes Failover Deployment support related features (#33) Operator crashes the when NATS unavailable in the cluster (#50) Versioning in siddhi application level (#42) Getting segmentation fault error when creating PVC automatically (#86) Stateful Siddhi Application fails deployment if persistence volume is unavailable (#92) Please find more details about the release here","title":"Release Notes"},{"location":"release-notes/#release-notes","text":"","title":"Release Notes"},{"location":"release-notes/#siddhi-distribution-releases","text":"","title":"Siddhi Distribution Releases"},{"location":"release-notes/#siddhi-distribution-511","text":"","title":"Siddhi Distribution 5.1.1"},{"location":"release-notes/#features-improvements","text":"Provide snapshot persistence support for Amazon S3 buckets. (#657) Support partial app creation for in-memory bridged apps. (#649)","title":"Features &amp; Improvements"},{"location":"release-notes/#bug-fixes","text":"Add proper names to zip files exported through Docker/K8S export wizard. (#613) Use file separator instead of / in Docker and K8s export files. (#614) Fix retry mechanism deploying a successful app again and again. (#615) Fix parsing Siddhi apps with zero Siddhi query elements. (#643) Avoid removing sink/sources while converting in-mem transports. (#650) Remove editor file system based hot deployment. (#651) Add triple quotes for highlighting rules. (#664) Exception occurs in the Siddhi-Editor while sending events through event simulator. (#668) ) Fix simulator listing siddhi apps with template variables as FAULTY. (#678) Use user-given topic for inMemory bridges during parsing. (#675) Refresh explorer simulator after file import. (#681) Autofocus fails in search bar of the Import Sample option in editor. (#682) Suppress log if the feed simulation is deployed with no relevant siddhi app in the workspace. (#684)","title":"Bug Fixes"},{"location":"release-notes/#highlighted-change","text":"This release onwards, debug feature is removed from Siddhi editor runtime. (#669)","title":"Highlighted Change"},{"location":"release-notes/#complete-changes","text":"Please find the complete changes here Please find more details about the release here Please find the details of the corresponding docker release here","title":"Complete Changes"},{"location":"release-notes/#siddhi-distribution-510","text":"","title":"Siddhi Distribution 5.1.0"},{"location":"release-notes/#highlights","text":"Siddhi 5.1.0 distribution release contains the features and bug fixes done in Siddhi core 5.1.x releases. Other than that, it provides a lot of new features and improvements that help for the cloud native stream processing deployments. Please find more details in below.","title":"Highlights"},{"location":"release-notes/#features-improvements_1","text":"Caching support to retain on-demand queries provided by users through editor. (#499) Add alerts for the docker push process. (#507) Improve logging and default selections in K8/Docker export. (#516) Add validations to the export docker config step. (#526) Improve auto completion in editor to include latest keywords. (#528) Support relative path for -Dapps and -Dconfig parameters. (#543) Make templated Siddhi apps collapsable. (#592) Add alert dialog to see the console if artifacts are pushed to Docker. (#598) Docker image push support and set values for templated variables in Siddhi apps (#485) Improvements to perform list operations with events. Check here Add styling for template option in Docker/Kubernetes export dialog (#442) Add side panel to the editor which allows the user to set values for templated variables in Siddhi apps (#446) Deprecate existing query API and introduce on-demand queries (#481) Add preview support to receive/publish gRpc events. (#396) Skip empty jar/bundle selections in Docker/Kubernetes export. (#367) UI/UX improvements in k8/Docker export feature. (#373) , (#369) Improve startup scripts carbon.sh and carbon.bat to support JDK 11. (#394) Add support for offset in siddhi parser (#291) Add overload param support for source view editor (#310) Improve design view to show the connection between *-call-request and *-call-response IOs. (#310) Feature to support downloading docker and Kubernetes artifacts from Tooling UI (#349)","title":"Features &amp; Improvements"},{"location":"release-notes/#bug-fixes_1","text":"Fix samples not loading in welcome page. (#491) Remove unnecessary console logs. (#494) Fix multiple template value showing issue. (#495) Docker build fails due to in-proper . handling of user inputs. (#496) Avoid syntax validation while app is running and validate when stopped. (#501) More Info on Bug Fixes Refactor gRPC related samples. (#502) Fix UI issues in Docker/Kubernets export. (#504) Add proper error handling to Docker/Kubernets export backend services. (#503) Maintain templated Siddhi app in runtime without overriding with the populated app. (#505) Fix console reconnection issue in editor. (#506) Show only the started siddhi apps in on-demand query dialog box. (#522) Avoid passing empty values for unset variables in Docker/Kubernetes export. (#525) Prettify error messages in editor console. (#531) Sample test client failures in windows environment. (#534) Log error trace (if any) while trying to start Siddhi app from editor. (#542) Jar to Bundle conversion failure in Windows environment. (#554) Maintain a single Siddhi manager and reuse it across components. (#556) Faulty Siddhi app not recovers even after the valid changes. (#559) Siddhi app cannot be saved is the app name annotation is not given. (#563) Docker push is not working in Widnows environment. (#565) CSS issue when deleting elements in editor design view. (#571) Simulator controls are not get disabled when app becomes faulty. (#574) Case sensitivity issue in fault stream annotations. (#578) Fix K8s unexpected character issue and set default messaging. (#583) Issue in docker export with 'Could not acquire image ID or digest following build'. (#587) Duplicate ports being added to docker file and readme. (#589) Docker export does not list all the Siddhi apps. (#591) Avoid parser creating multiple passthrough queries. (#593) Add validation in docker export to select either download or push. (#597) Duplicate stream definitions being added to Siddhi topology. (#599) Bug fixes related to Docker/Kubernetes artifacts export features. (#372) Change export file names and replace init script from the Dockerfile. (#383) Remove unnecessary and unprotected API from runner distribution. (#389) Bug: Switching from design view to source view after changing any element causes the conversion of triple-quotes into single ones in the avro scheme definition. (#353) Fix for snakeyaml dependency issue. (#310) Fix design view toggle button position (#243)","title":"Bug Fixes"},{"location":"release-notes/#complete-changes_1","text":"Please find the complete changes here Please find more details about the release here Please find the details of the corresponding docker release here","title":"Complete Changes"},{"location":"release-notes/#siddhi-core-libraries-releases","text":"","title":"Siddhi Core Libraries Releases"},{"location":"release-notes/#siddhi-core-518","text":"","title":"Siddhi Core 5.1.8"},{"location":"release-notes/#features-improvements_2","text":"Support mapping transport properties to any data type. (#1560)","title":"Features &amp; Improvements"},{"location":"release-notes/#bug-fixes_2","text":"NPE being thrown intermittently while removing bundles in shutdown process. (#1541) Avoid triggers to start before other elements. (#1543) SiddhiQL.g4 grammar file not building with antlr. (#1550) Event chunk breaks for scatter-gather data processing with joins. (#1559)","title":"Bug Fixes"},{"location":"release-notes/#complete-changes_2","text":"Please find the complete changes here Please find more details about the release here","title":"Complete Changes"},{"location":"release-notes/#siddhi-core-517","text":"","title":"Siddhi Core 5.1.7"},{"location":"release-notes/#bug-fixes_3","text":"Fix aggregation definition error logs. (#1538)","title":"Bug Fixes"},{"location":"release-notes/#complete-changes_3","text":"Please find the complete changes here Please find more details about the release here","title":"Complete Changes"},{"location":"release-notes/#siddhi-core-516","text":"","title":"Siddhi Core 5.1.6"},{"location":"release-notes/#highlights_1","text":"Bug fixes related to Siddhi sink retry implementation, logging and aggregations. Most importantly, it contains a bug fix for the Siddhi extension loading issue in slow environments.","title":"Highlights"},{"location":"release-notes/#features-improvements_3","text":"Improve error with context when updating environment variable (#1523)","title":"Features &amp; Improvements"},{"location":"release-notes/#bug-fixes_4","text":"Fix reconnection logic when publish() always throw connection unavailable exception. (#1525) Fix Siddhi extension loading issue in slow environments. (#1529) Fix the inconsistent behaviour aggregation optimization. (#1533)","title":"Bug Fixes"},{"location":"release-notes/#complete-changes_4","text":"Please find the complete changes here Please find more details about the release here","title":"Complete Changes"},{"location":"release-notes/#siddhi-core-515","text":"","title":"Siddhi Core 5.1.5"},{"location":"release-notes/#highlights_2","text":"Bug fixes covering various execution parts of Siddhi; mainly it contains fixes related to in-memory event table, error handling, extension loading, and event synchronization.","title":"Highlights"},{"location":"release-notes/#features-improvements_4","text":"Improve logs for duplicate extension additions (#1521) Code refactoring changes to rename store query to On-Demand query (#1506)","title":"Features &amp; Improvements"},{"location":"release-notes/#bug-fixes_5","text":"Fix for NPE when using stream name to refer to attributes in aggregation join queries. (#1503) Fix update or insert operation in InMemoryTable for EventChunks. (#1497) , (#1512) Fix for extension loading issue in certain OS environments (slow environments) (#1507) Bug fixes related to error handling in Triggers (#1515) Stop running on-demand queries if the Siddhi app has shut down (#1515) Fix input handler being silent when siddhi app is not running (Throw error when input handler used without staring the Siddhi App runtime) (#1518) Fix synchronization issues BaseIncrementalValueStore class (#1520)","title":"Bug Fixes"},{"location":"release-notes/#complete-changes_5","text":"Please find the complete changes here Please find more details about the release here","title":"Complete Changes"},{"location":"release-notes/#siddhi-core-514","text":"","title":"Siddhi Core 5.1.4"},{"location":"release-notes/#highlights_3","text":"Improvements related to @index annotation usage in stores and some dependency upgrades.","title":"Highlights"},{"location":"release-notes/#features-improvements_5","text":"Change the behavior of in-memory tables to support multiple '@index' annotations. (#1491)","title":"Features &amp; Improvements"},{"location":"release-notes/#bug-fixes_6","text":"Fix NPE when count() AttributeFunction is used (#1485)","title":"Bug Fixes"},{"location":"release-notes/#complete-changes_6","text":"Please find the complete changes here Please find more details about the release here","title":"Complete Changes"},{"location":"release-notes/#siddhi-core-513","text":"","title":"Siddhi Core 5.1.3"},{"location":"release-notes/#highlights_4","text":"Improvements done for use cases such as throttling, continuous testing integration and error handling.","title":"Highlights"},{"location":"release-notes/#features-improvements_6","text":"Introduce RESET processing mode to preserve memory optimization. (#1444) Add support YAML Config Manager for easy setting of system properties in SiddhiManager through a YAML file (#1446) Support to create a Sandbox SiddhiAppRuntime for testing purposes (#1451) Improve convert function to provide message cause for Throwable objects (#1463) Support a way to retrieve the sink options and type at sink mapper. (#1473) Support error handling (log/wait/fault-stream) when event sinks publish data asynchronously. (#1473)","title":"Features &amp; Improvements"},{"location":"release-notes/#bug-fixes_7","text":"Fixes to TimeBatchWindow to process events in a streaming manner, when it's enabled to send current events in streaming mode. This makes sure all having conditions are matched against the output, whereby allowing users to effectively implement throttling use cases with alert suppression. (#1441)","title":"Bug Fixes"},{"location":"release-notes/#complete-changes_7","text":"Please find the complete changes here Please find more details about the release here","title":"Complete Changes"},{"location":"release-notes/#siddhi-core-512","text":"","title":"Siddhi Core 5.1.2"},{"location":"release-notes/#highlights_5","text":"There is an improvement done for Template Builder by removing Java Message Format dependency since it is causing some inconsistencies with performing custom mapping for float, double and long values. Due to this fix, there might be some differences (corrected proper output) in the output that you get for custom output mapping with Text, XML, JSON, and CSV. ( #1431 ) There is a behavioral change introduced with the improvements done with ( #1421 ). When counting patterns are used such as e1=StockStream 2:8 and when they are referred without indexes such as e1.price it collects the price values from all the events in the counting pattern e1 and produces it as a list. Since the list is not native to Siddhi the attribute will have the object as its type. In older Siddhi version, it will output the last matching event\u2019s attribute value.","title":"Highlights"},{"location":"release-notes/#features-improvements_7","text":"SiddhiManager permits user-defined data to be propagated throughout the stack ( #1406 ) API to check whether the Siddhi App is stateful or not ( #1413 ) Support outputting the events collected in counting-pattern as a list ( #1421 ) Support API docs having multiline code segments ( #1430 ) Improve TemplateBuilder remove Java MessageFormat dependency ( #1431 ) Support pattern \u2018every\u2019 clause containing multiple state elements with within condition ( #1435 )","title":"Features &amp; Improvements"},{"location":"release-notes/#bug-fixes_8","text":"Siddhi Error Handlers not getting engaged ( #1419 ) Incremental persistence to work on Windows Environment ( 9c37b0d8fc8ce271551d4106bb20231334846f59 )","title":"Bug Fixes"},{"location":"release-notes/#complete-changes_8","text":"Please find the complete changes here Please find more details about the release here","title":"Complete Changes"},{"location":"release-notes/#siddhi-core-511","text":"","title":"Siddhi Core 5.1.1"},{"location":"release-notes/#features-improvements_8","text":"Siddhi store join query optimizations ( #1382 )","title":"Features &amp; Improvements"},{"location":"release-notes/#bug-fixes_9","text":"Log Rolling when aggregation query runs when Database is down ( #1380 ) Fix to avoid API changes introduced for Siddhi store implementation in Siddhi 5.1.0 ( #1388 ) Counting pattern issue with \u2018every\u2019 ( #1392 )","title":"Bug Fixes"},{"location":"release-notes/#complete-changes_9","text":"Please find the complete changes here Please find more details about the release here","title":"Complete Changes"},{"location":"release-notes/#siddhi-core-510","text":"","title":"Siddhi Core 5.1.0"},{"location":"release-notes/#features-improvements_9","text":"Minor improvements related to error messages used for the no param case when paramOverload annotation is in place. ( #1375 )","title":"Features &amp; Improvements"},{"location":"release-notes/#complete-changes_10","text":"Please find the complete changes here Please find more details about the release here","title":"Complete Changes"},{"location":"release-notes/#siddhi-k8s-operator-releases","text":"","title":"Siddhi K8s Operator Releases"},{"location":"release-notes/#siddhi-operator-021","text":"","title":"Siddhi Operator 0.2.1"},{"location":"release-notes/#bug-fixes_10","text":"Enable users to view deployed partial Siddhi apps directly using SiddhiProcess (#101) Enable users to view the READY status when pods become available (#80) Add initial pending state and intermediate updating state to the SiddhiProcess custom resource object (#90) Please find more details about the release here","title":"Bug Fixes"},{"location":"release-notes/#siddhi-operator-020","text":"Siddhi team is excited to announce the Siddhi Operator Release 0.2.0. Please find the major improvements and features introduced in this release.","title":"Siddhi Operator 0.2.0"},{"location":"release-notes/#compatibility-support","text":"There are specification changes in Siddhi Process Custom Resource Definition. You have to use siddhi.io/v1alpha2 custom resources with this release.","title":"Compatibility &amp; Support"},{"location":"release-notes/#features-improvements_10","text":"SiddhiProcess Spec Changes from 0.1.1 Aggregate previous apps and query specs to a single spec called apps . apps: - configMap: app - script: |- @App:name(\"MonitorApp\") @App:description(\"Description of the plan\") @sink(type='log', prefix='LOGGER') @source( type='http', receiver.url='http://0.0.0.0:8080/example', basic.auth.enabled='false', @map(type='json') ) define stream DevicePowerStream (type string, deviceID string, power int); @sink(type='log', prefix='LOGGER') define stream MonitorDevicesPowerStream(sumPower long); @info(name='monitored-filter') from DevicePowerStream#window.time(100 min) select sum(power) as sumPower insert all events into MonitorDevicesPowerStream; Replace previous pod spec with the container spec. container: env: - name: RECEIVER_URL value: \"http://0.0.0.0:8080/example\" - name: BASIC_AUTH_ENABLED value: \"false\" - name: NATS_URL value: \"nats://siddhi-nats:4222\" - name: NATS_DEST value: siddhi - name: NATS_CLUSTER_ID value: siddhi-stan image: \"siddhiio/siddhi-runner-ubuntu:latest\" The imagePullSecret under pod spec which was in previous releases move to the upper level in the YAML. (i.e Directly under the spec of CR ) Remove previous tls spec. Now you can configure ingress TLS secret using the siddhi-operator-config config map. Change YAML naming convention to the Camel case. messagingSystem: type: nats config: bootstrapServers: - \"nats://nats-siddhi:4222\" streamingClusterId: stan-siddhi persistentVolumeClaim: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi storageClassName: standard volumeMode: Filesystem Enable the version controlling to the SiddhiProcesses. (#57) , (#66) NGINX ingress 0.22.0+ support. Adding readiness and liveness probes to the Siddhi runner. (#46) Change previous static Siddhi parser to a dynamic one which embedded with the Siddhi runner. (#71)","title":"Features &amp; Improvements"},{"location":"release-notes/#bug-fixes_11","text":"Failover Deployment support related features (#33) Operator crashes the when NATS unavailable in the cluster (#50) Versioning in siddhi application level (#42) Getting segmentation fault error when creating PVC automatically (#86) Stateful Siddhi Application fails deployment if persistence volume is unavailable (#92) Please find more details about the release here","title":"Bug Fixes"},{"location":"development/","text":"Siddhi 5.1 Development Guide Obtaining and Building Project Source code Find the project source code here and the instruction to building the project repos here . Getting Involved in Project Development Siddhi design-related discussions are carried out in the Siddhi-Dev Google Group , you can subscribe to it to get notifications on the discussions and please feel free to get involved by contributing and sharing your thoughts and ideas. You can also propose changes or improvements by starting a thread in the Siddhi-Dev Google Group, and also by reporting issues in the Siddhi GitHub repository with the label type/improvement or type/new-feature . Project Architecture Find out about the architecture of Siddhi for the Siddhi Architecture documentation.","title":"Introduction"},{"location":"development/#siddhi-51-development-guide","text":"","title":"Siddhi 5.1 Development Guide"},{"location":"development/#obtaining-and-building-project-source-code","text":"Find the project source code here and the instruction to building the project repos here .","title":"Obtaining and Building Project Source code"},{"location":"development/#getting-involved-in-project-development","text":"Siddhi design-related discussions are carried out in the Siddhi-Dev Google Group , you can subscribe to it to get notifications on the discussions and please feel free to get involved by contributing and sharing your thoughts and ideas. You can also propose changes or improvements by starting a thread in the Siddhi-Dev Google Group, and also by reporting issues in the Siddhi GitHub repository with the label type/improvement or type/new-feature .","title":"Getting Involved in Project Development"},{"location":"development/#project-architecture","text":"Find out about the architecture of Siddhi for the Siddhi Architecture documentation.","title":"Project Architecture"},{"location":"development/architecture/","text":"Siddhi 5.1 Architecture Siddhi is an open source, cloud-native, stream processing and complex event processing engine. It can be utilized in any of the following ways: Run as a server on its own Run as a micro service on bare metal, VM, Docker and natively in Kubernetes Embedded into any Java or Python based application Run on an Android application Siddhi provides streaming data integration and data analytical operators. It connects multiple disparate live data sources, orchestrates data flows, calculates analytics, and also detects complex event patterns. This allows developers to build applications that collect data, perform data transformation and analytics, and publish the results to data sinks in real time. This section illustrates the architecture of the Siddhi Engine and guides you through its key functionality. We hope this article helps developers to understand Siddhi and its codebase better, and also help them to contribute and improve Siddhi. Main Design Decisions Event-by-event processing of real-time streaming data to achieve low latency. Ease of use with Streaming SQL providing an intuitive way to express stream processing logic and complex event processing constructs such as Patterns. Achieve high performance by processing events in-memory and using data stores for long term data storage. Optimize performance by enforcing a strict event stream schema and by pre-compiling the queries. Optimize memory consumption by having only the absolutely necessary information in-memory and dropping the rest as soon as possible. Supporting multiple extension points to accommodate a diverse set of functionality such as supporting multiple sources, sinks, functions, aggregation operations, windows, etc. High-Level Architecture At a high level, Siddhi consumes events from various events sources, processes them according to the defined Siddhi application, and produces results to the subscribed event sinks. Siddhi can store and consume events from in-memory tables or from external data stores such as RDBMS , MongoDB , Hazelcast in-memory grid, etc. (i.e., when configured to do so). Siddhi also allows applications and users to query Siddhi via its On-Demand Query API to interactively retrieve data from in-memory and other stores. Main Modules in Siddhi Engine Siddhi Engine comprises four main modules, they are: Siddhi Query API : This allows users to define the execution logic of the Siddhi application as queries and definitions using POJOs (Plain Old Java Objects). Internally, Siddhi uses these objects to identify the logic that it is expected to perform. Siddhi Query Compiler : This allows users to define the Siddhi application using the Siddhi Streaming SQL, and it compiles the Streaming SQL script to Siddhi Query API POJOs so that Siddhi can execute them. Siddhi Core : This builds the execution runtime based on the defined Siddhi Application POJOs and processes the incoming events as and when they arrive. Siddhi Annotation : This is a helper module that allows all extensions to be annotated so that they can be picked by Siddhi Core for processing. This also helps Siddhi to generate the extension documentation. Siddhi Component Architecture The following diagram illustrates the main components of Siddhi and how they work together. Here the Siddhi Core module maintains the execution logic. It also interacts with the external environment and systems for consuming, processing and publishing events. It uses the following components to achieve its tasks: SiddhiManager : This is a key component of Siddhi Core that manages Siddhi Application Runtimes and facilitates their functionality via Siddhi Context with periodic state persistence, statistics reporting and extension loading. It is recommended to use one Siddhi Manager for a single JVM. SiddhiAppRuntime : Siddhi Application Runtime can be generated for each Siddhi Application through the Siddhi Manager. Siddhi Application Runtimes provide an isolated execution environment for each defined Siddhi Application. These Siddhi Application Runtimes can have their own lifecycle and they execute based on the logic defined in their Siddhi Application. SiddhiContext : This is a shared object across all the Siddhi Application Runtimes within the same Siddhi manager. It contains references to the persistence store for periodic persistence, statistics manager to report performance statistics of Siddhi Application Runtimes, and extension holders for loading Siddhi extensions. Siddhi Application Creation Execution logic of the Siddhi Engine is composed as a Siddhi Application, and this is usually passed as a string to SiddhiManager to create the SiddhiAppRuntime for execution. When a Siddhi Application is passed to the SiddhiManager.createSiddhiAppRuntime() , it is processed internally with the SiddhiCompiler . Here, the SiddhiApp String is compiled to SiddhiApp object model by the SiddhiQLBaseVisitorImpl class. This validates the syntax of the given Siddhi Application. The model is then passed to the SiddhiAppParser to create the SiddhiAppRuntime . During this phase, the semantics of the Siddhi Application is validated and the execution logic of the Siddhi Application is optimized. Siddhi App Execution Flow Following diagram depicts the execution flow within a Siddhi App Runtime. The path taken by events within Siddhi Engine is indicated in blue. The components that are involved in handling the events are the following: StreamJunction This routes events of a particular stream to various components within the Siddhi App Runtime. A stream junction is generated for each defined or inferred Stream in the Siddhi Application. A stream junction by default uses the incoming event's thread and passes all the events to its subscribed components as soon as they arrive, but this behaviour can be altered by configuring @Async annotation to buffer the events at the and stream junction and to use another one or more threads to collect the events from the buffer and process the subsequent executions. InputHandler Input handler is used to push Event and Event[] objects into stream junctions from defined event sources, and from Java/Python programmes. StreamCallback This receives Event[] s from stream junction and passes them to event sinks to publish to external endpoints, and/or passes them to subscribed Java/Python programmes for further processing. Queries Partitions These components process events by filtering, transforming, aggregating, joining, pattern matching, etc. They consume events from one or more stream junctions, process them and publish the processed events into a set of stream junctions based on the defined queries or partitions. Source Sources consume events from external sources in various data formats, convert them into Siddhi events using SourceMapper s and pass them to corresponding stream junction via their associated input handlers. A source is generated for each @Source annotation defined above a stream definition. SourceMapper A source mapper is a sub-component of source, and it needs to be configured for each source in order to convert the incoming event into Siddhi event. The source mapper type can be configured using the @Map annotation within the @Source annotation. When the @Map annotation is not defined, Siddhi uses the PassThroughSourceMapper , where it assumes that the incoming message is already in the Siddhi Event format (i.e Event or Event[] ), and therefore makes no changes to the incoming event format. Sink Sinks consumes events from its associated stream junction, convert them to various data formats via SinkMapper and publish them to external endpoints as defined in the @Sink annotation. A sink is generated for each @Sink annotation defined above a stream definition. SinkMapper A sink mapper is a sub-component of sink. and its need to be configured for each sink in order to map the Siddhi events to the specified data format so that they can be published via the sink. The sink mapper type can be configured using the @Map annotation within the @Sink annotation. When the @Map annotation is not defined, Siddhi uses PassThroughSinkMapper , where it passes the Siddhi Event (i.e Event or Event[] ) without any formatting to the Sink. Table Tables are used to store events. When tables are defined by default, Siddhi uses the InMemoryTable implementation to store events in-memory. When @Store annotation is used on top of the table definition, it loads the associated external data store connector based on the defined store type. Most table implementations are extended from either AbstractRecordTable or AbstractQueryableRecordTable abstract classes the former provides the functionality to query external data store based on a given filtering condition, and the latter queries external data store by providing projection, limits, and ordering parameters in addition to data filter condition. Window Windows store events as and when they arrive and automatically expire/clean them based on the given window constraint. Multiple types of windows are can be implemented by extending the WindowProcessor abstract class. IncrementalAggregation Long running time series aggregates defined via the aggregation definition is calculated in an incremental manner using the Incremental Aggregation Processor for the defined time periods. Incremental aggregation functions can be implemented by extending IncrementalAttributeAggregator . By default, incremental aggregations aggregate all the values in-memory, but when it is associated with a store by adding @store annotation it uses in-memory to aggregate partial results and uses data stores to persist those increments. When requested for aggregate results it retrieves data from data stores and (if needed from) in-memory, computes combined aggregate results and provides as the output. Trigger A trigger triggers events at a given interval as given in the trigger definition. The triggered events are pushed to a stream junction having the same name as the trigger. QueryCallback A query callback taps into the events that are emitted by a particular query. It notifies the event occurrence timestamp and classifies the output events into currentEvents , and expiredEvents . Siddhi Query Execution Siddhi QueryRuntimes can be categorized into three main types: SingleInputStream : Queries that consist of query types such as filters and windows. JoinInputStream : Queries that consist of joins. StateInputStream : Queries that consist of patterns and sequences. The following section explains the internals of each query type. SingleInputStream Query Runtime (Filter Windows) A single input stream query runtime is generated for filter and window queries. They consume events from a stream junction or a window and convert the incoming events according to the expected output stream format at the ProcessStreamReceiver by dropping all the unrelated incoming stream attributes. Then the converted events are passed through a few Processors such as FilterProcessor , StreamProcessor , StreamFunctionProcessor , WindowProcessor , and QuerySelector . Here, the StreamProcessor , StreamFunctionProcessor , and WindowProcessor can be extended with various stream processing capabilities. The last processor of the chain of processors must always be a QuerySelector and it can't appear anywhere else. When the query runtime consumes events from a stream, its processor chain can maximum contain one WindowProcessor , and when query runtime consumes events from a window, its chain of processors cannot contain any WindowProcessor . The FilterProcessor is implemented using expressions that return a boolean value. ExpressionExecutor is used to process conditions, mathematical operations, unary operations, constant values, variables, and functions. Expressions have a tree structure, and they are processed based using the Depth First search algorithm. To achieve high performance, Siddhi currently depends on the user to formulate the least successful case in the leftmost side of the condition, thereby increasing the chance of early false detection. The condition expression price = 100 and ( Symbol == 'IBM' or Symbol == 'MSFT' ) is represented as shown below. These expressions also support the execution of user-defined functions (UDFs), and they can be implemented by extending the FunctionExecutor class. After getting processed by all the processors, events reach the QuerySelector for transformation. At the QuerySelector , events are transformed based on the select clause of the query. The select clause produces one AttributeProcessor for each output stream attribute, and these AttributeProcessor s contain expressions defining data transformation including constant values, variables, user-defined functions, etc. They can also contain AttributeAggregatorExecutor s to process aggregation operations such as sum , count , etc. If there is a Group By clause defined, then the GroupByKeyGenerator is used to identify the composite group-by key, and then for each key, an AttributeAggregatorExecutor state is generated to maintain per group-by key aggregations. When each time AttributeProcessor is executed the AttributeAggregatorExecutor calculates per group-by aggregation results and output the values. When AttributeAggregatorExecutor group-by states become obsolete, they are destroyed and automatically cleaned. After an event is transformed to the output format through the above process, it is evaluated against the having condition executor if a having clause is provided. The succeeding events are then ordered, and limited based on order by , limit and offset clauses before they pushed to the OutputRateLimiter . At OutputRateLimiter , the event output is controlled before sending the events to the stream junction or to the query callback. When the output clause is not defined, the PassThroughOutputRateLimiter is used by passing all the events without any rate limiting. Temporal Processing with Windows The temporal event processing aspect is achieved via Window and AttributeAggregators To achieve temporal processing, Siddhi uses the following four type of events: Current Events : Events that are newly arriving to the query from streams. Expired Events : Events that have expired from a window. Timer Events : Events that inform the query about an update of execution time. These events are usually generated by schedulers. Reset Events : Events that resets the Siddhi query states. In Siddhi, when an event comes into a WindowProcessor , it creates an appropriate expired event corresponding to the incoming current event with the expiring timestamp, and stores that event in the window. At the same time, WindowProcessor also forwards the current event to the next processor for further processing. It uses a scheduler or some other counting approach to determine when to emit the events that are stored in in-memory. When the expired events meet the condition for expiry based on the window contains, it emits the expired events to the next processor. At times like in window.timeBatch() there can be cases that need emitting all the events in-memory at once and the output does not need individual expired events values, in this cases the window emits a single reset event instead of sending one expired event for each event it has stored, so that it can reset the states in one go. For the QuerySelector aggregations to work correctly the window must emit a corresponding expired event for each current event it has emitted or it must send a reset event . In the QuerySelector , the arrived current events increase the aggregation values, expired events decrease the values, and reset events reset the aggregation calculation to produce correct query output. For example, the sliding TimeWindow ( window.time() ) creates a corresponding expired event for each current event that arrives, adds the expired event s to the window, adds an entry to the scheduler to notify when that event need to be expired, and finally sends the current event to the next processor for subsequent processing. The scheduler notifies the window by sending a timer event , and when the window receives an indication that the expected expiry time has come for the oldest event in the window via a timer event or by other means, it removes the expired event from the window and passes that to the next processor. JoinInputStream Query Runtime (Join) Join input stream query runtime is generated for join queries. This can consume events from two stream junctions and perform a join operation as depicted above. It can also perform a join by consuming events from one stream junction and join against itself, or it can also join against a table, window or an aggregation. When a join is performed with a table, window or aggregation, the WindowProcessor in the above image is replaced with the corresponding table, window or aggregation and no basic processors are used on their side. The joining operation is triggered by the events that arrive from the stream junction. Here, when an event from one stream reaches the pre JoinProcessor , it matches against all the available events of the other stream's WindowProcessor . When a match is found, those matched events are sent to the QuerySelector as current events , and at the same time, the original event is added to the WindowProcessor where it remains until it expires. Similarly, when an event expires from the WindowProcessor , it matches against all the available events of the other stream's WindowProcessor , and when a match is found, those matched events are sent to the QuerySelector as expired events . Note Despite the optimizations, a join query is quite expensive when it comes to performance. This is because the WindowProcessor is locked during the matching process to avoid race conditions and to achieve accuracy while joining. Therefore, when possible avoid matching large (time or length) windows in high volume streams. StateInputStream Query Runtime (Pattern Sequence) The state input stream query runtime is generated for pattern and sequence queries. This consumes events from one or more stream junctions via ProcessStreamReceiver s and checks whether the events match each pattern or sequence condition by processing the set of basic processors associated with each ProcessStreamReceiver . The PreStateProcessor s usually contains lists of state events that are already matched by previous conditions, and if its the first condition then it will have an empty state event in its list. When ProcessStreamReceiver consumes an event, it passes the event to the PreStateProcessor which updates the list of state events it has with the incoming event and executes the condition by passing the events to the basic processors. The state events that match the conditions reach the PostStateProcessor which will then stores the events to the state event list of the following PreStateProcessor . If it is the final condition's PostStateProcessor , then it will pass the state event to the QuerySelector to generate and emit the output. Siddhi Partition Execution A partition is a wrapper around one or more Siddhi queries and inner streams that connect them. A partition is implemented in Siddhi as a PartitionRuntime which contains multiple QueryRuntime s and inner stream junctions. Each partitioned stream entering the partition goes through a designated PartitionStreamReceiver . The PartitionExecutor of PartitionStreamReceiver evaluates the incoming events to identify their associated partition-key using either RangePartitionExecutor or ValuePartitionExecutor . The identified partition-key is then set as thread local variable and the event is passed to the QueryRuntime s of processing. The QueryRuntime s process events by maintaining separate states for each partition-key such that producing separate output per partition. When a partition query consumes a non-partitioned global stream, the QueryRuntime s are executed for each available partition-key in the system such that allowing all partitions to receive the same event. When the partitions are obsolete PartitionRuntime deletes all the partition states from its QueryRuntime s. Siddhi Aggregation Siddhi supports long duration time series aggregations via its aggregation definition. AggregationRuntime implements this by the use of streaming lambda architecture , where it processes part of the data in-memory and gets part of the data from data stores. AggregationRuntime creates an in-memory table or external store for each time granularity (i.e seconds, minutes, days, etc) it has to process the events, and when events enter it calculates the aggregations in-memory for its least granularity (usually seconds) using the IncrementalExecutor and maintains the running aggregation values in its BaseIncrementalValueStore . At each clock end time of the granularity (end of each second) IncrementalExecutor stores the summarized values to the associated granularity table and also passes the summarized values to the IncrementalExecutor of the next granularity level, which also follows the same methodology in processing the events. Through this approach each time granularities, the current time duration will be in-memory and all the historical time durations will be in stored in the tables. The aggregations results are calculated by IncrementalAttributeAggregator s and stored in such a way that allows proper data composition upon retrial, for example, avg() is stored as sum and count . This allows data composition across various granularity time durations when retrieving, for example, results for avg() composed by returning sum of sum s divided by the sum of count s. Aggregation can also work in a distributed manner and across system restarts. This is done by storing node specific IDs and granularity time duration information in the tables. To make sure tables do not go out of memory IncrementalDataPurger is used to purge old data. When aggregation is queried through join or on-demand query for a given time granularity it reads the data from the in-memory BaseIncrementalValueStore and from the tables computes the composite results as described, and presents the results. Siddhi Event Formats Siddhi has three event formats. Event This is the format exposed to external systems when they send events via Input Handler and consume events via Stream Callback or Query Callback. This consists of a timestamp and an Object[] that contains all the values in accordance to the corresponding stream. StreamEvent (Subtype of ComplexEvent ) This is used within queries. This contains a timestamp and the following three Object[] s: beforeWindowData : This contains values that are only used in processors that are executed before the WindowProcessor . onAfterWindowData : This contains values that are only used by the WindowProcessor and the other processors that follow it, but not sent as output. outputData : This contains the values that are sent via the output stream of the query. In order to optimize the amount of data that is stored in the in-memory at windows, the content in beforeWindowData is cleared before the event enters the WindowProcessor . StreamEvents can also be chained by linking each other via the next property in them. StateEvent (Subtype of ComplexEvent ) This is used in joins, patterns and sequences queries when we need to associate events of multiple streams, tables, windows or aggregations together. This contains a timestamp , a collection of StreamEvent s representing different streams, tables, etc, that are used in the query, and an Object[] to contain outputData values that are needed for query output. The StreamEvent s within the StateEvent and the StateEvent themselves can be chained by linking each other with the next property in them. Event Chunks Event Chunks provide an easier way of manipulating the chain of StreamEvent s and StateEvent s so that they are be easily iterated, inserted and removed. Summary This article focuses on describing the architecture of Siddhi and rationalizing some of the architectural decisions made when implementing the system. It also explains the key features of Siddhi. We hope this will be a good starting point for new developers to understand Siddhi and to start contributing to it.","title":"Architecture"},{"location":"development/architecture/#siddhi-51-architecture","text":"Siddhi is an open source, cloud-native, stream processing and complex event processing engine. It can be utilized in any of the following ways: Run as a server on its own Run as a micro service on bare metal, VM, Docker and natively in Kubernetes Embedded into any Java or Python based application Run on an Android application Siddhi provides streaming data integration and data analytical operators. It connects multiple disparate live data sources, orchestrates data flows, calculates analytics, and also detects complex event patterns. This allows developers to build applications that collect data, perform data transformation and analytics, and publish the results to data sinks in real time. This section illustrates the architecture of the Siddhi Engine and guides you through its key functionality. We hope this article helps developers to understand Siddhi and its codebase better, and also help them to contribute and improve Siddhi.","title":"Siddhi 5.1 Architecture"},{"location":"development/architecture/#main-design-decisions","text":"Event-by-event processing of real-time streaming data to achieve low latency. Ease of use with Streaming SQL providing an intuitive way to express stream processing logic and complex event processing constructs such as Patterns. Achieve high performance by processing events in-memory and using data stores for long term data storage. Optimize performance by enforcing a strict event stream schema and by pre-compiling the queries. Optimize memory consumption by having only the absolutely necessary information in-memory and dropping the rest as soon as possible. Supporting multiple extension points to accommodate a diverse set of functionality such as supporting multiple sources, sinks, functions, aggregation operations, windows, etc.","title":"Main Design Decisions"},{"location":"development/architecture/#high-level-architecture","text":"At a high level, Siddhi consumes events from various events sources, processes them according to the defined Siddhi application, and produces results to the subscribed event sinks. Siddhi can store and consume events from in-memory tables or from external data stores such as RDBMS , MongoDB , Hazelcast in-memory grid, etc. (i.e., when configured to do so). Siddhi also allows applications and users to query Siddhi via its On-Demand Query API to interactively retrieve data from in-memory and other stores.","title":"High-Level Architecture"},{"location":"development/architecture/#main-modules-in-siddhi-engine","text":"Siddhi Engine comprises four main modules, they are: Siddhi Query API : This allows users to define the execution logic of the Siddhi application as queries and definitions using POJOs (Plain Old Java Objects). Internally, Siddhi uses these objects to identify the logic that it is expected to perform. Siddhi Query Compiler : This allows users to define the Siddhi application using the Siddhi Streaming SQL, and it compiles the Streaming SQL script to Siddhi Query API POJOs so that Siddhi can execute them. Siddhi Core : This builds the execution runtime based on the defined Siddhi Application POJOs and processes the incoming events as and when they arrive. Siddhi Annotation : This is a helper module that allows all extensions to be annotated so that they can be picked by Siddhi Core for processing. This also helps Siddhi to generate the extension documentation.","title":"Main Modules in Siddhi Engine"},{"location":"development/architecture/#siddhi-component-architecture","text":"The following diagram illustrates the main components of Siddhi and how they work together. Here the Siddhi Core module maintains the execution logic. It also interacts with the external environment and systems for consuming, processing and publishing events. It uses the following components to achieve its tasks: SiddhiManager : This is a key component of Siddhi Core that manages Siddhi Application Runtimes and facilitates their functionality via Siddhi Context with periodic state persistence, statistics reporting and extension loading. It is recommended to use one Siddhi Manager for a single JVM. SiddhiAppRuntime : Siddhi Application Runtime can be generated for each Siddhi Application through the Siddhi Manager. Siddhi Application Runtimes provide an isolated execution environment for each defined Siddhi Application. These Siddhi Application Runtimes can have their own lifecycle and they execute based on the logic defined in their Siddhi Application. SiddhiContext : This is a shared object across all the Siddhi Application Runtimes within the same Siddhi manager. It contains references to the persistence store for periodic persistence, statistics manager to report performance statistics of Siddhi Application Runtimes, and extension holders for loading Siddhi extensions.","title":"Siddhi Component Architecture"},{"location":"development/architecture/#siddhi-application-creation","text":"Execution logic of the Siddhi Engine is composed as a Siddhi Application, and this is usually passed as a string to SiddhiManager to create the SiddhiAppRuntime for execution. When a Siddhi Application is passed to the SiddhiManager.createSiddhiAppRuntime() , it is processed internally with the SiddhiCompiler . Here, the SiddhiApp String is compiled to SiddhiApp object model by the SiddhiQLBaseVisitorImpl class. This validates the syntax of the given Siddhi Application. The model is then passed to the SiddhiAppParser to create the SiddhiAppRuntime . During this phase, the semantics of the Siddhi Application is validated and the execution logic of the Siddhi Application is optimized.","title":"Siddhi Application Creation"},{"location":"development/architecture/#siddhi-app-execution-flow","text":"Following diagram depicts the execution flow within a Siddhi App Runtime. The path taken by events within Siddhi Engine is indicated in blue. The components that are involved in handling the events are the following: StreamJunction This routes events of a particular stream to various components within the Siddhi App Runtime. A stream junction is generated for each defined or inferred Stream in the Siddhi Application. A stream junction by default uses the incoming event's thread and passes all the events to its subscribed components as soon as they arrive, but this behaviour can be altered by configuring @Async annotation to buffer the events at the and stream junction and to use another one or more threads to collect the events from the buffer and process the subsequent executions. InputHandler Input handler is used to push Event and Event[] objects into stream junctions from defined event sources, and from Java/Python programmes. StreamCallback This receives Event[] s from stream junction and passes them to event sinks to publish to external endpoints, and/or passes them to subscribed Java/Python programmes for further processing. Queries Partitions These components process events by filtering, transforming, aggregating, joining, pattern matching, etc. They consume events from one or more stream junctions, process them and publish the processed events into a set of stream junctions based on the defined queries or partitions. Source Sources consume events from external sources in various data formats, convert them into Siddhi events using SourceMapper s and pass them to corresponding stream junction via their associated input handlers. A source is generated for each @Source annotation defined above a stream definition. SourceMapper A source mapper is a sub-component of source, and it needs to be configured for each source in order to convert the incoming event into Siddhi event. The source mapper type can be configured using the @Map annotation within the @Source annotation. When the @Map annotation is not defined, Siddhi uses the PassThroughSourceMapper , where it assumes that the incoming message is already in the Siddhi Event format (i.e Event or Event[] ), and therefore makes no changes to the incoming event format. Sink Sinks consumes events from its associated stream junction, convert them to various data formats via SinkMapper and publish them to external endpoints as defined in the @Sink annotation. A sink is generated for each @Sink annotation defined above a stream definition. SinkMapper A sink mapper is a sub-component of sink. and its need to be configured for each sink in order to map the Siddhi events to the specified data format so that they can be published via the sink. The sink mapper type can be configured using the @Map annotation within the @Sink annotation. When the @Map annotation is not defined, Siddhi uses PassThroughSinkMapper , where it passes the Siddhi Event (i.e Event or Event[] ) without any formatting to the Sink. Table Tables are used to store events. When tables are defined by default, Siddhi uses the InMemoryTable implementation to store events in-memory. When @Store annotation is used on top of the table definition, it loads the associated external data store connector based on the defined store type. Most table implementations are extended from either AbstractRecordTable or AbstractQueryableRecordTable abstract classes the former provides the functionality to query external data store based on a given filtering condition, and the latter queries external data store by providing projection, limits, and ordering parameters in addition to data filter condition. Window Windows store events as and when they arrive and automatically expire/clean them based on the given window constraint. Multiple types of windows are can be implemented by extending the WindowProcessor abstract class. IncrementalAggregation Long running time series aggregates defined via the aggregation definition is calculated in an incremental manner using the Incremental Aggregation Processor for the defined time periods. Incremental aggregation functions can be implemented by extending IncrementalAttributeAggregator . By default, incremental aggregations aggregate all the values in-memory, but when it is associated with a store by adding @store annotation it uses in-memory to aggregate partial results and uses data stores to persist those increments. When requested for aggregate results it retrieves data from data stores and (if needed from) in-memory, computes combined aggregate results and provides as the output. Trigger A trigger triggers events at a given interval as given in the trigger definition. The triggered events are pushed to a stream junction having the same name as the trigger. QueryCallback A query callback taps into the events that are emitted by a particular query. It notifies the event occurrence timestamp and classifies the output events into currentEvents , and expiredEvents .","title":"Siddhi App Execution Flow"},{"location":"development/architecture/#siddhi-query-execution","text":"Siddhi QueryRuntimes can be categorized into three main types: SingleInputStream : Queries that consist of query types such as filters and windows. JoinInputStream : Queries that consist of joins. StateInputStream : Queries that consist of patterns and sequences. The following section explains the internals of each query type.","title":"Siddhi Query Execution"},{"location":"development/architecture/#singleinputstream-query-runtime-filter-windows","text":"A single input stream query runtime is generated for filter and window queries. They consume events from a stream junction or a window and convert the incoming events according to the expected output stream format at the ProcessStreamReceiver by dropping all the unrelated incoming stream attributes. Then the converted events are passed through a few Processors such as FilterProcessor , StreamProcessor , StreamFunctionProcessor , WindowProcessor , and QuerySelector . Here, the StreamProcessor , StreamFunctionProcessor , and WindowProcessor can be extended with various stream processing capabilities. The last processor of the chain of processors must always be a QuerySelector and it can't appear anywhere else. When the query runtime consumes events from a stream, its processor chain can maximum contain one WindowProcessor , and when query runtime consumes events from a window, its chain of processors cannot contain any WindowProcessor . The FilterProcessor is implemented using expressions that return a boolean value. ExpressionExecutor is used to process conditions, mathematical operations, unary operations, constant values, variables, and functions. Expressions have a tree structure, and they are processed based using the Depth First search algorithm. To achieve high performance, Siddhi currently depends on the user to formulate the least successful case in the leftmost side of the condition, thereby increasing the chance of early false detection. The condition expression price = 100 and ( Symbol == 'IBM' or Symbol == 'MSFT' ) is represented as shown below. These expressions also support the execution of user-defined functions (UDFs), and they can be implemented by extending the FunctionExecutor class. After getting processed by all the processors, events reach the QuerySelector for transformation. At the QuerySelector , events are transformed based on the select clause of the query. The select clause produces one AttributeProcessor for each output stream attribute, and these AttributeProcessor s contain expressions defining data transformation including constant values, variables, user-defined functions, etc. They can also contain AttributeAggregatorExecutor s to process aggregation operations such as sum , count , etc. If there is a Group By clause defined, then the GroupByKeyGenerator is used to identify the composite group-by key, and then for each key, an AttributeAggregatorExecutor state is generated to maintain per group-by key aggregations. When each time AttributeProcessor is executed the AttributeAggregatorExecutor calculates per group-by aggregation results and output the values. When AttributeAggregatorExecutor group-by states become obsolete, they are destroyed and automatically cleaned. After an event is transformed to the output format through the above process, it is evaluated against the having condition executor if a having clause is provided. The succeeding events are then ordered, and limited based on order by , limit and offset clauses before they pushed to the OutputRateLimiter . At OutputRateLimiter , the event output is controlled before sending the events to the stream junction or to the query callback. When the output clause is not defined, the PassThroughOutputRateLimiter is used by passing all the events without any rate limiting.","title":"SingleInputStream Query Runtime (Filter &amp; Windows)"},{"location":"development/architecture/#temporal-processing-with-windows","text":"The temporal event processing aspect is achieved via Window and AttributeAggregators To achieve temporal processing, Siddhi uses the following four type of events: Current Events : Events that are newly arriving to the query from streams. Expired Events : Events that have expired from a window. Timer Events : Events that inform the query about an update of execution time. These events are usually generated by schedulers. Reset Events : Events that resets the Siddhi query states. In Siddhi, when an event comes into a WindowProcessor , it creates an appropriate expired event corresponding to the incoming current event with the expiring timestamp, and stores that event in the window. At the same time, WindowProcessor also forwards the current event to the next processor for further processing. It uses a scheduler or some other counting approach to determine when to emit the events that are stored in in-memory. When the expired events meet the condition for expiry based on the window contains, it emits the expired events to the next processor. At times like in window.timeBatch() there can be cases that need emitting all the events in-memory at once and the output does not need individual expired events values, in this cases the window emits a single reset event instead of sending one expired event for each event it has stored, so that it can reset the states in one go. For the QuerySelector aggregations to work correctly the window must emit a corresponding expired event for each current event it has emitted or it must send a reset event . In the QuerySelector , the arrived current events increase the aggregation values, expired events decrease the values, and reset events reset the aggregation calculation to produce correct query output. For example, the sliding TimeWindow ( window.time() ) creates a corresponding expired event for each current event that arrives, adds the expired event s to the window, adds an entry to the scheduler to notify when that event need to be expired, and finally sends the current event to the next processor for subsequent processing. The scheduler notifies the window by sending a timer event , and when the window receives an indication that the expected expiry time has come for the oldest event in the window via a timer event or by other means, it removes the expired event from the window and passes that to the next processor.","title":"Temporal Processing with Windows"},{"location":"development/architecture/#joininputstream-query-runtime-join","text":"Join input stream query runtime is generated for join queries. This can consume events from two stream junctions and perform a join operation as depicted above. It can also perform a join by consuming events from one stream junction and join against itself, or it can also join against a table, window or an aggregation. When a join is performed with a table, window or aggregation, the WindowProcessor in the above image is replaced with the corresponding table, window or aggregation and no basic processors are used on their side. The joining operation is triggered by the events that arrive from the stream junction. Here, when an event from one stream reaches the pre JoinProcessor , it matches against all the available events of the other stream's WindowProcessor . When a match is found, those matched events are sent to the QuerySelector as current events , and at the same time, the original event is added to the WindowProcessor where it remains until it expires. Similarly, when an event expires from the WindowProcessor , it matches against all the available events of the other stream's WindowProcessor , and when a match is found, those matched events are sent to the QuerySelector as expired events . Note Despite the optimizations, a join query is quite expensive when it comes to performance. This is because the WindowProcessor is locked during the matching process to avoid race conditions and to achieve accuracy while joining. Therefore, when possible avoid matching large (time or length) windows in high volume streams.","title":"JoinInputStream Query Runtime (Join)"},{"location":"development/architecture/#stateinputstream-query-runtime-pattern-sequence","text":"The state input stream query runtime is generated for pattern and sequence queries. This consumes events from one or more stream junctions via ProcessStreamReceiver s and checks whether the events match each pattern or sequence condition by processing the set of basic processors associated with each ProcessStreamReceiver . The PreStateProcessor s usually contains lists of state events that are already matched by previous conditions, and if its the first condition then it will have an empty state event in its list. When ProcessStreamReceiver consumes an event, it passes the event to the PreStateProcessor which updates the list of state events it has with the incoming event and executes the condition by passing the events to the basic processors. The state events that match the conditions reach the PostStateProcessor which will then stores the events to the state event list of the following PreStateProcessor . If it is the final condition's PostStateProcessor , then it will pass the state event to the QuerySelector to generate and emit the output.","title":"StateInputStream Query Runtime (Pattern &amp; Sequence)"},{"location":"development/architecture/#siddhi-partition-execution","text":"A partition is a wrapper around one or more Siddhi queries and inner streams that connect them. A partition is implemented in Siddhi as a PartitionRuntime which contains multiple QueryRuntime s and inner stream junctions. Each partitioned stream entering the partition goes through a designated PartitionStreamReceiver . The PartitionExecutor of PartitionStreamReceiver evaluates the incoming events to identify their associated partition-key using either RangePartitionExecutor or ValuePartitionExecutor . The identified partition-key is then set as thread local variable and the event is passed to the QueryRuntime s of processing. The QueryRuntime s process events by maintaining separate states for each partition-key such that producing separate output per partition. When a partition query consumes a non-partitioned global stream, the QueryRuntime s are executed for each available partition-key in the system such that allowing all partitions to receive the same event. When the partitions are obsolete PartitionRuntime deletes all the partition states from its QueryRuntime s.","title":"Siddhi Partition Execution"},{"location":"development/architecture/#siddhi-aggregation","text":"Siddhi supports long duration time series aggregations via its aggregation definition. AggregationRuntime implements this by the use of streaming lambda architecture , where it processes part of the data in-memory and gets part of the data from data stores. AggregationRuntime creates an in-memory table or external store for each time granularity (i.e seconds, minutes, days, etc) it has to process the events, and when events enter it calculates the aggregations in-memory for its least granularity (usually seconds) using the IncrementalExecutor and maintains the running aggregation values in its BaseIncrementalValueStore . At each clock end time of the granularity (end of each second) IncrementalExecutor stores the summarized values to the associated granularity table and also passes the summarized values to the IncrementalExecutor of the next granularity level, which also follows the same methodology in processing the events. Through this approach each time granularities, the current time duration will be in-memory and all the historical time durations will be in stored in the tables. The aggregations results are calculated by IncrementalAttributeAggregator s and stored in such a way that allows proper data composition upon retrial, for example, avg() is stored as sum and count . This allows data composition across various granularity time durations when retrieving, for example, results for avg() composed by returning sum of sum s divided by the sum of count s. Aggregation can also work in a distributed manner and across system restarts. This is done by storing node specific IDs and granularity time duration information in the tables. To make sure tables do not go out of memory IncrementalDataPurger is used to purge old data. When aggregation is queried through join or on-demand query for a given time granularity it reads the data from the in-memory BaseIncrementalValueStore and from the tables computes the composite results as described, and presents the results.","title":"Siddhi Aggregation"},{"location":"development/architecture/#siddhi-event-formats","text":"Siddhi has three event formats. Event This is the format exposed to external systems when they send events via Input Handler and consume events via Stream Callback or Query Callback. This consists of a timestamp and an Object[] that contains all the values in accordance to the corresponding stream. StreamEvent (Subtype of ComplexEvent ) This is used within queries. This contains a timestamp and the following three Object[] s: beforeWindowData : This contains values that are only used in processors that are executed before the WindowProcessor . onAfterWindowData : This contains values that are only used by the WindowProcessor and the other processors that follow it, but not sent as output. outputData : This contains the values that are sent via the output stream of the query. In order to optimize the amount of data that is stored in the in-memory at windows, the content in beforeWindowData is cleared before the event enters the WindowProcessor . StreamEvents can also be chained by linking each other via the next property in them. StateEvent (Subtype of ComplexEvent ) This is used in joins, patterns and sequences queries when we need to associate events of multiple streams, tables, windows or aggregations together. This contains a timestamp , a collection of StreamEvent s representing different streams, tables, etc, that are used in the query, and an Object[] to contain outputData values that are needed for query output. The StreamEvent s within the StateEvent and the StateEvent themselves can be chained by linking each other with the next property in them. Event Chunks Event Chunks provide an easier way of manipulating the chain of StreamEvent s and StateEvent s so that they are be easily iterated, inserted and removed.","title":"Siddhi Event Formats"},{"location":"development/architecture/#summary","text":"This article focuses on describing the architecture of Siddhi and rationalizing some of the architectural decisions made when implementing the system. It also explains the key features of Siddhi. We hope this will be a good starting point for new developers to understand Siddhi and to start contributing to it.","title":"Summary"},{"location":"development/build/","text":"Building Siddhi 5.1 Repos Building Java Repos Prerequisites Oracle JDK 8 , OpenJDK 8 , or JDK 11 (Java 8 should be used for building in order to support both Java 8 and Java 11 at runtime) Maven 3.5.x or later version Steps to Build Get a clone or download source from Github repo, E.g. git clone https://github.com/siddhi-io/siddhi.git Run the Maven command mvn clean install from the root directory Command Description mvn clean install Build and install the artifacts into the local repository. mvn clean install -Dmaven.test.skip=true Build and install the artifacts into the local repository, without running any of the unit tests.","title":"Build"},{"location":"development/build/#building-siddhi-51-repos","text":"","title":"Building Siddhi 5.1 Repos"},{"location":"development/build/#building-java-repos","text":"","title":"Building Java Repos"},{"location":"development/build/#prerequisites","text":"Oracle JDK 8 , OpenJDK 8 , or JDK 11 (Java 8 should be used for building in order to support both Java 8 and Java 11 at runtime) Maven 3.5.x or later version","title":"Prerequisites"},{"location":"development/build/#steps-to-build","text":"Get a clone or download source from Github repo, E.g. git clone https://github.com/siddhi-io/siddhi.git Run the Maven command mvn clean install from the root directory Command Description mvn clean install Build and install the artifacts into the local repository. mvn clean install -Dmaven.test.skip=true Build and install the artifacts into the local repository, without running any of the unit tests.","title":"Steps to Build"},{"location":"development/roadmap/","text":"Siddhi Roadmap The Siddhi road map shows the key features and improvements that are in the pipeline for future releases. We have only listed the high level features and issues in below; we will certainly work on other minor improvements, bug fixes and etc\u2026 as well in future releases. Latest (Siddhi 5.1.0) Siddhi Core 5.1.7 Introduce RESET processing mode to preserve memory optimization. Support to create a Sandbox SiddhiAppRuntime for testing purposes Support error handling (log/wait/fault-stream) when event sinks publish data asynchronously. Siddhi Extension gRPC IO connector S3 IO connector GCS IO connector Execution List Connector Deduplicate support in Unique Extension Siddhi Tooling Support K8s/Docker artifacts export in Siddhi editor Overload parameter support in Siddhi source design editor CRDs to support Kubernetes deployments natively Support High-Available, Fault Tolerant Siddhi deployment with NATS Siddhi Test Framework: Provides the capability to write integration tests using Docker containers CI/CD deployment story for Siddhi Siddhi use case guides - https://siddhi.io/en/v5.1/docs/ 2019-Q4 Prometheus for metrics collection Support distributed Siddhi deployment with NATS Siddhi plugin for VSCode JDBC driver for Siddhi Query APIs Support templating Siddhi apps and configurations in Tooling 2020 + Kafka support for Siddhi K8s deployment Siddhi support in Golang Enhance management of secrets with vault services Evaluate Istio integration Allow to specify dependencies in the Siddhi Custom resource Cloud Foundry installation support If you have any queries or comments on the roadmap, please let us know via GitHub here . You can also always communicate through Google Group or Slack with us on our community page . You feedback and contribution is always welcome.","title":"Roadmap"},{"location":"development/roadmap/#siddhi-roadmap","text":"The Siddhi road map shows the key features and improvements that are in the pipeline for future releases. We have only listed the high level features and issues in below; we will certainly work on other minor improvements, bug fixes and etc\u2026 as well in future releases.","title":"Siddhi Roadmap"},{"location":"development/roadmap/#latest-siddhi-510","text":"Siddhi Core 5.1.7 Introduce RESET processing mode to preserve memory optimization. Support to create a Sandbox SiddhiAppRuntime for testing purposes Support error handling (log/wait/fault-stream) when event sinks publish data asynchronously. Siddhi Extension gRPC IO connector S3 IO connector GCS IO connector Execution List Connector Deduplicate support in Unique Extension Siddhi Tooling Support K8s/Docker artifacts export in Siddhi editor Overload parameter support in Siddhi source design editor CRDs to support Kubernetes deployments natively Support High-Available, Fault Tolerant Siddhi deployment with NATS Siddhi Test Framework: Provides the capability to write integration tests using Docker containers CI/CD deployment story for Siddhi Siddhi use case guides - https://siddhi.io/en/v5.1/docs/","title":"Latest (Siddhi 5.1.0)"},{"location":"development/roadmap/#2019-q4","text":"Prometheus for metrics collection Support distributed Siddhi deployment with NATS Siddhi plugin for VSCode JDBC driver for Siddhi Query APIs Support templating Siddhi apps and configurations in Tooling","title":"2019-Q4"},{"location":"development/roadmap/#2020","text":"Kafka support for Siddhi K8s deployment Siddhi support in Golang Enhance management of secrets with vault services Evaluate Istio integration Allow to specify dependencies in the Siddhi Custom resource Cloud Foundry installation support If you have any queries or comments on the roadmap, please let us know via GitHub here . You can also always communicate through Google Group or Slack with us on our community page . You feedback and contribution is always welcome.","title":"2020 +"},{"location":"development/source/","text":"Siddhi 5.1 Source Code Project Source Code Siddhi Core Java Library https://github.com/siddhi-io/siddhi (Java) Siddhi repo, containing the core Java libraries of Siddhi. PySiddhi https://github.com/siddhi-io/pysiddhi (Python) The Python wrapper for Siddhi core Java libraries. This depends on the siddhi-io/siddhi repo. Siddhi Local Microservice Distribution https://github.com/siddhi-io/distribution (Java) The Microservice distribution of the Siddhi Tooling and Siddhi Runtime. This depends on the siddhi-io/siddhi repo. Siddhi Docker Microservice Distribution https://github.com/siddhi-io/docker-siddhi (Docker) The Docker wrapper for the Siddhi Tooling and Siddhi Runtime. This depends on the siddhi-io/siddhi and siddhi-io/distribution repos. Siddhi Kubernetes Operator https://github.com/siddhi-io/siddhi-operator (Go) The Siddhi Kubernetes CRD repo deploying Siddhi on Kubernetes. This depends on the siddhi-io/siddhi , siddhi-io/distribution and siddhi-io/docker-siddhi repos. Siddhi Extensions Find the supported Siddhi extensions and source here","title":"Source"},{"location":"development/source/#siddhi-51-source-code","text":"","title":"Siddhi 5.1 Source Code"},{"location":"development/source/#project-source-code","text":"","title":"Project Source Code"},{"location":"development/source/#siddhi-core-java-library","text":"https://github.com/siddhi-io/siddhi (Java) Siddhi repo, containing the core Java libraries of Siddhi.","title":"Siddhi Core Java Library"},{"location":"development/source/#pysiddhi","text":"https://github.com/siddhi-io/pysiddhi (Python) The Python wrapper for Siddhi core Java libraries. This depends on the siddhi-io/siddhi repo.","title":"PySiddhi"},{"location":"development/source/#siddhi-local-microservice-distribution","text":"https://github.com/siddhi-io/distribution (Java) The Microservice distribution of the Siddhi Tooling and Siddhi Runtime. This depends on the siddhi-io/siddhi repo.","title":"Siddhi Local Microservice Distribution"},{"location":"development/source/#siddhi-docker-microservice-distribution","text":"https://github.com/siddhi-io/docker-siddhi (Docker) The Docker wrapper for the Siddhi Tooling and Siddhi Runtime. This depends on the siddhi-io/siddhi and siddhi-io/distribution repos.","title":"Siddhi Docker Microservice Distribution"},{"location":"development/source/#siddhi-kubernetes-operator","text":"https://github.com/siddhi-io/siddhi-operator (Go) The Siddhi Kubernetes CRD repo deploying Siddhi on Kubernetes. This depends on the siddhi-io/siddhi , siddhi-io/distribution and siddhi-io/docker-siddhi repos.","title":"Siddhi Kubernetes Operator"},{"location":"development/source/#siddhi-extensions","text":"Find the supported Siddhi extensions and source here","title":"Siddhi Extensions"},{"location":"docs/","text":"Siddhi 5.1 Documentation This section provides overview and information on using Siddhi. \"Siddhi\" is named after \"Event\" from the Sinhalese language. Siddhi is a stream processing and complex event processing platform that can be used to build fully-fledged event-driven applications. It can be embedded in Java and Python applications, run as a microservices on bare-metal, VM, or Docker, and run natively at scale in Kubernetes. Checkout the Siddhi features to get an idea on what it can do in brief. How Siddhi Works Siddhi Application Siddhi Application is the artifact that defines the real-time event processing logic of Siddhi as a SQL like script with .siddhi file extension. It contains consumers(sources), producers(sinks), streams, queries, tables, functions and other necessary contracts depicting how the events should be consumed, processed and published. To write Siddhi Applications using Siddhi Streaming SQL refer Siddhi Query Guide for details. For specific API information on Siddhi functions and features refer Siddhi API Guide . Find out about the supported Siddhi extensions and their versions here . Executing Siddhi Applications Siddhi can run in multiple environments as follows. As a Java Library As a Local Microservice As a Docker Microservice As a Kubernetes Microservice As a Python Library Siddhi Configurations Refer the Siddhi Config Guide for information on advance Siddhi execution configurations. System Requirements For all Siddhi execution modes following are the general system requirements. Memory - 128 MB (minimum), 500 MB (recommended), higher memory might be needed based on in-memory data stored for processing Cores - 2 cores (recommended), use lower number of cores after testing Siddhi Apps for performance JDK - 8 or 11 To build Siddhi from the Source distribution, it is necessary that you have JDK version 8 or 11 and Maven 3.0.4 or later","title":"Introduction"},{"location":"docs/#siddhi-51-documentation","text":"This section provides overview and information on using Siddhi. \"Siddhi\" is named after \"Event\" from the Sinhalese language. Siddhi is a stream processing and complex event processing platform that can be used to build fully-fledged event-driven applications. It can be embedded in Java and Python applications, run as a microservices on bare-metal, VM, or Docker, and run natively at scale in Kubernetes. Checkout the Siddhi features to get an idea on what it can do in brief.","title":"Siddhi 5.1 Documentation"},{"location":"docs/#how-siddhi-works","text":"","title":"How Siddhi Works"},{"location":"docs/#siddhi-application","text":"Siddhi Application is the artifact that defines the real-time event processing logic of Siddhi as a SQL like script with .siddhi file extension. It contains consumers(sources), producers(sinks), streams, queries, tables, functions and other necessary contracts depicting how the events should be consumed, processed and published. To write Siddhi Applications using Siddhi Streaming SQL refer Siddhi Query Guide for details. For specific API information on Siddhi functions and features refer Siddhi API Guide . Find out about the supported Siddhi extensions and their versions here .","title":"Siddhi Application"},{"location":"docs/#executing-siddhi-applications","text":"Siddhi can run in multiple environments as follows. As a Java Library As a Local Microservice As a Docker Microservice As a Kubernetes Microservice As a Python Library","title":"Executing Siddhi Applications"},{"location":"docs/#siddhi-configurations","text":"Refer the Siddhi Config Guide for information on advance Siddhi execution configurations.","title":"Siddhi Configurations"},{"location":"docs/#system-requirements","text":"For all Siddhi execution modes following are the general system requirements. Memory - 128 MB (minimum), 500 MB (recommended), higher memory might be needed based on in-memory data stored for processing Cores - 2 cores (recommended), use lower number of cores after testing Siddhi Apps for performance JDK - 8 or 11 To build Siddhi from the Source distribution, it is necessary that you have JDK version 8 or 11 and Maven 3.0.4 or later","title":"System Requirements"},{"location":"docs/config-guide/","text":"Siddhi 5.1 Config Guide Configuring Databases Applicable only for Local, Docker, and Kubernetes modes. This section is not applicable for Java and Python modes. It is recommended to configure RDBMS databases as datasources under datasources section of Siddhi configuration yaml, and pass it during startup, this will allow database to reuse connections across multiple Siddhi Apps. By default Siddhi stores product-specific data in predefined embedded H2 database located in SIDDHI_RUNNER_HOME /wso2/runner/database directory. Here, the default H2 database is only suitable for development, testing, and some production environments which do not store data. However, for most production environments we recommend using industry-standard RDBMS such as Oracle, PostgreSQL, MySQL, or MSSQL. In this case users are expected to add the relevant database drivers to Siddhi's class-path. Including database drivers. The database driver corresponding to the database should be an OSGi bundle and it need to be added to SIDDHI_RUNNER_HOME /lib/ directory. If the driver is a jar then this should be converted to an OSGi bundle before adding . Converting Non OSGi drivers. If the database driver is not an OSGi bundle, then it should be converted to OSGi. Please refer Converting Jars to OSGi Bundles documentation for details. The necessary table schemas are self generated by the features themselves, other than the tables needed for statistics reporting via databases . Below are the sample datasource configuration for each supported database types: MySQL dataSources: - name: SIDDHI_TEST_DB description: The datasource used for test database jndiConfig: name: jdbc/SIDDHI_TEST_DB definition: type: RDBMS configuration: jdbcUrl: jdbc:mysql://hostname:port/testdb username: root password: root driverClassName: com.mysql.jdbc.Driver maxPoolSize: 10 idleTimeout: 60000 connectionTestQuery: SELECT 1 validationTimeout: 30000 isAutoCommit: false Oracle There are two ways to configure Oracle. If you have a System Identifier (SID), use this (older) format: jdbc:oracle:thin:@[HOST][:PORT]:SID dataSources: - name: SIDDHI_TEST_DB description: The datasource used for test database jndiConfig: name: jdbc/SIDDHI_TEST_DB definition: type: RDBMS configuration: jdbcUrl: jdbc:oracle:thin:@hostname:port:SID username: testdb password: root driverClassName: oracle.jdbc.driver.OracleDriver maxPoolSize: 10 idleTimeout: 60000 connectionTestQuery: SELECT 1 validationTimeout: 30000 isAutoCommit: false If you have an Oracle service name, use this (newer) format: jdbc:oracle:thin:@//[HOST][:PORT]/SERVICE dataSources: - name: SIDDHI_TEST_DB description: The datasource used for test database jndiConfig: name: jdbc/SIDDHI_TEST_DB definition: type: RDBMS configuration: jdbcUrl: jdbc:oracle:thin:@hostname:port/SERVICE username: testdb password: root driverClassName: oracle.jdbc.driver.OracleDriver maxPoolSize: 50 idleTimeout: 60000 connectionTestQuery: SELECT 1 validationTimeout: 30000 isAutoCommit: false PostgreSQL dataSources: - name: SIDDHI_TEST_DB description: The datasource used for test database jndiConfig: name: jdbc/SIDDHI_TEST_DB definition: type: RDBMS configuration: jdbcUrl: jdbc:postgresql://hostname:port/testdb username: root password: root driverClassName: org.postgresql.Driver maxPoolSize: 10 idleTimeout: 60000 connectionTestQuery: SELECT 1 validationTimeout: 30000 isAutoCommit: false MSSQL dataSources: - name: SIDDHI_TEST_DB description: The datasource used for test database jndiConfig: name: jdbc/SIDDHI_TEST_DB definition: type: RDBMS configuration: jdbcUrl: jdbc:sqlserver://hostname:port;databaseName=testdb username: root password: root driverClassName: com.microsoft.sqlserver.jdbc.SQLServerDriver maxPoolSize: 10 idleTimeout: 60000 connectionTestQuery: SELECT 1 validationTimeout: 30000 isAutoCommit: false Configuring Periodic State Persistence Applicable only for Local, Docker, and Kubernetes modes. This section is not applicable for Java and Python modes. This explains how to periodically persisting the state of Siddhi either into a database system or file system, in order to prevent data losses that can result from a system failure. Persistence on Database To perform periodic state persistence on a database, the database should be configured as a datasource and the relevant jdbc drivers should be added to Siddhi's class-path. Refer Database Configuration section for more information. To configure database based periodic data persistence, add statePersistence section with the following properties on the Siddhi configuration yaml, and pass that during startup. Parameter Purpose Required Value enabled This enables data persistence. true intervalInMin The time interval in minutes that defines the interval in which state of Siddhi applications should be persisted 1 revisionsToKeep The number of revisions to keep in the system. Here when a new persistence takes place, the older revisions are removed. 3 persistenceStore The persistence store io.siddhi.distribution.core.persistence.DBPersistenceStore config datasource The datasource to be used in persisting the state. The datasource should be defined in the Siddhi configuration yaml. For detailed instructions of how to configure a datasource, see Database Configuration . SIDDHI_PERSISTENCE_DB (A datasource that is defined in datasources in Siddhi configuration yaml) config table The table that should be created and used for persisting states. PERSISTENCE_TABLE The following is a sample configuration for database based state persistence. statePersistence: enabled: true intervalInMin: 1 revisionsToKeep: 3 persistenceStore: io.siddhi.distribution.core.persistence.DBPersistenceStore config: datasource: DATASOURCE NAME # A datasource with this name should be defined in datasources namespace table: TABLE NAME Persistence on File System To configure file system based periodic data persistence, add statePersistence section with the following properties on the Siddhi configuration yaml, and pass that during startup. Parameter Purpose Required Value enabled This enables data persistence. true intervalInMin The time interval in minutes that defines the interval in which state of Siddhi applications should be persisted 1 revisionsToKeep The number of revisions to keep in the system. Here when a new persistence takes place, the older revisions are removed. 3 persistenceStore The persistence store io.siddhi.distribution.core.persistence.FileSystemPersistenceStore config location A fully qualified folder location to where the revision files should be persisted. siddhi-app-persistence The following is a sample configuration for file system based state persistence. statePersistence: enabled: true intervalInMin: 1 revisionsToKeep: 2 persistenceStore: io.siddhi.distribution.core.persistence.FileSystemPersistenceStore config: location: siddhi-app-persistence Persistence on AWS-S3 To configure AWS-S3 based periodic data persistence, add statePersistence section with the following properties on the Siddhi configuration yaml, and pass that during startup. Parameter Purpose Required Value enabled This enables data persistence. true intervalInMin The time interval in minutes that defines the interval in which state of Siddhi applications should be persisted 1 revisionsToKeep The number of revisions to keep in the system. Here when a new persistence takes place, the older revisions are removed. 3 persistenceStore The persistence store io.siddhi.distribution.core.persistence.S3PersistenceStore config credentialProvideClass CredentialProviderClass name. software.amazon.awssdk.auth.credentials.ProfileCredentialsProvider config accessKey Access key of the user (only if credentialProvideClass property is not provided. *****access-key***** config secretKey Secret key of the user (only if credentialProvideClass property is not provided. *****secret-key***** config bucketName Name of the bucket where revision files should be persisted. siddhi-app-persistence config region Name of the region where bucket belongs to. us-west-2 The following are some samples configuration for aws-s3 based state persistence. Sample with credential provider class statePersistence: enabled: true intervalInMin: 1 revisionsToKeep: 2 persistenceStore: io.siddhi.distribution.core.persistence.S3PersistenceStore config: credentialProvideClass: software.amazon.awssdk.auth.credentials.ProfileCredentialsProvider region: us-west-2 bucketName: siddhi-app-persistence Sample with secret-key and access-key statePersistence: enabled: true intervalInMin: 1 revisionsToKeep: 2 persistenceStore: io.siddhi.distribution.core.persistence.S3PersistenceStore config: accessKey: access-key secretKey: secret-key region: us-west-2 bucketName: siddhi-app-persistence Configuring Siddhi Elements Applicable only for Local, Docker, and Kubernetes modes. This section is not applicable for Java and Python modes. You can configure some of there environment specific configurations in the Siddhi Configuration yaml rather than configuring in-line, such that your Siddhi Application can become potable between environments. Configuring Sources, Sinks and Stores References Multiple sources, sinks, and stores could be defined in Siddhi Configuration yaml as ref , and referred by several Siddhi Applications as described below. The following is the syntax for the configuration. refs: - ref: name: ' name ' type: ' type ' properties: property1 : value1 property2 : value2 For each separate refs you want to configure, add a sub-section named ref under the refs subsection. The ref configured in Siddhi Configuration yaml can be referred from a Siddhi Application Source as follows. @Source(ref=' name ', @map(type='json', @attributes( name='$.name', amount='$.quantity'))) define stream SweetProductionStream (name string, amount double); Similarly Sinks and Store Tables can also be configured and referred from Siddhi Apps. Example : Configuring http source using ref Following configuration defines the url and details about basic.auth , in the Siddhi Configuration yaml. refs: - ref: name: 'http-passthrough' type: 'http' properties: receiver.url: 'http://0.0.0.0:8008/sweet-production' basic.auth.enabled: false This can be referred in the Siddhi Applications as follows. @Source(ref='http-passthrough', @map(type='json', @attributes( name='$.name', amount='$.quantity'))) define stream SweetProductionStream (name string, amount double); Configuring Extensions System Parameters Siddhi extensions cater use-case specific logic that is not available by default in Siddhi. Some of these extensions have system parameter configurations to define/modify their behavior. These extensions usually have default values for the parameters, but when needed, they can be overridden by configuring the parameters in Siddhi Configuration yaml and passing it at startup. The following is the syntax for the configuration. extensions: - extension: name: extension name namespace: extension namespace properties: key : value For each separate extension you want to configure, add a sub-section named extension under the extensions subsection. Following are some examples on overriding default system properties via Siddhi Configuration yaml Example 1 : Defining service host and port for the TCP source extensions: - extension: name: tcp namespace: source properties: host: 0.0.0.0 port: 5511 Example 2 : Overwriting the default RDBMS extension configuration extensions: - extension: name: rdbms namespace: store properties: mysql.batchEnable: true mysql.batchSize: 1000 mysql.indexCreateQuery: \"CREATE INDEX {{TABLE_NAME}}_INDEX ON {{TABLE_NAME}} ({{INDEX_COLUMNS}})\" mysql.recordDeleteQuery: \"DELETE FROM {{TABLE_NAME}} {{CONDITION}}\" mysql.recordExistsQuery: \"SELECT 1 FROM {{TABLE_NAME}} {{CONDITION}} LIMIT 1\" Configuring Siddhi Properties Siddhi supports setting following properties to be specify distribution based behaviours, for instance all Named Aggregation in the distribution can be changed to Distributed Named Aggregation with the following siddhi properties. System Property Description Possible Values Optional Default Value shardId The id of the shard one of the distributed aggregation is running in. This should be unique to a single shard Any string No partitionById This allows user to enable/disable distributed aggregation for all aggregations running in one siddhi manager .(Available from v4.3.3) true/false Yes false Following is the example of setting Distributed Named Aggregation properties: partitionById : true shardId : shard1 Configuring Authentication Applicable only for Local, Docker, and Kubernetes modes. This section is not applicable for Java and Python modes. Siddhi is configured with user name admin , and password admin . This can be updated by adding related user management configuration as authentication to the Siddhi Configuration yaml, and pass it at startup. A sample authentication is as follows. # Authentication configuration authentication: type: 'local' # Type of the IdP client used userManager: adminRole: admin # Admin role which is granted all permissions userStore: # User store users: - user: username: admin password: YWRtaW4= roles: 1 roles: - role: id: 1 displayName: admin Adding Extensions and Third Party Dependencies Applicable for all modes. For certain use-cases, Siddhi might require extensions and/or third party dependencies to fulfill some characteristics that it does not provide by default. This section provides details on how to add or update extension and/or third party dependencies that is needed by Siddhi. Adding to Siddhi Java Program When running Siddhi as a Java library, the extension jars and/or third-party dependencies needed for Siddhi can be simply added to Siddhi class-path. When Maven is used as the build tool add them to the pom.xml file along with the other mandatory jars needed by Siddhi as given is Using Siddhi as a library guide. A sample on adding siddhi-io-http extension to the Maven pom.xml is as follows. !--HTTP extension-- dependency groupId org.wso2.extension.siddhi.io.http /groupId artifactId siddhi-io-http /artifactId version ${siddhi.io.http.version} /version /dependency Refer guide for more details on using Siddhi as a Java Library. Adding to Siddhi Local Microservice The most used Siddhi extensions are packed by default with the Siddhi Local Microservice distribution. To add or update Siddhi extensions and/or third-party dependencies, you can use SIDDHI_RUNNER_HOME /jars and SIDDHI_RUNNER_HOME /bundles directories. SIDDHI_RUNNER_HOME /jars directory : Maintained for Jar files which may not have their corresponding OSGi bundle implementation. These Jars will be converted as OSGI bundles and copied to Siddhi Runner distribution during server startup. SIDDHI_RUNNER_HOME /bundles directory : Maintained for OSGI bundles which you need to copy to Siddhi Runner distribution during server startup. Updates to these directories will be adapted after a server restart. Refer guide for more details on using Siddhi as Local Microservice. Adding to Siddhi Docker Microservice The most used Siddhi extensions are packed by default with the Siddhi Docker Microservice distribution. To add or update Siddhi extensions and/or third-party dependencies, a new docker image has to be built from either siddhi-runner-base-ubuntu or siddhi-runner-base-alpine images. These images contain Linux OS, JDK and the Siddhi distribution. Sample docker file using siddhi-runner-base-alpine is as follows. # use siddhi-runner-base FROM siddhiio/siddhi-runner-base-alpine:5.1.0-alpha MAINTAINER Siddhi IO Docker Maintainers \"siddhi-dev@googlegroups.com\" ARG HOST_BUNDLES_DIR=./files/bundles ARG HOST_JARS_DIR=./files/jars ARG JARS=${RUNTIME_SERVER_HOME}/jars ARG BUNDLES=${RUNTIME_SERVER_HOME}/bundles # copy bundles jars to the siddhi-runner distribution COPY --chown=siddhi_user:siddhi_io ${HOST_BUNDLES_DIR}/ ${BUNDLES} COPY --chown=siddhi_user:siddhi_io ${HOST_JARS_DIR}/ ${JARS} # expose ports EXPOSE 9090 9443 9712 9612 7711 7611 7070 7443 RUN bash ${RUNTIME_SERVER_HOME}/bin/install-jars.sh STOPSIGNAL SIGINT ENTRYPOINT [\"/home/siddhi_user/siddhi-runner/bin/runner.sh\", \"--\"] Find the necessary artifacts to build the docker from docker-siddhi repository. DOCKERFILE_HOME gt/siddhi-runner/files contains two directories (bundles and jars directories) where you can copy the Jars and Bundles you need to bundle into the docker image. Jars directory - Maintained for Jar files which may not have their corresponding OSGi bundle implementation. These Jars will be converted as OSGI bundles and copied to Siddhi Runner docker image during docker build phase. Bundles directory - Maintained for OSGI bundles which you need to copy to Siddhi Runner docker image directory during docker build phase. Refer guide for more details on using Siddhi as Docker Microservice. Adding to Siddhi Kubernetes Microservice To add or update Siddhi extensions and/or third-party dependencies, a custom docker image has to be created using the steps described in Adding to Siddhi Docker Microservice documentation including the necessary extensions and dependencies. The created image can be then referenced in the sepc.pod subsection in the SiddhiProcess Kubernetes artifact created to deploy Siddhi in Kubernetes. For details on creating the Kubernetes artifacts refer Using Siddhi as Kubernetes Microservice documentation. Configuring Statistics Applicable only for Local, Docker, and Kubernetes modes. This section is not applicable for Java and Python modes. Siddhi uses dropwizard metrics library to calculate Siddhi and JVM statistics, and it can report the results via JMX Mbeans, console or database. To enable statistics, the relevant configuration under metrics section should be added to the Siddhi Configuration yaml as follows, and at the same time the statistics collection should be enabled in the Siddhi Application which is being monitored. Refer Siddhi Application Statistics documentation for enabling Siddhi Application level statistics. Configuring Metrics reporting level. To modify the statistics reporting, relevant metric names can be added under the metrics.levels subsection in the Siddhi Configurations yaml, along with the metrics level (i.e., OFF, INFO, DEBUG, TRACE, or ALL) as given below. metrics: # Metrics Levels are organized from most specific to least: # OFF (most specific, no metrics) # INFO # DEBUG # TRACE (least specific, a lot of data) # ALL (least specific, all data) levels: # The root level configured for Metrics rootLevel: INFO # Metric Levels levels: jvm.buffers: 'OFF' jvm.class-loading: INFO jvm.gc: DEBUG jvm.memory: INFO The available metrics reporting options are as follows. Reporting via JMX Mbeans JMX Mbeans is the default statistics reporting option of Siddhi. To enable stats with the default configuration add the metric-related properties under metrics section in the Siddhi Configurations yaml file, and pass that during startup. A sample configuration is as follows. metrics: enabled: true This will report JMX Mbeans in the name of org.wso2.carbon.metrics . However, in this default configuration the JVM metrics will not be measured. A detail JMX configuration along with the metrics reporting level is as follows. metrics: # Enable Metrics enabled: true jmx: # Register MBean when initializing Metrics registerMBean: true # MBean Name name: org.wso2.carbon:type=Metrics # Metrics Levels are organized from most specific to least: # OFF (most specific, no metrics) # INFO # DEBUG # TRACE (least specific, a lot of data) # ALL (least specific, all data) levels: # The root level configured for Metrics rootLevel: INFO # Metric Levels levels: jvm.buffers: 'OFF' jvm.class-loading: INFO jvm.gc: DEBUG jvm.memory: INFO Reporting via Console To enable statistics by periodically printing the metrics on console add the following configuration to the the Siddhi Configurations yaml file, and pass that during startup. # This is the main configuration for metrics metrics: # Enable Metrics enabled: false reporting: console: - # The name for the Console Reporter name: Console # Enable Console Reporter enabled: false # Polling Period in seconds. # This is the period for polling metrics from the metric registry and printing in the console pollingPeriod: 5 Reporting via Database To enable JDBC reporting and to periodically clean up the outdated statistics from the database, first a datasource should be created with the relevant database configurations and then the related metrics properties as given below should be added to in the Siddhi Configurations yaml file, and pass that during startup. The below sample is referring to the datasource with JNDI name jdbc/SiddhiMetricsDB , hence the datasource configuration in yaml should have jndiConfig.name as jdbc/SiddhiMetricsDB . For detailed instructions on configuring a datasource, refer Configuring Databases . . The scripts to create these tables are provided in the SIDDHI_RUNNER_HOME /wso2/runner/dbscripts directory. Sample configuration of reporting via database. metrics: enabled: true jdbc: # Data Source Configurations for JDBC Reporters dataSource: - JDBC01 dataSourceName: java:comp/env/jdbc/SiddhiMetricsDB scheduledCleanup: enabled: false daysToKeep: 7 scheduledCleanupPeriod: 86400 reporting: jdbc: - # The name for the JDBC Reporter name: JDBC enabled: true dataSource: *JDBC01 pollingPeriod: 60 Metrics history and reporting interval If the metrics.reporting.jdbc subsection is not enabled, the information relating to metrics history will not be persisted for future references. Also note the that the reporting will only start to update the database after the given pollingPeriod time has elapsed. Information about the parameters configured under the jdbc.dataSource subsection in the Siddhi Configuration yaml is as follows. Parameter Default Value Description dataSourceName java:comp/env/jdbc/SiddhiMetricsDB java:comp/env/ datasource JNDI name . The JNDI name of the datasource used to store metric data. scheduledCleanup.enabled false If this is set to true, metrics data stored in the database is cleared periodically based on scheduled time interval. scheduledCleanup.daysToKeep 3 If scheduled clean-up of metric data is enabled, all metric data in the database that are older than the number of days specified in this parameter are deleted. scheduledCleanup.scheduledCleanupPeriod 86400 The parameter specifies the time interval in seconds at which metric data should be cleaned. Converting Jars to OSGi Bundles To convert jar files to OSGi bundles, first download and save the non-OSGi jar it in a preferred directory in your machine. Then from the CLI, navigate to the SIDDHI_RUNNER_HOME /bin directory, and issue the following command. ./jartobundle.sh path to non OSGi jar ../lib This converts the Jar to OSGi bundles and place it in SIDDHI_RUNNER_HOME /lib directory. Encrypt sensitive deployment configurations Cipher tool is used to encrypt sensitive data in deployment configurations. This tool works in conjunction with Secure Vault to replace sensitive data that is in plain text with an alias. The actual value is then encrypted and securely stored in the SecureVault. At runtime, the actual value is retrieved from the alias and used. For more information, see Secure Vault . Below is the default configurations for Secure Vault # Secure Vault Configuration securevault: secretRepository: type: org.wso2.carbon.secvault.repository.DefaultSecretRepository parameters: privateKeyAlias: wso2carbon keystoreLocation: ${SIDDHI_RUNNER_HOME}/resources/security/securevault.jks secretPropertiesFile: ${SIDDHI_RUNNER_HOME}/conf/runner/secrets.properties masterKeyReader: type: org.wso2.carbon.secvault.reader.DefaultMasterKeyReader parameters: masterKeyReaderFile: ${SIDDHI_RUNNER_HOME}/conf/runner/master-keys.yaml Information about the parameters configured under the securevault subsection in the Siddhi Configuration yaml is as follows. Parameter Default Value Description secretRepository type org.wso2.carbon.secvault.repository.DefaultSecretRepository The default implementation of Secret Repository is based on the passwords and aliases given in the secrets.properties file and the JKS that is configured in the secure-vault.yaml file secretPropertiesFile ${SIDDHI_RUNNER_HOME}/conf/runner/secrets.properties Location of the secrect.properties file which matches alias with encrypted data secretPropertiesFile ${SIDDHI_RUNNER_HOME}/resources/security/securevault.jks Keystore which contains the certificate to encrypt sensitive data privateKeyAlias wso2carbon Alias of the certificate in the key store used for encryption masterKeyReader type org.wso2.carbon.secvault.reader.DefaultMasterKeyReader The default implementation of MasterKeyReader gets a list of required passwords from the Secret Repository and provides the values for those passwords by reading system properties, environment variables and the master-keys.yaml file. masterKeyReaderFile ${SIDDHI_RUNNER_HOME\\}/conf/runner/master-keys.yaml Location of master-keys.yaml file which contains password used to access the key store to decrypt the encrypted passwords at runtime Configuring server properties Siddhi runner and tooling distribution is based on WSO2 Carbon 5 Kernel platform. The properties for the server can be configure under wso2.carbon namespace. Sample configurations is as follows, wso2.carbon: id: siddhi-runner name: Siddhi Runner Distribution Configure port offset Port offset defines the number by which all ports defined in the runtime such as the HTTP/S ports will be offset. For example, if the default HTTP port is 9090 and the ports offset is 1, the effective HTTP port will be 9091. This configuration allows to change ports in a uniform manner across the transports. Below is the sample configurations for offsets, wso2.carbon: id: siddhi-runner name: Siddhi Runner Distribution ports: offset: 1 Disabling host name verification Hostname verification can be disabled in Admin APIs in analytics server side, with hostnameVerificationEnabled Below is the sample configuration, wso2.carbon: id: siddhi-runner name: Siddhi Runner Distribution hostnameVerificationEnabled: false Configuring Admin REST APIs Admin API can be configured under the namespace transports http . Sample Config and the parameters are as follows, transports: http: listenerConfigurations: - id: \"default\" host: \"0.0.0.0\" port: 9090 - id: \"msf4j-https\" host: \"0.0.0.0\" port: 9443 scheme: https sslConfig: keyStore: \"${carbon.home}/resources/security/wso2carbon.jks\" keyStorePassword: wso2carbon transportProperties: - name: \"server.bootstrap.socket.timeout\" value: 60 - name: \"latency.metrics.enabled\" value: false Parameter Default Value Description id default Id of the server host 0.0.0.0 Hostname of the server port 8080 Port of the APIs scheme http Scheme of the APIs. It can be either http or https httpTraceLogEnabled false Enable HTTP trace logs httpAccessLogEnabled false Enable HTTP access logs socketIdleTimeout 0 Timeout for socket for which requests received. Not set by default. SSL configurations (listenerConfigurations sslConfig) Parameter Default Value Description keyStore ${carbon.home}/resources/security/wso2carbon.jks The file containing the private key of the client keyStorePass wso2carbon Password of the private key if it is encrypted enableProtocols All SSL/TLS protocols to be enabled (e.g.: TLSv1,TLSv1.1,TLSv1.2) cipherSuites All List of ciphers to be used eg: TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256, TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA enableSessionCreation Enable/disable new SSL session creation sessionTimeOut 0 SSL session time out. Not set by default. handshakeTimeOut 0 SSL handshake time out. Not set by default. Transport Properties (transportProperties) Parameter Default Value Description server.bootstrap.connect.timeout 15000 Timout in millisecond to establish connection server.bootstrap.socket.timeout 60 Socket connection timeouts latency.metrics.enabled false Enable/Disable latency metrics by carbon metrics component Configuring Databridge Transport Siddhi uses Databridge transport to send and receive events over Thrift/Binary protocols, This can be used through siddhi-io-wso2event extension. Sample Configuration is as follows, transports: databridge: # Configuration used for the databridge communication listenerConfigurations: workerThreads: 10 . . . senderConfigurations: # Configuration of the Data Agents - to publish events through databridge agents: agentConfiguration: name: Thrift dataEndpointClass: org.wso2.carbon.databridge.agent.endpoint.thrift.ThriftDataEndpoint . . . Here, transports databridge includes listenerConfigurations, to configure databridge receiver in WSO2Event Source, and senderConfigurations, to configure agents used to publish events over databridge in WSO2Event Sink Configuring databridge listener Sample configuration for databridge listener and properties are as follows, transports: databridge: listenerConfigurations: workerThreads: 10 maxEventBufferCapacity: 10 eventBufferSize: 2000 keyStoreLocation: ${sys:carbon.home}/resources/security/wso2carbon.jks keyStorePassword: wso2carbon clientTimeoutMin: 30 # Data receiver configurations dataReceivers: - dataReceiver: type: Thrift properties: tcpPort: '7611' sslPort: '7711' - dataReceiver: type: Binary properties: tcpPort: '9611' sslPort: '9711' tcpReceiverThreadPoolSize: '100' sslReceiverThreadPoolSize: '100' hostName: 0.0.0.0 Parameter Default Value Description workerThreads 10 No of worker threads to consume events maxEventBufferCapacity 10 Maximum amount of messages that can be queued internally in Message Buffer eventBufferSize 2000 Maximum number of events that can be stored in the queue clientTimeoutMin 30 Session timeout value in minutes keyStoreLocation ${SIDDHIRUNNER_HOME}/resources/security/wso2carbon.jks Keystore file path Keystore password wo2carbon Keystore password dataReceivers Generalised configuration for different types of data receivers dataReceivers dataReceiver type Type of the data receiver Parameters for Thrift data receiver, Parameter Default Value Description tcpPort 7611 TCP port for the Thrift data receiver sslPort 7711 SSL port for the Thrift data receiver Parameters for Binary data receiver, Parameter Default Value Description tcpPort 7611 TCP port for the Binary data receiver sslPort 7711 SSL port for the Binary data receiver tcpReceiverThreadPoolSize 100 Receiver pool size for Thrift TCP protocol sslReceiverThreadPoolSize 100 Receiver pool size for Thrift SSL protocol hostname 0.0.0.0 Hostname for the Thrift receiver Configuring databridge publisher Note By default both Thrift and Binary agents will be started. Sample configuration for databridge agent(publisher) and properties are as follows, transports: databridge: senderConfigurations: agents: - agentConfiguration: name: Thrift dataEndpointClass: org.wso2.carbon.databridge.agent.endpoint.thrift.ThriftDataEndpoint publishingStrategy: async trustStorePath: '${sys:carbon.home}/resources/security/client-truststore.jks' trustStorePassword: 'wso2carbon' queueSize: 32768 batchSize: 200 corePoolSize: 1 socketTimeoutMS: 30000 maxPoolSize: 1 keepAliveTimeInPool: 20 reconnectionInterval: 30 maxTransportPoolSize: 250 maxIdleConnections: 250 evictionTimePeriod: 5500 minIdleTimeInPool: 5000 secureMaxTransportPoolSize: 250 secureMaxIdleConnections: 250 secureEvictionTimePeriod: 5500 secureMinIdleTimeInPool: 5000 sslEnabledProtocols: TLSv1.1,TLSv1.2 ciphers: TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256,TLS_DHE_RSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA,TLS_DHE_RSA_WITH_AES_128_CBC_SHA,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_DHE_RSA_WITH_AES_128_GCM_SHA256 - agentConfiguration: name: Binary dataEndpointClass: org.wso2.carbon.databridge.agent.endpoint.binary.BinaryDataEndpoint publishingStrategy: async trustStorePath: '${sys:carbon.home}/resources/security/client-truststore.jks' trustStorePassword: 'wso2carbon' queueSize: 32768 batchSize: 200 corePoolSize: 1 socketTimeoutMS: 30000 maxPoolSize: 1 keepAliveTimeInPool: 20 reconnectionInterval: 30 maxTransportPoolSize: 250 maxIdleConnections: 250 evictionTimePeriod: 5500 minIdleTimeInPool: 5000 secureMaxTransportPoolSize: 250 secureMaxIdleConnections: 250 secureEvictionTimePeriod: 5500 secureMinIdleTimeInPool: 5000 sslEnabledProtocols: TLSv1.1,TLSv1.2 ciphers: TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256,TLS_DHE_RSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA,TLS_DHE_RSA_WITH_AES_128_CBC_SHA,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_DHE_RSA_WITH_AES_128_GCM_SHA256 Parameter Default Value Description name Thrift / Binary Name of the databridge agent dataEndpointClass org.wso2.carbon.databridge.agent.endpoint.thrift.ThriftDataEndpoint / org.wso2.carbon.databridge.agent.endpoint.thrift.ThriftDataEndpoint Class of the databridge agent initialised publishingStrategy async Strategy used for publishing. Can be either sync or async trustStorePath ${sys:carbon.home\\}/resources/security/client-truststore.jks Truststore file path trustStorePassword wso2carbon Trust store password queueSize 32768 Queue size used to hold events before publishing batchSize 200 Size of a publishing batch of events corePoolSize 1 Pool size of the threads used to buffer before publishing maxPoolSize 1 Maximum pool size for threads used to buffer before publishing socketTimeoutMS 30000 Time for socket to timeout in Milliseconds keepAliveTimeInPool 20 Time used to keep the threads live reconnectionInterval 30 Reconnection interval in case of lost transmission maxTransportPoolSize 250 Transport threads used for publishing maxIdleConnections 250 Maximum idle connections maintained in the databridge evictionTimePeriod 5500 Eviction time interval minIdleTimeInPool 5500 Min idle time in pool secureMaxTransportPoolSize 250 Max transport pool size in SSL publishing secureMaxIdleConnections 250 Max idle connections in SSL publishing secureEvictionTimePeriod 5500 Eviction time period in SSL publishing secureMinIdleTimeInPool 5500 Min idle time in pool in SSL publishing sslEnabledProtocols TLSv1.1,TLSv1.2 SSL enabled protocols ciphers TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256, TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256, TLS_DHE_RSA_WITH_AES_128_CBC_SHA256, TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA, TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA, TLS_DHE_RSA_WITH_AES_128_CBC_SHA, TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256, TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256, TLS_DHE_RSA_WITH_AES_128_GCM_SHA256 Ciphers used in transmission","title":"Configuration Guide"},{"location":"docs/config-guide/#siddhi-51-config-guide","text":"","title":"Siddhi 5.1 Config Guide"},{"location":"docs/config-guide/#configuring-databases","text":"Applicable only for Local, Docker, and Kubernetes modes. This section is not applicable for Java and Python modes. It is recommended to configure RDBMS databases as datasources under datasources section of Siddhi configuration yaml, and pass it during startup, this will allow database to reuse connections across multiple Siddhi Apps. By default Siddhi stores product-specific data in predefined embedded H2 database located in SIDDHI_RUNNER_HOME /wso2/runner/database directory. Here, the default H2 database is only suitable for development, testing, and some production environments which do not store data. However, for most production environments we recommend using industry-standard RDBMS such as Oracle, PostgreSQL, MySQL, or MSSQL. In this case users are expected to add the relevant database drivers to Siddhi's class-path. Including database drivers. The database driver corresponding to the database should be an OSGi bundle and it need to be added to SIDDHI_RUNNER_HOME /lib/ directory. If the driver is a jar then this should be converted to an OSGi bundle before adding . Converting Non OSGi drivers. If the database driver is not an OSGi bundle, then it should be converted to OSGi. Please refer Converting Jars to OSGi Bundles documentation for details. The necessary table schemas are self generated by the features themselves, other than the tables needed for statistics reporting via databases . Below are the sample datasource configuration for each supported database types: MySQL dataSources: - name: SIDDHI_TEST_DB description: The datasource used for test database jndiConfig: name: jdbc/SIDDHI_TEST_DB definition: type: RDBMS configuration: jdbcUrl: jdbc:mysql://hostname:port/testdb username: root password: root driverClassName: com.mysql.jdbc.Driver maxPoolSize: 10 idleTimeout: 60000 connectionTestQuery: SELECT 1 validationTimeout: 30000 isAutoCommit: false Oracle There are two ways to configure Oracle. If you have a System Identifier (SID), use this (older) format: jdbc:oracle:thin:@[HOST][:PORT]:SID dataSources: - name: SIDDHI_TEST_DB description: The datasource used for test database jndiConfig: name: jdbc/SIDDHI_TEST_DB definition: type: RDBMS configuration: jdbcUrl: jdbc:oracle:thin:@hostname:port:SID username: testdb password: root driverClassName: oracle.jdbc.driver.OracleDriver maxPoolSize: 10 idleTimeout: 60000 connectionTestQuery: SELECT 1 validationTimeout: 30000 isAutoCommit: false If you have an Oracle service name, use this (newer) format: jdbc:oracle:thin:@//[HOST][:PORT]/SERVICE dataSources: - name: SIDDHI_TEST_DB description: The datasource used for test database jndiConfig: name: jdbc/SIDDHI_TEST_DB definition: type: RDBMS configuration: jdbcUrl: jdbc:oracle:thin:@hostname:port/SERVICE username: testdb password: root driverClassName: oracle.jdbc.driver.OracleDriver maxPoolSize: 50 idleTimeout: 60000 connectionTestQuery: SELECT 1 validationTimeout: 30000 isAutoCommit: false PostgreSQL dataSources: - name: SIDDHI_TEST_DB description: The datasource used for test database jndiConfig: name: jdbc/SIDDHI_TEST_DB definition: type: RDBMS configuration: jdbcUrl: jdbc:postgresql://hostname:port/testdb username: root password: root driverClassName: org.postgresql.Driver maxPoolSize: 10 idleTimeout: 60000 connectionTestQuery: SELECT 1 validationTimeout: 30000 isAutoCommit: false MSSQL dataSources: - name: SIDDHI_TEST_DB description: The datasource used for test database jndiConfig: name: jdbc/SIDDHI_TEST_DB definition: type: RDBMS configuration: jdbcUrl: jdbc:sqlserver://hostname:port;databaseName=testdb username: root password: root driverClassName: com.microsoft.sqlserver.jdbc.SQLServerDriver maxPoolSize: 10 idleTimeout: 60000 connectionTestQuery: SELECT 1 validationTimeout: 30000 isAutoCommit: false","title":"Configuring Databases"},{"location":"docs/config-guide/#configuring-periodic-state-persistence","text":"Applicable only for Local, Docker, and Kubernetes modes. This section is not applicable for Java and Python modes. This explains how to periodically persisting the state of Siddhi either into a database system or file system, in order to prevent data losses that can result from a system failure.","title":"Configuring Periodic State Persistence"},{"location":"docs/config-guide/#persistence-on-database","text":"To perform periodic state persistence on a database, the database should be configured as a datasource and the relevant jdbc drivers should be added to Siddhi's class-path. Refer Database Configuration section for more information. To configure database based periodic data persistence, add statePersistence section with the following properties on the Siddhi configuration yaml, and pass that during startup. Parameter Purpose Required Value enabled This enables data persistence. true intervalInMin The time interval in minutes that defines the interval in which state of Siddhi applications should be persisted 1 revisionsToKeep The number of revisions to keep in the system. Here when a new persistence takes place, the older revisions are removed. 3 persistenceStore The persistence store io.siddhi.distribution.core.persistence.DBPersistenceStore config datasource The datasource to be used in persisting the state. The datasource should be defined in the Siddhi configuration yaml. For detailed instructions of how to configure a datasource, see Database Configuration . SIDDHI_PERSISTENCE_DB (A datasource that is defined in datasources in Siddhi configuration yaml) config table The table that should be created and used for persisting states. PERSISTENCE_TABLE The following is a sample configuration for database based state persistence. statePersistence: enabled: true intervalInMin: 1 revisionsToKeep: 3 persistenceStore: io.siddhi.distribution.core.persistence.DBPersistenceStore config: datasource: DATASOURCE NAME # A datasource with this name should be defined in datasources namespace table: TABLE NAME","title":"Persistence on Database"},{"location":"docs/config-guide/#persistence-on-file-system","text":"To configure file system based periodic data persistence, add statePersistence section with the following properties on the Siddhi configuration yaml, and pass that during startup. Parameter Purpose Required Value enabled This enables data persistence. true intervalInMin The time interval in minutes that defines the interval in which state of Siddhi applications should be persisted 1 revisionsToKeep The number of revisions to keep in the system. Here when a new persistence takes place, the older revisions are removed. 3 persistenceStore The persistence store io.siddhi.distribution.core.persistence.FileSystemPersistenceStore config location A fully qualified folder location to where the revision files should be persisted. siddhi-app-persistence The following is a sample configuration for file system based state persistence. statePersistence: enabled: true intervalInMin: 1 revisionsToKeep: 2 persistenceStore: io.siddhi.distribution.core.persistence.FileSystemPersistenceStore config: location: siddhi-app-persistence","title":"Persistence on File System"},{"location":"docs/config-guide/#persistence-on-aws-s3","text":"To configure AWS-S3 based periodic data persistence, add statePersistence section with the following properties on the Siddhi configuration yaml, and pass that during startup. Parameter Purpose Required Value enabled This enables data persistence. true intervalInMin The time interval in minutes that defines the interval in which state of Siddhi applications should be persisted 1 revisionsToKeep The number of revisions to keep in the system. Here when a new persistence takes place, the older revisions are removed. 3 persistenceStore The persistence store io.siddhi.distribution.core.persistence.S3PersistenceStore config credentialProvideClass CredentialProviderClass name. software.amazon.awssdk.auth.credentials.ProfileCredentialsProvider config accessKey Access key of the user (only if credentialProvideClass property is not provided. *****access-key***** config secretKey Secret key of the user (only if credentialProvideClass property is not provided. *****secret-key***** config bucketName Name of the bucket where revision files should be persisted. siddhi-app-persistence config region Name of the region where bucket belongs to. us-west-2 The following are some samples configuration for aws-s3 based state persistence. Sample with credential provider class statePersistence: enabled: true intervalInMin: 1 revisionsToKeep: 2 persistenceStore: io.siddhi.distribution.core.persistence.S3PersistenceStore config: credentialProvideClass: software.amazon.awssdk.auth.credentials.ProfileCredentialsProvider region: us-west-2 bucketName: siddhi-app-persistence Sample with secret-key and access-key statePersistence: enabled: true intervalInMin: 1 revisionsToKeep: 2 persistenceStore: io.siddhi.distribution.core.persistence.S3PersistenceStore config: accessKey: access-key secretKey: secret-key region: us-west-2 bucketName: siddhi-app-persistence","title":"Persistence on AWS-S3"},{"location":"docs/config-guide/#configuring-siddhi-elements","text":"Applicable only for Local, Docker, and Kubernetes modes. This section is not applicable for Java and Python modes. You can configure some of there environment specific configurations in the Siddhi Configuration yaml rather than configuring in-line, such that your Siddhi Application can become potable between environments.","title":"Configuring Siddhi Elements"},{"location":"docs/config-guide/#configuring-sources-sinks-and-stores-references","text":"Multiple sources, sinks, and stores could be defined in Siddhi Configuration yaml as ref , and referred by several Siddhi Applications as described below. The following is the syntax for the configuration. refs: - ref: name: ' name ' type: ' type ' properties: property1 : value1 property2 : value2 For each separate refs you want to configure, add a sub-section named ref under the refs subsection. The ref configured in Siddhi Configuration yaml can be referred from a Siddhi Application Source as follows. @Source(ref=' name ', @map(type='json', @attributes( name='$.name', amount='$.quantity'))) define stream SweetProductionStream (name string, amount double); Similarly Sinks and Store Tables can also be configured and referred from Siddhi Apps. Example : Configuring http source using ref Following configuration defines the url and details about basic.auth , in the Siddhi Configuration yaml. refs: - ref: name: 'http-passthrough' type: 'http' properties: receiver.url: 'http://0.0.0.0:8008/sweet-production' basic.auth.enabled: false This can be referred in the Siddhi Applications as follows. @Source(ref='http-passthrough', @map(type='json', @attributes( name='$.name', amount='$.quantity'))) define stream SweetProductionStream (name string, amount double);","title":"Configuring Sources, Sinks and Stores References"},{"location":"docs/config-guide/#configuring-extensions-system-parameters","text":"Siddhi extensions cater use-case specific logic that is not available by default in Siddhi. Some of these extensions have system parameter configurations to define/modify their behavior. These extensions usually have default values for the parameters, but when needed, they can be overridden by configuring the parameters in Siddhi Configuration yaml and passing it at startup. The following is the syntax for the configuration. extensions: - extension: name: extension name namespace: extension namespace properties: key : value For each separate extension you want to configure, add a sub-section named extension under the extensions subsection. Following are some examples on overriding default system properties via Siddhi Configuration yaml Example 1 : Defining service host and port for the TCP source extensions: - extension: name: tcp namespace: source properties: host: 0.0.0.0 port: 5511 Example 2 : Overwriting the default RDBMS extension configuration extensions: - extension: name: rdbms namespace: store properties: mysql.batchEnable: true mysql.batchSize: 1000 mysql.indexCreateQuery: \"CREATE INDEX {{TABLE_NAME}}_INDEX ON {{TABLE_NAME}} ({{INDEX_COLUMNS}})\" mysql.recordDeleteQuery: \"DELETE FROM {{TABLE_NAME}} {{CONDITION}}\" mysql.recordExistsQuery: \"SELECT 1 FROM {{TABLE_NAME}} {{CONDITION}} LIMIT 1\"","title":"Configuring Extensions System Parameters"},{"location":"docs/config-guide/#configuring-siddhi-properties","text":"Siddhi supports setting following properties to be specify distribution based behaviours, for instance all Named Aggregation in the distribution can be changed to Distributed Named Aggregation with the following siddhi properties. System Property Description Possible Values Optional Default Value shardId The id of the shard one of the distributed aggregation is running in. This should be unique to a single shard Any string No partitionById This allows user to enable/disable distributed aggregation for all aggregations running in one siddhi manager .(Available from v4.3.3) true/false Yes false Following is the example of setting Distributed Named Aggregation properties: partitionById : true shardId : shard1","title":"Configuring Siddhi Properties"},{"location":"docs/config-guide/#configuring-authentication","text":"Applicable only for Local, Docker, and Kubernetes modes. This section is not applicable for Java and Python modes. Siddhi is configured with user name admin , and password admin . This can be updated by adding related user management configuration as authentication to the Siddhi Configuration yaml, and pass it at startup. A sample authentication is as follows. # Authentication configuration authentication: type: 'local' # Type of the IdP client used userManager: adminRole: admin # Admin role which is granted all permissions userStore: # User store users: - user: username: admin password: YWRtaW4= roles: 1 roles: - role: id: 1 displayName: admin","title":"Configuring Authentication"},{"location":"docs/config-guide/#adding-extensions-and-third-party-dependencies","text":"Applicable for all modes. For certain use-cases, Siddhi might require extensions and/or third party dependencies to fulfill some characteristics that it does not provide by default. This section provides details on how to add or update extension and/or third party dependencies that is needed by Siddhi.","title":"Adding Extensions and Third Party Dependencies"},{"location":"docs/config-guide/#adding-to-siddhi-java-program","text":"When running Siddhi as a Java library, the extension jars and/or third-party dependencies needed for Siddhi can be simply added to Siddhi class-path. When Maven is used as the build tool add them to the pom.xml file along with the other mandatory jars needed by Siddhi as given is Using Siddhi as a library guide. A sample on adding siddhi-io-http extension to the Maven pom.xml is as follows. !--HTTP extension-- dependency groupId org.wso2.extension.siddhi.io.http /groupId artifactId siddhi-io-http /artifactId version ${siddhi.io.http.version} /version /dependency Refer guide for more details on using Siddhi as a Java Library.","title":"Adding to Siddhi Java Program"},{"location":"docs/config-guide/#adding-to-siddhi-local-microservice","text":"The most used Siddhi extensions are packed by default with the Siddhi Local Microservice distribution. To add or update Siddhi extensions and/or third-party dependencies, you can use SIDDHI_RUNNER_HOME /jars and SIDDHI_RUNNER_HOME /bundles directories. SIDDHI_RUNNER_HOME /jars directory : Maintained for Jar files which may not have their corresponding OSGi bundle implementation. These Jars will be converted as OSGI bundles and copied to Siddhi Runner distribution during server startup. SIDDHI_RUNNER_HOME /bundles directory : Maintained for OSGI bundles which you need to copy to Siddhi Runner distribution during server startup. Updates to these directories will be adapted after a server restart. Refer guide for more details on using Siddhi as Local Microservice.","title":"Adding to Siddhi Local Microservice"},{"location":"docs/config-guide/#adding-to-siddhi-docker-microservice","text":"The most used Siddhi extensions are packed by default with the Siddhi Docker Microservice distribution. To add or update Siddhi extensions and/or third-party dependencies, a new docker image has to be built from either siddhi-runner-base-ubuntu or siddhi-runner-base-alpine images. These images contain Linux OS, JDK and the Siddhi distribution. Sample docker file using siddhi-runner-base-alpine is as follows. # use siddhi-runner-base FROM siddhiio/siddhi-runner-base-alpine:5.1.0-alpha MAINTAINER Siddhi IO Docker Maintainers \"siddhi-dev@googlegroups.com\" ARG HOST_BUNDLES_DIR=./files/bundles ARG HOST_JARS_DIR=./files/jars ARG JARS=${RUNTIME_SERVER_HOME}/jars ARG BUNDLES=${RUNTIME_SERVER_HOME}/bundles # copy bundles jars to the siddhi-runner distribution COPY --chown=siddhi_user:siddhi_io ${HOST_BUNDLES_DIR}/ ${BUNDLES} COPY --chown=siddhi_user:siddhi_io ${HOST_JARS_DIR}/ ${JARS} # expose ports EXPOSE 9090 9443 9712 9612 7711 7611 7070 7443 RUN bash ${RUNTIME_SERVER_HOME}/bin/install-jars.sh STOPSIGNAL SIGINT ENTRYPOINT [\"/home/siddhi_user/siddhi-runner/bin/runner.sh\", \"--\"] Find the necessary artifacts to build the docker from docker-siddhi repository. DOCKERFILE_HOME gt/siddhi-runner/files contains two directories (bundles and jars directories) where you can copy the Jars and Bundles you need to bundle into the docker image. Jars directory - Maintained for Jar files which may not have their corresponding OSGi bundle implementation. These Jars will be converted as OSGI bundles and copied to Siddhi Runner docker image during docker build phase. Bundles directory - Maintained for OSGI bundles which you need to copy to Siddhi Runner docker image directory during docker build phase. Refer guide for more details on using Siddhi as Docker Microservice.","title":"Adding to Siddhi Docker Microservice"},{"location":"docs/config-guide/#adding-to-siddhi-kubernetes-microservice","text":"To add or update Siddhi extensions and/or third-party dependencies, a custom docker image has to be created using the steps described in Adding to Siddhi Docker Microservice documentation including the necessary extensions and dependencies. The created image can be then referenced in the sepc.pod subsection in the SiddhiProcess Kubernetes artifact created to deploy Siddhi in Kubernetes. For details on creating the Kubernetes artifacts refer Using Siddhi as Kubernetes Microservice documentation.","title":"Adding to Siddhi Kubernetes Microservice"},{"location":"docs/config-guide/#configuring-statistics","text":"Applicable only for Local, Docker, and Kubernetes modes. This section is not applicable for Java and Python modes. Siddhi uses dropwizard metrics library to calculate Siddhi and JVM statistics, and it can report the results via JMX Mbeans, console or database. To enable statistics, the relevant configuration under metrics section should be added to the Siddhi Configuration yaml as follows, and at the same time the statistics collection should be enabled in the Siddhi Application which is being monitored. Refer Siddhi Application Statistics documentation for enabling Siddhi Application level statistics. Configuring Metrics reporting level. To modify the statistics reporting, relevant metric names can be added under the metrics.levels subsection in the Siddhi Configurations yaml, along with the metrics level (i.e., OFF, INFO, DEBUG, TRACE, or ALL) as given below. metrics: # Metrics Levels are organized from most specific to least: # OFF (most specific, no metrics) # INFO # DEBUG # TRACE (least specific, a lot of data) # ALL (least specific, all data) levels: # The root level configured for Metrics rootLevel: INFO # Metric Levels levels: jvm.buffers: 'OFF' jvm.class-loading: INFO jvm.gc: DEBUG jvm.memory: INFO The available metrics reporting options are as follows.","title":"Configuring Statistics"},{"location":"docs/config-guide/#reporting-via-jmx-mbeans","text":"JMX Mbeans is the default statistics reporting option of Siddhi. To enable stats with the default configuration add the metric-related properties under metrics section in the Siddhi Configurations yaml file, and pass that during startup. A sample configuration is as follows. metrics: enabled: true This will report JMX Mbeans in the name of org.wso2.carbon.metrics . However, in this default configuration the JVM metrics will not be measured. A detail JMX configuration along with the metrics reporting level is as follows. metrics: # Enable Metrics enabled: true jmx: # Register MBean when initializing Metrics registerMBean: true # MBean Name name: org.wso2.carbon:type=Metrics # Metrics Levels are organized from most specific to least: # OFF (most specific, no metrics) # INFO # DEBUG # TRACE (least specific, a lot of data) # ALL (least specific, all data) levels: # The root level configured for Metrics rootLevel: INFO # Metric Levels levels: jvm.buffers: 'OFF' jvm.class-loading: INFO jvm.gc: DEBUG jvm.memory: INFO","title":"Reporting via JMX Mbeans"},{"location":"docs/config-guide/#reporting-via-console","text":"To enable statistics by periodically printing the metrics on console add the following configuration to the the Siddhi Configurations yaml file, and pass that during startup. # This is the main configuration for metrics metrics: # Enable Metrics enabled: false reporting: console: - # The name for the Console Reporter name: Console # Enable Console Reporter enabled: false # Polling Period in seconds. # This is the period for polling metrics from the metric registry and printing in the console pollingPeriod: 5","title":"Reporting via Console"},{"location":"docs/config-guide/#reporting-via-database","text":"To enable JDBC reporting and to periodically clean up the outdated statistics from the database, first a datasource should be created with the relevant database configurations and then the related metrics properties as given below should be added to in the Siddhi Configurations yaml file, and pass that during startup. The below sample is referring to the datasource with JNDI name jdbc/SiddhiMetricsDB , hence the datasource configuration in yaml should have jndiConfig.name as jdbc/SiddhiMetricsDB . For detailed instructions on configuring a datasource, refer Configuring Databases . . The scripts to create these tables are provided in the SIDDHI_RUNNER_HOME /wso2/runner/dbscripts directory. Sample configuration of reporting via database. metrics: enabled: true jdbc: # Data Source Configurations for JDBC Reporters dataSource: - JDBC01 dataSourceName: java:comp/env/jdbc/SiddhiMetricsDB scheduledCleanup: enabled: false daysToKeep: 7 scheduledCleanupPeriod: 86400 reporting: jdbc: - # The name for the JDBC Reporter name: JDBC enabled: true dataSource: *JDBC01 pollingPeriod: 60 Metrics history and reporting interval If the metrics.reporting.jdbc subsection is not enabled, the information relating to metrics history will not be persisted for future references. Also note the that the reporting will only start to update the database after the given pollingPeriod time has elapsed. Information about the parameters configured under the jdbc.dataSource subsection in the Siddhi Configuration yaml is as follows. Parameter Default Value Description dataSourceName java:comp/env/jdbc/SiddhiMetricsDB java:comp/env/ datasource JNDI name . The JNDI name of the datasource used to store metric data. scheduledCleanup.enabled false If this is set to true, metrics data stored in the database is cleared periodically based on scheduled time interval. scheduledCleanup.daysToKeep 3 If scheduled clean-up of metric data is enabled, all metric data in the database that are older than the number of days specified in this parameter are deleted. scheduledCleanup.scheduledCleanupPeriod 86400 The parameter specifies the time interval in seconds at which metric data should be cleaned.","title":"Reporting via Database"},{"location":"docs/config-guide/#converting-jars-to-osgi-bundles","text":"To convert jar files to OSGi bundles, first download and save the non-OSGi jar it in a preferred directory in your machine. Then from the CLI, navigate to the SIDDHI_RUNNER_HOME /bin directory, and issue the following command. ./jartobundle.sh path to non OSGi jar ../lib This converts the Jar to OSGi bundles and place it in SIDDHI_RUNNER_HOME /lib directory.","title":"Converting Jars to OSGi Bundles"},{"location":"docs/config-guide/#encrypt-sensitive-deployment-configurations","text":"Cipher tool is used to encrypt sensitive data in deployment configurations. This tool works in conjunction with Secure Vault to replace sensitive data that is in plain text with an alias. The actual value is then encrypted and securely stored in the SecureVault. At runtime, the actual value is retrieved from the alias and used. For more information, see Secure Vault . Below is the default configurations for Secure Vault # Secure Vault Configuration securevault: secretRepository: type: org.wso2.carbon.secvault.repository.DefaultSecretRepository parameters: privateKeyAlias: wso2carbon keystoreLocation: ${SIDDHI_RUNNER_HOME}/resources/security/securevault.jks secretPropertiesFile: ${SIDDHI_RUNNER_HOME}/conf/runner/secrets.properties masterKeyReader: type: org.wso2.carbon.secvault.reader.DefaultMasterKeyReader parameters: masterKeyReaderFile: ${SIDDHI_RUNNER_HOME}/conf/runner/master-keys.yaml Information about the parameters configured under the securevault subsection in the Siddhi Configuration yaml is as follows. Parameter Default Value Description secretRepository type org.wso2.carbon.secvault.repository.DefaultSecretRepository The default implementation of Secret Repository is based on the passwords and aliases given in the secrets.properties file and the JKS that is configured in the secure-vault.yaml file secretPropertiesFile ${SIDDHI_RUNNER_HOME}/conf/runner/secrets.properties Location of the secrect.properties file which matches alias with encrypted data secretPropertiesFile ${SIDDHI_RUNNER_HOME}/resources/security/securevault.jks Keystore which contains the certificate to encrypt sensitive data privateKeyAlias wso2carbon Alias of the certificate in the key store used for encryption masterKeyReader type org.wso2.carbon.secvault.reader.DefaultMasterKeyReader The default implementation of MasterKeyReader gets a list of required passwords from the Secret Repository and provides the values for those passwords by reading system properties, environment variables and the master-keys.yaml file. masterKeyReaderFile ${SIDDHI_RUNNER_HOME\\}/conf/runner/master-keys.yaml Location of master-keys.yaml file which contains password used to access the key store to decrypt the encrypted passwords at runtime","title":"Encrypt sensitive deployment configurations"},{"location":"docs/config-guide/#configuring-server-properties","text":"Siddhi runner and tooling distribution is based on WSO2 Carbon 5 Kernel platform. The properties for the server can be configure under wso2.carbon namespace. Sample configurations is as follows, wso2.carbon: id: siddhi-runner name: Siddhi Runner Distribution","title":"Configuring server properties"},{"location":"docs/config-guide/#configure-port-offset","text":"Port offset defines the number by which all ports defined in the runtime such as the HTTP/S ports will be offset. For example, if the default HTTP port is 9090 and the ports offset is 1, the effective HTTP port will be 9091. This configuration allows to change ports in a uniform manner across the transports. Below is the sample configurations for offsets, wso2.carbon: id: siddhi-runner name: Siddhi Runner Distribution ports: offset: 1","title":"Configure port offset"},{"location":"docs/config-guide/#disabling-host-name-verification","text":"Hostname verification can be disabled in Admin APIs in analytics server side, with hostnameVerificationEnabled Below is the sample configuration, wso2.carbon: id: siddhi-runner name: Siddhi Runner Distribution hostnameVerificationEnabled: false","title":"Disabling host name verification"},{"location":"docs/config-guide/#configuring-admin-rest-apis","text":"Admin API can be configured under the namespace transports http . Sample Config and the parameters are as follows, transports: http: listenerConfigurations: - id: \"default\" host: \"0.0.0.0\" port: 9090 - id: \"msf4j-https\" host: \"0.0.0.0\" port: 9443 scheme: https sslConfig: keyStore: \"${carbon.home}/resources/security/wso2carbon.jks\" keyStorePassword: wso2carbon transportProperties: - name: \"server.bootstrap.socket.timeout\" value: 60 - name: \"latency.metrics.enabled\" value: false Parameter Default Value Description id default Id of the server host 0.0.0.0 Hostname of the server port 8080 Port of the APIs scheme http Scheme of the APIs. It can be either http or https httpTraceLogEnabled false Enable HTTP trace logs httpAccessLogEnabled false Enable HTTP access logs socketIdleTimeout 0 Timeout for socket for which requests received. Not set by default. SSL configurations (listenerConfigurations sslConfig) Parameter Default Value Description keyStore ${carbon.home}/resources/security/wso2carbon.jks The file containing the private key of the client keyStorePass wso2carbon Password of the private key if it is encrypted enableProtocols All SSL/TLS protocols to be enabled (e.g.: TLSv1,TLSv1.1,TLSv1.2) cipherSuites All List of ciphers to be used eg: TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256, TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA enableSessionCreation Enable/disable new SSL session creation sessionTimeOut 0 SSL session time out. Not set by default. handshakeTimeOut 0 SSL handshake time out. Not set by default. Transport Properties (transportProperties) Parameter Default Value Description server.bootstrap.connect.timeout 15000 Timout in millisecond to establish connection server.bootstrap.socket.timeout 60 Socket connection timeouts latency.metrics.enabled false Enable/Disable latency metrics by carbon metrics component","title":"Configuring Admin REST APIs"},{"location":"docs/config-guide/#configuring-databridge-transport","text":"Siddhi uses Databridge transport to send and receive events over Thrift/Binary protocols, This can be used through siddhi-io-wso2event extension. Sample Configuration is as follows, transports: databridge: # Configuration used for the databridge communication listenerConfigurations: workerThreads: 10 . . . senderConfigurations: # Configuration of the Data Agents - to publish events through databridge agents: agentConfiguration: name: Thrift dataEndpointClass: org.wso2.carbon.databridge.agent.endpoint.thrift.ThriftDataEndpoint . . . Here, transports databridge includes listenerConfigurations, to configure databridge receiver in WSO2Event Source, and senderConfigurations, to configure agents used to publish events over databridge in WSO2Event Sink","title":"Configuring Databridge Transport"},{"location":"docs/config-guide/#configuring-databridge-listener","text":"Sample configuration for databridge listener and properties are as follows, transports: databridge: listenerConfigurations: workerThreads: 10 maxEventBufferCapacity: 10 eventBufferSize: 2000 keyStoreLocation: ${sys:carbon.home}/resources/security/wso2carbon.jks keyStorePassword: wso2carbon clientTimeoutMin: 30 # Data receiver configurations dataReceivers: - dataReceiver: type: Thrift properties: tcpPort: '7611' sslPort: '7711' - dataReceiver: type: Binary properties: tcpPort: '9611' sslPort: '9711' tcpReceiverThreadPoolSize: '100' sslReceiverThreadPoolSize: '100' hostName: 0.0.0.0 Parameter Default Value Description workerThreads 10 No of worker threads to consume events maxEventBufferCapacity 10 Maximum amount of messages that can be queued internally in Message Buffer eventBufferSize 2000 Maximum number of events that can be stored in the queue clientTimeoutMin 30 Session timeout value in minutes keyStoreLocation ${SIDDHIRUNNER_HOME}/resources/security/wso2carbon.jks Keystore file path Keystore password wo2carbon Keystore password dataReceivers Generalised configuration for different types of data receivers dataReceivers dataReceiver type Type of the data receiver Parameters for Thrift data receiver, Parameter Default Value Description tcpPort 7611 TCP port for the Thrift data receiver sslPort 7711 SSL port for the Thrift data receiver Parameters for Binary data receiver, Parameter Default Value Description tcpPort 7611 TCP port for the Binary data receiver sslPort 7711 SSL port for the Binary data receiver tcpReceiverThreadPoolSize 100 Receiver pool size for Thrift TCP protocol sslReceiverThreadPoolSize 100 Receiver pool size for Thrift SSL protocol hostname 0.0.0.0 Hostname for the Thrift receiver","title":"Configuring databridge listener"},{"location":"docs/config-guide/#configuring-databridge-publisher","text":"Note By default both Thrift and Binary agents will be started. Sample configuration for databridge agent(publisher) and properties are as follows, transports: databridge: senderConfigurations: agents: - agentConfiguration: name: Thrift dataEndpointClass: org.wso2.carbon.databridge.agent.endpoint.thrift.ThriftDataEndpoint publishingStrategy: async trustStorePath: '${sys:carbon.home}/resources/security/client-truststore.jks' trustStorePassword: 'wso2carbon' queueSize: 32768 batchSize: 200 corePoolSize: 1 socketTimeoutMS: 30000 maxPoolSize: 1 keepAliveTimeInPool: 20 reconnectionInterval: 30 maxTransportPoolSize: 250 maxIdleConnections: 250 evictionTimePeriod: 5500 minIdleTimeInPool: 5000 secureMaxTransportPoolSize: 250 secureMaxIdleConnections: 250 secureEvictionTimePeriod: 5500 secureMinIdleTimeInPool: 5000 sslEnabledProtocols: TLSv1.1,TLSv1.2 ciphers: TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256,TLS_DHE_RSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA,TLS_DHE_RSA_WITH_AES_128_CBC_SHA,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_DHE_RSA_WITH_AES_128_GCM_SHA256 - agentConfiguration: name: Binary dataEndpointClass: org.wso2.carbon.databridge.agent.endpoint.binary.BinaryDataEndpoint publishingStrategy: async trustStorePath: '${sys:carbon.home}/resources/security/client-truststore.jks' trustStorePassword: 'wso2carbon' queueSize: 32768 batchSize: 200 corePoolSize: 1 socketTimeoutMS: 30000 maxPoolSize: 1 keepAliveTimeInPool: 20 reconnectionInterval: 30 maxTransportPoolSize: 250 maxIdleConnections: 250 evictionTimePeriod: 5500 minIdleTimeInPool: 5000 secureMaxTransportPoolSize: 250 secureMaxIdleConnections: 250 secureEvictionTimePeriod: 5500 secureMinIdleTimeInPool: 5000 sslEnabledProtocols: TLSv1.1,TLSv1.2 ciphers: TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256,TLS_DHE_RSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA,TLS_DHE_RSA_WITH_AES_128_CBC_SHA,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_DHE_RSA_WITH_AES_128_GCM_SHA256 Parameter Default Value Description name Thrift / Binary Name of the databridge agent dataEndpointClass org.wso2.carbon.databridge.agent.endpoint.thrift.ThriftDataEndpoint / org.wso2.carbon.databridge.agent.endpoint.thrift.ThriftDataEndpoint Class of the databridge agent initialised publishingStrategy async Strategy used for publishing. Can be either sync or async trustStorePath ${sys:carbon.home\\}/resources/security/client-truststore.jks Truststore file path trustStorePassword wso2carbon Trust store password queueSize 32768 Queue size used to hold events before publishing batchSize 200 Size of a publishing batch of events corePoolSize 1 Pool size of the threads used to buffer before publishing maxPoolSize 1 Maximum pool size for threads used to buffer before publishing socketTimeoutMS 30000 Time for socket to timeout in Milliseconds keepAliveTimeInPool 20 Time used to keep the threads live reconnectionInterval 30 Reconnection interval in case of lost transmission maxTransportPoolSize 250 Transport threads used for publishing maxIdleConnections 250 Maximum idle connections maintained in the databridge evictionTimePeriod 5500 Eviction time interval minIdleTimeInPool 5500 Min idle time in pool secureMaxTransportPoolSize 250 Max transport pool size in SSL publishing secureMaxIdleConnections 250 Max idle connections in SSL publishing secureEvictionTimePeriod 5500 Eviction time period in SSL publishing secureMinIdleTimeInPool 5500 Min idle time in pool in SSL publishing sslEnabledProtocols TLSv1.1,TLSv1.2 SSL enabled protocols ciphers TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256, TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256, TLS_DHE_RSA_WITH_AES_128_CBC_SHA256, TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA, TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA, TLS_DHE_RSA_WITH_AES_128_CBC_SHA, TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256, TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256, TLS_DHE_RSA_WITH_AES_128_GCM_SHA256 Ciphers used in transmission","title":"Configuring databridge publisher"},{"location":"docs/extensions/","text":"Siddhi Extensions Siddhi provides an extension architecture to enhance its functionality by incorporating other libraries and custom logic in a seamless manner. Each extension has a namespace that can be used to identify and specifically access the functionality of the relevant extension. Following are the supported Siddhi extension types: Execution extension types Function extension. Aggregate Function extension. Window extension. Stream Function extension. Stream Processor extension. IO extension types Source extension. Sink extension. Map extension types Source Mapper extension. Sink Mapper extension. Store extension types Store extension. Script extension types Script extension. Available Extensions All the Siddhi extensions are released under Apache 2.0 License. Execution Extensions Name Description Latest Tested Version execution-string Provides basic string handling capabilities such as concat, length, replace all, etc. 5.0.7 execution-regex Provides basic RegEx execution capabilities. 5.0.5 execution-math Provides useful mathematical functions. 5.0.4 execution-time Provides time related functionality such as getting current time, current date, manipulating/formatting dates, etc. 5.0.4 execution-map Provides the capability to generate and manipulate map data objects. 5.0.4 execution-json Provides the capability to retrieve, insert, and modify JSON elements. 2.0.4 execution-unitconversion Converts various units such as length, mass, time and volume. 2.0.2 execution-reorder Orders out-of-order event arrivals using algorithms such as K-Slack and alpha K-Stack. 5.0.3 execution-unique Retains and process unique events based on the given parameters. 5.0.4 execution-list Provides the capability to send an array object inside Siddhi stream definitions and use it within queries. 1.0.0 execution-streamingml Performs streaming machine learning (clustering, classification and regression) on event streams. 2.0.3 execution-tensorflow Provides support for running pre-built TensorFlow models. 2.0.2 execution-pmml Evaluates Predictive Model Markup Language (PMML). It is under GPL license. 5.0.1 Input/Output Extensions Name Description Latest Tested Version io-http Receives and publishes events via http and https transports, calls external services, and serves incoming requests and provide synchronous responses. 2.1.2 io-nats Receives and publishes events from/to NATS. 2.0.6 io-kafka Receives and publishes events from/to Kafka. 5.0.4 io-email Receives and publishes events via email using smtp , pop3 and imap protocols. 2.0.4 io-cdc Captures change data from databases such as MySQL, MS SQL, Postgresql, H2 and Oracle. 2.0.3 io-tcp Receives and publishes events through TCP transport. 3.0.4 io-googlepubsub Receives and publishes events through Google Pub/Sub. 2.0.2 io-rabbitmq Receives and publishes events from/to RabbitMQ. 3.0.2 io-file Receives and publishes event data from/to files. 2.0.3 io-jms Receives and publishes events via Java Message Service (JMS), supporting Message brokers such as ActiveMQ 2.0.2 io-prometheus Consumes and expose Prometheus metrics from/to Prometheus server. 2.1.0 io-grpc Receives and publishes events via gRpc. 1.0.2 io-mqtt Allows to receive and publish events from/to mqtt broker. 3.0.0 io-sqs Subscribes to a SQS queue and receive/publish SQS messages. 3.0.0 io-s3 Allows you to publish/retrieve events to/from Amazon AWS S3. 1.0.1 io-gcs Receives/publishes events from/to Google Cloud Storage bucket. 1.0.0 Data Mapping Extensions Name Description Latest Tested Version map-json Converts JSON messages to/from Siddhi events. 5.0.4 map-xml Converts XML messages to/from Siddhi events. 5.0.3 map-text Converts text messages to/from Siddhi events. 2.0.4 map-avro Converts AVRO messages to/from Siddhi events. 2.0.5 map-keyvalue Converts events having Key-Value maps to/from Siddhi events. 2.0.4 map-csv Converts messages with CSV format to/from Siddhi events. 2.0.3 map-binary Converts binary events that adheres to Siddhi format to/from Siddhi events. 2.0.4 map-protobuf Converts protobuf messages to/from Siddhi events.. 1.0.1 Store Extensions Name Description Latest Tested Version store-rdbms Optimally stores, retrieves, and manipulates data on RDBMS databases such as MySQL, MS SQL, Postgresql, H2 and Oracle. 7.0.0 store-mongodb Stores, retrieves, and manipulates data on MongoDB. 2.0.3 store-redis Stores, retrieves, and manipulates data on Redis. 3.1.1 store-elasticsearch Stores, retrieves, and manipulates data on Elasticsearch. 3.1.2 Script Extensions Name Description Latest Tested Version script-js Allows writing user defined JavaScript functions within Siddhi Applications to process events. 5.0.2 Configure System Parameters for Extensions Siddhi extensions cater use-case specific logic that is not available by default in Siddhi. Some of these extensions have system parameter configurations to define/modify their behavior. These extensions usually have default values for the parameters, but when needed, they can be overridden by configuring the parameters in Siddhi Configuration yaml and passing it at startup. Refer more details on this in here Writing Custom Extensions Custom extensions can be written in order to cater use case specific logics that are not by default available in Siddhi core, or in existing extensions. The following Maven Archetypes can be used to generate the necessary Maven project to create the relevant extensions. Writing Execution Executions Maven archetype for execution execution that is used to generate the extension types such as: Function extension, by extending io.siddhi.core.executor.function.FunctionExecutor . Aggregate Function extension, by extending io.siddhi.core.query.selector.attribute.aggregator.AttributeAggregatorExecutor . Window extension, by extending io.siddhi.core.query.processor.stream.window.WindowProcessor . Stream Function extension, by extending io.siddhi.core.query.processor.stream.function.StreamFunctionProcessor . Stream Processor extension, by extending io.siddhi.core.query.processor.stream.StreamProcessor . The CLI command to generate the Maven project for the extension is as follows. mvn archetype:generate -DarchetypeGroupId=io.siddhi.extension.archetype -DarchetypeArtifactId=siddhi-archetype-execution -DgroupId=io.siddhi.extension.execution -Dversion=1.0.0-SNAPSHOT When the command is executed, the following properties will be prompted. At last, confirm that all the property values are correct by typing Y for yes, and N if they are not. Property Description Mandatory Default Value _nameOfFunction The name of the custom function (function, aggregate function, window, stream function, or stream processor) that is being created. Yes - _nameSpaceOfFunction The namespace of the function that groups similar custom functions. Yes - groupIdPostfix Postfix for the group ID. (As a convention, the function namespace is used). No ${_nameSpaceOfFunction} artifactId Artifact ID for the project. No siddhi-execution-${_nameSpaceOfFunction} classNameOfFunction Class name of the Function. No ${_nameOfFunction}Function classNameOfAggregateFunction Class name of the Aggregate Function. No ${_nameOfFunction}AggregateFunction classNameOfWindow Class name of the Window. No ${_nameOfFunction}Window classNameOfStreamFunction Class name of the Stream Function. No ${_nameOfFunction}StreamFunction classNameOfStreamProcessor Class name of the Stream Processor. No ${_nameOfFunction}StreamProcessor Writing Input/Output Executions Maven archetype for input/output execution that is used to generate the extension types such as: Source extension, by extending io.siddhi.core.stream.input.source.Source . Sink extension, by extending io.siddhi.core.stream.output.sink.Sink . The CLI command to generate the Maven project for the extension is as follows. mvn archetype:generate -DarchetypeGroupId=io.siddhi.extension.archetype -DarchetypeArtifactId=siddhi-archetype-io -DgroupId=io.siddhi.extension.io -Dversion=1.0.0-SNAPSHOT When the command is executed, the following properties will be prompted. At last, confirm that all the property values are correct by typing Y for yes, and N if they are not. Property Description Mandatory Default Value _IOType The name of the custom IO type that is being created. Yes - groupIdPostfix Postfix for the group ID. (As a convention, the name of the IO type is used). No ${_IOType} artifactId Artifact ID for the project. No siddhi-io-${_IOType} classNameOfSink Class name of the Sink. No ${_IOType}Sink classNameOfSource Class name of the Source. No ${_IOType}Source Writing Data Mapping Executions Maven archetype for data mapping execution that is used to generate the extension types such as: Source Mapper extension, by extending io.siddhi.core.stream.output.sink.SourceMapper . Sink Mapper extension, by extending io.siddhi.core.stream.output.sink.SinkMapper . The CLI command to generate the Maven project for the extension is as follows. mvn archetype:generate -DarchetypeGroupId=io.siddhi.extension.archetype -DarchetypeArtifactId=siddhi-archetype-map -DgroupId=io.siddhi.extension.map -Dversion=1.0.0-SNAPSHOT When the command is executed, the following properties will be prompted. At last, confirm that all the property values are correct by typing Y for yes, and N if they are not. Property Description Mandatory Default Value _mapType The name of the custom mapper type that is being created. Yes - groupIdPostfix Postfix for the group ID. (As a convention, the name of the mapper type is used). No ${_mapType} artifactId Artifact ID of the project. No siddhi-map-${_mapType} classNameOfSinkMapper Class name of the Sink Mapper. No ${_mapType}SinkMapper classNameOfSourceMapper Class name of the Source Mapper. No ${_mapType}SourceMapper Writing Store Executions Maven archetype to generate store extension, by extending either io.siddhi.core.table.record.AbstractRecordTable or io.siddhi.core.table.record.AbstractQueryableRecordTable . Here, the former allows Siddhi queries to perform conditional filters on the store while the latter allows performing both the conditional filters and select aggregations directly on the store. The CLI command to generate the Maven project for the extension is as follows. mvn archetype:generate -DarchetypeGroupId=io.siddhi.extension.archetype -DarchetypeArtifactId=siddhi-archetype-store -DgroupId=io.siddhi.extension.store -Dversion=1.0.0-SNAPSHOT When the command is executed, the following properties will be prompted. At last, confirm that all the property values are correct by typing Y for yes, and N if they are not. Property Description Mandatory Default Value _storeType The name of the custom store that is being created. Yes - groupIdPostfix Postfix for the group ID. (As a convention, the name of the store type is used). No ${_storeType} artifactId Artifact ID for the project. No siddhi-store-${_storeType} className Class name of the store. No ${_storeType}EventTable Writing Script Executions Maven archetype to generate script extension, by extending either io.siddhi.core.function.Script . The CLI command to generate the Maven project for the extension is as follows. mvn archetype:generate -DarchetypeGroupId=io.siddhi.extension.archetype -DarchetypeArtifactId=siddhi-archetype-script -DgroupId=io.siddhi.extension.script -Dversion=1.0.0-SNAPSHOT When the command is executed, the following properties will be prompted. At last, confirm that all the property values are correct by typing Y for yes, and N if they are not. Property Description Mandatory Default Value _nameOfScript The name of the custom script that is being created. Yes - groupIdPostfix Postfix for the group ID. (As a convention, the name of the script type is used). No ${_nameOfScript} artifactId Artifact ID for the project. No siddhi-script-${_nameOfScript} classNameOfScript Class name of the Script. No Eval${_nameOfScript}","title":"Extensions"},{"location":"docs/extensions/#siddhi-extensions","text":"Siddhi provides an extension architecture to enhance its functionality by incorporating other libraries and custom logic in a seamless manner. Each extension has a namespace that can be used to identify and specifically access the functionality of the relevant extension. Following are the supported Siddhi extension types: Execution extension types Function extension. Aggregate Function extension. Window extension. Stream Function extension. Stream Processor extension. IO extension types Source extension. Sink extension. Map extension types Source Mapper extension. Sink Mapper extension. Store extension types Store extension. Script extension types Script extension.","title":"Siddhi Extensions"},{"location":"docs/extensions/#available-extensions","text":"All the Siddhi extensions are released under Apache 2.0 License.","title":"Available Extensions"},{"location":"docs/extensions/#execution-extensions","text":"Name Description Latest Tested Version execution-string Provides basic string handling capabilities such as concat, length, replace all, etc. 5.0.7 execution-regex Provides basic RegEx execution capabilities. 5.0.5 execution-math Provides useful mathematical functions. 5.0.4 execution-time Provides time related functionality such as getting current time, current date, manipulating/formatting dates, etc. 5.0.4 execution-map Provides the capability to generate and manipulate map data objects. 5.0.4 execution-json Provides the capability to retrieve, insert, and modify JSON elements. 2.0.4 execution-unitconversion Converts various units such as length, mass, time and volume. 2.0.2 execution-reorder Orders out-of-order event arrivals using algorithms such as K-Slack and alpha K-Stack. 5.0.3 execution-unique Retains and process unique events based on the given parameters. 5.0.4 execution-list Provides the capability to send an array object inside Siddhi stream definitions and use it within queries. 1.0.0 execution-streamingml Performs streaming machine learning (clustering, classification and regression) on event streams. 2.0.3 execution-tensorflow Provides support for running pre-built TensorFlow models. 2.0.2 execution-pmml Evaluates Predictive Model Markup Language (PMML). It is under GPL license. 5.0.1","title":"Execution Extensions"},{"location":"docs/extensions/#inputoutput-extensions","text":"Name Description Latest Tested Version io-http Receives and publishes events via http and https transports, calls external services, and serves incoming requests and provide synchronous responses. 2.1.2 io-nats Receives and publishes events from/to NATS. 2.0.6 io-kafka Receives and publishes events from/to Kafka. 5.0.4 io-email Receives and publishes events via email using smtp , pop3 and imap protocols. 2.0.4 io-cdc Captures change data from databases such as MySQL, MS SQL, Postgresql, H2 and Oracle. 2.0.3 io-tcp Receives and publishes events through TCP transport. 3.0.4 io-googlepubsub Receives and publishes events through Google Pub/Sub. 2.0.2 io-rabbitmq Receives and publishes events from/to RabbitMQ. 3.0.2 io-file Receives and publishes event data from/to files. 2.0.3 io-jms Receives and publishes events via Java Message Service (JMS), supporting Message brokers such as ActiveMQ 2.0.2 io-prometheus Consumes and expose Prometheus metrics from/to Prometheus server. 2.1.0 io-grpc Receives and publishes events via gRpc. 1.0.2 io-mqtt Allows to receive and publish events from/to mqtt broker. 3.0.0 io-sqs Subscribes to a SQS queue and receive/publish SQS messages. 3.0.0 io-s3 Allows you to publish/retrieve events to/from Amazon AWS S3. 1.0.1 io-gcs Receives/publishes events from/to Google Cloud Storage bucket. 1.0.0","title":"Input/Output Extensions"},{"location":"docs/extensions/#data-mapping-extensions","text":"Name Description Latest Tested Version map-json Converts JSON messages to/from Siddhi events. 5.0.4 map-xml Converts XML messages to/from Siddhi events. 5.0.3 map-text Converts text messages to/from Siddhi events. 2.0.4 map-avro Converts AVRO messages to/from Siddhi events. 2.0.5 map-keyvalue Converts events having Key-Value maps to/from Siddhi events. 2.0.4 map-csv Converts messages with CSV format to/from Siddhi events. 2.0.3 map-binary Converts binary events that adheres to Siddhi format to/from Siddhi events. 2.0.4 map-protobuf Converts protobuf messages to/from Siddhi events.. 1.0.1","title":"Data Mapping Extensions"},{"location":"docs/extensions/#store-extensions","text":"Name Description Latest Tested Version store-rdbms Optimally stores, retrieves, and manipulates data on RDBMS databases such as MySQL, MS SQL, Postgresql, H2 and Oracle. 7.0.0 store-mongodb Stores, retrieves, and manipulates data on MongoDB. 2.0.3 store-redis Stores, retrieves, and manipulates data on Redis. 3.1.1 store-elasticsearch Stores, retrieves, and manipulates data on Elasticsearch. 3.1.2","title":"Store Extensions"},{"location":"docs/extensions/#script-extensions","text":"Name Description Latest Tested Version script-js Allows writing user defined JavaScript functions within Siddhi Applications to process events. 5.0.2 Configure System Parameters for Extensions Siddhi extensions cater use-case specific logic that is not available by default in Siddhi. Some of these extensions have system parameter configurations to define/modify their behavior. These extensions usually have default values for the parameters, but when needed, they can be overridden by configuring the parameters in Siddhi Configuration yaml and passing it at startup. Refer more details on this in here","title":"Script Extensions"},{"location":"docs/extensions/#writing-custom-extensions","text":"Custom extensions can be written in order to cater use case specific logics that are not by default available in Siddhi core, or in existing extensions. The following Maven Archetypes can be used to generate the necessary Maven project to create the relevant extensions. Writing Execution Executions Maven archetype for execution execution that is used to generate the extension types such as: Function extension, by extending io.siddhi.core.executor.function.FunctionExecutor . Aggregate Function extension, by extending io.siddhi.core.query.selector.attribute.aggregator.AttributeAggregatorExecutor . Window extension, by extending io.siddhi.core.query.processor.stream.window.WindowProcessor . Stream Function extension, by extending io.siddhi.core.query.processor.stream.function.StreamFunctionProcessor . Stream Processor extension, by extending io.siddhi.core.query.processor.stream.StreamProcessor . The CLI command to generate the Maven project for the extension is as follows. mvn archetype:generate -DarchetypeGroupId=io.siddhi.extension.archetype -DarchetypeArtifactId=siddhi-archetype-execution -DgroupId=io.siddhi.extension.execution -Dversion=1.0.0-SNAPSHOT When the command is executed, the following properties will be prompted. At last, confirm that all the property values are correct by typing Y for yes, and N if they are not. Property Description Mandatory Default Value _nameOfFunction The name of the custom function (function, aggregate function, window, stream function, or stream processor) that is being created. Yes - _nameSpaceOfFunction The namespace of the function that groups similar custom functions. Yes - groupIdPostfix Postfix for the group ID. (As a convention, the function namespace is used). No ${_nameSpaceOfFunction} artifactId Artifact ID for the project. No siddhi-execution-${_nameSpaceOfFunction} classNameOfFunction Class name of the Function. No ${_nameOfFunction}Function classNameOfAggregateFunction Class name of the Aggregate Function. No ${_nameOfFunction}AggregateFunction classNameOfWindow Class name of the Window. No ${_nameOfFunction}Window classNameOfStreamFunction Class name of the Stream Function. No ${_nameOfFunction}StreamFunction classNameOfStreamProcessor Class name of the Stream Processor. No ${_nameOfFunction}StreamProcessor Writing Input/Output Executions Maven archetype for input/output execution that is used to generate the extension types such as: Source extension, by extending io.siddhi.core.stream.input.source.Source . Sink extension, by extending io.siddhi.core.stream.output.sink.Sink . The CLI command to generate the Maven project for the extension is as follows. mvn archetype:generate -DarchetypeGroupId=io.siddhi.extension.archetype -DarchetypeArtifactId=siddhi-archetype-io -DgroupId=io.siddhi.extension.io -Dversion=1.0.0-SNAPSHOT When the command is executed, the following properties will be prompted. At last, confirm that all the property values are correct by typing Y for yes, and N if they are not. Property Description Mandatory Default Value _IOType The name of the custom IO type that is being created. Yes - groupIdPostfix Postfix for the group ID. (As a convention, the name of the IO type is used). No ${_IOType} artifactId Artifact ID for the project. No siddhi-io-${_IOType} classNameOfSink Class name of the Sink. No ${_IOType}Sink classNameOfSource Class name of the Source. No ${_IOType}Source Writing Data Mapping Executions Maven archetype for data mapping execution that is used to generate the extension types such as: Source Mapper extension, by extending io.siddhi.core.stream.output.sink.SourceMapper . Sink Mapper extension, by extending io.siddhi.core.stream.output.sink.SinkMapper . The CLI command to generate the Maven project for the extension is as follows. mvn archetype:generate -DarchetypeGroupId=io.siddhi.extension.archetype -DarchetypeArtifactId=siddhi-archetype-map -DgroupId=io.siddhi.extension.map -Dversion=1.0.0-SNAPSHOT When the command is executed, the following properties will be prompted. At last, confirm that all the property values are correct by typing Y for yes, and N if they are not. Property Description Mandatory Default Value _mapType The name of the custom mapper type that is being created. Yes - groupIdPostfix Postfix for the group ID. (As a convention, the name of the mapper type is used). No ${_mapType} artifactId Artifact ID of the project. No siddhi-map-${_mapType} classNameOfSinkMapper Class name of the Sink Mapper. No ${_mapType}SinkMapper classNameOfSourceMapper Class name of the Source Mapper. No ${_mapType}SourceMapper Writing Store Executions Maven archetype to generate store extension, by extending either io.siddhi.core.table.record.AbstractRecordTable or io.siddhi.core.table.record.AbstractQueryableRecordTable . Here, the former allows Siddhi queries to perform conditional filters on the store while the latter allows performing both the conditional filters and select aggregations directly on the store. The CLI command to generate the Maven project for the extension is as follows. mvn archetype:generate -DarchetypeGroupId=io.siddhi.extension.archetype -DarchetypeArtifactId=siddhi-archetype-store -DgroupId=io.siddhi.extension.store -Dversion=1.0.0-SNAPSHOT When the command is executed, the following properties will be prompted. At last, confirm that all the property values are correct by typing Y for yes, and N if they are not. Property Description Mandatory Default Value _storeType The name of the custom store that is being created. Yes - groupIdPostfix Postfix for the group ID. (As a convention, the name of the store type is used). No ${_storeType} artifactId Artifact ID for the project. No siddhi-store-${_storeType} className Class name of the store. No ${_storeType}EventTable Writing Script Executions Maven archetype to generate script extension, by extending either io.siddhi.core.function.Script . The CLI command to generate the Maven project for the extension is as follows. mvn archetype:generate -DarchetypeGroupId=io.siddhi.extension.archetype -DarchetypeArtifactId=siddhi-archetype-script -DgroupId=io.siddhi.extension.script -Dversion=1.0.0-SNAPSHOT When the command is executed, the following properties will be prompted. At last, confirm that all the property values are correct by typing Y for yes, and N if they are not. Property Description Mandatory Default Value _nameOfScript The name of the custom script that is being created. Yes - groupIdPostfix Postfix for the group ID. (As a convention, the name of the script type is used). No ${_nameOfScript} artifactId Artifact ID for the project. No siddhi-script-${_nameOfScript} classNameOfScript Class name of the Script. No Eval${_nameOfScript}","title":"Writing Custom Extensions"},{"location":"docs/features/","text":"Siddhi 5.1 Features New features of Siddhi 5.1 Information on new features of Siddhi 5.1 can be found in the release blog . Development and deployment SQL like language for event processing Siddhi Streaming SQL allows writing processing logic for event consumption, processing, integration, and publishing as a SQL script. Web based editor Siddhi Editor provides graphical drag-and-drop and source-based query building capability, with event flow visualization, syntax highlighting, auto-completion, and error handling support. Event simulation support to test the apps by sending events one by one, as a feed, or from CSV or database. Ability to export the developed apps as .siddhi files, or as Docker or Kubernetes artifacts with necessary extensions. CI/CD pipeline Siddhi test framework provides tools to build unit, integration, and black-box tests, to achieve CI/CD pipeline with agile DevOps workflow. App version management supported by manging .siddhi files in a preferred version control system. Multiple deployment options Embedded execution in Java and Python as libraries. Run as a standalone microservice in bare-metal, VM , or Docker . Deploy and run as a standalone or as distributed microservices natively in Kubernetes , using Siddhi Kubernetes operator. Functionality Consume and publish events with various data formats Consume and publish events via NATS , Kafka , RabbitMQ , HTTP , gRPC , TCP , JMS , IBM MQ , MQTT , Amazon SQS , Google Pub/Sub , Email , WebSocket , File , Change Data Capture (CDC) (From MySQL , Oracle , MSSQL , DB2 , Postgre ), S3 , Google Cloud Storage , and in-memory . Support message formats such as JSON , XML , Avro , Protobuf , Text , Binary , Key-value , and CSV . Rate-limit the output based on time and number of events . Perform load balancing and failover when publishing events to endpoints. Data filtering and preprocessing Filter event based on conditions such as value ranges, string matching, regex, and others. Clean data by setting defaults, and handling nulls, using default , if-then-else functions, and many others. Date transformation Support data extraction and reconstruction of massages . Inline mathematical and logical operations . Inbuilt functions and 60+ extensions for processing JSON , string , time , math , regex , and others. Ability to write custom functions in JavaScript , and Java . Data store integration with caching Query, modify, and join the data stored in-memory tables which support primary key constraints and indexing. Query, modify, and join the data stored in stores backed by systems such as RDBMS ( MySQL , Oracle , MSSQL , DB2 , Postgre , H2 ), Redis , Hazelcast , MongoDB , HBase , Cassandra , Solr , and Elasticsearch . Support low latency processing by preloading and caching data using caching modes such as FIFO , LRU , and RFU . Service integration and error handling Support for calling HTTP and gRPC services in a non-blocking manner to fetch data and enrich events. Handle responses accordingly for different response status codes. Various error handling options to handle endpoint unavailability while retrying to connect, such as; Logging and dropping the events. Waiting indefinitely and not consuming events from the sources. Divert the events to error stream to handle the errors gracefully. Data Summarization Aggregate of data using sum , count , average ( avg ), min , max , distinctCount , and standard deviation ( StdDev ) operators. Event summarization based on time intervals using sliding time , or tumbling/batch time windows . Event summarization based on number of events using sliding length , and tumbling/batch length windows . Support for data summarization based on sessions and uniqueness. Ability to run long running aggregations with time granularities from seconds to years using both in-memory and databases via named aggregation . Support to aggregate data based on group by fields , filter aggregated data using having conditions , and sort limit the aggregated output using order by limit keywords. Rule processing Execution of rules based on single event using filter operator, if-then-else and match functions , and many others. Rules based on collection of events using data summarization , and joins with streams , tables , windows or aggregations . Rules to detect event occurrence patterns, trends, or non-occurrence of a critical events using complex event processing constructs such as pattern , and sequence . Serving online and predefined ML models Serve pre-created ML models based on TensorFlow or PMML that are built via Python, R, Spark, H2O.ai, or others. Ability to call models via HTTP or gRPC for decision making. Online machine learning for clustering, classification, and regression. Anomaly detection using markov chains. Scatter-gather and data pipeline Process complex messages by dividing them into simple messages using tokenize function, process or transform them in isolation, and group them back using the batch window and group aggregation. Ability to modularize the execution logic of each usecase into a single .siddhi file (Siddhi Application), and connect them using in-memory source and in-memory sink to build a composite event-driven application. Provide execution isolation and parallel processing by partitioning the events using keys or value ranges. Periodically trigger data pipelines based on time intervals, and cron expression, or at App startup using triggers . Synchronize and parallelize event processing using @sync annotations . Ability to alter the speed of processing based on event generation time. Realtime decisions as a service Act as an HTTP or gRPC service to provide realtime synchronous decisions via service source and service-Response sink . Provide REST APIs to query in-memory tables , windows , named-aggregations , and database backed stores such as RDBMS, NoSQL DBs to make decisions based on the state of the system. Snapshot and restore Support for periodic incremental state persistence to allow state restoration during failures.","title":"Features"},{"location":"docs/features/#siddhi-51-features","text":"New features of Siddhi 5.1 Information on new features of Siddhi 5.1 can be found in the release blog .","title":"Siddhi 5.1 Features"},{"location":"docs/features/#development-and-deployment","text":"","title":"Development and deployment"},{"location":"docs/features/#sql-like-language-for-event-processing","text":"Siddhi Streaming SQL allows writing processing logic for event consumption, processing, integration, and publishing as a SQL script.","title":"SQL like language for event processing"},{"location":"docs/features/#web-based-editor","text":"Siddhi Editor provides graphical drag-and-drop and source-based query building capability, with event flow visualization, syntax highlighting, auto-completion, and error handling support. Event simulation support to test the apps by sending events one by one, as a feed, or from CSV or database. Ability to export the developed apps as .siddhi files, or as Docker or Kubernetes artifacts with necessary extensions.","title":"Web based editor"},{"location":"docs/features/#cicd-pipeline","text":"Siddhi test framework provides tools to build unit, integration, and black-box tests, to achieve CI/CD pipeline with agile DevOps workflow. App version management supported by manging .siddhi files in a preferred version control system.","title":"CI/CD pipeline"},{"location":"docs/features/#multiple-deployment-options","text":"Embedded execution in Java and Python as libraries. Run as a standalone microservice in bare-metal, VM , or Docker . Deploy and run as a standalone or as distributed microservices natively in Kubernetes , using Siddhi Kubernetes operator.","title":"Multiple deployment options"},{"location":"docs/features/#functionality","text":"","title":"Functionality"},{"location":"docs/features/#consume-and-publish-events-with-various-data-formats","text":"Consume and publish events via NATS , Kafka , RabbitMQ , HTTP , gRPC , TCP , JMS , IBM MQ , MQTT , Amazon SQS , Google Pub/Sub , Email , WebSocket , File , Change Data Capture (CDC) (From MySQL , Oracle , MSSQL , DB2 , Postgre ), S3 , Google Cloud Storage , and in-memory . Support message formats such as JSON , XML , Avro , Protobuf , Text , Binary , Key-value , and CSV . Rate-limit the output based on time and number of events . Perform load balancing and failover when publishing events to endpoints.","title":"Consume and publish events with various data formats"},{"location":"docs/features/#data-filtering-and-preprocessing","text":"Filter event based on conditions such as value ranges, string matching, regex, and others. Clean data by setting defaults, and handling nulls, using default , if-then-else functions, and many others.","title":"Data filtering and preprocessing"},{"location":"docs/features/#date-transformation","text":"Support data extraction and reconstruction of massages . Inline mathematical and logical operations . Inbuilt functions and 60+ extensions for processing JSON , string , time , math , regex , and others. Ability to write custom functions in JavaScript , and Java .","title":"Date transformation"},{"location":"docs/features/#data-store-integration-with-caching","text":"Query, modify, and join the data stored in-memory tables which support primary key constraints and indexing. Query, modify, and join the data stored in stores backed by systems such as RDBMS ( MySQL , Oracle , MSSQL , DB2 , Postgre , H2 ), Redis , Hazelcast , MongoDB , HBase , Cassandra , Solr , and Elasticsearch . Support low latency processing by preloading and caching data using caching modes such as FIFO , LRU , and RFU .","title":"Data store integration with caching"},{"location":"docs/features/#service-integration-and-error-handling","text":"Support for calling HTTP and gRPC services in a non-blocking manner to fetch data and enrich events. Handle responses accordingly for different response status codes. Various error handling options to handle endpoint unavailability while retrying to connect, such as; Logging and dropping the events. Waiting indefinitely and not consuming events from the sources. Divert the events to error stream to handle the errors gracefully.","title":"Service integration and error handling"},{"location":"docs/features/#data-summarization","text":"Aggregate of data using sum , count , average ( avg ), min , max , distinctCount , and standard deviation ( StdDev ) operators. Event summarization based on time intervals using sliding time , or tumbling/batch time windows . Event summarization based on number of events using sliding length , and tumbling/batch length windows . Support for data summarization based on sessions and uniqueness. Ability to run long running aggregations with time granularities from seconds to years using both in-memory and databases via named aggregation . Support to aggregate data based on group by fields , filter aggregated data using having conditions , and sort limit the aggregated output using order by limit keywords.","title":"Data Summarization"},{"location":"docs/features/#rule-processing","text":"Execution of rules based on single event using filter operator, if-then-else and match functions , and many others. Rules based on collection of events using data summarization , and joins with streams , tables , windows or aggregations . Rules to detect event occurrence patterns, trends, or non-occurrence of a critical events using complex event processing constructs such as pattern , and sequence .","title":"Rule processing"},{"location":"docs/features/#serving-online-and-predefined-ml-models","text":"Serve pre-created ML models based on TensorFlow or PMML that are built via Python, R, Spark, H2O.ai, or others. Ability to call models via HTTP or gRPC for decision making. Online machine learning for clustering, classification, and regression. Anomaly detection using markov chains.","title":"Serving online and predefined ML models"},{"location":"docs/features/#scatter-gather-and-data-pipeline","text":"Process complex messages by dividing them into simple messages using tokenize function, process or transform them in isolation, and group them back using the batch window and group aggregation. Ability to modularize the execution logic of each usecase into a single .siddhi file (Siddhi Application), and connect them using in-memory source and in-memory sink to build a composite event-driven application. Provide execution isolation and parallel processing by partitioning the events using keys or value ranges. Periodically trigger data pipelines based on time intervals, and cron expression, or at App startup using triggers . Synchronize and parallelize event processing using @sync annotations . Ability to alter the speed of processing based on event generation time.","title":"Scatter-gather and data pipeline"},{"location":"docs/features/#realtime-decisions-as-a-service","text":"Act as an HTTP or gRPC service to provide realtime synchronous decisions via service source and service-Response sink . Provide REST APIs to query in-memory tables , windows , named-aggregations , and database backed stores such as RDBMS, NoSQL DBs to make decisions based on the state of the system.","title":"Realtime decisions as a service"},{"location":"docs/features/#snapshot-and-restore","text":"Support for periodic incremental state persistence to allow state restoration during failures.","title":"Snapshot and restore"},{"location":"docs/query-guide/","text":"Siddhi 5.1 Streaming SQL Guide Introduction Siddhi Streaming SQL is designed to process streams of events. It can be used to implement streaming data integration, streaming analytics, rule based and adaptive decision making use cases. It is an evolution of Complex Event Processing (CEP) and Stream Processing systems, hence it can also be used to process stateful computations, detecting of complex event patterns, and sending notifications in real-time. Siddhi Streaming SQL uses SQL like syntax, and annotations to consume events from diverse event sources with various data formats, process then using stateful and stateless operators and send outputs to multiple endpoints according to their accepted event formats. It also supports exposing rule based and adaptive decision making as service endpoints such that external programs and systems can synchronously get decision support form Siddhi. The following sections explains how to write event processing logic using Siddhi Streaming SQL. Siddhi Application The processing logic for your program can be written using the Streaming SQL and put together as a single file with .siddhi extension. This file is called as the Siddhi Application or the SiddhiApp . SiddhiApps are named by adding @app:name(' name ') annotation on the top of the SiddhiApp file. When the annotation is not added Siddhi assigns a random UUID as the name of the SiddhiApp. Purpose SiddhiApp provides an isolated execution environment for your processing logic that allows you to deploy and execute processing logic independent of other SiddhiApp in the system. Therefore it's always recommended to have a processing logic related to single use case in a single SiddhiApp. This will help you to group processing logic and easily manage addition and removal of various use cases. Have different business use cases in separate Siddhi Applications. This is recommended as it allows users to selectively deploy the applications based their on business needs. It is also recommended to move the repeated steam processing logic that exist in multiple Siddhi Applications such as message retrieval and preprocessing, to a common Siddhi Application, whereby reducing code duplication and improving maintainability. In this case, to pass the events from one Siddhi App to another, configure them using a common topic using In-Memory Sink and In-Memory Source . The following diagram depicts some of the key Siddhi Streaming SQL elements of Siddhi Application and how event flows through the elements. Below table provides brief description of a few key elements in the Siddhi Streaming SQL Language. Elements Description Stream A logical series of events ordered in time with a uniquely identifiable name, and a defined set of typed attributes defining its schema. Event An event is a single event object associated with a stream. All events of a stream contains a timestamp and an identical set of typed attributes based on the schema of the stream they belong to. Table A structured representation of data stored with a defined schema. Stored data can be backed by In-Memory , or external data stores such as RDBMS , MongoDB , etc. The tables can be accessed and manipulated at runtime. Named-Window A structured representation of data stored with a defined schema and eviction policy. Window data is stored In-Memory and automatically cleared by the named-window constrain. Other siddhi elements can only query the values in windows at runtime but they cannot modify them. Named-Aggregation A structured representation of data that's incrementally aggregated and stored with a defined schema and aggregation granularity such as seconds, minutes, hours, etc. Aggregation data is stored both In-Memory and in external data stores such as RDBMS . Other siddhi elements can only query the values in windows at runtime but they cannot modify them. Query A logical construct that processes events in streaming manner by by consuming data from one or more streams, tables, windows and aggregations, and publishes output events into a stream, table or a window. Source A construct that consumes data from external sources (such as TCP , Kafka , HTTP , etc) with various event formats such as XML , JSON , binary , etc, convert then to Siddhi events, and passes into streams for processing. Sink A construct that consumes events arriving at a stream, maps them to a predefined data format (such as XML , JSON , binary , etc), and publishes them to external endpoints (such as E-mail , TCP , Kafka , HTTP , etc). Input Handler A mechanism to programmatically inject events into streams. Stream/Query Callback A mechanism to programmatically consume output events from streams or queries. Partition A logical container that isolates the processing of queries based on the partition keys derived from the events. Inner Stream A positionable stream that connects portioned queries with each other within the partition. Grammar SiddhiApp is a collection of Siddhi Streaming SQL elements composed together as a script. Here each Siddhi element must be separated by a semicolon ; . Hight level syntax of SiddhiApp is as follows. siddhi app : app annotation * ( stream definition | table definition | ... ) + ( query | partition ) + ; Example Siddhi Application with name Temperature-Analytics defined with a stream named TempStream and a query named 5minAvgQuery . @app:name('Temperature-Analytics') define stream TempStream (deviceID long, roomNo int, temp double); @info(name = '5minAvgQuery') from TempStream#window.time(5 min) select roomNo, avg(temp) as avgTemp group by roomNo insert into OutputStream; Stream A stream is a logical series of events ordered in time. Its schema is defined via the stream definition . A stream definition contains the stream name and a set of attributes having a specific type and a uniquely identifiable name within the scope of the stream. All events associated with the stream will have the same schema (i.e., have the same attributes in the same order). Purpose Stream groups common types of events together with a schema. This helps in various ways such as, processing all events in queries together and performing data format transformations together when they are consumed and published via sources and sinks. Syntax The syntax for defining a stream is as follows. define stream stream name ( attribute name attribute type , attribute name attribute type , ... ); The following parameters are used to configure a stream definition. Parameter Description stream name The name of the stream created. (It is recommended to define a stream name in PascalCase .) attribute name Uniquely identifiable name of the stream attribute. (It is recommended to define attribute names in camelCase .) attribute type The type of each attribute defined in the schema. This can be STRING , INT , LONG , DOUBLE , FLOAT , BOOL or OBJECT . To use and refer stream and attribute names that do not follow [a-zA-Z_][a-zA-Z_0-9]* format enclose them in ` . E.g. `$test(0)` . To make the stream process events in multi-threading and asynchronous way use the @async annotation as shown in Threading and synchronization configuration section. Example define stream TempStream (deviceID long, roomNo int, temp double); The above creates a stream with name TempStream having the following attributes. deviceID of type long roomNo of type int temp of type double Source Sources receive events via multiple transports and in various data formats, and direct them into streams for processing. A source configuration allows to define a mapping in order to convert each incoming event from its native data format to a Siddhi event. When customizations to such mappings are not provided, Siddhi assumes that the arriving event adheres to the predefined format based on the stream definition and the configured message mapping type. Purpose Source provides a way to consume events from external systems and convert them to be processed by the associated stream. Syntax To configure a stream that consumes events via a source, add the source configuration to a stream definition by adding the @source annotation with the required parameter values. The source syntax is as follows: @source(type=' source type ', static.key =' value ', static.key =' value ', @map(type=' map type ', static.key =' value ', static.key =' value ', @attributes( attribute1 =' attribute mapping ', attributeN =' attribute mapping ') ) ) define stream stream name ( attribute1 type , attributeN type ); This syntax includes the following annotations. Source The type parameter of @source annotation defines the source type that receives events. The other parameters of @source annotation depends upon the selected source type, and here some of its parameters can be optional. For detailed information about the supported parameters see the documentation of the relevant source. The following is the list of source types supported by Siddhi: Source type Description In-memory Allow SiddhiApp to consume events from other SiddhiApps running on the same JVM. HTTP Expose an HTTP service to consume messages. Kafka Subscribe to Kafka topic to consume events. TCP Expose a TCP service to consume messages. Email Consume emails via POP3 and IMAP protocols. JMS Subscribe to JMS topic or queue to consume events. File Reads files by tailing or as a whole to extract events out of them. CDC Perform change data capture on databases. Prometheus Consume data from Prometheus agent. In-memory is the only source inbuilt in Siddhi, and all other source types are implemented as extensions. Source Mapper Each @source configuration can have a mapping denoted by the @map annotation that defines how to convert the incoming event format to Siddhi events. The type parameter of the @map defines the map type to be used in converting the incoming events. The other parameters of @map annotation depends on the mapper selected, and some of its parameters can be optional. For detailed information about the parameters see the documentation of the relevant mapper. Map Attributes @attributes is an optional annotation used with @map to define custom mapping. When @attributes is not provided, each mapper assumes that the incoming events adheres to its own default message format and attempt to convert the events from that format. By adding the @attributes annotation, users can selectively extract data from the incoming message and assign them to the attributes. There are two ways to configure @attributes . Define attribute names as keys, and mapping configurations as values: @attributes( attribute1 =' mapping ', attributeN =' mapping ') Define the mapping configurations in the same order as the attributes defined in stream definition: @attributes( ' mapping for attribute1 ', ' mapping for attributeN ') Supported source mapping types The following is the list of source mapping types supported by Siddhi: Source mapping type Description PassThrough Omits data conversion on Siddhi events. JSON Converts JSON messages to Siddhi events. XML Converts XML messages to Siddhi events. TEXT Converts plain text messages to Siddhi events. Avro Converts Avro events to Siddhi events. Binary Converts Siddhi specific binary events to Siddhi events. Key Value Converts key-value HashMaps to Siddhi events. CSV Converts CSV like delimiter separated events to Siddhi events. Tip When the @map annotation is not provided @map(type='passThrough') is used as default, that passes the consumed Siddhi events directly to the streams without any data conversion. PassThrough is the only source mapper inbuilt in Siddhi, and all other source mappers are implemented as extensions. Example 1 Receive JSON messages by exposing an HTTP service, and direct them to InputStream stream for processing. Here the HTTP service will be secured with basic authentication, receives events on all network interfaces on port 8080 and context /foo . The service expects the JSON messages to be on the default data format that's supported by the JSON mapper as follows. { \"name\":\"Paul\", \"age\":20, \"country\":\"UK\" } The configuration of the HTTP source and JSON source mapper to achieve the above is as follows. @source(type='http', receiver.url='http://0.0.0.0:8080/foo', @map(type='json')) define stream InputStream (name string, age int, country string); Example 2 Receive JSON messages by exposing an HTTP service, and direct them to StockStream stream for processing. Here the incoming JSON , as given below, do not adhere to the default data format that's supported by the JSON mapper. { \"portfolio\":{ \"stock\":{ \"volume\":100, \"company\":{ \"symbol\":\"FB\" }, \"price\":55.6 } } } The configuration of the HTTP source and the custom JSON source mapping to achieve the above is as follows. @source(type='http', receiver.url='http://0.0.0.0:8080/foo', @map(type='json', enclosing.element=\"$.portfolio\", @attributes(symbol = \"stock.company.symbol\", price = \"stock.price\", volume = \"stock.volume\"))) define stream StockStream (symbol string, price float, volume long); The same can also be configured by omitting the attribute names as below. @source(type='http', receiver.url='http://0.0.0.0:8080/foo', @map(type='json', enclosing.element=\"$.portfolio\", @attributes(\"stock.company.symbol\", \"stock.price\", \"stock.volume\"))) define stream StockStream (symbol string, price float, volume long); Sink Sinks consumes events from streams and publish them via multiple transports to external endpoints in various data formats. A sink configuration allows users to define a mapping to convert the Siddhi events in to the required output data format (such as JSON , TEXT , XML , etc.) and publish the events to the configured endpoints. When customizations to such mappings are not provided, Siddhi converts events to the predefined event format based on the stream definition and the configured mapper type, before publishing the events. Purpose Sink provides a way to publish Siddhi events of a stream to external systems by converting events to their supported format. Syntax To configure a stream to publish events via a sink, add the sink configuration to a stream definition by adding the @sink annotation with the required parameter values. The sink syntax is as follows: @sink(type=' sink type ', static.key =' value ', dynamic.key ='{{ value }}', @map(type=' map type ', static.key =' value ', dynamic.key ='{{ value }}', @payload(' payload mapping ') ) ) define stream stream name ( attribute1 type , attributeN type ); Dynamic Properties The sink and sink mapper properties that are categorized as dynamic have the ability to absorb attribute values dynamically from the Siddhi events of their associated streams. This can be configured by enclosing the relevant attribute names in double curly braces as {{...}} , and using it within the property values. Some valid dynamic properties values are: '{{attribute1}}' 'This is {{attribute1}}' {{attribute1}} {{attributeN}} Here the attribute names in the double curly braces will be replaced with the values from the events before they are published. This syntax includes the following annotations. Sink The type parameter of the @sink annotation defines the sink type that publishes the events. The other parameters of the @sink annotation depends upon the selected sink type, and here some of its parameters can be optional and/or dynamic. For detailed information about the supported parameters see documentation of the relevant sink. The following is a list of sink types supported by Siddhi: Source type Description In-memory Allow SiddhiApp to publish events to other SiddhiApps running on the same JVM. Log Logs the events appearing on the streams. HTTP Publish events to an HTTP endpoint. Kafka Publish events to Kafka topic. TCP Publish events to a TCP service. Email Send emails via SMTP protocols. JMS Publish events to JMS topics or queues. File Writes events to files. Prometheus Expose data through Prometheus agent. Distributed Sink Distributed Sinks publish events from a defined stream to multiple endpoints using load balancing or partitioning strategies. Any sink can be used as a distributed sink. A distributed sink configuration allows users to define a common mapping to convert and send the Siddhi events for all its destination endpoints. Purpose Distributed sink provides a way to publish Siddhi events to multiple endpoints in the configured event format. Syntax To configure distributed sink add the sink configuration to a stream definition by adding the @sink annotation and add the configuration parameters that are common of all the destination endpoints inside it, along with the common parameters also add the @distribution annotation specifying the distribution strategy (i.e. roundRobin or partitioned ) and @destination annotations providing each endpoint specific configurations. The distributed sink syntax is as follows: RoundRobin Distributed Sink Publishes events to defined destinations in a round robin manner. @sink(type=' sink type ', common.static.key =' value ', common.dynamic.key ='{{ value }}', @map(type=' map type ', static.key =' value ', dynamic.key ='{{ value }}', @payload(' payload mapping ') ) @distribution(strategy='roundRobin', @destination( destination.specific.key =' value '), @destination( destination.specific.key =' value '))) ) define stream stream name ( attribute1 type , attributeN type ); Partitioned Distributed Sink Publish events to the defined destinations by partitioning them based on a partitioning key. @sink(type=' sink type ', common.static.key =' value ', common.dynamic.key ='{{ value }}', @map(type=' map type ', static.key =' value ', dynamic.key ='{{ value }}', @payload(' payload mapping ') ) @distribution(strategy='partitioned', partitionKey=' partition key ', @destination( destination.specific.key =' value '), @destination( destination.specific.key =' value '))) ) define stream stream name ( attribute1 type , attributeN type ); Sink Mapper Each @sink configuration can have a mapping denoted by the @map annotation that defines how to convert Siddhi events to outgoing messages with the defined format. The type parameter of the @map defines the map type to be used in converting the outgoing events, and other parameters of @map annotation depend on the mapper selected, where some of these parameters can be optional and/or dynamic. For detailed information about the parameters see the documentation of the relevant mapper. Map Payload @payload is an optional annotation used with @map to define custom mapping. When the @payload annotation is not provided, each mapper maps the outgoing events to its own default event format. The @payload annotation allow users to configure mappers to produce the output payload of their choice, and by using dynamic properties within the payload they can selectively extract and add data from the published Siddhi events. There are two ways you to configure @payload annotation. Some mappers such as XML , JSON , and Test only accept one output payload: @payload( 'This is a test message from {{user}}.') Some mappers such key-value accept series of mapping values: @payload( key1='mapping_1', 'key2'='user : {{user}}') Here, the keys of payload mapping can be defined using the dot notation as a.b.c , or using any constant string value as '$abc' . Supported sink mapping types The following is a list of sink mapping types supported by Siddhi: Sink mapping type Description PassThrough Omits data conversion on outgoing Siddhi events. JSON Converts Siddhi events to JSON messages. XML Converts Siddhi events to XML messages. TEXT Converts Siddhi events to plain text messages. Avro Converts Siddhi events to Avro Events. Binary Converts Siddhi events to Siddhi specific binary events. Key Value Converts Siddhi events to key-value HashMaps. CSV Converts Siddhi events to CSV like delimiter separated events. Tip When the @map annotation is not provided @map(type='passThrough') is used as default, that passes the outgoing Siddhi events directly to the sinks without any data conversion. PassThrough is the only sink mapper inbuilt in Siddhi, and all other sink mappers are implemented as extensions. Example 1 Sink to publish OutputStream events by converting them to JSON messages with the default format, and by sending to an HTTP endpoint http://localhost:8005/endpoint1 , using POST method, Accept header, and basic authentication having admin is both username and password. The configuration of the HTTP sink and JSON sink mapper to achieve the above is as follows. @sink(type='http', publisher.url='http://localhost:8005/endpoint', method='POST', headers='Accept-Date:20/02/2017', basic.auth.enabled='true', basic.auth.username='admin', basic.auth.password='admin', @map(type='json')) define stream OutputStream (name string, age int, country string); This will publish a JSON message on the following format: { \"event\":{ \"name\":\"Paul\", \"age\":20, \"country\":\"UK\" } } Example 2 Sink to publish StockStream events by converting them to user defined JSON messages, and sending them to an HTTP endpoint http://localhost:8005/stocks . The configuration of the HTTP sink and custom JSON sink mapping to achieve the above is as follows. @sink(type='http', publisher.url='http://localhost:8005/stocks', @map(type='json', validate.json='true', enclosing.element='$.Portfolio', @payload(\"\"\"{\"StockData\":{ \"Symbol\":\"{{symbol}}\", \"Price\":{{price}} }}\"\"\"))) define stream StockStream (symbol string, price float, volume long); This will publish a single event as the JSON message on the following format: { \"Portfolio\":{ \"StockData\":{ \"Symbol\":\"GOOG\", \"Price\":55.6 } } } This can also publish multiple events together as a JSON message on the following format: { \"Portfolio\":[ { \"StockData\":{ \"Symbol\":\"GOOG\", \"Price\":55.6 } }, { \"StockData\":{ \"Symbol\":\"FB\", \"Price\":57.0 } } ] } Example 3 Sink to publish events from the OutputStream stream to multiple HTTP endpoints using a partitioning strategy. Here, the events are sent to either http://localhost:8005/endpoint1 or http://localhost:8006/endpoint2 based on the partitioning key country . It uses default JSON mapping, POST method, and admin as both the username and the password when publishing to both the endpoints. The configuration of the distributed HTTP sink and JSON sink mapper to achieve the above is as follows. @sink(type='http', method='POST', basic.auth.enabled='true', basic.auth.username='admin', basic.auth.password='admin', @map(type='json'), @distribution(strategy='partitioned', partitionKey='country', @destination(publisher.url='http://localhost:8005/endpoint1'), @destination(publisher.url='http://localhost:8006/endpoint2'))) define stream OutputStream (name string, age int, country string); This will partition the outgoing events and publish all events with the same country attribute value to the same endpoint. The JSON message published will be on the following format: { \"event\":{ \"name\":\"Paul\", \"age\":20, \"country\":\"UK\" } } Error Handling Errors in Siddhi can be handled at the Streams and in Sinks. Error Handling at Stream When errors are thrown by Siddhi elements subscribed to the stream, the error gets propagated up to the stream that delivered the event to those Siddhi elements. By default the error is logged and dropped at the stream, but this behavior can be altered by by adding @OnError annotation to the corresponding stream definition. @OnError annotation can help users to capture the error and the associated event, and handle them gracefully by sending them to a fault stream. The @OnError annotation and the required action to be specified as below. @OnError(action='on error action') define stream stream name ( attribute name attribute type , attribute name attribute type , ... ); The action parameter of the @OnError annotation defines the action to be executed during failure scenarios. The following actions can be specified to @OnError annotation to handle erroneous scenarios. LOG : Logs the event with the error, and drops the event. This is the default action performed even when @OnError annotation is not defined. STREAM : Creates a fault stream and redirects the event and the error to it. The created fault stream will have all the attributes defined in the base stream to capture the error causing event, and in addition it also contains _error attribute of type object to containing the error information. The fault stream can be referred by adding ! in front of the base stream name as ! stream name . Example Handle errors in TempStream by redirecting the errors to a fault stream. The configuration of TempStream stream and @OnError annotation is as follows. @OnError(action='STREAM') define stream TempStream (deviceID long, roomNo int, temp double); Siddhi will infer and automatically defines the fault stream of TempStream as given below. define stream !TempStream (deviceID long, roomNo int, temp double, _error object); The SiddhiApp extending the above the use-case by adding failure generation and error handling with the use of queries is as follows. Note: Details on writing processing logics via queries will be explained in later sections. -- Define fault stream to handle error occurred at TempStream subscribers @OnError(action='STREAM') define stream TempStream (deviceID long, roomNo int, temp double); -- Error generation through a custom function `createError()` @info(name = 'error-generation') from TempStream#custom:createError() insert into IgnoreStream1; -- Handling error by simply logging the event and error. @info(name = 'handle-error') from !TempStream#log(\"Error Occurred!\") select deviceID, roomNo, temp, _error insert into IgnoreStream2; Error Handling at Sink There can be cases where external systems becoming unavailable or coursing errors when the events are published to them. By default sinks log and drop the events causing event losses, and this can be handled gracefully by configuring on.error parameter of the @sink annotation. The on.error parameter of the @sink annotation can be specified as below. @sink(type=' sink type ', on.error=' on error action ', key =' value ', ...) define stream stream name ( attribute name attribute type , attribute name attribute type , ... ); The following actions can be specified to on.error parameter of @sink annotation to handle erroneous scenarios. LOG : Logs the event with the error, and drops the event. This is the default action performed even when on.error parameter is not defined on the @sink annotation. WAIT : Publishing threads wait in back-off and re-trying mode, and only send the events when the connection is re-established. During this time the threads will not consume any new messages causing the systems to introduce back pressure on the systems that publishes to it. STREAM : Pushes the failed events with the corresponding error to the associated fault stream the sink belongs to. Example 1 Introduce back pressure on the threads who bring events via TempStream when the system cannot connect to Kafka. The configuration of TempStream stream and @sink Kafka annotation with on.error property is as follows. @sink(type='kafka', on.error='WAIT', topic='{{roomNo}}', bootstrap.servers='localhost:9092', @map(type='xml')) define stream TempStream (deviceID long, roomNo int, temp double); Example 2 Send events to the fault stream of TempStream when the system cannot connect to Kafka. The configuration of TempStream stream with associated fault stream, @sink Kafka annotation with on.error property and a queries to handle the error is as follows. Note: Details on writing processing logics via queries will be explained in later sections. @OnError(action='STREAM') @sink(type='kafka', on.error='STREAM', topic='{{roomNo}}', bootstrap.servers='localhost:9092', @map(type='xml')) define stream TempStream (deviceID long, roomNo int, temp double); -- Handling error by simply logging the event and error. @info(name = 'handle-error') from !TempStream#log(\"Error Occurred!\") select deviceID, roomNo, temp, _error insert into IgnoreStream; Query Query defines the processing logic in Siddhi. It consumes events from one or more streams, named-windows , tables , and/or named-aggregations , process the events in a streaming manner, and generate output events into a stream , named-window , or table . Purpose A query provides a way to process the events in the order they arrive and produce output using both stateful and stateless complex event processing and stream processing operations. Syntax The high level query syntax for defining processing logics is as follows: @info(name = ' query name ') from input projection output action The following parameters are used to configure a stream definition. Parameter Description query name The name of the query. Since naming the query (i.e the @info(name = ' query name ') annotation) is optional, when the name is not provided Siddhi assign a system generated name for the query. input Defines the means of event consumption via streams , named-windows , tables , and/or named-aggregations , and defines the processing logic using filters , windows , stream-functions , joins , patterns and sequences . projection Generates output event attributes using select , functions , aggregation-functions , and group by operations, and filters the generated the output using having , limit offset , order by , and output rate limiting operations before sending them out. Here the projection is optional and when it is omitted all the input events will be sent to the output as it is. output action Defines output action (such as insert into , update , delete , etc) that needs to be performed by the generated events on a stream , named-window , or table From All Siddhi queries must always have at least one stream or named-window as an input (some queries can consume more than one stream or named-window ), and only join query can consume events via tables , or named-aggregations as the second input. The input stream , named-window , table , and/or named-aggregation should be defined before they can be used in a query. Syntax A high level syntax for consuming events from a stream, named-window, table, and/or named-aggregation is as follow; from (( stream | named-window ) handler *) ((join ( stream | named-window | table | named-aggregation ) handler *)|((,|- )( stream | named-window ) handler *)+)? projection insert into ( stream | named-window | table ) Here the handler denotes the processing logic using filters , windows , and stream-functions , join for joins , - for patterns , and , for sequences . More information on writing queries used these will be discussed in later sections. Insert Allows events to be inserted directly into streams , named-windows , or tables . When a query is defined to insert events into a stream that is not already defined, Siddhi infers and automatically defines its stream definition, such that queries defined below the current query can use the stream like any other predefined streams. Syntax Syntax to insert events into a stream, named-window or table from another stream is as follows; from input projection insert into ( stream | named-window | table ) This inserts all the newly arrived events ( current events ) in to a stream, named-window, or table. There are also other types of events other than current events that are produced by queries, the Event Type section provides more information on, how insertion operation can be modified to support those. Example A query to consume events from the TempStream stream and output only the roomNo and temp attributes to the RoomTempStream stream, from which another query to consume the events and send all its attributes to AnotherRoomTempStream stream. define stream TempStream (deviceID long, roomNo int, temp double); from TempStream select roomNo, temp insert into RoomTempStream; from RoomTempStream insert into AnotherRoomTempStream; Inferred Stream Here, the RoomTempStream and AnotherRoomTempStream streams are an inferred streams, which means their stream definitions are inferred from the queries and hence they can be used the same as any other defined streams without any restrictions. Value Values are typed data, which can be manipulated, transferred, and stored. Values can be referred by the attributes defined in definitions such as streams, and tables. Siddhi supports values of type STRING , INT (Integer), LONG , DOUBLE , FLOAT , BOOL (Boolean) and OBJECT . The syntax of each type and their example use as a constant value is as follows, Attribute Type Format Example int + 123 , -75 , +95 long +L 123000L , -750l , +154L float ( +)?('.' *)? (E(-|+)? +)?F 123.0f , -75.0e-10F , +95.789f double ( +)?('.' *)? (E(-|+)? +)?D? 123.0 , 123.0D , -75.0e-10D , +95.789d bool (true|false) true , false , TRUE , FALSE string '( char * !('|\"|\"\"\"| line ))' or \"( char * !(\"|\"\"\"| line ))\" or \"\"\"( char * !(\"\"\"))\"\"\" 'Any text.' , \"Text with 'single' quotes.\" , \"\"\" Text with 'single' quotes, \"double\" quotes, and new lines. \"\"\" Time Time is a special type of LONG value that denotes time using digits and their unit in the format ( digit + unit )+ . At execution, the time gets converted into milliseconds and returns a LONG value. Unit Syntax Year year | years Month month | months Week week | weeks Day day | days Hour hour | hours Minutes minute | minutes | min Seconds second | seconds | sec Milliseconds millisecond | milliseconds Example 1 hour and 25 minutes can by written as 1 hour and 25 minutes which is equal to the LONG value 5100000 . Select The select clause in Siddhi query defines the output event attributes of the query. Following are some basic query projection operations supported by select. Action Description Select specific attributes for projection Only select some of the input attributes as query output attributes. E.g., Select and output only roomNo and temp attributes from the TempStream stream. from TempStream select roomNo, temp insert into RoomTempStream; Select all attributes for projection Select all input attributes as query output attributes. This can be done by using asterisk ( * ) or by omitting the select clause itself. E.g., Both following queries select all attributes of TempStream input stream and output all attributes to NewTempStream stream. from TempStream select * insert into NewTempStream; or from TempStream insert into NewTempStream; Name attribute Provide a unique name for each output attribute generated by the query. This can help to rename the selected input attributes or assign an attribute name to a projection operation such as function, aggregate-function, mathematical operation, etc, using as keyword. E.g., Query that renames input attribute temp to temperature and function currentTimeMillis() as time . from TempStream select roomNo, temp as temperature, currentTimeMillis() as time insert into RoomTempStream; Constant values as attributes Creates output attributes with a constant value. Any constant value of type STRING , INT , LONG , DOUBLE , FLOAT , BOOL , and time as given in the values section can be defined. E.g., Query specifying 'C' as the constant value for the scale attribute. from TempStream select roomNo, temp, 'C' as scale insert into RoomTempStream; Mathematical and logical expressions in attributes Defines the mathematical and logical operations that need to be performed to generating output attribute values. These expressions are executed in the precedence order given below. Operator precedence Operator Distribution Example () Scope (cost + tax) * 0.05 IS NULL Null check deviceID is null NOT Logical NOT not (price > 10) * , / , % Multiplication, division, modulus temp * 9/5 + 32 + , - Addition, subtraction temp * 9/5 - 32 < , < = , > , >= Comparators: less-than, greater-than-equal, greater-than, less-than-equal totalCost >= price * quantity == , != Comparisons: equal, not equal totalCost != price * quantity IN Checks if value exist in the table roomNo in ServerRoomsTable AND Logical AND temp < 40 and humidity < 40 OR Logical OR humidity < 40 or humidity >= 60 E.g., Query converts temperature from Celsius to Fahrenheit, and identifies rooms with room number between 10 and 15 as server rooms. from TempStream select roomNo, temp * 9/5 + 32 as temp, 'F' as scale, roomNo > 10 and roomNo < 15 as isServerRoom insert into RoomTempStream; Function Functions are pre-configured operations that can consumes zero, or more parameters and always produce a single value as result. It can be used anywhere an attribute can be used. Purpose It encapsulate pre-configured reusable execution logic allowing users to execute the logic anywhere just by calling the function. This also make writing SiddhiApps simple and easy to understand. Syntax The syntax of function is as follows, ( namespace :)? function name ( ( parameter (, parameter )*)? ) Here, the namespace and function name together uniquely identifies the function. The function name is used to specify the operation provided by the function, and the namespace is used to identify the extension where the function exists. The inbuilt functions do not belong to a namespace, and hence namespace is omitted when they are defined. The parameter s define the input parameters that the function accepts. The input parameters can be attributes, constant values, results of other functions, results of mathematical or logical expressions, or time values. The number and type of parameters a function accepts depend on the function itself. Note Functions, mathematical expressions, and logical expressions can be used in a nested manner. Inbuilt functions Following are some inbuilt Siddhi functions. Inbuilt function Description eventTimestamp Returns event's timestamp. currentTimeMillis Returns current time of SiddhiApp runtime. default Returns a default value if the parameter is null. ifThenElse Returns parameters based on a conditional parameter. UUID Generates a UUID. cast Casts parameter type. convert Converts parameter type. coalesce Returns first not null input parameter. maximum Returns the maximum value of all parameters. minimum Returns the minimum value of all parameters. instanceOfBoolean Checks if the parameter is an instance of Boolean. instanceOfDouble Checks if the parameter is an instance of Double. instanceOfFloat Checks if the parameter is an instance of Float. instanceOfInteger Checks if the parameter is an instance of Integer. instanceOfLong Checks if the parameter is an instance of Long. instanceOfString Checks if the parameter is an instance of String. createSet Creates HashSet with given input parameters. sizeOfSet Returns number of items in the HashSet, that's passed as a parameter. Extension functions Several pre written functions can be found in the Siddhi extensions available here . Several pre written functions can be found under siddhi-execution-* extensions available here . Example 1 Function with name ifThenElse accepting three input parameters, first parameter being a bool condition price 700 and the second and the third parameters being the output for if case 'high' , and else case 'low' . ifThenElse(price 700, 'high', 'low') Example 2 math:ceil(inValue) Function with name ceil in math namespace accepting a single input parameters 56.89 and produces ceiling value 57 as output. math:ceil(56.89) Example 3 Query to convert the roomNo to string using convert function, find the maximum temperature reading with maximum function, and to add a unique messageID using the UUID function. from TempStream select convert(roomNo, 'string') as roomNo, maximum(tempReading1, tempReading2) as temp, UUID() as messageID insert into RoomTempStream; Filter Filters filter events arriving on input streams based on specified conditions. They accept any type of condition including a combination of attributes, constants, functions, and others, that produces a Boolean result. Filters allow events to pass through if the condition results in true , and drops if it results in a false . Purpose Filter helps to select the events that are relevant for processing and omit the ones that are not. Syntax Filter conditions should be defined in square brackets ( [] ) next to the input stream as shown below. from input stream [ filter condition ] select attribute name , attribute name , ... insert into output stream Example Query to filter TempStream stream events, having roomNo within the range of 100-210 and temperature greater than 40 degrees, and insert the filtered results into HighTempStream stream. from TempStream[(roomNo = 100 and roomNo 210) and temp 40] select roomNo, temp insert into HighTempStream; Stream Function Stream functions process the events that arrive via the input stream (or named-window ), to generate zero or more new events with one or more additional output attributes for each event. Unlike the standard functions, they operate directly on the streams or (or named-windows ) and can add the function outputs via predefined attributes on the generated events. Purpose Stream function is useful when a function produces more than one output for the given input parameters. In this case, the outputs are added to the event, using newly introduced attributes with predefined attribute names. Syntax Stream function should be defined next to the input stream or named-windows along the # prefix as shown below. from input stream #( namespace :)? stream function name ( parameter , parameter , ... ) select attribute name , attribute name , ... insert into output stream Here, the namespace and stream function name together uniquely identifies the stream function. The stream function name is used to specify the operation provided by the window, and the namespace is used to identify the extension where the stream function exists. The inbuilt stream functions do not belong to a namespace, and hence namespace is omitted when they are defined. The parameter s define the input parameters that the stream function accepts. The input parameters can be attributes, constant values, functions, mathematical or logical expressions, or time values. The number and type of parameters a stream function accepts depend on the stream function itself. Inbuilt stream functions Following is an inbuilt Siddhi stream function. Inbuilt stream function Description pol2Cart Calculates cartesian coordinates x and y for the given theta , and rho coordinates. Extension stream functions Several pre written stream functions can be found in the Siddhi extensions available here . Example A query to calculate cartesian coordinates from theta , and rho attribute values optioned from the PolarStream stream, and to insert the results x and y via CartesianStream stream. define stream PolarStream (theta double, rho double); from PolarStream#pol2Cart(theta, rho) select x, y insert into CartesianStream; Here, the pol2Cart stream function amend the events with x and y attributes with respective cartesian values. Window Windows capture a subset of events from input streams and retain them for a period of time based on a specified criterion. The criterion defines when and how the events should be evicted from the window. Such as events getting evicted based on time duration, or number of events in the window, and the way they get evicted is in sliding (one by one) or tumbling (batch) manner. In a query, each input stream can at most have only one window associated with it. Purpose Windows help to retain events based on a criterion, such that the values of those events can be aggregated, correlated or checked, if the event of interest is in the window. Syntax Window should be defined next to the input stream along the #window prefix as shown below. from input stream #window.( namespace :)? window name ( parameter , parameter , ... ) select attribute name , attribute name , ... insert output event type ? into output stream Here, the namespace and window name together uniquely identifies the window. The window name is used to specify the operation provided by the window, and the namespace is used to identify the extension where the window exists. The inbuilt windows do not belong to a namespace, and hence namespace is omitted when they are defined. The parameter s define the input parameters that the window accepts. The input parameters can be attributes, constant values, functions, mathematical or logical expressions, or time values. The number and type of parameters a window accepts depend on the window itself. Note Filter conditions and stream functions can be applied both before and/or after the window. Inbuilt windows Following are some inbuilt Siddhi windows. Inbuilt function Description time Retains events based on time in a sliding manner. timeBatch Retains events based on time in a tumbling/batch manner. length Retains events based on number of events in a sliding manner. lengthBatch Retains events based on number of events in a tumbling/batch manner. timeLength Retains events based on time and number of events in a sliding manner. session Retains events for each session based on session key. batch Retains events of last arrived event chunk. sort Retains top-k or bottom-k events based on a parameter value. cron Retains events based on cron time in a tumbling/batch manner. externalTime Retains events based on event time value passed as a parameter in a sliding manner. externalTimeBatch Retains events based on event time value passed as a parameter in a a tumbling/batch manner. delay Retains events and delays the output by the given time period in a sliding manner. Extension windows Several pre written windows can be found under siddhi-execution-* extensions available here . Example 1 Query to find out the maximum temperature out of the last 10 events , using the window of length 10 and max() aggregation function, from the TempStream stream and insert the results into the MaxTempStream stream. from TempStream#window.length(10) select max(temp) as maxTemp insert into MaxTempStream; Here, the length window operates in a sliding manner where the following 3 event subsets are calculated and outputted when a list of 12 events are received in sequential order. Subset Event Range 1 1 - 10 2 2 - 11 3 3 - 12 Example 2 Query to find out the maximum temperature out of the every 10 events , using the window of lengthBatch 10 and max() aggregation function, from the TempStream stream and insert the results into the MaxTempStream stream. from TempStream#window.lengthBatch(10) select max(temp) as maxTemp insert into MaxTempStream; Here, the window operates in a batch/tumbling manner where the following 3 event subsets are calculated and outputted when a list of 30 events are received in a sequential order. Subset Event Range 1 1 - 10 2 11 - 20 3 21 - 30 Example 3 Query to find out the maximum temperature out of the events arrived during last 10 minutes , using the window of time 10 minutes and max() aggregation function, from the TempStream stream and insert the results into the MaxTempStream stream. from TempStream#window.time(10 min) select max(temp) as maxTemp insert into MaxTempStream; Here, the time window operates in a sliding manner with millisecond accuracy, where it will process events in the following 3 time durations and output aggregated events when a list of events are received in a sequential order. Subset Time Range (in ms) 1 1:00:00.001 - 1:10:00.000 2 1:00:01.001 - 1:10:01.000 3 1:00:01.033 - 1:10:01.034 Example 4 Query to find out the maximum temperature out of the events arriving every 10 minutes , using the window of timeBatch 10 and max() aggregation function, from the TempStream stream and insert the results into the MaxTempStream stream. from TempStream#window.timeBatch(10 min) select max(temp) as maxTemp insert into MaxTempStream; Here, the window operates in a batch/tumbling manner where the window will process events in the following 3 time durations and output aggregated events when a list of events are received in a sequential order. Subset Time Range (in ms) 1 1:00:00.001 - 1:10:00.000 2 1:10:00.001 - 1:20:00.000 3 1:20:00.001 - 1:30:00.000 Example 5 Query to find out the unique number of deviceID s arrived over last 1 minute , using the time window in the unique extension, and to insert the results into the UniqueCountStream stream. define stream TempStream (deviceID long, roomNo int, temp double); from TempStream#window.unique:time(deviceID, 1 sec) select count(deviceID) as deviceIDs insert into UniqueCountStream ; Event Type Query output depends on the current and expired event types produced by the query based on its internal processing state. By default all queries produce current events upon event arrival. The queries containing windows additionally produce expired events when events expire from those windows. Purpose Event type helps to identify how the events were produced and to specify when a query should output such events to the output stream, such as output processed events only upon new event arrival to the query, upon event expiry from the window, or upon both cases. Syntax Event type should be defined in between insert and into keywords for insert queries as follows. from input stream #window. window name ( parameter , parameter , ... ) select attribute name , attribute name , ... insert event type into output stream Event type should be defined next to the for keyword for delete queries as follows. from input stream #window. window name ( parameter , parameter , ... ) select attribute name , attribute name , ... delete table (for event type )? on condition Event type should be defined next to the for keyword for update queries as follows. from input stream #window. window name ( parameter , parameter , ... ) select attribute name , attribute name , ... update table (for event type )? set table . attribute name = ( attribute name | expression )?, table . attribute name = ( attribute name | expression )?, ... on condition Event type should be defined next to the for keyword for update or insert queries as follows. from input stream #window. window name ( parameter , parameter , ... ) select attribute name , attribute name , ... update or insert into table (for event type )? set table . attribute name = expression , table . attribute name = expression , ... on condition The event types can be defined using the following keywords to manipulate query output. Event types Description current events Outputs processed events only upon new event arrival at the query. This is default behavior when no specific event type is specified. expired events Outputs processed events only upon event expiry from the window. all events Outputs processed events when both new events arrive at the query as well as when events expire from the window. Note Controlling query output based on the event types neither alters query execution nor its accuracy. Example Query to output processed events only upon event expiry from the 1 minute time window to the DelayedTempStream stream. This query helps to delay events by a minute. from TempStream#window.time(1 min) select * insert expired events into DelayedTempStream Note This is just to illustrate how expired events work, it is recommended to use delay window for use cases where we need to delay events by a given time period of time. Aggregate Function Aggregate functions are pre-configured aggregation operations that can consume zero, or more parameters from multiple events and produce a single value as result. They can be only used in query projection (as part of the select clause). When a query comprises a window, the aggregation will be constrained to the events in the window, and when it does not have a window, the aggregation is performed from the first event the query has received. Purpose Aggregate functions encapsulate pre-configured reusable aggregate logic allowing users to aggregate values of multiple events together. When used with batch/tumbling windows this will also reduce the number of output events produced. Syntax Aggregate function can be used in query projection (as part of the select clause) alone or as a part of another expression. In all cases, the output produced should be properly mapped to the output stream attribute of the query using the as keyword. The syntax of aggregate function is as follows, from input stream #window. window name ( parameter , parameter , ... ) select ( namespace :)? aggregate function name ( parameter , parameter , ... ) as attribute name , attribute2 name , ... insert into output stream ; Here, the namespace and aggregate function name together uniquely identifies the aggregate function. The aggregate function name is used to specify the operation provided by the aggregate function, and the namespace is used to identify the extension where the aggregate function exists. The inbuilt aggregate functions do not belong to a namespace, and hence namespace is omitted when they are defined. The parameter s define the input parameters the aggregate function accepts. The input parameters can be attributes, constant values, results of other functions or aggregate functions, results of mathematical or logical expressions, or time values. The number and type of parameters an aggregate function accepts depend on the aggregate function itself. Inbuilt aggregate functions Following are some inbuilt aggregation functions. Inbuilt aggregate function Description sum Calculates the sum from a set of values. count Calculates the count from a set of values. distinctCount Calculates the distinct count based on a parameter from a set of values. avg Calculates the average from a set of values. max Finds the maximum value from a set of values. max Finds the minimum value from a set of values. maxForever Finds the maximum value from all events throughout its lifetime irrespective of the windows. minForever Finds the minimum value from all events throughout its lifetime irrespective of the windows. stdDev Calculates the standard deviation from a set of values. and Calculates boolean and from a set of values. or Calculates boolean or from a set of values. unionSet Constructs a Set by unioning set of values. Extension aggregate functions Several pre written aggregate functions can be found under siddhi-execution-* extensions available here . Example Query to calculate average, maximum, minimum and 97 th percentile values on temp attribute of the TempStream stream in a sliding manner, from the events arrived over the last 10 minutes and to produce output events with attributes avgTemp , maxTemp , minTemp and percentile97 respectively to the AggTempStream stream. from TempStream#window.time(10 min) select avg(temp) as avgTemp, max(temp) as maxTemp, min(temp) as minTemp, math:percentile(temp, 97.0) as percentile97 insert into AggTempStream; Group By Group By groups events based on one or more specified attributes to perform aggregate operations. Purpose Group By helps to perform aggregate functions independently for each given group-by key combination. Syntax The syntax for the Group By with aggregate function is as follows. from input stream #window. window name (...) select aggregate function ( parameter , parameter , ...) as attribute1 name , attribute2 name , ... group by attribute1 name , attribute2 name , ... insert into output stream ; Here the group by attributes should be defined next to the group by keyword separating each attribute by a comma. Example Query to calculate the average temp per each roomNo and deviceID combination, from the events arrived from TempStream stream, during the last 10 minutes time-window in a sliding manner. from TempStream#window.time(10 min) select roomNo, deviceID, avg(temp) as avgTemp group by roomNo, deviceID insert into AvgTempStream; Having Having filters events at the query output using a specified condition on query output stream attributes. It accepts any type of condition including a combination of output stream attributes, constants, and/or functions that produces a Boolean result. Having, allow events to passthrough if the condition results in true , and drops if it results in a false . Purpose Having helps to select the events that are relevant for the output based on the attributes those are produced by the select clause and omit the ones that are not. Syntax The syntax for the Having clause is as follows. from input stream #window. window name ( ... ) select aggregate function ( parameter , parameter , ...) as attribute1 name , attribute2 name , ... group by attribute1 name , attribute2 name ... having condition insert into output stream ; Here the having condition should be defined next to the having keyword, and it can be used with or without group by clause. Example Query to calculate the average temp per roomNo for the events arrived on the last 10 minutes, and send alerts for each event having avgTemp more than 30 degrees. from TempStream#window.time(10 min) select roomNo, avg(temp) as avgTemp group by roomNo having avgTemp 30 insert into AlertStream; Order By Order By, orders the query results in ascending or descending order based on one or more specified attributes. By default the order by attribute orders the events in ascending order, and by adding desc keyword, the events can be ordered in descending order. When more than one attribute is defined the attributes defined towards the left will have more precedence in ordering than the ones defined in right. Purpose Order By helps to sort the events in the query output chunks. Order By will only be effective when query outputs a lot of events together such as in batch windows than for sliding windows where events are emitted one at a time. Syntax The syntax for the Order By clause is as follows: from input stream #window. window name ( ... ) select aggregate function ( parameter , parameter , ...) as attribute1 name , attribute2 name , ... group by attribute1 name , attribute2 name ... having condition order by attribute1 name (asc|desc)?, attribute2 name (asc|desc)?, ... insert into output stream ; Here, the order by attributes ( attributeN name ) should be defined next to the order by keyword separating each by a comma, and optionally the event ordering can be specified using asc (default) or desc keywords to respectively define ascending and descending. Example Query to calculate the average temp , per roomNo and deviceID combination, on every 10 minutes batches, and order the generated output events in ascending order by avgTemp and then in descending order of roomNo (if there are more events having the same avgTemp value) before emitting them to the AvgTempStream stream. from TempStream#window.timeBatch(10 min) select roomNo, deviceID, avg(temp) as avgTemp group by roomNo, deviceID order by avgTemp, roomNo desc insert into AvgTempStream; Limit Offset These provide a way to select a limited number of events (via limit) from the desired index (using an offset) from the output event chunks produced by the query. Purpose Limit Offset helps to output only the selected set of events from large event batches. This will be very useful with Order By clause where one can order the output and extract the topK or bottomK events, and even use it to paginate through the dataset by obtaining set of events from the middle. Syntax The syntax for the Limit Offset clauses is as follows: from input stream #window. window name ( ... ) select aggregate function ( parameter , parameter , ...) as attribute1 name , attribute2 name , ... group by attribute1 name , attribute2 name ... having condition order by attribute1 name (asc | desc)?, attribute2 name ( ascend/descend )?, ... limit positive integer ? offset positive integer ? insert into output stream ; Here both limit and offset are optional and both can be defined by adding a positive integer next to their keywords, when limit is omitted the query will output all the events, and when offset is omitted 0 is taken as the default offset value. Example 1 Query to calculate the average temp , per roomNo and deviceID combination, for every 10 minutes batches, from the events arriving at the TempStream stream, and emit only two events having the highest avgTemp value. from TempStream#window.timeBatch(10 min) select roomNo, deviceID, avg(temp) as avgTemp group by roomNo, deviceID order by avgTemp desc limit 2 insert into HighestAvgTempStream; Example 2 Query to calculate the average temp , per roomNo and deviceID combination, for every 10 minutes batches, for the events arriving at the TempStream stream, and emits only the third, forth and fifth events when sorted in descending order based on their avgTemp value. from TempStream#window.timeBatch(10 min) select roomNo, deviceID, avg(temp) as avgTemp group by roomNo, deviceID order by avgTemp desc limit 3 offset 2 insert into HighestAvgTempStream; Stream Processor Stream processors are a combination of stream stream functions and windows . They work directly on the input streams (or named-windows ), to generate zero or more new events with zero or more additional output attributes while having the ability to retain and arbitrarily emit events. They are more advanced than stream functions as they can retain and arbitrarily emit events, and they are more advanced than windows because they can add additional attributes to the events. Purpose Stream processors help to achieve complex execution logics that cannot be achieved by other constructs such as functions , aggregate functions , stream functions and windows . Syntax Stream processor should be defined next to the input stream or named-windows along the # prefix as shown below. from input stream #( namespace :)? stream processor name ( parameter , parameter , ... ) select attribute name , attribute name , ... insert into output stream Here, the namespace and stream processor name together uniquely identifies the stream processor. The stream processor name is used to specify the operation provided by the window, and the namespace is used to identify the extension where the stream processor exists. The inbuilt stream processors do not belong to a namespace, and hence namespace is omitted when they are defined. The parameter s define the input parameters that the stream processor accepts. The input parameters can be attributes, constant values, processors, mathematical or logical expressions, or time values. The number and type of parameters a stream processor accepts depend on the stream processor itself. Inbuilt stream processors Following is an inbuilt Siddhi stream processor. Inbuilt stream processor Description log Logs the message on the given priority with or without the processed event. Extension stream processors Several pre written stream processors can be found in the Siddhi extensions available here . Note Stream processors can be used before or after filters , stream functions , windows , or other stream processors. Example A query to log a message \"Sample Event :\" along with the event on \"INFO\" log level for all events of InputStream Stream. from InputStream#log(\"INFO\", \"Sample Event :\", true) select * insert into IgnoreStream; Join (Stream) Joins combine events from two streams in real-time based on a specified condition. Purpose Join provides a way of correlating events of two steams and in addition aggregating them based on the defined windows. Two streams cannot directly join as they are stateless, and they do not retain events. Therefore, each stream needs to be associated with a window for joining as it can retain events. Join also accepts a condition to match events against each event stream window. During the joining process each incoming event of each stream is matched against all the events in the other stream's window based on the given condition, and the output events are generated for all the matching event pairs. When there is no window associated with the joining steam, and empty window with length zero is assigned to the steam by default, to enable join process while preserving stream's stateless nature. Note Join can also be performed with table , named-aggregation , or named-windows . Syntax The syntax to join two streams is as follows: from input stream ( non window handler )*(#window. window name ( parameter , ... ))? (unidirectional)? (as reference )? join type input stream ( non window handler )*(#window. window name ( parameter , ... ))? (unidirectional)? (as reference )? (on join condition )? select reference . attribute name , reference . attribute name , ... insert into output stream Here, both the streams can have optional non window handlers (filters, stream functions, and stream processors) followed by a window associated with them. They can also have an optional join condition next to the on keyword to match events from both windows to generate combined output events. Window should be defined as the last element of each joining stream. Join query expects a window to be defined as the last element of each joining stream, therefore a filter cannot be defined after the window. Supported join types Following are the supported join operations. Inner join (join) This is the default behavior of a join operation, and the join keyword is used to join both the streams. The output is generated only if there is a matching event in both the stream windows when either of the streams triggers the join operation. Left outer join The left outer join keyword is used to join two streams while producing all left stream events to the output. Here, the output is generated when right stream triggers the join operation and finds matching events in the left stream window to perform the join, and in all cases where the left stream triggers the join operation. Here, when the left stream finds matching events in the right stream window, it uses them for the join, and if there are no matching events, then it uses null values for the join operation. Right outer join This is similar to a left outer join and the right outer join keyword is used to join two streams while producing all right stream events to the output. It generate output in all cases where the right stream triggers the join operation even if there are no matching events in the left stream window. Full outer join The full outer join combines the results of left outer join and right outer join. The full outer join keyword is used to join the streams while producing both left and stream events to the output. Here, the output is generated in all cases where the left or right stream triggers the join operation, and when a stream finds matching events in the other stream window, it uses them for the join, and if there are no matching events, then it uses null values instead. Cross join In either of these cases, when the join condition is omitted, the triggering event will successfully match against all the events in the other stream window, producing a cross join behavior. Unidirectional join operation By default, events arriving on either stream trigger the join operation and generate the corresponding output. However, this join behavior can be controlled by adding the unidirectional keyword next to one of the streams as depicted in the join query syntax above. This enables only the stream with the unidirectional to trigger the join operation. Therefore the events arriving on the other stream will neither trigger the join operation nor produce any output, but rather they only update their stream's window state. The unidirectional keyword cannot be applied on both join streams. This is because the default behavior already allows both the streams to trigger the join operation. Example 1 (join) A query to generate output when there is a matching event having equal symbol and companyID combination from the events arrived in the last 10 minutes on StockStream stream and the events arrived in the last 20 minutes on TwitterStream stream. define stream StockStream (symbol string, price float, volume long); define stream TwitterStream (companyID string, tweet string); from StockStream#window.time(10 min) as S join TwitterStream#window.time(20 min) as T on S.symbol== T.companyID select S.symbol as symbol, T.tweet, S.price insert into OutputStream ; Possible OutputStream outputs as follows (\"FB\", \"FB is great!\", 23.5f) (\"GOOG\", \"Its time to Google!\", 54.5f) Example 2 (with no join condition) A query to generate output for all possible event combinations from the last 5 events of the StockStream stream and the events arrived in the last 1 minutes on TwitterStream stream. define stream StockStream (symbol string, price float, volume long); define stream TwitterStream (companyID string, tweet string); from StockStream#window.length(5) as S join TwitterStream#window.time(1 min) as T select S.symbol as symbol, T.tweet, S.price insert into OutputStream ; Possible OutputStream outputs as follows, (\"FB\", \"FB is great!\", 23.5f) (\"FB\", \"Its time to Google!\", 23.5f) (\"GOOG\", \"FB is great!\", 54.5f) (\"GOOG\", \"Its time to Google!\", 54.5f) Example 3 (left outer join) A query to generate output for all events arriving in the StockStream stream regardless of whether there is a matching companyID for symbol exist in the events arrived in the last 20 minutes on TwitterStream stream, and generate output for the events arriving in the StockStream stream only when there is a matchine symbol and companyID combination exist in the events arrived in the last 10 minutes on StockStream stream. define stream StockStream (symbol string, price float, volume long); define stream TwitterStream (companyID string, tweet string); from StockStream#window.time(10 min) as S left outer join TwitterStream#window.time(20 min) as T on S.symbol== T.companyID select S.symbol as symbol, T.tweet, S.price insert into OutputStream ; Possible OutputStream outputs as follows, (\"FB\", \"FB is great!\", 23.5f) (\"GOOG\", null, 54.5f) //when there are no matching event in TwitterStream Example 3 (full outer join) A query to generate output for all events arriving in the StockStream stream and in the TwitterStream stream regardless of whether there is a matching companyID for symbol exist in the other stream window or not. define stream StockStream (symbol string, price float, volume long); define stream TwitterStream (companyID string, tweet string); from StockStream#window.time(10 min) as S full outer join TwitterStream#window.time(20 min) as T on S.symbol== T.companyID select S.symbol as symbol, T.tweet, S.price insert into OutputStream ; Possible OutputStream outputs as follows, (\"FB\", \"FB is great!\", 23.5f) (\"GOOG\", null, 54.5f) //when there are no matching event in TwitterStream (null, \"I like to tweet!\", null) //when there are no matching event in StockStream Example 3 (unidirectional join) A query to generate output only when events arrive on StockStream stream find a matching event having equal symbol and companyID combination against the events arrived in the last 20 minutes on TwitterStream stream. define stream StockStream (symbol string, price float, volume long); define stream TwitterStream (companyID string, tweet string); from StockStream#window.time(10 min) unidirectional as S join TwitterStream#window.time(20 min) as T on S.symbol== T.companyID select S.symbol as symbol, T.tweet, S.price insert into OutputStream ; Possible OutputStream outputs as follows, (\"FB\", \"FB is great!\", 23.5f) (\"GOOG\", \"Its time to Google!\", 54.5f) Here both outputs will be initiated by events arriving on StockStream . Pattern The pattern is a state machine implementation that detects event occurrences from events arrived via one or more event streams over time. It can repetitively match patterns, count event occurrences, and use logical event ordering (using and , or , and not ). Purpose The pattern helps to achieve Complex Event Processing (CEP) capabilities by detecting various pre-defined event occurrence patterns in realtime. Pattern query does not expect the matching events to occur immediately after each other, and it can successfully correlate the events who are far apart and having other events in between. Syntax The syntax for a pattern query is as follows, from ( (every)? ( event reference =)? input stream [ filter condition ]( min count : max count )? | (every)? ( event reference =)? input stream [ filter condition ] (and|or) ( event reference =)? input stream [ filter condition ] | (every)? not input stream [ filter condition ] (and event reference = input stream [ filter condition ] | for time gap ) ) - ... (within time gap )? select event reference . attribute name , event reference . attribute name , ... insert into output stream Items Description -> Indicates an event will follow the given event. The subsequent event does not necessarily have to occur immediately after the preceding event. The condition to be met by the preceding event should be added before the -> , and the condition to be met by the subsequent event should be added after the -> . every An optional keyword defining when a new event matching state-machine should be initiated to repetitively match the pattern. When this keyword is not used, the event matching state-machine will be initiated only once. within time gap An optional within clause that defines the time duration within which all the matching events should occur. min count : max count Determines the number of minimum and maximum number of events that should the matched at the given condition. Possible values for the min and max count and their behavior is as follows, Syntex Description Example n1:n2 Matches n1 to n2 events (including n1 and not more than n2 ). 1:4 matches 1 to 4 events. n: Matches n or more events (including n ). 2:> matches 2 or more events. :n Matches up to n events (excluding n ). :5 matches up to 5 events. n Matches exactly n events. 5 matches exactly 5 events. and Allows both of its condition to be matched by two distinct events in any order. or Only expects one of its condition to be matched by an event. Here the event reference of the unmatched condition will be null . not condition1 and condition2 Detects the event matching condition2 before any event matching condition1 . not condition1> for time period> Detects no event matching on condition1 for the specified time period . event reference An optional reference to access the matching event for further processing. All conditions can be assigned to an event reference to collect the matching event occurrences, other than the condition used for not case (as there will not be any event matched against it). Non occurrence of events. Siddhi detects non-occurrence of events using the not keyword, and its effective non-occurrence checking period is bounded either by fulfillment of a condition associated by and or via an expiry time using time period . Logical correlation of multiple conditions. Siddhi can only logically correlate two conditions at a time using keywords such as and , or , and not . When more than two conditions need to be logically correlated, use multiple pattern queries in a chaining manner, at a time correlating two logical conditions and streaming the output to a downstream query to logically correlate the results with other logical conditions. Event selection The event reference in pattern queries is used to retrieve the matched events. When a pattern condition is intended to match only a single event, then its attributes can be retrieved by referring to its reference as event reference . attribute name . An example of this is as follows. e1.symbol , refers to the symbol attribute value of the matching event e1 . But when the pattern condition is associated with min count : max count , it is expected to match against on multiple events. Therefore, an event from the matched event collection should be retrieved using the event index from its reference. Here the indexes are specified in square brackets next to event reference, where index 0 referring to the first event, and a special index last referring to the last available event in the collection. Attribute values of all the events in the matching event collection can be accessed a list, by referring to their event reference without an index. Some possible indexes and their behavior is as follows. e1[0].symbol , refers to the symbol attribute value of the 1 st event in reference e1 . e1[3].price , refers to the price attribute value of the 4 th event in reference e1 . e1[last].symbol , refers to the symbol attribute value of the last event in reference e1 . e1[last - 1].symbol , refers to the symbol attribute value of one before the last event in reference e1 . e1.symbol , refers to the list of symbol attribute values of all events in the event collection in reference e1 , as a list object. The system returns null when accessing attribute values, when no matching event is assigned to the event reference (as in when two conditions are combined using or ) or when the provided index is greater than the last event index in the event collection. Example 1 (Every) A query to send an alerts when temperature of a room increases by 5 degrees within 10 min. from every( e1=TempStream ) - e2=TempStream[ e1.roomNo == roomNo and (e1.temp + 5) = temp ] within 10 min select e1.roomNo, e1.temp as initialTemp, e2.temp as finalTemp insert into AlertStream; Here, the matching process begins for each event in the TempStream stream (as every is used with e1=TempStream ), and if another event arrives within 10 minutes with a value for temp attribute being greater than or equal to e1.temp + 5 of the initial event e1 , an output is generated via the AlertStream . Example 2 (Event collection) A query to find the temperature difference between two regulator events. define stream TempStream (deviceID long, roomNo int, temp double); define stream RegulatorStream (deviceID long, roomNo int, tempSet double, isOn bool); from every e1=RegulatorStream - e2=TempStream[e1.roomNo==roomNo] 1: - e3=RegulatorStream[e1.roomNo==roomNo] select e1.roomNo, e2[0].temp - e2[last].temp as tempDiff insert into TempDiffStream; Here, one or more TempStream events having the same roomNo as of the RegulatorStream stream event matched in e1 is collected, and among them, the first and the last was retrieved to find the temperature difference. Example 3 (Logical or condition) Query to send the stop control action to the regulator via RegulatorActionStream when the key is removed from the hotel room. Here the key actions are monitored via RoomKeyStream stream, and the regulator state is monitored through RegulatorStateChangeStream stream. define stream RegulatorStateChangeStream(deviceID long, roomNo int, tempSet double, action string); define stream RoomKeyStream(deviceID long, roomNo int, action string); from every e1=RegulatorStateChangeStream[ action == 'on' ] - e2=RoomKeyStream[ e1.roomNo == roomNo and action == 'removed' ] or e3=RegulatorStateChangeStream[ e1.roomNo == roomNo and action == 'off'] select e1.roomNo, ifThenElse( e2 is null, 'none', 'stop' ) as action having action != 'none' insert into RegulatorActionStream; Here, the query sends a stop action on RegulatorActionStream stream, if a removed action is triggered in the RoomKeyStream stream before the regulator state changing to off which is notified via RegulatorStateChangeStream stream. Example 4 (Logical not condition) Query to generate alerts if the regulator gets switched off before the temperature reaches 12 degrees. define stream RegulatorStateChangeStream(deviceID long, roomNo int, tempSet double, action string); define stream TempStream (deviceID long, roomNo int, temp double); from every e1=RegulatorStateChangeStream[action == 'start'] - not TempStream[e1.roomNo == roomNo and temp = 12] and e2=RegulatorStateChangeStream[e1.roomNo == roomNo and action == 'off'] select e1.roomNo as roomNo insert into AlertStream; Here, the query alerts the roomNo via AlertStream stream, when no temperature events having less than 12 arrived in the TempStream between the start and off actions of the regulator, notified via RegulatorActionStream stream. Example 5 (Logical not condition) Query to alert if the room temperature does not reduce to the set value within 5 minutes after switching on the regulator. define stream RegulatorStateChangeStream(deviceID long, roomNo int, tempSet double, action string); define stream TempStream (deviceID long, roomNo int, temp double); from e1=RegulatorStateChangeStream[action == 'start'] - not TempStream[e1.roomNo == roomNo and temp = e1.tempSet] for '5 min' select e1.roomNo as roomNo insert into AlertStream; Here, the query alerts the roomNo via AlertStream stream, when no temperature events having less than tempSet temperature arrived in the TempStream within 5 minutes of the regulator start action arrived via RegulatorActionStream stream. Example 6 (Detecting event non-occurrence) Following table presents some non-occurrence event matching scenarios that can be implemented using patterns. Pattern Description Sample Scenario not A for time period The non-occurrence of event A within time period after system start up. Alerts if the taxi has not reached its destination within 30 minutes, indicating that the passenger might be in danger. not A for time period and B Event A does not occur within time period , but event B occurs at some point in time. Alerts if the taxi has not reached its destination within 30 minutes, and the passenger has marked that he/she is in danger at some point in time. not A for time period or B Either event A does not occur within time period , or event B occurs at some point in time. Alerts if the taxi has not reached its destination within 30 minutes, or if the passenger has marked that he/she is in danger at some point in time. not A for time period 1 and not B for time period 2 Event A does not occur within time period 1 , and event B also does not occur within time period 2 . Alerts if the taxi has not reached its destination within 30 minutes, and the passenger has not marked himself/herself not in danger within the same time period. not A for time period 1 or not B for time period 2 Either event A does not occur within time period 1 , or event B occurs within time period 2 . Alerts if the taxi has not reached its destination A within 20 minutes, or reached its destination B within 30 minutes. A \u2192 not B for time period Event B does not occur within time period after the occurrence of event A. Alerts if the taxi has reached its destination, but it has been not followed by a payment record within 10 minutes. not A and B or A and not B Event A does not occur before event B. Alerts if the taxi is stated before activating the taxi fare calculator. Sequence The sequence is a state machine implementation that detects consecutive event occurrences from events arrived via one or more event streams over time. Here all matching events need to arrive consecutively , and there should not be any non-matching events in between the matching sequence of events. The sequence can repetitively match event sequences, count event occurrences, and use logical event ordering (using and , or , and not ). Purpose The sequence helps to achieve Complex Event Processing (CEP) capabilities by detecting various pre-defined consecutive event occurrence sequences in realtime. Sequence query does expect the matching events to occur immediately after each other, and it can successfully correlate the events who do not have other events in between. Syntax The syntax for a sequence query is as follows: from ( (every)? ( event reference =)? input stream [ filter condition ] (+|*|?)? | ( event reference =)? input stream [ filter condition ] (and|or) ( event reference =)? input stream [ filter condition ] | not input stream [ filter condition ] (and event reference = input stream [ filter condition ] | for time gap ) ), ... (within time gap )? select event reference . attribute name , event reference . attribute name , ... insert into output stream Items Description , Indicates the immediate next event that follows the given event. The condition to be met by the preceding event should be added before the , , and the condition to be met by the subsequent event should be added after the , . every An optional keyword defining when a new event matching state-machine should be initiated to repetitively match the sequence. When this keyword is not used, the event matching state-machine will be initiated only once. within time gap An optional within clause that defines the time duration within which all the matching events should occur. + Matches **one or more** events to the given condition. * Matches **zero or more** events to the given condition. ? Matches **zero or one** events to the given condition. and Allows both of its condition to be matched by two distinct events in any order. or Only expects one of its condition to be matched by an event. Here the event reference of the unmatched condition will be null . not condition1 and condition2 Detects the event matching condition2 before any event matching condition1 . not condition1> for time period> Detects no event matching on condition1 for the specified time period . event reference An optional reference to access the matching event for further processing. All conditions can be assigned to an event reference to collect the matching event occurrences, other than the condition used for not case (as there will not be any event matched against it). Non occurrence of events. Siddhi detects non-occurrence of events using the not keyword, and its effective non-occurrence checking period is bounded either by fulfillment of a condition associated by and or via an expiry time using time period . Logical correlation of multiple conditions. Siddhi can only logically correlate two conditions at a time using keywords such as and , or , and not . When more than two conditions need to be logically correlated, use multiple pattern queries in a chaining manner, at a time correlating two logical conditions and streaming the output to a downstream query to logically correlate the results with other logical conditions. Event selection The event reference in sequence queries is used to retrieve the matched events. When a sequence condition is intended to match only a single event, then its attributes can be retrieved by referring to its reference as event reference . attribute name . An example of this is as follows. e1.symbol , refers to the symbol attribute value of the matching event e1 . But when the pattern condition is associated with min count : max count , it is expected to match against on multiple events. Therefore, an event from the matched event collection should be retrieved using the event index from its reference. Here the indexes are specified in square brackets next to event reference, where index 0 referring to the first event, and a special index last referring to the last available event in the collection. Attribute values of all the events in the matching event collection can be accessed a list, by referring to their event reference without an index. Some possible indexes and their behavior is as follows. e1[0].symbol , refers to the symbol attribute value of the 1 st event in reference e1 . e1[3].price , refers to the price attribute value of the 4 th event in reference e1 . e1[last].symbol , refers to the symbol attribute value of the last event in reference e1 . e1[last - 1].symbol , refers to the symbol attribute value of one before the last event in reference e1 . e1.symbol , refers to the list of symbol attribute values of all events in the event collection in reference e1 , as a list object. The system returns null when accessing attribute values, when no matching event is assigned to the event reference (as in when two conditions are combined using or ) or when the provided index is greater than the last event index in the event collection. Example 1 (Every) Query to send alerts when temperature increases at least by one degree between two consecutive temperature events. from every e1=TempStream, e2=TempStream[temp e1.temp + 1] select e1.temp as initialTemp, e2.temp as finalTemp insert into AlertStream; Here, the matching process begins for each event in the TempStream stream (as every is used with e1=TempStream ), and if the immediate next event with a value for temp attribute being greater than e1.temp + 1 of the initial event e1 , then an output is generated via the AlertStream . Example 2 (Every collection) Query to identify temperature peeks by monitoring continuous increases in temp attribute and alerts upon the first drop. define stream TempStream(deviceID long, roomNo int, temp double); @info(name = 'query1') from every e1=TempStream, e2=TempStream[ifThenElse(e2[last].temp is null, e1.temp = temp, e2[last].temp = temp)]+, e3=TempStream[e2[last].temp temp] select e1.temp as initialTemp, e2[last].temp as peekTemp, e3.price as firstDropTemp insert into PeekTempStream ; Here, the matching process begins for each event in the TempStream stream (as every is used with e1=TempStream ). It checks if the temp attribute value of the second event is greater than or equal to the temp attribute value of the first event ( e1.temp ), then for all the following events, their temp attribute value is checked if they are greater than or equal to their previous event's temp attribute value ( e2[last].temp ), and when the temp attribute value becomes less than its previous events temp attribute value value an output is generated via the AlertStream stream. Example 3 (Logical and condition) A query to identify a regulator activation event immediately followed by both temperature sensor and humidity sensor activation events in either order. define stream TempStream(deviceID long, isActive bool); define stream HumidStream(deviceID long, isActive bool); define stream RegulatorStream(deviceID long, isOn bool); from every e1=RegulatorStream[isOn == true], e2=TempStream and e3=HumidStream select e2.isActive as tempSensorActive, e3.isActive as humidSensorActive insert into StateNotificationStream; Here, the matching process begins for each event in the RegulatorStream stream having the isOn attribute true . It generates an output via the AlertStream stream when an event from both TempStream stream and HumidStream stream arrives immediately after the first event in either order. Output Rate Limiting Output rate-limiting limits the number of events emitted by the queries based on a specified criterion such as time, and number of events. Purpose Output rate-limiting helps to reduce the load on the subsequent executions such as query processing, I/O operations, and notifications by reducing the output frequency of the events. Syntax The syntax for output rate limiting is as follows: from input stream ... select attribute name , attribute name , ... output rate limiting configuration insert into output stream Here, the output rate limiting configuration ( rate limiting configuration ) should be defined next to the output keyword and the supported output rate limiting types are explained in the following table: Rate limiting configuration Syntax Description Time based ( output event selection )? every time interval Outputs output event selection every time interval time interval. Number of events based ( output event selection )? every event interval events Outputs output event selection for every event interval number of events. Snapshot based snapshot every time interval Outputs all events currently in the query window (or outputs only the last event if no window is defined in the query) for every given time interval time interval. The output event selection specifies the event(s) that are selected to be outputted from the query, here when no output event selection is defined, all is used by default. The possible values for the output event selection and their behaviors are as follows: * first : The first query output is published as soon as it is generated and the subsequent events are dropped until the specified time interval or the number of events are reached before sending the next event as output. * last : Emits only the last output event generated during the specified time or event interval. * all : Emits all the output events together which are generated during the specified time or event interval. Example 1 (Time based first event) Query to calculate the average temp per roomNo for the events arrived on the last 10 minutes, and send alerts once every 15 minutes of the events having avgTemp more than 30 degrees. define stream TempStream(deviceID long, roomNo int, temp double); from TempStream#window.time(10 min) select roomNo, avg(temp) as avgTemp group by roomNo having avgTemp 30 output first every 15 min insert into AlertStream; Here the first event having avgTemp 30 is emitted immediately and the next event is only emitted after 15 minutes. Example 2 (Event based first event) A query to output the initial event, and from there onwards every 5 th event of TempStream stream. define stream TempStream(deviceID long, roomNo int, temp double); from TempStream output first every 5 events insert into FiveEventBatchStream; Example 3 (Event based all events) Query to collect last 5 TempStream stream events and send them together as a single batch. define stream TempStream(deviceID long, roomNo int, temp double); from TempStream output every 5 events insert into FiveEventBatchStream; As no output event selection is defined, the behavior of all is applied in this case. Example 4 (Time based last event) Query to emit only the last event of TempStream stream for every 10 minute interval. define stream TempStream(deviceID long, roomNo int, temp double); from TempStream output last every 10 min insert into FiveEventBatchStream; Example 5 (Snapshot based) Query to emit the snapshot of events retained by its last 5 minutes window defined on TempStream stream, every second. define stream TempStream(deviceID long, roomNo int, temp double); from TempStream#window.time(5 sec) output snapshot every 1 sec insert into SnapshotTempStream; Here, the query emits all the current events generated which do not have a corresponding expired event at the predefined time interval. Example 6 (Snapshot based) Query to emit the snapshot of events retained every second, when no window is defined on TempStream stream. define stream TempStream(deviceID long, roomNo int, temp double); from TempStream output snapshot every 5 sec insert into SnapshotTempStream; Here, the query outputs the last seen event at the end of each time interval as there are no events stored in no window defined. Partition Partition provides data parallelism by categorizing events into various isolated partition instance based on their attribute values and by processing each partition instance in isolation. Here each partition instance is tagged with a partition key, and they only process events that match to the corresponding partition key. Purpose Partition provide ways to segment events into groups and allow them to process the same set of queries in parallel and in isolation without redefining the queries for each segment. Here, events form multiple streams generating the same partition key will result in the same instance of the partition, and executed together. When a stream is used within the partition block without configuring a partition key, all of its events will be executed in all available partition instances. Syntax The syntax for a partition is as follows: @purge(enable='true', interval=' purge interval ', idle.period=' idle period of partition instance ') partition with ( key selection of stream name , key selection of stream name , ... ) begin from stream name ... select attribute name , attribute name , ... insert into (#)? stream name from (#)? stream name ... select attribute name , attribute name , ... insert into stream name ... end; Here, a new instance of a partition will be dynamically created for each unique partition key that is generated based on the key selection applied on the events of their associated streams ( stream name ). These created partition instances will exist in the system forever unless otherwise a purging policy is defined using the @purge annotation. The inner streams denoted by # stream name can be used to chain multiple queries within a partition block without leaving the isolation of the partition instance. The key selection defines the partition key for each event based on the event attribute value or using range expressions as listed below. Key selection type Syntax description Partition by value attribute name Attribute value of the event is used as its partition key. Partition by range compare condition as 'value' or compare condition as 'value' or ... Event is executed against all compare conditions , and the values associated with the matching conditions are used as its partition key. Here, when the event is matched against multiple conditions, it is processed on all the partition instances that are associated with those matching conditions. When there are multiple queries within a partition block, and they can be chained without leaving the isolation of the partition instance using the inner streams denoted by # . More information on inner Streams will be covered in the following sections. Inner Stream Inner stream connects the queries inside a partition instance to one another while preserving partition isolation. These are denoted by a # placed before the stream name, and these streams cannot be accessed outside the partition block. Through this, without repartitioning the streams, the output of a query instance can be used as the input of another query instance that is also in the same partition instance. Using non inner streams to chain queries within a partition block. When the connecting stream is not an inner stream and if it is not configured to generate a partition key, then it outputs events to all available partition instances . However, when the non-inner stream is configured to generate a partition key, it only outputs to the partition instances that are selected based on the repartitioned partition key. Purge Partition Purge partition purges partitions that are not being used for a given period on a regular interval. This is because, by default, when partition instances are created for each unique partition key they exist forever if their queries contain stateful information, and there are use cases (such as partitioning events by date value) where an extremely large number of unique partition keys are used, which generates a large number of partition instances, and this eventually leading to system out of memory. The partition instances that will not be used anymore can purged using the @purge annotation. The elements of the annotation and their behavior is as follows. Purge partition configuration Description enable To enable partition purging. internal Periodic time interval to purge the purgeable partition instances. idle.period The idle period, a particular partition instance (for a given partition key) needs to be idle before it becomes purgeable. Example 1 (Partition by value) Query to calculate the maximum temperature of each deviceID , among its last 10 events. partition with ( deviceID of TempStream ) begin from TempStream#window.length(10) select roomNo, deviceID, max(temp) as maxTemp insert into DeviceTempStream; end; Here, each unique deviceID will create a partition instance which retains the last 10 events arrived for its corresponding partition key and calculates the maximum values without interfering with the events of other partition instances. Example 2 (Partition by range) Query to calculate the average temperature for the last 10 minutes per each office area, where the office areas are identified based on the roomNo attribute ranges from the events of TempStream stream. partition with ( roomNo = 1030 as 'serverRoom' or roomNo 1030 and roomNo = 330 as 'officeRoom' or roomNo 330 as 'lobby' of TempStream) begin from TempStream#window.time(10 min) select roomNo, deviceID, avg(temp) as avgTemp insert into AreaTempStream end; Here, partition instances are created for each office area type such as serverRoom , officeRoom , and lobby . Events are processed only in the partition instances which are associated with matching compare condition values that are satisfied by the event's roomNo attribute, and within each partition instance, the average temp value is calculated based on the events arrived over the last 10 minutes. Example 3 (Inner streams) A partition to calculate the average temperature of every 10 events for each sensor, and send the output via the DeviceTempIncreasingStream stream if consecutive average temperature ( avgTemp ) values increase by more than 5 degrees. partition with ( deviceID of TempStream ) begin from TempStream#window.lengthBatch(10) select roomNo, deviceID, avg(temp) as avgTemp insert into #AvgTempStream from every e1=#AvgTempStream, e2=#AvgTempStream[e1.avgTemp + 5 avgTemp] select e1.deviceID, e1.avgTemp as initialAvgTemp, e2.avgTemp as finalAvgTemp insert into DeviceTempIncreasingStream end; Here, the first query calculates the avgTemp for every 10 events for each unique deviceID and passes the output via the inner stream #AvgTempStream to the second query that is also in the same partition instance. The second query then identifies a pair of consecutive events from #AvgTempStream , where the latter event having 5 degrees more on avgTemp value than its previous event. Example 4 (Purge partition) A partition to identify consecutive three login failure attempts for each session within 1 hour. Here, the number of sessions can be infinite. define stream LoginStream ( sessionID string, loginSuccessful bool); @purge(enable='true', interval='10 sec', idle.period='1 hour') partition with ( sessionID of LoginStream ) begin from every e1=LoginStream[loginSuccessful==false], e2=LoginStream[loginSuccessful==false], e3=LoginStream[loginSuccessful==false] within 1 hour select e1.sessionID as sessionID insert into LoginFailureStream; end; Here, the events in LoginStream is partitioned by their sessionID attribute and matched for consecutive occurrences of events having loginSuccessful==false with 1 hour using a sequence query and inserts the matching pattern's sessionID to LoginFailureStream . As the number of sessions is infinite the @purge annotation is enabled to purge the partition instances. The instances are marked for purging if there are no events from a particular sessionID for the last 1 hour, and the marked instances are periodically purged once every 10 seconds. Table A table is a stored collection of events, and its schema is defined via the table definition . A table definition is similar to the stream definition where it contains the table name and a set of attributes having specific types and uniquely identifiable names within the scope of the table. Here, all events associated with the table will have the same schema (i.e., have the same attributes in the same order). The events of the table are stored in-memory , but Siddhi also provides store extensions to mirror the table to external databases such as RDBMS, MongoDB, and others, while allowing the events to be stored on those databases. Table supports primary keys to enforce uniqueness on stored events/recodes, and indexes to improve their searchability. Purpose Tables help to work with stored events. They allow to pick and choose the events that need to be stored by performing insert, update, and delete operations, and help to retrieve necessarily events when by performing read operations. Managing events stored in table The events in the table can be managed using queries that perform join, insert, update, insert or update, and delete operates, which are either initiated by events arriving in Streams or through on-demand queries . Syntax The syntax for defining a table is as follows: @primaryKey( key , key , ... ) @index( key , key , ...) define table table name ( attribute name attribute type , attribute name attribute type , ... ); The following parameters are used to configure the table definition: Parameter Description table nam The name of the table created. (It is recommended to define a table name in PascalCase .) attribute name Uniquely identifiable name of the table attribute. (It is recommended to define attribute names in camelCase .) attribute type The type of each attribute defined in the schema. This can be STRING , INT , LONG , DOUBLE , FLOAT , BOOL or OBJECT . To use and refer table and attribute names that do not follow [a-zA-Z_][a-zA-Z_0-9]* format enclose them in ` . E.g. `$test(0)` . Primary Keys Primary keys help to avoid duplication of data by enforcing nor two events to have the same value for the selected primary key attributes. They also index the table to access the events much faster. Primary keys are optional, and they can be configured using the @primaryKey annotation. Here, each table can only have at most one @primaryKey annotation, which can have one or more attribute name s defined as primary keys (The number of attribute name supported can differ based on the differet store implementations). When more than one attribute is used, the uniqueness of the events stored in the table is determined based on the composite value for those attributes. When more than one events having the same primary keys are inserted to the table, the latter event replaces the event/record that already exists in the table. Indexes Indexes allow events in the tables to be searched/modified much faster, but unlike primary keys, the indexed attributes support duplicate values. Indexes are optional, and they can be configured using the @index annotation. Here, each @index annotation creates an index in the table, and the tables only support one attribute name for each index (The number of @Index annotations and attribute name inside the annotation can differ based on different store implementations). Example 1 (Primary key) define table RoomTypeTable ( roomNo int, type string ); The above table definition defines an in-memory table named RoomTypeTable having the following attributes. roomNo of type int Room type of type string Example 2 (Primary key) @primaryKey('symbol') define table StockTable (symbol string, price float, volume long); The above table definition defines an in-memory table named StockTable having the following attributes. symbol of type string price of type float volume of type long As this table is configured with the primary key symbol , there will be only one record/event exist in the table for a particular value of the symbol attribute. Example 3 (Index) @index('username') @index('salary') define table SalaryTable (username string, salary double); The above table definition defines an in-memory table named SalaryTable having the following attributes. username of type string salary of type double As this table is configured with indexes for username and salary , the search operations on username and/or salary attributes will be much faster than the non-indexed case. Here, the table can contain duplicate events having the same value for username and/or salary. Example 3 (Primary key and index) @primaryKey('username') @index('salary') define table SalaryTable (username string, salary double); The above table definition defines an in-memory table named SalaryTable having the following attributes. username of type string salary of type double As this table is configured with the primary key username and index salary . Hence, there can be only one record/event exist in the table having a particular username value, and the search operations on username and/or salary attributes will be much faster than the non-indexed case. Store Stores allow creating, reading, updating, and deleting events/recodes stored on external data stores such as RDBMS, MongoDB, and others. They produce these functionalities by using the Siddhi tables as a proxy to external databases. Stores depending on their implementation and the connected external data store, some supports primary keys to enforce uniqueness on stored events/recodes, and indexes to improve their searchability. Since stores work with external data stores, the i/o latency can be quite higher than in-memory tables, the increase in latency can be eliminated by defining a cache, such that recently accessed data will be cached in-memory providing faster data retrievals. Purpose Stores allow searching retrieving and manipulating data stored in external data stores through queries. This is useful for use cases when there is a need to access a common database used by various other systems, to retrieve and transfer data. Syntax The syntax for defining a store along with is associated table is as follows: @store(type=' store type ', common.static.key =' value ', common.static.key =' value ' @cache(size=' cache size ', cache.policy=' cache policy ', retention.period=' retention period ', purge.interval=\" purge interval \")) @primaryKey( key , key , ... ) @index( key , key , ...) define table table name ( attribute name attribute type , attribute name attribute type , ... ); Here the store is defined via the @store annotation, and the schema of the store is defined via the table definition associated with it. In this case the table definition will not create an in-memory table but rather used as a poxy to read, write, and modify data stored in external store. The type parameter of the @store defines the store type to be used to connect to the external data store, and the other parameters of @store annotation other than @cache depend on the store selected, where some of these parameters can be optional. The @primaryKey and @index annotations are optional, and supported by some store implementations. The @primaryKey annotation can be defined at most once, and it can have one or more attribute name s as composed primary keys based on the implementation. At the same time, @index annotation can be defined several times, and it can also have one or more attribute name s as composed indexes if the implementation supports them. Cache The @cache annotation inside @store defines the behavior of the cache. @cache is an optional annotation that can be applied to all store implementations, where when this is not defined, the cache will not be enabled. The parameters defining the cache behavior via the @cache annotation is as follows. Parameter Mandatory/Optional Default Value Description size Mandatory - Maximum number of events/records stored in the cache. cache.policy Optional FIFO Policy to remove elements from the cache when the cache is at its maximum size and new entries need to added due to cache miss. Supported policies are FIFO (First-In First-Out), LRU (Least Recently Used) LFU (Least Frequently Used) retention.period Optional - The period after an event/record will become eligible for removal from the cached irrespective of the case size. This allows the cache to fetch the recent database updates made by other systems. purge.interval Optional Equal to retention period. The periodic time interval the cached events/records that are eligible for removal will be purge. Even though the cache is enabled, its behavior and usage depend on the number of recodes in the external store relative to the maximum cache size defined as follows: Cache size being greater than or equal to the number of recodes in the external store: At startup, all the recodes of the external store data will be preloaded to cached. The cache is used to process all type of data retrieval operations. When retention.period (and purge.interval ) is configured, all records the cache are periodically deleted and reloaded from the external store. Cache size is smaller than the number of recodes in the external store: At startup, the number of recodes equal to the maximum cache size is preloaded from the external store. The cache is used to process only the data retrieval operations that use all defined primary keys in equal ( == ) comparisons, and when there are multiple comparisons, those are combined using and , (For example when customerID and companyID are defined as primary keys then the data retrieval operations with condition customerID == 'John' and companyID == 'Google' and age 28 can be executed in the cache) . All other operations are directly executed in the external data store. If the cache is full and when a cache miss occurs, a record is removed from the cache based on the defined cache expiry policy before adding the missed record from the external data store. When retention.period (and purge.interval ) is configured, the data is cache that are loaded earlier than retention period are periodically deleted. Here, no reloading will be done from the external data store. Supported store types The following is a list of store types supported by Siddhi: Sink mapping type Description RDBMS Optimally stores, retrieves, and manipulates data on RDBMS databases such as MySQL, MS SQL, Postgresql, H2 and Oracle. MongoDB Stores, retrieves, and manipulates data on MongoDB. Redis Stores, retrieves, and manipulates data on Redis. Elasticsearch Supports data access and manipulation operators on Elasticsearch. Example 1 An RDBMS Store configuration to work with MySQL database. @store(type=\"rdbms\", jdbc.url=\"jdbc:mysql://localhost:3306/hotel\", username=\"siddhi\", password=\"123\", jdbc.driver.name=\"com.mysql.jdbc.Driver\") define table RoomTypeTable ( roomNo int, type string ); Here, the store connects to the MySQL table RoomTypeTable in the database hotel hosted on localhost:3306 , and its columns mapped as follows. roomNo of type INTEGER mapped to int type of type VARCHAR(255) mapped to string Example 2 An RDBMS Store configuration to work with an indexed MySQL database using a cache. @store(type=\"rdbms\", jdbc.url=\"jdbc:mysql://localhost:3306/hotel\", username=\"siddhi\", password=\"123\", jdbc.driver.name=\"com.mysql.jdbc.Driver\" @cache(size=\"100\", retention.period=\"5 min\", purge.interval=\"1 min\")) @primaryKey('username') @index('salary') define table SalaryTable (username string, salary double); Here, an RDBMS store is defined with a cache of size 100 that every minute removes the entries added to the cache which are older than 5 minutes. The store connects to the MySQL table named SalaryTable , that is configured with the primary key username and index salary , and located in a MySQL the database hotel hosted on localhost:3306 . Its columns mapped as follows. username of type VARCHAR(255) mapped to string salary of type VARCHAR(255) mapped to string Table (and store) operators The following operations can be performed on tables (and stores). Insert Allows events (records) to be inserted into tables/stores. This is similar to inserting events into streams. Primary Keys If the table is defined with primary keys, and multiple records are inserted with the same primary key, a primary key constrain violations can can occur. In such cases use the update or insert into operation. Syntax Syntax to insert events into a table from a stream is as follows; from input stream select attribute name , attribute name , ... insert into table Similar to streams, the current events , expired events or the all events keyword can be used between insert and into keywords in order to insert only the specific event types. For more information, refer Event Type section. Example Query to inserts all the events from the TempStream stream to the TempTable table. define stream TempStream(tempId string, temp double); define table TempTable(tempId string, temp double); from TempStream select * insert into TempTable; Join (Table) Allows stream or named-window to retrieve events (records) from a table. Other Join Functions Joins can also be performed among two streams , with named-aggregation , or named-window . Syntax The syntax for a stream or a named-window to join with a table is as follows: from ( input stream ( non window handler )*( window )?| named-window ) (as reference )? join type table (as reference )? (on join condition )? select reference . attribute name , reference . attribute name , ... insert into output stream A join with table is similar to the join of two streams , where one of the inputs is a table and other can be either a stream or a named-window . Here, the table and named-window cannot have any optional handlers associated with it. Two tables cannot be joined. A table can only be joint with a stream or named-window. Two tables, or table and named-aggregation cannot be joint because there must be at least one active entity to trigger the join operation. Supported join types Table join supports following join operations. Inner join (join) This is the default behavior of a join operation, and the join keyword is used to join a stream with a table. The output is generated only if there is a matching event in both the stream and the table. Left outer join The left outer join keyword is used to join a stream on the left side with a table on the right side based on a condition. It returns all the events of the left stream even if there are no matching events in the right table by having null values for the attributes of the table on the right. Right outer join This is similar to a left outer join and the right outer join keyword is used to join a stream on right side with a table on the left side based on a condition. It returns all the events of the right stream even if there are no matching events in the left table by having null values for the attributes of the table on the left. Cross join In either of these cases, when the join condition is omitted, the triggering event will successfully match against all the events in the table, producing a cross join behavior. Example A query to join and retrieve the room type from RoomTypeTable table based on equal roomNo attribute of TempStream , and to insert the results into RoomTempStream steam. define table RoomTypeTable (roomNo int, type string); define stream TempStream (deviceID long, roomNo int, temp double); from TempStream as t join RoomTypeTable as r on t.roomNo == r.roomNo select t.deviceID, t.roomNo, r.type as roomType, t.temp insert into RoomTempStream; Delete Allows a stream to delete selected events (records) form a table. Syntax Syntax to delete selected events in a table based on the events in a stream is as follows; from input stream select attribute name , attribute name , ... delete table (for event type )? (on condition )? The condition element specifies the basis on which the events in the table are selected to be deleted. When specifying the condition, the table attributes should always be referred with the table name , and and when a condition is not defined, all the events in the table will be deleted. To execute delete, only for specific event types, use the current events , expired events or the all events keyword can be used with for as shown in the syntax. For more information refer Event Type . Note When defining the condition, the table attributes must be always referred with the table name as follows: table name . attribute name Example 1 A query to delete the records in the RoomTypeTable table that has matching values for the roomNo attribute against the values of roomNumber attribute of the events in the DeleteStream stream. define table RoomTypeTable (roomNo int, type string); define stream DeleteStream (roomNumber int); from DeleteStream delete RoomTypeTable on RoomTypeTable.roomNo == roomNumber; Example 2 A query to delete all the records in the BlacklistTable table when an event arrives in the ClearStream stream. define table BlacklistTable (ip string); define stream ClearStream (source string); from ClearStream delete BlacklistTable; Update Allows a stream to update selected events (records) form a table. Syntax Syntax to update events on a table is as follows; from input stream select attribute name , attribute name , ... update table (for event type )? (set table . attribute name = ( attribute name | expression ), table . attribute name = ( attribute name | expression ), ...)? (on condition )? The condition element specifies the basis on which the events in the table are selected to be updated. When referring the table attributes in the update clause, they must always be referred to with the table name , and when a condition is not defined, all the events in the table will be updated. The set keyword can be used to update only the selected attributes from the table. Here, for each assignment, the left side should contain the table attribute that is being updated, and the right side can contain a query output attribute, a table attribute, a mathematical operation, or any other. When the set clause is not provided, all attributes in the table will be updated based on the query output. To execute update, only for specific event types, use the current events , expired events or the all events keyword can be used with for as shown in the syntax. For more information refer Event Type . Note In the update clause, the table attributes must be always referred with the table name as follows: table name . attribute name Example 1 A query to update the latestHeartbeatTime on the ServerInfoTable against each serverIP for every event on the HeartbeatStream . define table ServerInfoTable (serverIP string, host string, port int, latestHeartbeatTime long); define stream HeartbeatStream (serverIP string, timestamp long); from HeartbeatStream select * update ServerInfoTable set ServerInfoTable.latestHeartbeatTime = timestamp on ServerInfoTable.serverIP == serverIP; Example 2 A query to update the peoplePresent in the RoomOccupancyTable table for each roomNo based on new people arrival and exit values from events of the UpdateStream stream. define table RoomOccupancyTable (roomNo int, peoplePresent int); define stream UpdateStream (roomNumber int, arrival int, exit int); from UpdateStream select * update RoomOccupancyTable set RoomOccupancyTable.peoplePresent = RoomOccupancyTable.peoplePresent + arrival - exit on RoomOccupancyTable.roomNo == roomNumber; Example 3 A query to update the latestHeartbeatTime on the HeartbeatTable for each event on the HeartbeatStream . define table HeartbeatTable (serverIP string, latestHeartbeatTime long); define stream HeartbeatStream (serverIP string, timestamp long); from HeartbeatStream select serverIP, timestamp as latestHeartbeatTime update HeartbeatTable on ServerInfoTable.serverIP == serverIP; Update or Insert Allows a stream to update the events (records) that already exist in the table based on a condition, else inserts the event as a new entry to the table. Syntax Syntax to update or insert events on a table is as follows; from input stream select attribute name , attribute name , ... update or insert into table (for event type )? (set table . attribute name = ( attribute name | expression ), table . attribute name = ( attribute name | expression ), ...)? (on condition )? The condition element specifies the basis on which the events in the table are selected to be updated. When referring the table attributes in the update clause, they must always be referred with the table name , and when the condition does not match with any event in the table, then a new event (a record) is inserted into the table. Here, when a condition is not defined, all the events in the table will be updated. The set clause is only used when an update is performed in the update or insert operation. In this case, the set keyword can be used to update only the selected attributes from the table. Here, for each assignment, the left side should contain the table attribute that is being updated, and the right side can contain a query output attribute, a table attribute, a mathematical operation, or any other. When the set clause is not provided, all attributes in the table will be updated based on the query output. To execute update or insert, only for specific event types, use the current events , expired events or the all events keyword can be used with for as shown in the syntax. For more information refer Event Type . Note In the update or insert clause, the table attributes must be always referred with the table name as follows: table name . attribute name Example A query to update assignee information in the RoomAllocationTable table for the corresponding roomNumber from the RoomAllocationStream stream when at least one matching record is present in the table, and when there are no matching records it inserts a new record to the RoomAllocationTable table based on the query output. define table RoomAllocationTable (roomNo int, type string, assignee string); define stream RoomAllocationStream (roomNumber int, type string, assignee string); from RoomAllocationStream select roomNumber as roomNo, type, assignee update or insert into RoomAllocationTable set RoomAllocationTable.assignee = assignee on RoomAllocationTable.roomNo == roomNo; In (Table) Allows the query to check whether the expected value exists in the table using a condition operation. Syntax from input stream [ condition in table ] select attribute name , attribute name , ... insert into output stream The condition element specifies the basis on which the events in the table are checked for existence. When referring the table attributes in the condition , they must always be referred with the table name as table name . attribute name . Example 1 A query to filter only the events of server rooms from the TempStream stream using the ServerRoomTable table, and pass them for further processing via ServerRoomTempStream stream. define table ServerRoomTable (roomNo int); define stream TempStream (deviceID long, roomNo int, temp double); from TempStream[ServerRoomTable.roomNo == roomNo in ServerRoomTable] insert into ServerRoomTempStream; Example 2 A query to filter out the blacklisted serverIP s from the RequestStream stream using the BlacklistTable table, and only pass events having IPs that are not blacklisted, for further processing via FilteredRequestStream stream. define table BlacklistTable (serverIP string); define stream RequestStream (ip string, request string); from RequestStream[not (BlacklistTable.serverIP == ip in BlacklistTable)] insert into FilteredRequestStream; Named-Aggregation Named-aggregation aggregates events incrementally for a specified set of time granularities, and allows interactively accessing them to produce reports, dashboards, and to take decisions in realtime with millisecond accuracy. The aggregation logic and schema is defined via the aggregation definition . A aggregation definition contains the aggregation name, input, aggregation logic, the time granularities on which the aggregations are calculated, and the set of aggregated output attributes having specific types and uniquely identifiable names within the scope of the named-aggregation. The aggregated events of the named-aggregation are stored by default in-memory , but Siddhi also provides store extensions to mirror the named-aggregation to external databases such as RDBMS, while allowing the aggregated events to be stored on databases such that allowing it to hold data for longer durations, preserve data at failures, and to aggregate data in a distributed manner. The historical data stored in named-aggregations are purged automatically to limit data growth overtime, and when purging is not configured, system automatically purges the data every 15 minutes, by only retaining the default number of records for each time granularity. Purpose Named-aggregations helps to calculate aggregations over long durations and retrieve the aggregated values over various time ranges, It can perform aggregation using operations such as sum , count , avg , min , max , count and distinctCount on stream attributes for time granularities such as sec , min , hour , day , month , and year . This can be used for in many analytics scenarios as this provides time-series aggregates on calendar time, over long durations, even for out-of-order events, and helps to retrieve historical data for selected time range and time granularity. Syntax The syntax for defining a named-aggregation is as follows: @store(type=' store type ', ...) @purge(enable=\" enable purging \", interval=' purging interval ', @retentionPeriod( granularity = ' retention period ', ...)) @PartitionById(enable=\" enable distributed aggregation \") define aggregation aggregation name from ( stream | named-window ) select attribute name , aggregate function ( attribute name ) as attribute name , ... group by attribute name aggregate by timestamp attribute every time granularities ; The following parameters are used to configure the aggregation definition: Parameter Description @store Annotation to configure the data store to store the aggregated records. This annotation is optional and when not provided, the results are stored in in-memory tables. @purge Annotation to configure data purging on aggregated data. This annotation is optional, and when not provided, the default data purging configuration is enabled. To disable data purging, use @purge(enable='false') configuration, and also make sure to disable when named-aggregation is used only for read-only purposes. Detail information on data purging is explained in the following section. @PartitionById Annotation to enable multiple named-aggregations to process in a distributed manner. Detail information on this is discussed in the distributed named-aggregation section. aggregation name The name of the named-aggregation created. (It is recommended to define an aggregation name in PascalCase .) input stream The stream that feeds the named-aggregation, and this stream must be defined before the aggregation definition . group by attribute name The group by clause to aggregate the events per each unique group by attribute value combination. This is optional, and when not provided, all events are aggregated together. by timestamp attribute Configures a stream attribute to be used as the event timestamp in the aggregation. This is optional, and if not provided, the event time is used by default. When the stream attribute is used as the event timestamp it could be either a long in Unix timestamp in milliseconds (e.g. 1496289950000 ), or a string in the format yyyy - MM - dd HH : mm : ss (if time is in GMT) or yyyy - MM - dd HH : mm : ss Z (if the time is not in GMT) with ISO 8601 UTC offset for Z (e.g., +05:30 , -11:00 ). time granularities Defines the granularity ranges on which the aggregations should be performed using second , minute , hour , day , month , and/or year keywords. Here, the granularity range can be defined with minimum and maximum granularities separating them with three dots (e.g. sec ... year where the aggregation will be performed per each second, minute, hour, day, month, and year), or using comma-separated granularities (e.g. min, hour where the aggregation will be only performed per each minute and hour). The named-aggregation uses calendar time. The named-aggregations aggregate events at calendar start times for each granularity based on GMT timezone. Handles out-of-order event arrival. Named-aggregations aggregates out-of-order event arrivals into their corresponding time range and granularity. Data Purging Data purging on named-aggregations are enabled by default with 15 min purging interval and the following retention periods; Time granularity Default retention period Minimum retention period second 120 seconds 120 seconds minute 24 hours 120 minutes hour 30 days 25 hours day 1 year 32 days month All 13 month year All none This can be modified using the @purge annotation by optionally providing interval parameter to configure the purging interval, and by optionally configuring the @retentionPeriod annotation, the duration the aggregated data needs to be retained when carrying out data purging is defined for each time granularity period using the granularity = ' retention period ' pairs. Here for each granularity, the configured granularity period should be greater than or equal to the respective minimum retention period, and when not defined, the default retention period is applied as specified in the above table. Beware of defining the same named-aggregation in multiple SiddhiApps. The same named-aggregation can be defined in multiple SiddhiApps for data aggregation and data retrieval purposes. In this case, make sure all the named-aggregations to have the same purging configuration or enable purging only in one of the named-aggregations to ensure that data is purged as expected. Further, when these named-aggregations are configured to use the same physical data store using the @stroe annotation while the distributed named-aggregation configuration discussed in the following sections is not used, make sure a named-aggregation in only one of the SiddhiApps performs data aggregation (i.e., the aggregation input stream only feeds events into one of the aggregation definitions) while others are only used for data retrieval either using join, or on-demand select queries. Distributed Named-Aggregations The system will result in an error when more than one named-aggregation, with same aggregation name pointing to the same physical store using the @store annotation, is defined on multiple SiddhiApps unless otherwise Siddhi is configured to perform aggregations in a distributed manner. Distributed named-aggregation configurations allow each SiddhiApp to work as independent shards by partially aggregating the data fed to them. These partial results are combined during data retrieval. Named-aggregations can be configured to process data parallelly and in a distributed manner, by adding the following Siddhi properties to Siddhi configuration. Refer Siddhi configuration guide for detail steps. Siddhi Property Description Possible Values Optional Default Value shardId An ID to uniquely identify the running process (Siddhi Manager/ Siddhi Runner). This helps different instances of the same SiddhiApp running on separate processes to aggregate and store data separately. Any string No \"\" (Empty string) partitionById Enables all named-aggregations on the running process (Siddhi Manager/ Siddhi Runner) to aggregate data in a distributed manner. true , false Yes false The named-aggregations that are enabled to process in a distributed manner using the Siddhi properties can be selectively disabled by adding the @PartitionById annotation to the corresponding aggregation definition and setting its enable property to false as @PartitionById(enable='false') . Once a shardId is introduced it should not be dropped arbitrarily! When a process (Siddhi Manager/ Siddhi Runner) configured with a specific shardId is permanently removed, it will result in unexpected aggregate results unless otherwise the data belonging to that shard is migrated or cleaned in the data store. Example 1 An in-memory named-aggregation with default default purging named as TradeAggregation to calculate the average and sum for price attribute for each unique symbol for all time granularities from second to year using timestamp attribute as the event time, on the events arriving via the TradeStream stream. define stream TradeStream (symbol string, price double, volume long, timestamp long); define aggregation TradeAggregation from TradeStream select symbol, avg(price) as avgPrice, sum(price) as total group by symbol aggregate by timestamp every sec ... year; Example 2 A custom purging enabled RDBMS store based named-aggregation with name TradeAggregation to calculate the min and max price for each unique symbol for time granularities hour, day, and month using Siddhi event timestamp, on the events arriving via the TradeStream stream. define stream TradeStream (symbol string, price double, volume long, timestamp long); @store(type=\"rdbms\", jdbc.url=\"jdbc:mysql://localhost:3306/sweetFactoryDB\", username=\"root\", password=\"root\", jdbc.driver.name=\"com.mysql.jdbc.Driver\") @purge(enable='true', interval='10 min', @retentionPeriod(hour='24 hours', days='1 year', months='all')) define aggregation TradeMinMax from TradeStream select symbol, min(price) as minPrice, max(price) as maxPrice group by symbol aggregate every hour, day, month; Here, the aggregated data is stored in a MySQL store hosted at mysql://localhost:3306/sweetFactoryDB and the data is periodically purged for every 10 min while retaining data for hour, day, and month granularities for 24 hours , 1 year , and forever respectively. Named-aggregation operators The following operation can be performed on named-aggregation. Join (Named-Aggregation) Allows stream or named-window to retrieve aggregated results from the named-aggregation. Other Join Functions Joins can also be performed among two streams , with table , or named-window . Syntax The syntax for a stream or a named-window to join with a named-aggregation is as follows: from ( input stream ( non window handler )*( window )?| named-window ) (as reference )? join type named-aggregation (as reference )? on join condition within time range per time granularity select reference . attribute name , reference . attribute name , ... insert into output stream ; A join with named-aggregation is similar to the table join with additional within and per clauses, where table is being replaced by a named-aggregation. Here, the named-aggregation cannot have any optional handlers associated with it. Apart from the standard join constructs this supports the within and per clauses as follows. Item Description within time range Specifies the time interval for which the aggregate values should to be retrieved. This can be specified either by providing a start and an end timestamps (in string or long values) separating them by a comma as in \"2014-02-15 00:00:00 +05:30\", \"2014-03-16 00:00:00 +05:30\" and 1496200000000L, 1596434876000L , or by using a wildcard string specifying the data range as in \"2014-02-15 **:**:** +05:30\" . per time granularity Specifies the time granularity by which the data should be grouped and aggregated when data is retrieved. For instance, when days is specified for granularity, the named-aggregation returns aggregated results grouped for each day within the selected time interval. Here, the timestamp of each group can be obtained using the AGG_TIMESTAMP attribute, that is internal to the named-aggregation. Named-aggregations can only be joint with a stream or named-window. Two named-aggregations, or table and named-aggregation cannot be joint because there must be at least one active entity to trigger the join operation. Supported join types Named-aggregation join supports inner join ( join ), left outer join , right outer join , and cross join (when join condition is omitted) similar to the table join . Examples Following aggregation definition is used for all the examples. define stream TradeStream (symbol string, price double, volume long, timestamp long); define aggregation TradeAggregation from TradeStream select symbol, avg(price) as avgPrice, sum(price) as total group by symbol aggregate by timestamp every sec ... year; Example 1 A query to join and retrieve daily aggregations within the time range of \"2014-02-15 00:00:00 +05:30\", \"2014-03-16 00:00:00 +05:30\" from TradeAggregation based on equal symbol attribute of StockStream , and to insert the results into AggregateStockStream steam. Here, +05:30 in time range can be omitted if the timezone is GMT. define stream StockStream (symbol string, value int); from StockStream as s join TradeAggregation as t on s.symbol == t.symbol within \"2014-02-15 00:00:00 +05:30\", \"2014-03-16 00:00:00 +05:30\" per \"days\" select AGG_TIMESTAMP as timestamp, s.symbol, t.total, t.avgPrice insert into AggregateStockStream; Example 2 A query to join and retrieve all the hourly aggregations within the day of 2014-02-15 from TradeAggregation each event in RequestStream stream, order the results by symbol , and to insert the results into AggregateStockStream steam. define stream RequestStream (request string); from RequestStream join TradeAggregation as t within \"2014-02-15 **:**:** +05:30\" per \"hours\" select AGG_TIMESTAMP as timestamp, t.symbol, t.total, t.avgPrice order by symbol insert into AggregateStockStream; Example 3 A query to join and retrieve aggregated results from TradeAggregation for respective granularity and symbol attributes between the start and the end timestamps of events arriving on StockStream , and to insert the results into AggregateStockStream steam. define stream StockStream (symbol string, granularity string, start long, end long); from StockStream as s join TradeAggregation as t on s.symbol == t.symbol within s.start, s.end per s.granularity select AGG_TIMESTAMP as timestamp, s.symbol, t.total, t.avgPrice insert into AggregateStockStream; Here, granularity , start and end can have values such as \"hour\" , 1496200000000 , and 1596434876000 respectively. Named-Window A named-window is a window that is shared across multiple queries, where multiple queries can insert, join and consume output from the window. Its schema is defined via the window definition . A window definition is similar to the stream definition where it contains the name of named-window, a of attributes having specific types and uniquely identifiable names within the scope of the named-window along with the window type and output event type . Here, all events associated with the named-window will have the same schema (i.e., have the same attributes in the same order). The events of named-window are expired automatically based on the configured window type, and they cannot be explicitly removed by other means. Purpose Named-windows help to use the same instance of a window in multiple queries, this reduces memory consumption, supports calculating various types of aggregations and output them via multiple streams, and allows multiple queries to query on the same window data. Cannot selectively remove events from named-window. The events in the named-window cannot be selectively removed using delete operations, and the only way they are removed is via the automatic expiry operations of the defined window type. Syntax The syntax for defining a named-window is as follows: define window window name ( attribute name attribute type , attribute name attribute type , ... ) window type ( parameter , parameter , \u2026) (output event type )?; The following parameters are used configure the window definition: Parameter Description window name The name of the named-window created. (It is recommended to define a window name in PascalCase .) attribute name Uniquely identifiable name of the named-window attribute. (It is recommended to define attribute names in camelCase .) attribute type The type of each attribute defined in the schema. This can be STRING , INT , LONG , DOUBLE , FLOAT , BOOL or OBJECT . window type ( parameter , ...) The window implementation associated with the named-window and its parameters. event type Defines when the window should be emitting the events, by specifying keywords such as current events , expired events , or all events . Here, when the output is omitted, all events are emitted by default. For more information, refer Event Type section. Example 1 define window SensorWindow (deviceID string, value float, roomNo int) timeBatch(1 second); The above window definition with the name SensorWindow defines a named-window that is configured to retain events for 1 second in timeBatch window, and produce output upon event arrival and expiry to the window. This named-window contains the following attributes. deviceID of type string value of type float roomNo of type int Example 2 define window SensorWindow (deviceID string, value float, roomNo int) time(1 min) output expired events; The above window definition with the name SensorWindow defines a named-window that is configured to retain events for last 1 minute via time window, and produce output upon event expiry form the window. This named-window contains the following attributes. deviceID of type string value of type float roomNo of type int Named-windows operators The following operations can be performed on named-windows. Insert Allows events to be inserted into named-windows. This is similar to inserting events into streams. Syntax Syntax to insert events into a named-window from a stream is as follows; from input stream select attribute name , attribute name , ... insert into window Similar to streams, the current events , expired events or the all events keyword can be used between insert and into keywords in order to insert only the specific event types. For more information, refer Event Type section. Example This query inserts all events from the TempStream stream to the OneMinTempWindow window. define stream TempStream(tempId string, temp double); define window OneMinTempWindow(tempId string, temp double) time(1 min); from TempStream select * insert into OneMinTempWindow; Join (Named-Window) Allows stream or named-window to retrieve events from another named-window. Other Join Functions Joins can also be performed among two streams , with named-aggregation , or table . Syntax The syntax for a stream or a named-window to join with another named-window is as follows: from ( input stream ( non window handler )*( window )?| named-window ) (as reference )? join type named-window (as reference )? on condition select reference . attribute name , reference . attribute name , ... insert into output stream A join with named-window is similar to the join of two streams , where either both the inputs are named-windows, or one is a stream and other is a named-window. Here, the named-window cannot have any optional handlers associated with it. Supported join types Named-window join supports inner join ( join ), left outer join , right outer join , full outer join , and cross join (when join condition is omitted) similar to the stream join . Example A query, for each event on CheckStream , to join and calculate the number of temperature events having greater than 40 degrees for the temp attribute value, within the last 2 minutes of the TwoMinTempWindow named-window, and to insert the results into HighTempCountStream steam. define window TwoMinTempWindow (roomNo int, temp double) time(2 min); define stream CheckStream (requestId string); from CheckStream as c join TwoMinTempWindow as t on t.temp 40 select requestId, count(t.temp) as count insert into HighTempCountStream; From (Named-Window) Named-windows can be used as an input to any query, similar to streams. Syntax Syntax for using named-window as an input to a simple query is as follows; from named-window non window handler * ((join ( stream handler *| named-window | table | named-aggregation ))|((,|- )( stream | named-window ) non window handler *)+)? projection output action Named-windows can be used as input for any query type, like how streams are being used . They can be associated with optional non window handlers (such as filters, stream functions, and stream processors) in queries other than when they are used in join query. Example Queries to calculate the max temperature among all rooms, and avg temperature per each room , in the last 5 minutes, using FiveMinTempWindow , and publish the results via MaxTempStream stream, and AvgTempStream stream respectively. define window FiveMinTempWindow (roomNo int, temp double) time(5 min); from FiveMinTempWindow select max(temp) as maxValue insert into MaxTempStream; from FiveMinTempWindow select roomNo, avg(temp) as avgTemp group by roomNo insert into AvgTempStream; Trigger Trigger produces events periodically based on a given internal with a predefined schema. They can be used in any query, similar to the streams, and defined via the trigger definition . Purpose Triggers help to periodically generate events based on a specified time interval or cron expression, to perform periodic execution of queries. They can also be produced at SiddhiApp startup to perform initialization operations. Syntax The syntax for defining a trigger is as follows: define trigger trigger name at ( 'start'| every time interval | ' cron expression '); Triggers can be used as input to any query, similar to the streams. Because, when defined, they are represented as a stream having one attribute with name triggered_time , and type long as follows. define stream trigger name (triggered_time long); The supported trigger types are as follows. Trigger type Description 'start' Produces a single event when SiddhiApp starts. every time interval Produces events periodically at the given time interval. ' cron expression ' Produces events periodically based on the given cron expression. For configuration details, refer quartz-scheduler . Example 1 A trigger to generate events every 5 minutes. define trigger FiveMinTrigger at every 5 min; Example 2 A trigger to generate events at 10.15 AM on every weekdays. define trigger WorkStartTrigger at '0 15 10 ? * MON-FRI'; Example 3 A trigger to generate an event at SiddhiApp startup. define trigger InitTrigger at 'start'; Script The script provides the ability to write custom functions in other programming languages and execute them within Siddhi queries. The custom functions using scripts can be defined via the function definitions and accessed in queries similar to any other inbuilt functions . Purpose Scripts help to define custom functions in other programming languages such as javascript. This can eliminate the need for writing extensions to fulfill the functionalities that are not provided in Siddhi core or by its extension. Syntax The syntax for defining the script is as follows. define function function name [ language name ] return return type { function logic }; The defined function can be used in the queries similar to inbuilt functions as follows. function name ( ( function parameter (, function parameter )*)? ) Here, the function parameter s are passed into the function logic of the definition as an Object[] with the name data . The functions defined via the function definitions have higher precedence compared to inbuilt functions and the functions provided via extensions. The following parameters are used to configure the function definition: Parameter Description function name The name of the function created. (It is recommended to define a function name in camelCase .) language name Name of the programming language used to define the script, such as javascript , r , or scala . return type The return type of the function. This can be int , long , float , double , string , bool , or object . Here, the function implementer is responsible for returning the output according on the defined return type to ensure proper functionality. function logic The execution logic that is written in the language specified under the language name , where it consumes the function parameter s through the data variable and returns the output in the type specified via the return type parameter. Example 1 A function to concatenate three strings into one using JavaScript. define function concatFn[javascript] return string { var str1 = data[0]; var str2 = data[1]; var str3 = data[2]; var response = str1 + str2 + str3; return response; }; define stream TempStream(deviceID long, roomNo int, temp double); from TempStream select concatFn(roomNo,'-',deviceID) as id, temp insert into DeviceTempStream; Here, the defined concatFn function is used in the query by passing three string parameters for concatenation. On-Demand Query On-demand queries provide a way of performing add hock operations on Siddhi tables ( stores ), named-windows , and named-aggregations . The On-demand query can be submitted to the SiddhiApp using the query() method of the respective Siddhi application runtime as follows. siddhiAppRuntime.query( on-demand query ); To successfully execute an on-demand query, the SiddhiApp of the respective siddhiAppRuntime should have the corresponding table, named-window, or named-aggregation defined, that is being used in the on-demand query. Purpose On-demand queries allow to retrieve, add, delete and update events/data in Siddhi tables ( stores ), named-windows , and named-aggregations without the intervention of streams. This can be used to retrieve the status of the system, extract information for reporting and dashboarding purposes, and many others. The operations supported on tables are: Select Insert Delete Update Update or insert The operation supported on named-windows , and named-aggregations is: Select On-Demand query operators The following operations can be performed via on-demand queries. Select (Table/Named-Window) On-demand query to retrieve records from the specified table (/ store ) or named-window , based on the given condition. To retrieve data from Named-Aggregation, refer the Named-Aggregation Select section. Syntax Syntax to retrieve events from table or named-window is as follows; from ( table | named-window ) (on condition )? select attribute name , attribute name , ... group by ? having ? order by ? limit ? Here, the input can be either table (/ store ) or named-window , and the other parameters are similar to the standard Siddhi query . Examples The on-demand queries used in the examples are performed on a SiddhiApp that contains a table definition similar to the following. define table RoomTypeTable (roomNo int, type string); Example 1 An on-demand query to retrieve room numbers and their types from the RoomTypeTable table, for all room numbers that are greater than or equal to 10. from RoomTypeTable on roomNo = 10; select roomNo, type Example 2 An on-demand query to calculate the total number of rooms in the RoomTypeTable table. from RoomTypeTable select count(roomNo) as totalRooms Select (Named-Aggregation) On-demand query to retrieve records from the specified named-aggregation , based on the time range, time granularity, and the given condition. To retrieve data from table (store), or named-window, refer the Table/Named-Window Select section. Syntax Syntax to retrieve events from named-aggregation is as follows; from aggregation (on condition )? within time range per time granularity select attribute name , attribute name , ... group by ? having ? order by ? limit ? This is similar to the Table/Named-Window Select , but the input should be a named-aggregation , and the within time range and per time granularity should be provided as in named-aggregation join specifying the time interval for which the aggregate values need to be retrieved, and the the time granularity by which the aggregate values must be grouped and returned respectively. Examples The on-demand queries used in the examples are performed on a SiddhiApp that contains an aggregation definition similar to the following. define stream TradeStream (symbol string, price double, volume long, timestamp long); define aggregation TradeAggregation from TradeStream select symbol, avg(price) as avgPrice, sum(price) as total group by symbol aggregate by timestamp every sec ... year; Example 1 An on-demand query to retrieve daily aggregations from the TradeAggregation within the time range of \"2014-02-15 00:00:00 +05:30\", \"2014-03-16 00:00:00 +05:30\" (Here, +05:30 can be omitted if timezone is in GMT). from TradeAggregation within \"2014-02-15 00:00:00 +05:30\", \"2014-03-16 00:00:00 +05:30\" per \"days\" select symbol, total, avgPrice; Example 2 An on-demand query to retrieve the hourly aggregations from the TradeAggregation for \"FB\" symbol within the day of 2014-02-15 . from TradeAggregation on symbol == \"FB\" within \"2014-02-15 **:**:** +05:30\" per \"hours\" select symbol, total, avgPrice; Insert On-demand query to insert a new record to a table (/ store ), based on the attribute values defined in the select clause. Syntax Syntax to insert events into a table (store) is as follows; select attribute name , attribute name , ... insert into table ; Example An on-demand query to insert a new record into the table RoomOccupancyTable having values for its roomNo and people attributes as 10 and 2 respectively. select 10 as roomNo, 2 as people insert into RoomOccupancyTable Here, the respective SiddhiApp should have a RoomOccupancyTable table something similar to the following. define table RoomOccupancyTable (roomNo int, people int); Delete On-demand query to delete records from a table (/ store ), based on the specified condition. Syntax Syntax to delete events from a table (store) is as follows; select ? delete table (on condition )? Here, the on condition specifies the basis on which records are selected to be deleted, and when omitted, all recodes in the table will be removed. Note In the delete clause, the table attributes must be always referred with the table name as follows: table name . attribute name . Example On-demand queries to delete records in the RoomTypeTable table that have 10 as the value for their roomNo attribute. select 10 as roomNumber delete RoomTypeTable on RoomTypeTable.roomNo == roomNumber; delete RoomTypeTable on RoomTypeTable.roomNo == 10; Both the above queries result in the same. For the above queries to be performed, the respective SiddhiApp should have a RoomTypeTable table defined something similar to the following. define table RoomTypeTable (roomNo int, type string); Update On-demand query to update selected attributes of records from a table (/ store ), based on the specified condition. Syntax Syntax to update events on a table (store) is as follows; select attribute name , attribute name , ...? update table (set table . attribute name = ( attribute name | expression ), table . attribute name = ( attribute name | expression ), ...)? (on condition )? The condition element specifies the basis on which the events in the table are selected to be updated. When referring the table attributes in the update clause, they must always be referred to with the table name , and when a condition is not defined, all the events in the table will be updated. The set keyword can be used to update only the selected attributes from the table. Here, for each assignment, the left side should contain the table attribute that is being updated, and the right side can contain a query output attribute, a table attribute, a mathematical operation, or any other. When the set clause is not provided, all attributes in the table will be updated based on the query output. Note In the update clause, the table attributes must be always referred with the table name as follows: table name . attribute name Examples The on-demand queries used in the examples are performed on a SiddhiApp that contains a table definition similar to the following. define table RoomOccupancyTable (roomNo int, people int); Example 1 An on-demand query to increment the number of people by 1 for the roomNo 10 , in the RoomOccupancyTable table. update RoomTypeTable set RoomTypeTable.people = RoomTypeTable.people + 1 on RoomTypeTable.roomNo == 10; Example 2 An on-demand query to increment the number of people by the arrival amount for the given roomNumber , in the RoomOccupancyTable table. select 10 as roomNumber, 1 as arrival update RoomTypeTable set RoomTypeTable.people = RoomTypeTable.people + arrival on RoomTypeTable.roomNo == roomNumber; Update or Insert On-demand query to update the events (records) that already exist in the table (/ store ) based on a condition, else inserts the event as a new entry to the table. Syntax Syntax to update or insert events on a table (store) is as follows; select attribute name , attribute name , ... update or insert into table (set table . attribute name = ( attribute name | expression ), table . attribute name = ( attribute name | expression ), ...)? (on condition )? The condition element specifies the basis on which the events in the table are selected to be updated. When referring the table attributes in the update clause, they must always be referred with the table name , and when the condition does not match with any event in the table, then a new event (a record) is inserted into the table. Here, when a condition is not defined, all the events in the table will be updated. The set clause is only used when an update is performed in the update or insert operation. In this case, the set keyword can be used to update only the selected attributes from the table. Here, for each assignment, the left side should contain the table attribute that is being updated, and the right side can contain a query output attribute, a table attribute, a mathematical operation, or any other. When the set clause is not provided, all attributes in the table will be updated based on the query output. Note In the update or insert clause, the table attributes must be always referred with the table name as follows: table name . attribute name Example An on-demand query to update the record with assignee \"John\" when there is already and record for roomNo 10 in the RoomAssigneeTable table, else to insert a new record with values 10 , \"single\" and \"John\" for the attributes roomNo , type , and assignee respectively. select 10 as roomNo, \"single\" as type, \"John\" as assignee update or insert into RoomAssigneeTable set RoomAssigneeTable.assignee = assignee on RoomAssigneeTable.roomNo == roomNo; For the above query to be performed, the respective SiddhiApp should have a RoomAssigneeTable table defined something similar to the following. define table RoomAssigneeTable (roomNo int, type string, assignee string); SiddhiApp Configuration and Monitoring Threading and Synchronization The threading and synchronization behavior of SiddhiApps can be modified by using the @async annotation on the Streams. By default, SiddhiApp uses the request threads for all the processing. Here, the request threads follow through the streams and process each query in the order they are defined. By using the @async annotation, processing of events can be handed over to a new set of worker threads. Purpose The @async annotation helps to improve the SiddhiApp performance using parallel processing and event chunking, and it can also help to synchronize the execution across multiple operations. Syntax The syntax for configuring threading in Siddhi is as follows. @async(buffer.size=' buffer size ', workers=' workers ', batch.size.max='max batch size ') define stream stream name ( attribute name attribute type , attribute name attribute type , ... ); The following parameters are used to configure the @async definition. Parameter Description Optional Default Value buffer.size The size of the event buffer (in power of 2) that holds the events until they are picked by worker threads for processing. No - workers The number of worker threads that process the buffered events. Yes 1 batch.size.max The maximum number of events that will be fetched from the event buffer to be processed together by a worker thread, at a given time. Yes buffer.size Parallel processing Parallel processing helps to improve the performance by letting multiple threads to process events in parallel. By default, Siddhi does not process events in parallel, unless otherwise, it uses multi-threaded windows, triggers, or the events are sent to Siddhi using multiple input threads either from the sources defined via @source annotation or from the applications calling the Siddhi via InputHander . Parallel processing within a SiddhiApp can be explicitly achieved by defining @async annotations on the appropriate streams with the number of workers being greater than 1 . Here, the whole execution flow beginning from that stream will be executed by multiple workers in parallel. Event chunking Event chunking helps to improve the performance by processing multiple events to together, especially when the operations are I/O bound. By default, Siddhi does not attempt to chunk/group events together. Event chunking in a SiddhiApp can be explicitly achieved, by defining @async annotations on appropriate streams with batch.size.max set to greater than one. Here, the whole execution flow beginning from those streams will execute multiple events together, where each event group will have up to batch.size.max number of events. Use a combination of parallel processing, and event chunking to achieve the best performance. The optimal values for buffer.size , workers and batch.size.max parameters vary depending on the use case and the environment. Therefore, they can be only identified by testing the setup in a staging environment. synchronized execution Synchronized execution eliminates possible concurrent updates and race conditions among queries. By default, Siddhi provides synchronization only within its operators and not across queries. Synchronized execution across multiple queries can be explicitly achieved by defining @async annotation on appropriate streams with workers set to 1 . Here, the whole execution flow beginning from that stream will be executed synchronously by a single thread. Too many async annotations can reduce performance! Having multiple @async annotations will result in many threads being used for processing, this increases the context switching overhead, and thereby reducing the overall performance of the SiddhiApp. Therefore, use @async annotation only when it is necessary . Statistics The throughput, latency, and memory consumption of SiddhiApps, and their internal operators can be monitored through Siddhi statistics. SiddhiApps can have preconfigured statistics configurations using the @app:statistics annotation applied on SiddhiApp level, and they can also be dynamically modified at runtime using the setStatisticsLevel() method available on the SiddhiAppRuntime . Purpose Siddhi statistics helps to identify the bottlenecks in the SiddhiApp execution flows, and thereby facilitate to improve SiddhiApp performance by appropriately handling them. The name of the statistics metrics follow the below format. io.siddhi.SiddhiApps. SiddhiApp name .Siddhi. component type . component name . metrics type . The following table lists the component types and their supported of metrics types. Component Type Metrics Type Stream throughput size (The number of buffered events when asynchronous processing is enabled via @async annotation. Trigger throughput (For both trigger and corresponding stream) Source throughput Sink throughput Mapper latency throughput (For both input and output) Table memory throughput (For all operations) Latency (For all operations) Query memory latency Window throughput (For all operations) latency (For all operation) Partition throughput (For all operations) latency (For all operation) Syntax The syntax for defining the statistics for SiddhiApps running on Java or Python modes is as follows. @app:statistics( reporter=' reporter ', interval=' internal ', include=' included metrics for reporting ') The following parameters are used to configure statistics in Java and Python modes. Parameter Description Default Value reporter The implementation of the statistics reporter. Supported values are: console jmx console interval The statistics reporting time interval (in seconds). 60 include Specifies the metricizes that should report statistics using a comma-separated list or via wildcards. *.* (All) The syntax for defining the statistics for SiddhiApps running on Local, Docker, or Kubernetes modes is as follows. @app:statistics(enable = ' is enable ', include = `' included metrics for reporting '`) The following parameters are used to configure statistics in Local, Docker, and Kubernetes modes. Parameter Description Default Value enable Enables statistics reporting. Supported values are: true , false false include Specifies the metricizes that should report statistics using a comma-separated list or via wildcards. *.* (All) Here, other statistics configurations are applied commonly to all SiddhiApps, as specified in the Configuration Guide . Example A SiddhiApp running on Java, to report statistics every minute, by logging its stats on the console. @App:name('TestMetrics') @App:Statistics(reporter = 'console') define stream TestStream (message string); @info(name='logQuery') from TestSream#log(\"Message:\") insert into TempSream; The statistics reported via console log will be as follows. Click to view the extract 11/26/17 8:01:20 PM ============================================================ -- Gauges ---------------------------------------------------------------------- io.siddhi.SiddhiApps.TestMetrics.Siddhi.Queries.logQuery.memory value = 5760 io.siddhi.SiddhiApps.TestMetrics.Siddhi.Streams.TestStream.size value = 0 -- Meters ---------------------------------------------------------------------- io.siddhi.SiddhiApps.TestMetrics.Siddhi.Sources.TestStream.http.throughput count = 0 mean rate = 0.00 events/second 1-minute rate = 0.00 events/second 5-minute rate = 0.00 events/second 15-minute rate = 0.00 events/second io.siddhi.SiddhiApps.TestMetrics.Siddhi.Streams.TempSream.throughput count = 2 mean rate = 0.04 events/second 1-minute rate = 0.03 events/second 5-minute rate = 0.01 events/second 15-minute rate = 0.00 events/second io.siddhi.SiddhiApps.TestMetrics.Siddhi.Streams.TestStream.throughput count = 2 mean rate = 0.04 events/second 1-minute rate = 0.03 events/second 5-minute rate = 0.01 events/second 15-minute rate = 0.00 events/second -- Timers ---------------------------------------------------------------------- io.siddhi.SiddhiApps.TestMetrics.Siddhi.Queries.logQuery.latency count = 2 mean rate = 0.11 calls/second 1-minute rate = 0.34 calls/second 5-minute rate = 0.39 calls/second 15-minute rate = 0.40 calls/second min = 0.61 milliseconds max = 1.08 milliseconds mean = 0.84 milliseconds stddev = 0.23 milliseconds median = 0.61 milliseconds 75% < = 1.08 milliseconds 95% < = 1.08 milliseconds 98% < = 1.08 milliseconds 99% < = 1.08 milliseconds 99.9% < = 1.08 milliseconds Event Playback The speed of time within the SiddhiApp can be altered based on the actual event timestamp, using the @app:playback SiddhiApp annotation. Here, the event playback updates the current SiddhiApp time to the largest event time seen so far. Purpose Event playback helps to reprocess previously consumed and stored events in much faster speed, without losing the time-based properties of Siddhi queries, by rapidly replaying the events. Syntax The syntax for defining event playback is as follows. @app:playback(idle.time = ' idle time before incrementing timestamp ', increment = ' incremented time interval ') The following parameters are used to configure this annotation. Parameter Description idle.time The time duration (in milliseconds), within which when no events arrive, the current SiddhiApp time will be incremented by the value specified under the increment parameter. increment The number of seconds, by which, the current SiddhiApp time must be incremented when no events receive during the idle.time period. Here, both the parameters are optional, and when omitted, the current SiddhiApp time will not be automatically incremented when events do not arrive. Example 1 SiddhiApp to perform playback while incrementing the current SiddhiApp time by 2 seconds when no events arrive for every 100 milliseconds . @app:playback(idle.time = '100 millisecond', increment = '2 sec') Example 2 SiddhiApp to perform playback while not incrementing the current SiddhiApp time when no events arrive. @app:playback()","title":"Query Guide"},{"location":"docs/query-guide/#siddhi-51-streaming-sql-guide","text":"","title":"Siddhi 5.1 Streaming SQL Guide"},{"location":"docs/query-guide/#introduction","text":"Siddhi Streaming SQL is designed to process streams of events. It can be used to implement streaming data integration, streaming analytics, rule based and adaptive decision making use cases. It is an evolution of Complex Event Processing (CEP) and Stream Processing systems, hence it can also be used to process stateful computations, detecting of complex event patterns, and sending notifications in real-time. Siddhi Streaming SQL uses SQL like syntax, and annotations to consume events from diverse event sources with various data formats, process then using stateful and stateless operators and send outputs to multiple endpoints according to their accepted event formats. It also supports exposing rule based and adaptive decision making as service endpoints such that external programs and systems can synchronously get decision support form Siddhi. The following sections explains how to write event processing logic using Siddhi Streaming SQL.","title":"Introduction"},{"location":"docs/query-guide/#siddhi-application","text":"The processing logic for your program can be written using the Streaming SQL and put together as a single file with .siddhi extension. This file is called as the Siddhi Application or the SiddhiApp . SiddhiApps are named by adding @app:name(' name ') annotation on the top of the SiddhiApp file. When the annotation is not added Siddhi assigns a random UUID as the name of the SiddhiApp. Purpose SiddhiApp provides an isolated execution environment for your processing logic that allows you to deploy and execute processing logic independent of other SiddhiApp in the system. Therefore it's always recommended to have a processing logic related to single use case in a single SiddhiApp. This will help you to group processing logic and easily manage addition and removal of various use cases. Have different business use cases in separate Siddhi Applications. This is recommended as it allows users to selectively deploy the applications based their on business needs. It is also recommended to move the repeated steam processing logic that exist in multiple Siddhi Applications such as message retrieval and preprocessing, to a common Siddhi Application, whereby reducing code duplication and improving maintainability. In this case, to pass the events from one Siddhi App to another, configure them using a common topic using In-Memory Sink and In-Memory Source . The following diagram depicts some of the key Siddhi Streaming SQL elements of Siddhi Application and how event flows through the elements. Below table provides brief description of a few key elements in the Siddhi Streaming SQL Language. Elements Description Stream A logical series of events ordered in time with a uniquely identifiable name, and a defined set of typed attributes defining its schema. Event An event is a single event object associated with a stream. All events of a stream contains a timestamp and an identical set of typed attributes based on the schema of the stream they belong to. Table A structured representation of data stored with a defined schema. Stored data can be backed by In-Memory , or external data stores such as RDBMS , MongoDB , etc. The tables can be accessed and manipulated at runtime. Named-Window A structured representation of data stored with a defined schema and eviction policy. Window data is stored In-Memory and automatically cleared by the named-window constrain. Other siddhi elements can only query the values in windows at runtime but they cannot modify them. Named-Aggregation A structured representation of data that's incrementally aggregated and stored with a defined schema and aggregation granularity such as seconds, minutes, hours, etc. Aggregation data is stored both In-Memory and in external data stores such as RDBMS . Other siddhi elements can only query the values in windows at runtime but they cannot modify them. Query A logical construct that processes events in streaming manner by by consuming data from one or more streams, tables, windows and aggregations, and publishes output events into a stream, table or a window. Source A construct that consumes data from external sources (such as TCP , Kafka , HTTP , etc) with various event formats such as XML , JSON , binary , etc, convert then to Siddhi events, and passes into streams for processing. Sink A construct that consumes events arriving at a stream, maps them to a predefined data format (such as XML , JSON , binary , etc), and publishes them to external endpoints (such as E-mail , TCP , Kafka , HTTP , etc). Input Handler A mechanism to programmatically inject events into streams. Stream/Query Callback A mechanism to programmatically consume output events from streams or queries. Partition A logical container that isolates the processing of queries based on the partition keys derived from the events. Inner Stream A positionable stream that connects portioned queries with each other within the partition. Grammar SiddhiApp is a collection of Siddhi Streaming SQL elements composed together as a script. Here each Siddhi element must be separated by a semicolon ; . Hight level syntax of SiddhiApp is as follows. siddhi app : app annotation * ( stream definition | table definition | ... ) + ( query | partition ) + ; Example Siddhi Application with name Temperature-Analytics defined with a stream named TempStream and a query named 5minAvgQuery . @app:name('Temperature-Analytics') define stream TempStream (deviceID long, roomNo int, temp double); @info(name = '5minAvgQuery') from TempStream#window.time(5 min) select roomNo, avg(temp) as avgTemp group by roomNo insert into OutputStream;","title":"Siddhi Application"},{"location":"docs/query-guide/#stream","text":"A stream is a logical series of events ordered in time. Its schema is defined via the stream definition . A stream definition contains the stream name and a set of attributes having a specific type and a uniquely identifiable name within the scope of the stream. All events associated with the stream will have the same schema (i.e., have the same attributes in the same order). Purpose Stream groups common types of events together with a schema. This helps in various ways such as, processing all events in queries together and performing data format transformations together when they are consumed and published via sources and sinks. Syntax The syntax for defining a stream is as follows. define stream stream name ( attribute name attribute type , attribute name attribute type , ... ); The following parameters are used to configure a stream definition. Parameter Description stream name The name of the stream created. (It is recommended to define a stream name in PascalCase .) attribute name Uniquely identifiable name of the stream attribute. (It is recommended to define attribute names in camelCase .) attribute type The type of each attribute defined in the schema. This can be STRING , INT , LONG , DOUBLE , FLOAT , BOOL or OBJECT . To use and refer stream and attribute names that do not follow [a-zA-Z_][a-zA-Z_0-9]* format enclose them in ` . E.g. `$test(0)` . To make the stream process events in multi-threading and asynchronous way use the @async annotation as shown in Threading and synchronization configuration section. Example define stream TempStream (deviceID long, roomNo int, temp double); The above creates a stream with name TempStream having the following attributes. deviceID of type long roomNo of type int temp of type double","title":"Stream"},{"location":"docs/query-guide/#source","text":"Sources receive events via multiple transports and in various data formats, and direct them into streams for processing. A source configuration allows to define a mapping in order to convert each incoming event from its native data format to a Siddhi event. When customizations to such mappings are not provided, Siddhi assumes that the arriving event adheres to the predefined format based on the stream definition and the configured message mapping type. Purpose Source provides a way to consume events from external systems and convert them to be processed by the associated stream. Syntax To configure a stream that consumes events via a source, add the source configuration to a stream definition by adding the @source annotation with the required parameter values. The source syntax is as follows: @source(type=' source type ', static.key =' value ', static.key =' value ', @map(type=' map type ', static.key =' value ', static.key =' value ', @attributes( attribute1 =' attribute mapping ', attributeN =' attribute mapping ') ) ) define stream stream name ( attribute1 type , attributeN type ); This syntax includes the following annotations. Source The type parameter of @source annotation defines the source type that receives events. The other parameters of @source annotation depends upon the selected source type, and here some of its parameters can be optional. For detailed information about the supported parameters see the documentation of the relevant source. The following is the list of source types supported by Siddhi: Source type Description In-memory Allow SiddhiApp to consume events from other SiddhiApps running on the same JVM. HTTP Expose an HTTP service to consume messages. Kafka Subscribe to Kafka topic to consume events. TCP Expose a TCP service to consume messages. Email Consume emails via POP3 and IMAP protocols. JMS Subscribe to JMS topic or queue to consume events. File Reads files by tailing or as a whole to extract events out of them. CDC Perform change data capture on databases. Prometheus Consume data from Prometheus agent. In-memory is the only source inbuilt in Siddhi, and all other source types are implemented as extensions.","title":"Source"},{"location":"docs/query-guide/#source-mapper","text":"Each @source configuration can have a mapping denoted by the @map annotation that defines how to convert the incoming event format to Siddhi events. The type parameter of the @map defines the map type to be used in converting the incoming events. The other parameters of @map annotation depends on the mapper selected, and some of its parameters can be optional. For detailed information about the parameters see the documentation of the relevant mapper. Map Attributes @attributes is an optional annotation used with @map to define custom mapping. When @attributes is not provided, each mapper assumes that the incoming events adheres to its own default message format and attempt to convert the events from that format. By adding the @attributes annotation, users can selectively extract data from the incoming message and assign them to the attributes. There are two ways to configure @attributes . Define attribute names as keys, and mapping configurations as values: @attributes( attribute1 =' mapping ', attributeN =' mapping ') Define the mapping configurations in the same order as the attributes defined in stream definition: @attributes( ' mapping for attribute1 ', ' mapping for attributeN ') Supported source mapping types The following is the list of source mapping types supported by Siddhi: Source mapping type Description PassThrough Omits data conversion on Siddhi events. JSON Converts JSON messages to Siddhi events. XML Converts XML messages to Siddhi events. TEXT Converts plain text messages to Siddhi events. Avro Converts Avro events to Siddhi events. Binary Converts Siddhi specific binary events to Siddhi events. Key Value Converts key-value HashMaps to Siddhi events. CSV Converts CSV like delimiter separated events to Siddhi events. Tip When the @map annotation is not provided @map(type='passThrough') is used as default, that passes the consumed Siddhi events directly to the streams without any data conversion. PassThrough is the only source mapper inbuilt in Siddhi, and all other source mappers are implemented as extensions. Example 1 Receive JSON messages by exposing an HTTP service, and direct them to InputStream stream for processing. Here the HTTP service will be secured with basic authentication, receives events on all network interfaces on port 8080 and context /foo . The service expects the JSON messages to be on the default data format that's supported by the JSON mapper as follows. { \"name\":\"Paul\", \"age\":20, \"country\":\"UK\" } The configuration of the HTTP source and JSON source mapper to achieve the above is as follows. @source(type='http', receiver.url='http://0.0.0.0:8080/foo', @map(type='json')) define stream InputStream (name string, age int, country string); Example 2 Receive JSON messages by exposing an HTTP service, and direct them to StockStream stream for processing. Here the incoming JSON , as given below, do not adhere to the default data format that's supported by the JSON mapper. { \"portfolio\":{ \"stock\":{ \"volume\":100, \"company\":{ \"symbol\":\"FB\" }, \"price\":55.6 } } } The configuration of the HTTP source and the custom JSON source mapping to achieve the above is as follows. @source(type='http', receiver.url='http://0.0.0.0:8080/foo', @map(type='json', enclosing.element=\"$.portfolio\", @attributes(symbol = \"stock.company.symbol\", price = \"stock.price\", volume = \"stock.volume\"))) define stream StockStream (symbol string, price float, volume long); The same can also be configured by omitting the attribute names as below. @source(type='http', receiver.url='http://0.0.0.0:8080/foo', @map(type='json', enclosing.element=\"$.portfolio\", @attributes(\"stock.company.symbol\", \"stock.price\", \"stock.volume\"))) define stream StockStream (symbol string, price float, volume long);","title":"Source Mapper"},{"location":"docs/query-guide/#sink","text":"Sinks consumes events from streams and publish them via multiple transports to external endpoints in various data formats. A sink configuration allows users to define a mapping to convert the Siddhi events in to the required output data format (such as JSON , TEXT , XML , etc.) and publish the events to the configured endpoints. When customizations to such mappings are not provided, Siddhi converts events to the predefined event format based on the stream definition and the configured mapper type, before publishing the events. Purpose Sink provides a way to publish Siddhi events of a stream to external systems by converting events to their supported format. Syntax To configure a stream to publish events via a sink, add the sink configuration to a stream definition by adding the @sink annotation with the required parameter values. The sink syntax is as follows: @sink(type=' sink type ', static.key =' value ', dynamic.key ='{{ value }}', @map(type=' map type ', static.key =' value ', dynamic.key ='{{ value }}', @payload(' payload mapping ') ) ) define stream stream name ( attribute1 type , attributeN type ); Dynamic Properties The sink and sink mapper properties that are categorized as dynamic have the ability to absorb attribute values dynamically from the Siddhi events of their associated streams. This can be configured by enclosing the relevant attribute names in double curly braces as {{...}} , and using it within the property values. Some valid dynamic properties values are: '{{attribute1}}' 'This is {{attribute1}}' {{attribute1}} {{attributeN}} Here the attribute names in the double curly braces will be replaced with the values from the events before they are published. This syntax includes the following annotations. Sink The type parameter of the @sink annotation defines the sink type that publishes the events. The other parameters of the @sink annotation depends upon the selected sink type, and here some of its parameters can be optional and/or dynamic. For detailed information about the supported parameters see documentation of the relevant sink. The following is a list of sink types supported by Siddhi: Source type Description In-memory Allow SiddhiApp to publish events to other SiddhiApps running on the same JVM. Log Logs the events appearing on the streams. HTTP Publish events to an HTTP endpoint. Kafka Publish events to Kafka topic. TCP Publish events to a TCP service. Email Send emails via SMTP protocols. JMS Publish events to JMS topics or queues. File Writes events to files. Prometheus Expose data through Prometheus agent.","title":"Sink"},{"location":"docs/query-guide/#distributed-sink","text":"Distributed Sinks publish events from a defined stream to multiple endpoints using load balancing or partitioning strategies. Any sink can be used as a distributed sink. A distributed sink configuration allows users to define a common mapping to convert and send the Siddhi events for all its destination endpoints. Purpose Distributed sink provides a way to publish Siddhi events to multiple endpoints in the configured event format. Syntax To configure distributed sink add the sink configuration to a stream definition by adding the @sink annotation and add the configuration parameters that are common of all the destination endpoints inside it, along with the common parameters also add the @distribution annotation specifying the distribution strategy (i.e. roundRobin or partitioned ) and @destination annotations providing each endpoint specific configurations. The distributed sink syntax is as follows: RoundRobin Distributed Sink Publishes events to defined destinations in a round robin manner. @sink(type=' sink type ', common.static.key =' value ', common.dynamic.key ='{{ value }}', @map(type=' map type ', static.key =' value ', dynamic.key ='{{ value }}', @payload(' payload mapping ') ) @distribution(strategy='roundRobin', @destination( destination.specific.key =' value '), @destination( destination.specific.key =' value '))) ) define stream stream name ( attribute1 type , attributeN type ); Partitioned Distributed Sink Publish events to the defined destinations by partitioning them based on a partitioning key. @sink(type=' sink type ', common.static.key =' value ', common.dynamic.key ='{{ value }}', @map(type=' map type ', static.key =' value ', dynamic.key ='{{ value }}', @payload(' payload mapping ') ) @distribution(strategy='partitioned', partitionKey=' partition key ', @destination( destination.specific.key =' value '), @destination( destination.specific.key =' value '))) ) define stream stream name ( attribute1 type , attributeN type );","title":"Distributed Sink"},{"location":"docs/query-guide/#sink-mapper","text":"Each @sink configuration can have a mapping denoted by the @map annotation that defines how to convert Siddhi events to outgoing messages with the defined format. The type parameter of the @map defines the map type to be used in converting the outgoing events, and other parameters of @map annotation depend on the mapper selected, where some of these parameters can be optional and/or dynamic. For detailed information about the parameters see the documentation of the relevant mapper. Map Payload @payload is an optional annotation used with @map to define custom mapping. When the @payload annotation is not provided, each mapper maps the outgoing events to its own default event format. The @payload annotation allow users to configure mappers to produce the output payload of their choice, and by using dynamic properties within the payload they can selectively extract and add data from the published Siddhi events. There are two ways you to configure @payload annotation. Some mappers such as XML , JSON , and Test only accept one output payload: @payload( 'This is a test message from {{user}}.') Some mappers such key-value accept series of mapping values: @payload( key1='mapping_1', 'key2'='user : {{user}}') Here, the keys of payload mapping can be defined using the dot notation as a.b.c , or using any constant string value as '$abc' . Supported sink mapping types The following is a list of sink mapping types supported by Siddhi: Sink mapping type Description PassThrough Omits data conversion on outgoing Siddhi events. JSON Converts Siddhi events to JSON messages. XML Converts Siddhi events to XML messages. TEXT Converts Siddhi events to plain text messages. Avro Converts Siddhi events to Avro Events. Binary Converts Siddhi events to Siddhi specific binary events. Key Value Converts Siddhi events to key-value HashMaps. CSV Converts Siddhi events to CSV like delimiter separated events. Tip When the @map annotation is not provided @map(type='passThrough') is used as default, that passes the outgoing Siddhi events directly to the sinks without any data conversion. PassThrough is the only sink mapper inbuilt in Siddhi, and all other sink mappers are implemented as extensions. Example 1 Sink to publish OutputStream events by converting them to JSON messages with the default format, and by sending to an HTTP endpoint http://localhost:8005/endpoint1 , using POST method, Accept header, and basic authentication having admin is both username and password. The configuration of the HTTP sink and JSON sink mapper to achieve the above is as follows. @sink(type='http', publisher.url='http://localhost:8005/endpoint', method='POST', headers='Accept-Date:20/02/2017', basic.auth.enabled='true', basic.auth.username='admin', basic.auth.password='admin', @map(type='json')) define stream OutputStream (name string, age int, country string); This will publish a JSON message on the following format: { \"event\":{ \"name\":\"Paul\", \"age\":20, \"country\":\"UK\" } } Example 2 Sink to publish StockStream events by converting them to user defined JSON messages, and sending them to an HTTP endpoint http://localhost:8005/stocks . The configuration of the HTTP sink and custom JSON sink mapping to achieve the above is as follows. @sink(type='http', publisher.url='http://localhost:8005/stocks', @map(type='json', validate.json='true', enclosing.element='$.Portfolio', @payload(\"\"\"{\"StockData\":{ \"Symbol\":\"{{symbol}}\", \"Price\":{{price}} }}\"\"\"))) define stream StockStream (symbol string, price float, volume long); This will publish a single event as the JSON message on the following format: { \"Portfolio\":{ \"StockData\":{ \"Symbol\":\"GOOG\", \"Price\":55.6 } } } This can also publish multiple events together as a JSON message on the following format: { \"Portfolio\":[ { \"StockData\":{ \"Symbol\":\"GOOG\", \"Price\":55.6 } }, { \"StockData\":{ \"Symbol\":\"FB\", \"Price\":57.0 } } ] } Example 3 Sink to publish events from the OutputStream stream to multiple HTTP endpoints using a partitioning strategy. Here, the events are sent to either http://localhost:8005/endpoint1 or http://localhost:8006/endpoint2 based on the partitioning key country . It uses default JSON mapping, POST method, and admin as both the username and the password when publishing to both the endpoints. The configuration of the distributed HTTP sink and JSON sink mapper to achieve the above is as follows. @sink(type='http', method='POST', basic.auth.enabled='true', basic.auth.username='admin', basic.auth.password='admin', @map(type='json'), @distribution(strategy='partitioned', partitionKey='country', @destination(publisher.url='http://localhost:8005/endpoint1'), @destination(publisher.url='http://localhost:8006/endpoint2'))) define stream OutputStream (name string, age int, country string); This will partition the outgoing events and publish all events with the same country attribute value to the same endpoint. The JSON message published will be on the following format: { \"event\":{ \"name\":\"Paul\", \"age\":20, \"country\":\"UK\" } }","title":"Sink Mapper"},{"location":"docs/query-guide/#error-handling","text":"Errors in Siddhi can be handled at the Streams and in Sinks. Error Handling at Stream When errors are thrown by Siddhi elements subscribed to the stream, the error gets propagated up to the stream that delivered the event to those Siddhi elements. By default the error is logged and dropped at the stream, but this behavior can be altered by by adding @OnError annotation to the corresponding stream definition. @OnError annotation can help users to capture the error and the associated event, and handle them gracefully by sending them to a fault stream. The @OnError annotation and the required action to be specified as below. @OnError(action='on error action') define stream stream name ( attribute name attribute type , attribute name attribute type , ... ); The action parameter of the @OnError annotation defines the action to be executed during failure scenarios. The following actions can be specified to @OnError annotation to handle erroneous scenarios. LOG : Logs the event with the error, and drops the event. This is the default action performed even when @OnError annotation is not defined. STREAM : Creates a fault stream and redirects the event and the error to it. The created fault stream will have all the attributes defined in the base stream to capture the error causing event, and in addition it also contains _error attribute of type object to containing the error information. The fault stream can be referred by adding ! in front of the base stream name as ! stream name . Example Handle errors in TempStream by redirecting the errors to a fault stream. The configuration of TempStream stream and @OnError annotation is as follows. @OnError(action='STREAM') define stream TempStream (deviceID long, roomNo int, temp double); Siddhi will infer and automatically defines the fault stream of TempStream as given below. define stream !TempStream (deviceID long, roomNo int, temp double, _error object); The SiddhiApp extending the above the use-case by adding failure generation and error handling with the use of queries is as follows. Note: Details on writing processing logics via queries will be explained in later sections. -- Define fault stream to handle error occurred at TempStream subscribers @OnError(action='STREAM') define stream TempStream (deviceID long, roomNo int, temp double); -- Error generation through a custom function `createError()` @info(name = 'error-generation') from TempStream#custom:createError() insert into IgnoreStream1; -- Handling error by simply logging the event and error. @info(name = 'handle-error') from !TempStream#log(\"Error Occurred!\") select deviceID, roomNo, temp, _error insert into IgnoreStream2; Error Handling at Sink There can be cases where external systems becoming unavailable or coursing errors when the events are published to them. By default sinks log and drop the events causing event losses, and this can be handled gracefully by configuring on.error parameter of the @sink annotation. The on.error parameter of the @sink annotation can be specified as below. @sink(type=' sink type ', on.error=' on error action ', key =' value ', ...) define stream stream name ( attribute name attribute type , attribute name attribute type , ... ); The following actions can be specified to on.error parameter of @sink annotation to handle erroneous scenarios. LOG : Logs the event with the error, and drops the event. This is the default action performed even when on.error parameter is not defined on the @sink annotation. WAIT : Publishing threads wait in back-off and re-trying mode, and only send the events when the connection is re-established. During this time the threads will not consume any new messages causing the systems to introduce back pressure on the systems that publishes to it. STREAM : Pushes the failed events with the corresponding error to the associated fault stream the sink belongs to. Example 1 Introduce back pressure on the threads who bring events via TempStream when the system cannot connect to Kafka. The configuration of TempStream stream and @sink Kafka annotation with on.error property is as follows. @sink(type='kafka', on.error='WAIT', topic='{{roomNo}}', bootstrap.servers='localhost:9092', @map(type='xml')) define stream TempStream (deviceID long, roomNo int, temp double); Example 2 Send events to the fault stream of TempStream when the system cannot connect to Kafka. The configuration of TempStream stream with associated fault stream, @sink Kafka annotation with on.error property and a queries to handle the error is as follows. Note: Details on writing processing logics via queries will be explained in later sections. @OnError(action='STREAM') @sink(type='kafka', on.error='STREAM', topic='{{roomNo}}', bootstrap.servers='localhost:9092', @map(type='xml')) define stream TempStream (deviceID long, roomNo int, temp double); -- Handling error by simply logging the event and error. @info(name = 'handle-error') from !TempStream#log(\"Error Occurred!\") select deviceID, roomNo, temp, _error insert into IgnoreStream;","title":"Error Handling"},{"location":"docs/query-guide/#query","text":"Query defines the processing logic in Siddhi. It consumes events from one or more streams, named-windows , tables , and/or named-aggregations , process the events in a streaming manner, and generate output events into a stream , named-window , or table . Purpose A query provides a way to process the events in the order they arrive and produce output using both stateful and stateless complex event processing and stream processing operations. Syntax The high level query syntax for defining processing logics is as follows: @info(name = ' query name ') from input projection output action The following parameters are used to configure a stream definition. Parameter Description query name The name of the query. Since naming the query (i.e the @info(name = ' query name ') annotation) is optional, when the name is not provided Siddhi assign a system generated name for the query. input Defines the means of event consumption via streams , named-windows , tables , and/or named-aggregations , and defines the processing logic using filters , windows , stream-functions , joins , patterns and sequences . projection Generates output event attributes using select , functions , aggregation-functions , and group by operations, and filters the generated the output using having , limit offset , order by , and output rate limiting operations before sending them out. Here the projection is optional and when it is omitted all the input events will be sent to the output as it is. output action Defines output action (such as insert into , update , delete , etc) that needs to be performed by the generated events on a stream , named-window , or table","title":"Query"},{"location":"docs/query-guide/#from","text":"All Siddhi queries must always have at least one stream or named-window as an input (some queries can consume more than one stream or named-window ), and only join query can consume events via tables , or named-aggregations as the second input. The input stream , named-window , table , and/or named-aggregation should be defined before they can be used in a query. Syntax A high level syntax for consuming events from a stream, named-window, table, and/or named-aggregation is as follow; from (( stream | named-window ) handler *) ((join ( stream | named-window | table | named-aggregation ) handler *)|((,|- )( stream | named-window ) handler *)+)? projection insert into ( stream | named-window | table ) Here the handler denotes the processing logic using filters , windows , and stream-functions , join for joins , - for patterns , and , for sequences . More information on writing queries used these will be discussed in later sections.","title":"From"},{"location":"docs/query-guide/#insert","text":"Allows events to be inserted directly into streams , named-windows , or tables . When a query is defined to insert events into a stream that is not already defined, Siddhi infers and automatically defines its stream definition, such that queries defined below the current query can use the stream like any other predefined streams. Syntax Syntax to insert events into a stream, named-window or table from another stream is as follows; from input projection insert into ( stream | named-window | table ) This inserts all the newly arrived events ( current events ) in to a stream, named-window, or table. There are also other types of events other than current events that are produced by queries, the Event Type section provides more information on, how insertion operation can be modified to support those. Example A query to consume events from the TempStream stream and output only the roomNo and temp attributes to the RoomTempStream stream, from which another query to consume the events and send all its attributes to AnotherRoomTempStream stream. define stream TempStream (deviceID long, roomNo int, temp double); from TempStream select roomNo, temp insert into RoomTempStream; from RoomTempStream insert into AnotherRoomTempStream; Inferred Stream Here, the RoomTempStream and AnotherRoomTempStream streams are an inferred streams, which means their stream definitions are inferred from the queries and hence they can be used the same as any other defined streams without any restrictions.","title":"Insert"},{"location":"docs/query-guide/#value","text":"Values are typed data, which can be manipulated, transferred, and stored. Values can be referred by the attributes defined in definitions such as streams, and tables. Siddhi supports values of type STRING , INT (Integer), LONG , DOUBLE , FLOAT , BOOL (Boolean) and OBJECT . The syntax of each type and their example use as a constant value is as follows, Attribute Type Format Example int + 123 , -75 , +95 long +L 123000L , -750l , +154L float ( +)?('.' *)? (E(-|+)? +)?F 123.0f , -75.0e-10F , +95.789f double ( +)?('.' *)? (E(-|+)? +)?D? 123.0 , 123.0D , -75.0e-10D , +95.789d bool (true|false) true , false , TRUE , FALSE string '( char * !('|\"|\"\"\"| line ))' or \"( char * !(\"|\"\"\"| line ))\" or \"\"\"( char * !(\"\"\"))\"\"\" 'Any text.' , \"Text with 'single' quotes.\" , \"\"\" Text with 'single' quotes, \"double\" quotes, and new lines. \"\"\" Time Time is a special type of LONG value that denotes time using digits and their unit in the format ( digit + unit )+ . At execution, the time gets converted into milliseconds and returns a LONG value. Unit Syntax Year year | years Month month | months Week week | weeks Day day | days Hour hour | hours Minutes minute | minutes | min Seconds second | seconds | sec Milliseconds millisecond | milliseconds Example 1 hour and 25 minutes can by written as 1 hour and 25 minutes which is equal to the LONG value 5100000 .","title":"Value"},{"location":"docs/query-guide/#select","text":"The select clause in Siddhi query defines the output event attributes of the query. Following are some basic query projection operations supported by select. Action Description Select specific attributes for projection Only select some of the input attributes as query output attributes. E.g., Select and output only roomNo and temp attributes from the TempStream stream. from TempStream select roomNo, temp insert into RoomTempStream; Select all attributes for projection Select all input attributes as query output attributes. This can be done by using asterisk ( * ) or by omitting the select clause itself. E.g., Both following queries select all attributes of TempStream input stream and output all attributes to NewTempStream stream. from TempStream select * insert into NewTempStream; or from TempStream insert into NewTempStream; Name attribute Provide a unique name for each output attribute generated by the query. This can help to rename the selected input attributes or assign an attribute name to a projection operation such as function, aggregate-function, mathematical operation, etc, using as keyword. E.g., Query that renames input attribute temp to temperature and function currentTimeMillis() as time . from TempStream select roomNo, temp as temperature, currentTimeMillis() as time insert into RoomTempStream; Constant values as attributes Creates output attributes with a constant value. Any constant value of type STRING , INT , LONG , DOUBLE , FLOAT , BOOL , and time as given in the values section can be defined. E.g., Query specifying 'C' as the constant value for the scale attribute. from TempStream select roomNo, temp, 'C' as scale insert into RoomTempStream; Mathematical and logical expressions in attributes Defines the mathematical and logical operations that need to be performed to generating output attribute values. These expressions are executed in the precedence order given below. Operator precedence Operator Distribution Example () Scope (cost + tax) * 0.05 IS NULL Null check deviceID is null NOT Logical NOT not (price > 10) * , / , % Multiplication, division, modulus temp * 9/5 + 32 + , - Addition, subtraction temp * 9/5 - 32 < , < = , > , >= Comparators: less-than, greater-than-equal, greater-than, less-than-equal totalCost >= price * quantity == , != Comparisons: equal, not equal totalCost != price * quantity IN Checks if value exist in the table roomNo in ServerRoomsTable AND Logical AND temp < 40 and humidity < 40 OR Logical OR humidity < 40 or humidity >= 60 E.g., Query converts temperature from Celsius to Fahrenheit, and identifies rooms with room number between 10 and 15 as server rooms. from TempStream select roomNo, temp * 9/5 + 32 as temp, 'F' as scale, roomNo > 10 and roomNo < 15 as isServerRoom insert into RoomTempStream;","title":"Select"},{"location":"docs/query-guide/#function","text":"Functions are pre-configured operations that can consumes zero, or more parameters and always produce a single value as result. It can be used anywhere an attribute can be used. Purpose It encapsulate pre-configured reusable execution logic allowing users to execute the logic anywhere just by calling the function. This also make writing SiddhiApps simple and easy to understand. Syntax The syntax of function is as follows, ( namespace :)? function name ( ( parameter (, parameter )*)? ) Here, the namespace and function name together uniquely identifies the function. The function name is used to specify the operation provided by the function, and the namespace is used to identify the extension where the function exists. The inbuilt functions do not belong to a namespace, and hence namespace is omitted when they are defined. The parameter s define the input parameters that the function accepts. The input parameters can be attributes, constant values, results of other functions, results of mathematical or logical expressions, or time values. The number and type of parameters a function accepts depend on the function itself. Note Functions, mathematical expressions, and logical expressions can be used in a nested manner. Inbuilt functions Following are some inbuilt Siddhi functions. Inbuilt function Description eventTimestamp Returns event's timestamp. currentTimeMillis Returns current time of SiddhiApp runtime. default Returns a default value if the parameter is null. ifThenElse Returns parameters based on a conditional parameter. UUID Generates a UUID. cast Casts parameter type. convert Converts parameter type. coalesce Returns first not null input parameter. maximum Returns the maximum value of all parameters. minimum Returns the minimum value of all parameters. instanceOfBoolean Checks if the parameter is an instance of Boolean. instanceOfDouble Checks if the parameter is an instance of Double. instanceOfFloat Checks if the parameter is an instance of Float. instanceOfInteger Checks if the parameter is an instance of Integer. instanceOfLong Checks if the parameter is an instance of Long. instanceOfString Checks if the parameter is an instance of String. createSet Creates HashSet with given input parameters. sizeOfSet Returns number of items in the HashSet, that's passed as a parameter. Extension functions Several pre written functions can be found in the Siddhi extensions available here . Several pre written functions can be found under siddhi-execution-* extensions available here . Example 1 Function with name ifThenElse accepting three input parameters, first parameter being a bool condition price 700 and the second and the third parameters being the output for if case 'high' , and else case 'low' . ifThenElse(price 700, 'high', 'low') Example 2 math:ceil(inValue) Function with name ceil in math namespace accepting a single input parameters 56.89 and produces ceiling value 57 as output. math:ceil(56.89) Example 3 Query to convert the roomNo to string using convert function, find the maximum temperature reading with maximum function, and to add a unique messageID using the UUID function. from TempStream select convert(roomNo, 'string') as roomNo, maximum(tempReading1, tempReading2) as temp, UUID() as messageID insert into RoomTempStream;","title":"Function"},{"location":"docs/query-guide/#filter","text":"Filters filter events arriving on input streams based on specified conditions. They accept any type of condition including a combination of attributes, constants, functions, and others, that produces a Boolean result. Filters allow events to pass through if the condition results in true , and drops if it results in a false . Purpose Filter helps to select the events that are relevant for processing and omit the ones that are not. Syntax Filter conditions should be defined in square brackets ( [] ) next to the input stream as shown below. from input stream [ filter condition ] select attribute name , attribute name , ... insert into output stream Example Query to filter TempStream stream events, having roomNo within the range of 100-210 and temperature greater than 40 degrees, and insert the filtered results into HighTempStream stream. from TempStream[(roomNo = 100 and roomNo 210) and temp 40] select roomNo, temp insert into HighTempStream;","title":"Filter"},{"location":"docs/query-guide/#stream-function","text":"Stream functions process the events that arrive via the input stream (or named-window ), to generate zero or more new events with one or more additional output attributes for each event. Unlike the standard functions, they operate directly on the streams or (or named-windows ) and can add the function outputs via predefined attributes on the generated events. Purpose Stream function is useful when a function produces more than one output for the given input parameters. In this case, the outputs are added to the event, using newly introduced attributes with predefined attribute names. Syntax Stream function should be defined next to the input stream or named-windows along the # prefix as shown below. from input stream #( namespace :)? stream function name ( parameter , parameter , ... ) select attribute name , attribute name , ... insert into output stream Here, the namespace and stream function name together uniquely identifies the stream function. The stream function name is used to specify the operation provided by the window, and the namespace is used to identify the extension where the stream function exists. The inbuilt stream functions do not belong to a namespace, and hence namespace is omitted when they are defined. The parameter s define the input parameters that the stream function accepts. The input parameters can be attributes, constant values, functions, mathematical or logical expressions, or time values. The number and type of parameters a stream function accepts depend on the stream function itself. Inbuilt stream functions Following is an inbuilt Siddhi stream function. Inbuilt stream function Description pol2Cart Calculates cartesian coordinates x and y for the given theta , and rho coordinates. Extension stream functions Several pre written stream functions can be found in the Siddhi extensions available here . Example A query to calculate cartesian coordinates from theta , and rho attribute values optioned from the PolarStream stream, and to insert the results x and y via CartesianStream stream. define stream PolarStream (theta double, rho double); from PolarStream#pol2Cart(theta, rho) select x, y insert into CartesianStream; Here, the pol2Cart stream function amend the events with x and y attributes with respective cartesian values.","title":"Stream Function"},{"location":"docs/query-guide/#window","text":"Windows capture a subset of events from input streams and retain them for a period of time based on a specified criterion. The criterion defines when and how the events should be evicted from the window. Such as events getting evicted based on time duration, or number of events in the window, and the way they get evicted is in sliding (one by one) or tumbling (batch) manner. In a query, each input stream can at most have only one window associated with it. Purpose Windows help to retain events based on a criterion, such that the values of those events can be aggregated, correlated or checked, if the event of interest is in the window. Syntax Window should be defined next to the input stream along the #window prefix as shown below. from input stream #window.( namespace :)? window name ( parameter , parameter , ... ) select attribute name , attribute name , ... insert output event type ? into output stream Here, the namespace and window name together uniquely identifies the window. The window name is used to specify the operation provided by the window, and the namespace is used to identify the extension where the window exists. The inbuilt windows do not belong to a namespace, and hence namespace is omitted when they are defined. The parameter s define the input parameters that the window accepts. The input parameters can be attributes, constant values, functions, mathematical or logical expressions, or time values. The number and type of parameters a window accepts depend on the window itself. Note Filter conditions and stream functions can be applied both before and/or after the window. Inbuilt windows Following are some inbuilt Siddhi windows. Inbuilt function Description time Retains events based on time in a sliding manner. timeBatch Retains events based on time in a tumbling/batch manner. length Retains events based on number of events in a sliding manner. lengthBatch Retains events based on number of events in a tumbling/batch manner. timeLength Retains events based on time and number of events in a sliding manner. session Retains events for each session based on session key. batch Retains events of last arrived event chunk. sort Retains top-k or bottom-k events based on a parameter value. cron Retains events based on cron time in a tumbling/batch manner. externalTime Retains events based on event time value passed as a parameter in a sliding manner. externalTimeBatch Retains events based on event time value passed as a parameter in a a tumbling/batch manner. delay Retains events and delays the output by the given time period in a sliding manner. Extension windows Several pre written windows can be found under siddhi-execution-* extensions available here . Example 1 Query to find out the maximum temperature out of the last 10 events , using the window of length 10 and max() aggregation function, from the TempStream stream and insert the results into the MaxTempStream stream. from TempStream#window.length(10) select max(temp) as maxTemp insert into MaxTempStream; Here, the length window operates in a sliding manner where the following 3 event subsets are calculated and outputted when a list of 12 events are received in sequential order. Subset Event Range 1 1 - 10 2 2 - 11 3 3 - 12 Example 2 Query to find out the maximum temperature out of the every 10 events , using the window of lengthBatch 10 and max() aggregation function, from the TempStream stream and insert the results into the MaxTempStream stream. from TempStream#window.lengthBatch(10) select max(temp) as maxTemp insert into MaxTempStream; Here, the window operates in a batch/tumbling manner where the following 3 event subsets are calculated and outputted when a list of 30 events are received in a sequential order. Subset Event Range 1 1 - 10 2 11 - 20 3 21 - 30 Example 3 Query to find out the maximum temperature out of the events arrived during last 10 minutes , using the window of time 10 minutes and max() aggregation function, from the TempStream stream and insert the results into the MaxTempStream stream. from TempStream#window.time(10 min) select max(temp) as maxTemp insert into MaxTempStream; Here, the time window operates in a sliding manner with millisecond accuracy, where it will process events in the following 3 time durations and output aggregated events when a list of events are received in a sequential order. Subset Time Range (in ms) 1 1:00:00.001 - 1:10:00.000 2 1:00:01.001 - 1:10:01.000 3 1:00:01.033 - 1:10:01.034 Example 4 Query to find out the maximum temperature out of the events arriving every 10 minutes , using the window of timeBatch 10 and max() aggregation function, from the TempStream stream and insert the results into the MaxTempStream stream. from TempStream#window.timeBatch(10 min) select max(temp) as maxTemp insert into MaxTempStream; Here, the window operates in a batch/tumbling manner where the window will process events in the following 3 time durations and output aggregated events when a list of events are received in a sequential order. Subset Time Range (in ms) 1 1:00:00.001 - 1:10:00.000 2 1:10:00.001 - 1:20:00.000 3 1:20:00.001 - 1:30:00.000 Example 5 Query to find out the unique number of deviceID s arrived over last 1 minute , using the time window in the unique extension, and to insert the results into the UniqueCountStream stream. define stream TempStream (deviceID long, roomNo int, temp double); from TempStream#window.unique:time(deviceID, 1 sec) select count(deviceID) as deviceIDs insert into UniqueCountStream ;","title":"Window"},{"location":"docs/query-guide/#event-type","text":"Query output depends on the current and expired event types produced by the query based on its internal processing state. By default all queries produce current events upon event arrival. The queries containing windows additionally produce expired events when events expire from those windows. Purpose Event type helps to identify how the events were produced and to specify when a query should output such events to the output stream, such as output processed events only upon new event arrival to the query, upon event expiry from the window, or upon both cases. Syntax Event type should be defined in between insert and into keywords for insert queries as follows. from input stream #window. window name ( parameter , parameter , ... ) select attribute name , attribute name , ... insert event type into output stream Event type should be defined next to the for keyword for delete queries as follows. from input stream #window. window name ( parameter , parameter , ... ) select attribute name , attribute name , ... delete table (for event type )? on condition Event type should be defined next to the for keyword for update queries as follows. from input stream #window. window name ( parameter , parameter , ... ) select attribute name , attribute name , ... update table (for event type )? set table . attribute name = ( attribute name | expression )?, table . attribute name = ( attribute name | expression )?, ... on condition Event type should be defined next to the for keyword for update or insert queries as follows. from input stream #window. window name ( parameter , parameter , ... ) select attribute name , attribute name , ... update or insert into table (for event type )? set table . attribute name = expression , table . attribute name = expression , ... on condition The event types can be defined using the following keywords to manipulate query output. Event types Description current events Outputs processed events only upon new event arrival at the query. This is default behavior when no specific event type is specified. expired events Outputs processed events only upon event expiry from the window. all events Outputs processed events when both new events arrive at the query as well as when events expire from the window. Note Controlling query output based on the event types neither alters query execution nor its accuracy. Example Query to output processed events only upon event expiry from the 1 minute time window to the DelayedTempStream stream. This query helps to delay events by a minute. from TempStream#window.time(1 min) select * insert expired events into DelayedTempStream Note This is just to illustrate how expired events work, it is recommended to use delay window for use cases where we need to delay events by a given time period of time.","title":"Event Type"},{"location":"docs/query-guide/#aggregate-function","text":"Aggregate functions are pre-configured aggregation operations that can consume zero, or more parameters from multiple events and produce a single value as result. They can be only used in query projection (as part of the select clause). When a query comprises a window, the aggregation will be constrained to the events in the window, and when it does not have a window, the aggregation is performed from the first event the query has received. Purpose Aggregate functions encapsulate pre-configured reusable aggregate logic allowing users to aggregate values of multiple events together. When used with batch/tumbling windows this will also reduce the number of output events produced. Syntax Aggregate function can be used in query projection (as part of the select clause) alone or as a part of another expression. In all cases, the output produced should be properly mapped to the output stream attribute of the query using the as keyword. The syntax of aggregate function is as follows, from input stream #window. window name ( parameter , parameter , ... ) select ( namespace :)? aggregate function name ( parameter , parameter , ... ) as attribute name , attribute2 name , ... insert into output stream ; Here, the namespace and aggregate function name together uniquely identifies the aggregate function. The aggregate function name is used to specify the operation provided by the aggregate function, and the namespace is used to identify the extension where the aggregate function exists. The inbuilt aggregate functions do not belong to a namespace, and hence namespace is omitted when they are defined. The parameter s define the input parameters the aggregate function accepts. The input parameters can be attributes, constant values, results of other functions or aggregate functions, results of mathematical or logical expressions, or time values. The number and type of parameters an aggregate function accepts depend on the aggregate function itself. Inbuilt aggregate functions Following are some inbuilt aggregation functions. Inbuilt aggregate function Description sum Calculates the sum from a set of values. count Calculates the count from a set of values. distinctCount Calculates the distinct count based on a parameter from a set of values. avg Calculates the average from a set of values. max Finds the maximum value from a set of values. max Finds the minimum value from a set of values. maxForever Finds the maximum value from all events throughout its lifetime irrespective of the windows. minForever Finds the minimum value from all events throughout its lifetime irrespective of the windows. stdDev Calculates the standard deviation from a set of values. and Calculates boolean and from a set of values. or Calculates boolean or from a set of values. unionSet Constructs a Set by unioning set of values. Extension aggregate functions Several pre written aggregate functions can be found under siddhi-execution-* extensions available here . Example Query to calculate average, maximum, minimum and 97 th percentile values on temp attribute of the TempStream stream in a sliding manner, from the events arrived over the last 10 minutes and to produce output events with attributes avgTemp , maxTemp , minTemp and percentile97 respectively to the AggTempStream stream. from TempStream#window.time(10 min) select avg(temp) as avgTemp, max(temp) as maxTemp, min(temp) as minTemp, math:percentile(temp, 97.0) as percentile97 insert into AggTempStream;","title":"Aggregate Function"},{"location":"docs/query-guide/#group-by","text":"Group By groups events based on one or more specified attributes to perform aggregate operations. Purpose Group By helps to perform aggregate functions independently for each given group-by key combination. Syntax The syntax for the Group By with aggregate function is as follows. from input stream #window. window name (...) select aggregate function ( parameter , parameter , ...) as attribute1 name , attribute2 name , ... group by attribute1 name , attribute2 name , ... insert into output stream ; Here the group by attributes should be defined next to the group by keyword separating each attribute by a comma. Example Query to calculate the average temp per each roomNo and deviceID combination, from the events arrived from TempStream stream, during the last 10 minutes time-window in a sliding manner. from TempStream#window.time(10 min) select roomNo, deviceID, avg(temp) as avgTemp group by roomNo, deviceID insert into AvgTempStream;","title":"Group By"},{"location":"docs/query-guide/#having","text":"Having filters events at the query output using a specified condition on query output stream attributes. It accepts any type of condition including a combination of output stream attributes, constants, and/or functions that produces a Boolean result. Having, allow events to passthrough if the condition results in true , and drops if it results in a false . Purpose Having helps to select the events that are relevant for the output based on the attributes those are produced by the select clause and omit the ones that are not. Syntax The syntax for the Having clause is as follows. from input stream #window. window name ( ... ) select aggregate function ( parameter , parameter , ...) as attribute1 name , attribute2 name , ... group by attribute1 name , attribute2 name ... having condition insert into output stream ; Here the having condition should be defined next to the having keyword, and it can be used with or without group by clause. Example Query to calculate the average temp per roomNo for the events arrived on the last 10 minutes, and send alerts for each event having avgTemp more than 30 degrees. from TempStream#window.time(10 min) select roomNo, avg(temp) as avgTemp group by roomNo having avgTemp 30 insert into AlertStream;","title":"Having"},{"location":"docs/query-guide/#order-by","text":"Order By, orders the query results in ascending or descending order based on one or more specified attributes. By default the order by attribute orders the events in ascending order, and by adding desc keyword, the events can be ordered in descending order. When more than one attribute is defined the attributes defined towards the left will have more precedence in ordering than the ones defined in right. Purpose Order By helps to sort the events in the query output chunks. Order By will only be effective when query outputs a lot of events together such as in batch windows than for sliding windows where events are emitted one at a time. Syntax The syntax for the Order By clause is as follows: from input stream #window. window name ( ... ) select aggregate function ( parameter , parameter , ...) as attribute1 name , attribute2 name , ... group by attribute1 name , attribute2 name ... having condition order by attribute1 name (asc|desc)?, attribute2 name (asc|desc)?, ... insert into output stream ; Here, the order by attributes ( attributeN name ) should be defined next to the order by keyword separating each by a comma, and optionally the event ordering can be specified using asc (default) or desc keywords to respectively define ascending and descending. Example Query to calculate the average temp , per roomNo and deviceID combination, on every 10 minutes batches, and order the generated output events in ascending order by avgTemp and then in descending order of roomNo (if there are more events having the same avgTemp value) before emitting them to the AvgTempStream stream. from TempStream#window.timeBatch(10 min) select roomNo, deviceID, avg(temp) as avgTemp group by roomNo, deviceID order by avgTemp, roomNo desc insert into AvgTempStream;","title":"Order By"},{"location":"docs/query-guide/#limit-offset","text":"These provide a way to select a limited number of events (via limit) from the desired index (using an offset) from the output event chunks produced by the query. Purpose Limit Offset helps to output only the selected set of events from large event batches. This will be very useful with Order By clause where one can order the output and extract the topK or bottomK events, and even use it to paginate through the dataset by obtaining set of events from the middle. Syntax The syntax for the Limit Offset clauses is as follows: from input stream #window. window name ( ... ) select aggregate function ( parameter , parameter , ...) as attribute1 name , attribute2 name , ... group by attribute1 name , attribute2 name ... having condition order by attribute1 name (asc | desc)?, attribute2 name ( ascend/descend )?, ... limit positive integer ? offset positive integer ? insert into output stream ; Here both limit and offset are optional and both can be defined by adding a positive integer next to their keywords, when limit is omitted the query will output all the events, and when offset is omitted 0 is taken as the default offset value. Example 1 Query to calculate the average temp , per roomNo and deviceID combination, for every 10 minutes batches, from the events arriving at the TempStream stream, and emit only two events having the highest avgTemp value. from TempStream#window.timeBatch(10 min) select roomNo, deviceID, avg(temp) as avgTemp group by roomNo, deviceID order by avgTemp desc limit 2 insert into HighestAvgTempStream; Example 2 Query to calculate the average temp , per roomNo and deviceID combination, for every 10 minutes batches, for the events arriving at the TempStream stream, and emits only the third, forth and fifth events when sorted in descending order based on their avgTemp value. from TempStream#window.timeBatch(10 min) select roomNo, deviceID, avg(temp) as avgTemp group by roomNo, deviceID order by avgTemp desc limit 3 offset 2 insert into HighestAvgTempStream;","title":"Limit &amp; Offset"},{"location":"docs/query-guide/#stream-processor","text":"Stream processors are a combination of stream stream functions and windows . They work directly on the input streams (or named-windows ), to generate zero or more new events with zero or more additional output attributes while having the ability to retain and arbitrarily emit events. They are more advanced than stream functions as they can retain and arbitrarily emit events, and they are more advanced than windows because they can add additional attributes to the events. Purpose Stream processors help to achieve complex execution logics that cannot be achieved by other constructs such as functions , aggregate functions , stream functions and windows . Syntax Stream processor should be defined next to the input stream or named-windows along the # prefix as shown below. from input stream #( namespace :)? stream processor name ( parameter , parameter , ... ) select attribute name , attribute name , ... insert into output stream Here, the namespace and stream processor name together uniquely identifies the stream processor. The stream processor name is used to specify the operation provided by the window, and the namespace is used to identify the extension where the stream processor exists. The inbuilt stream processors do not belong to a namespace, and hence namespace is omitted when they are defined. The parameter s define the input parameters that the stream processor accepts. The input parameters can be attributes, constant values, processors, mathematical or logical expressions, or time values. The number and type of parameters a stream processor accepts depend on the stream processor itself. Inbuilt stream processors Following is an inbuilt Siddhi stream processor. Inbuilt stream processor Description log Logs the message on the given priority with or without the processed event. Extension stream processors Several pre written stream processors can be found in the Siddhi extensions available here . Note Stream processors can be used before or after filters , stream functions , windows , or other stream processors. Example A query to log a message \"Sample Event :\" along with the event on \"INFO\" log level for all events of InputStream Stream. from InputStream#log(\"INFO\", \"Sample Event :\", true) select * insert into IgnoreStream;","title":"Stream Processor"},{"location":"docs/query-guide/#join-stream","text":"Joins combine events from two streams in real-time based on a specified condition. Purpose Join provides a way of correlating events of two steams and in addition aggregating them based on the defined windows. Two streams cannot directly join as they are stateless, and they do not retain events. Therefore, each stream needs to be associated with a window for joining as it can retain events. Join also accepts a condition to match events against each event stream window. During the joining process each incoming event of each stream is matched against all the events in the other stream's window based on the given condition, and the output events are generated for all the matching event pairs. When there is no window associated with the joining steam, and empty window with length zero is assigned to the steam by default, to enable join process while preserving stream's stateless nature. Note Join can also be performed with table , named-aggregation , or named-windows . Syntax The syntax to join two streams is as follows: from input stream ( non window handler )*(#window. window name ( parameter , ... ))? (unidirectional)? (as reference )? join type input stream ( non window handler )*(#window. window name ( parameter , ... ))? (unidirectional)? (as reference )? (on join condition )? select reference . attribute name , reference . attribute name , ... insert into output stream Here, both the streams can have optional non window handlers (filters, stream functions, and stream processors) followed by a window associated with them. They can also have an optional join condition next to the on keyword to match events from both windows to generate combined output events. Window should be defined as the last element of each joining stream. Join query expects a window to be defined as the last element of each joining stream, therefore a filter cannot be defined after the window. Supported join types Following are the supported join operations. Inner join (join) This is the default behavior of a join operation, and the join keyword is used to join both the streams. The output is generated only if there is a matching event in both the stream windows when either of the streams triggers the join operation. Left outer join The left outer join keyword is used to join two streams while producing all left stream events to the output. Here, the output is generated when right stream triggers the join operation and finds matching events in the left stream window to perform the join, and in all cases where the left stream triggers the join operation. Here, when the left stream finds matching events in the right stream window, it uses them for the join, and if there are no matching events, then it uses null values for the join operation. Right outer join This is similar to a left outer join and the right outer join keyword is used to join two streams while producing all right stream events to the output. It generate output in all cases where the right stream triggers the join operation even if there are no matching events in the left stream window. Full outer join The full outer join combines the results of left outer join and right outer join. The full outer join keyword is used to join the streams while producing both left and stream events to the output. Here, the output is generated in all cases where the left or right stream triggers the join operation, and when a stream finds matching events in the other stream window, it uses them for the join, and if there are no matching events, then it uses null values instead. Cross join In either of these cases, when the join condition is omitted, the triggering event will successfully match against all the events in the other stream window, producing a cross join behavior. Unidirectional join operation By default, events arriving on either stream trigger the join operation and generate the corresponding output. However, this join behavior can be controlled by adding the unidirectional keyword next to one of the streams as depicted in the join query syntax above. This enables only the stream with the unidirectional to trigger the join operation. Therefore the events arriving on the other stream will neither trigger the join operation nor produce any output, but rather they only update their stream's window state. The unidirectional keyword cannot be applied on both join streams. This is because the default behavior already allows both the streams to trigger the join operation. Example 1 (join) A query to generate output when there is a matching event having equal symbol and companyID combination from the events arrived in the last 10 minutes on StockStream stream and the events arrived in the last 20 minutes on TwitterStream stream. define stream StockStream (symbol string, price float, volume long); define stream TwitterStream (companyID string, tweet string); from StockStream#window.time(10 min) as S join TwitterStream#window.time(20 min) as T on S.symbol== T.companyID select S.symbol as symbol, T.tweet, S.price insert into OutputStream ; Possible OutputStream outputs as follows (\"FB\", \"FB is great!\", 23.5f) (\"GOOG\", \"Its time to Google!\", 54.5f) Example 2 (with no join condition) A query to generate output for all possible event combinations from the last 5 events of the StockStream stream and the events arrived in the last 1 minutes on TwitterStream stream. define stream StockStream (symbol string, price float, volume long); define stream TwitterStream (companyID string, tweet string); from StockStream#window.length(5) as S join TwitterStream#window.time(1 min) as T select S.symbol as symbol, T.tweet, S.price insert into OutputStream ; Possible OutputStream outputs as follows, (\"FB\", \"FB is great!\", 23.5f) (\"FB\", \"Its time to Google!\", 23.5f) (\"GOOG\", \"FB is great!\", 54.5f) (\"GOOG\", \"Its time to Google!\", 54.5f) Example 3 (left outer join) A query to generate output for all events arriving in the StockStream stream regardless of whether there is a matching companyID for symbol exist in the events arrived in the last 20 minutes on TwitterStream stream, and generate output for the events arriving in the StockStream stream only when there is a matchine symbol and companyID combination exist in the events arrived in the last 10 minutes on StockStream stream. define stream StockStream (symbol string, price float, volume long); define stream TwitterStream (companyID string, tweet string); from StockStream#window.time(10 min) as S left outer join TwitterStream#window.time(20 min) as T on S.symbol== T.companyID select S.symbol as symbol, T.tweet, S.price insert into OutputStream ; Possible OutputStream outputs as follows, (\"FB\", \"FB is great!\", 23.5f) (\"GOOG\", null, 54.5f) //when there are no matching event in TwitterStream Example 3 (full outer join) A query to generate output for all events arriving in the StockStream stream and in the TwitterStream stream regardless of whether there is a matching companyID for symbol exist in the other stream window or not. define stream StockStream (symbol string, price float, volume long); define stream TwitterStream (companyID string, tweet string); from StockStream#window.time(10 min) as S full outer join TwitterStream#window.time(20 min) as T on S.symbol== T.companyID select S.symbol as symbol, T.tweet, S.price insert into OutputStream ; Possible OutputStream outputs as follows, (\"FB\", \"FB is great!\", 23.5f) (\"GOOG\", null, 54.5f) //when there are no matching event in TwitterStream (null, \"I like to tweet!\", null) //when there are no matching event in StockStream Example 3 (unidirectional join) A query to generate output only when events arrive on StockStream stream find a matching event having equal symbol and companyID combination against the events arrived in the last 20 minutes on TwitterStream stream. define stream StockStream (symbol string, price float, volume long); define stream TwitterStream (companyID string, tweet string); from StockStream#window.time(10 min) unidirectional as S join TwitterStream#window.time(20 min) as T on S.symbol== T.companyID select S.symbol as symbol, T.tweet, S.price insert into OutputStream ; Possible OutputStream outputs as follows, (\"FB\", \"FB is great!\", 23.5f) (\"GOOG\", \"Its time to Google!\", 54.5f) Here both outputs will be initiated by events arriving on StockStream .","title":"Join (Stream)"},{"location":"docs/query-guide/#pattern","text":"The pattern is a state machine implementation that detects event occurrences from events arrived via one or more event streams over time. It can repetitively match patterns, count event occurrences, and use logical event ordering (using and , or , and not ). Purpose The pattern helps to achieve Complex Event Processing (CEP) capabilities by detecting various pre-defined event occurrence patterns in realtime. Pattern query does not expect the matching events to occur immediately after each other, and it can successfully correlate the events who are far apart and having other events in between. Syntax The syntax for a pattern query is as follows, from ( (every)? ( event reference =)? input stream [ filter condition ]( min count : max count )? | (every)? ( event reference =)? input stream [ filter condition ] (and|or) ( event reference =)? input stream [ filter condition ] | (every)? not input stream [ filter condition ] (and event reference = input stream [ filter condition ] | for time gap ) ) - ... (within time gap )? select event reference . attribute name , event reference . attribute name , ... insert into output stream Items Description -> Indicates an event will follow the given event. The subsequent event does not necessarily have to occur immediately after the preceding event. The condition to be met by the preceding event should be added before the -> , and the condition to be met by the subsequent event should be added after the -> . every An optional keyword defining when a new event matching state-machine should be initiated to repetitively match the pattern. When this keyword is not used, the event matching state-machine will be initiated only once. within time gap An optional within clause that defines the time duration within which all the matching events should occur. min count : max count Determines the number of minimum and maximum number of events that should the matched at the given condition. Possible values for the min and max count and their behavior is as follows, Syntex Description Example n1:n2 Matches n1 to n2 events (including n1 and not more than n2 ). 1:4 matches 1 to 4 events. n: Matches n or more events (including n ). 2:> matches 2 or more events. :n Matches up to n events (excluding n ). :5 matches up to 5 events. n Matches exactly n events. 5 matches exactly 5 events. and Allows both of its condition to be matched by two distinct events in any order. or Only expects one of its condition to be matched by an event. Here the event reference of the unmatched condition will be null . not condition1 and condition2 Detects the event matching condition2 before any event matching condition1 . not condition1> for time period> Detects no event matching on condition1 for the specified time period . event reference An optional reference to access the matching event for further processing. All conditions can be assigned to an event reference to collect the matching event occurrences, other than the condition used for not case (as there will not be any event matched against it). Non occurrence of events. Siddhi detects non-occurrence of events using the not keyword, and its effective non-occurrence checking period is bounded either by fulfillment of a condition associated by and or via an expiry time using time period . Logical correlation of multiple conditions. Siddhi can only logically correlate two conditions at a time using keywords such as and , or , and not . When more than two conditions need to be logically correlated, use multiple pattern queries in a chaining manner, at a time correlating two logical conditions and streaming the output to a downstream query to logically correlate the results with other logical conditions. Event selection The event reference in pattern queries is used to retrieve the matched events. When a pattern condition is intended to match only a single event, then its attributes can be retrieved by referring to its reference as event reference . attribute name . An example of this is as follows. e1.symbol , refers to the symbol attribute value of the matching event e1 . But when the pattern condition is associated with min count : max count , it is expected to match against on multiple events. Therefore, an event from the matched event collection should be retrieved using the event index from its reference. Here the indexes are specified in square brackets next to event reference, where index 0 referring to the first event, and a special index last referring to the last available event in the collection. Attribute values of all the events in the matching event collection can be accessed a list, by referring to their event reference without an index. Some possible indexes and their behavior is as follows. e1[0].symbol , refers to the symbol attribute value of the 1 st event in reference e1 . e1[3].price , refers to the price attribute value of the 4 th event in reference e1 . e1[last].symbol , refers to the symbol attribute value of the last event in reference e1 . e1[last - 1].symbol , refers to the symbol attribute value of one before the last event in reference e1 . e1.symbol , refers to the list of symbol attribute values of all events in the event collection in reference e1 , as a list object. The system returns null when accessing attribute values, when no matching event is assigned to the event reference (as in when two conditions are combined using or ) or when the provided index is greater than the last event index in the event collection. Example 1 (Every) A query to send an alerts when temperature of a room increases by 5 degrees within 10 min. from every( e1=TempStream ) - e2=TempStream[ e1.roomNo == roomNo and (e1.temp + 5) = temp ] within 10 min select e1.roomNo, e1.temp as initialTemp, e2.temp as finalTemp insert into AlertStream; Here, the matching process begins for each event in the TempStream stream (as every is used with e1=TempStream ), and if another event arrives within 10 minutes with a value for temp attribute being greater than or equal to e1.temp + 5 of the initial event e1 , an output is generated via the AlertStream . Example 2 (Event collection) A query to find the temperature difference between two regulator events. define stream TempStream (deviceID long, roomNo int, temp double); define stream RegulatorStream (deviceID long, roomNo int, tempSet double, isOn bool); from every e1=RegulatorStream - e2=TempStream[e1.roomNo==roomNo] 1: - e3=RegulatorStream[e1.roomNo==roomNo] select e1.roomNo, e2[0].temp - e2[last].temp as tempDiff insert into TempDiffStream; Here, one or more TempStream events having the same roomNo as of the RegulatorStream stream event matched in e1 is collected, and among them, the first and the last was retrieved to find the temperature difference. Example 3 (Logical or condition) Query to send the stop control action to the regulator via RegulatorActionStream when the key is removed from the hotel room. Here the key actions are monitored via RoomKeyStream stream, and the regulator state is monitored through RegulatorStateChangeStream stream. define stream RegulatorStateChangeStream(deviceID long, roomNo int, tempSet double, action string); define stream RoomKeyStream(deviceID long, roomNo int, action string); from every e1=RegulatorStateChangeStream[ action == 'on' ] - e2=RoomKeyStream[ e1.roomNo == roomNo and action == 'removed' ] or e3=RegulatorStateChangeStream[ e1.roomNo == roomNo and action == 'off'] select e1.roomNo, ifThenElse( e2 is null, 'none', 'stop' ) as action having action != 'none' insert into RegulatorActionStream; Here, the query sends a stop action on RegulatorActionStream stream, if a removed action is triggered in the RoomKeyStream stream before the regulator state changing to off which is notified via RegulatorStateChangeStream stream. Example 4 (Logical not condition) Query to generate alerts if the regulator gets switched off before the temperature reaches 12 degrees. define stream RegulatorStateChangeStream(deviceID long, roomNo int, tempSet double, action string); define stream TempStream (deviceID long, roomNo int, temp double); from every e1=RegulatorStateChangeStream[action == 'start'] - not TempStream[e1.roomNo == roomNo and temp = 12] and e2=RegulatorStateChangeStream[e1.roomNo == roomNo and action == 'off'] select e1.roomNo as roomNo insert into AlertStream; Here, the query alerts the roomNo via AlertStream stream, when no temperature events having less than 12 arrived in the TempStream between the start and off actions of the regulator, notified via RegulatorActionStream stream. Example 5 (Logical not condition) Query to alert if the room temperature does not reduce to the set value within 5 minutes after switching on the regulator. define stream RegulatorStateChangeStream(deviceID long, roomNo int, tempSet double, action string); define stream TempStream (deviceID long, roomNo int, temp double); from e1=RegulatorStateChangeStream[action == 'start'] - not TempStream[e1.roomNo == roomNo and temp = e1.tempSet] for '5 min' select e1.roomNo as roomNo insert into AlertStream; Here, the query alerts the roomNo via AlertStream stream, when no temperature events having less than tempSet temperature arrived in the TempStream within 5 minutes of the regulator start action arrived via RegulatorActionStream stream. Example 6 (Detecting event non-occurrence) Following table presents some non-occurrence event matching scenarios that can be implemented using patterns. Pattern Description Sample Scenario not A for time period The non-occurrence of event A within time period after system start up. Alerts if the taxi has not reached its destination within 30 minutes, indicating that the passenger might be in danger. not A for time period and B Event A does not occur within time period , but event B occurs at some point in time. Alerts if the taxi has not reached its destination within 30 minutes, and the passenger has marked that he/she is in danger at some point in time. not A for time period or B Either event A does not occur within time period , or event B occurs at some point in time. Alerts if the taxi has not reached its destination within 30 minutes, or if the passenger has marked that he/she is in danger at some point in time. not A for time period 1 and not B for time period 2 Event A does not occur within time period 1 , and event B also does not occur within time period 2 . Alerts if the taxi has not reached its destination within 30 minutes, and the passenger has not marked himself/herself not in danger within the same time period. not A for time period 1 or not B for time period 2 Either event A does not occur within time period 1 , or event B occurs within time period 2 . Alerts if the taxi has not reached its destination A within 20 minutes, or reached its destination B within 30 minutes. A \u2192 not B for time period Event B does not occur within time period after the occurrence of event A. Alerts if the taxi has reached its destination, but it has been not followed by a payment record within 10 minutes. not A and B or A and not B Event A does not occur before event B. Alerts if the taxi is stated before activating the taxi fare calculator.","title":"Pattern"},{"location":"docs/query-guide/#sequence","text":"The sequence is a state machine implementation that detects consecutive event occurrences from events arrived via one or more event streams over time. Here all matching events need to arrive consecutively , and there should not be any non-matching events in between the matching sequence of events. The sequence can repetitively match event sequences, count event occurrences, and use logical event ordering (using and , or , and not ). Purpose The sequence helps to achieve Complex Event Processing (CEP) capabilities by detecting various pre-defined consecutive event occurrence sequences in realtime. Sequence query does expect the matching events to occur immediately after each other, and it can successfully correlate the events who do not have other events in between. Syntax The syntax for a sequence query is as follows: from ( (every)? ( event reference =)? input stream [ filter condition ] (+|*|?)? | ( event reference =)? input stream [ filter condition ] (and|or) ( event reference =)? input stream [ filter condition ] | not input stream [ filter condition ] (and event reference = input stream [ filter condition ] | for time gap ) ), ... (within time gap )? select event reference . attribute name , event reference . attribute name , ... insert into output stream Items Description , Indicates the immediate next event that follows the given event. The condition to be met by the preceding event should be added before the , , and the condition to be met by the subsequent event should be added after the , . every An optional keyword defining when a new event matching state-machine should be initiated to repetitively match the sequence. When this keyword is not used, the event matching state-machine will be initiated only once. within time gap An optional within clause that defines the time duration within which all the matching events should occur. + Matches **one or more** events to the given condition. * Matches **zero or more** events to the given condition. ? Matches **zero or one** events to the given condition. and Allows both of its condition to be matched by two distinct events in any order. or Only expects one of its condition to be matched by an event. Here the event reference of the unmatched condition will be null . not condition1 and condition2 Detects the event matching condition2 before any event matching condition1 . not condition1> for time period> Detects no event matching on condition1 for the specified time period . event reference An optional reference to access the matching event for further processing. All conditions can be assigned to an event reference to collect the matching event occurrences, other than the condition used for not case (as there will not be any event matched against it). Non occurrence of events. Siddhi detects non-occurrence of events using the not keyword, and its effective non-occurrence checking period is bounded either by fulfillment of a condition associated by and or via an expiry time using time period . Logical correlation of multiple conditions. Siddhi can only logically correlate two conditions at a time using keywords such as and , or , and not . When more than two conditions need to be logically correlated, use multiple pattern queries in a chaining manner, at a time correlating two logical conditions and streaming the output to a downstream query to logically correlate the results with other logical conditions. Event selection The event reference in sequence queries is used to retrieve the matched events. When a sequence condition is intended to match only a single event, then its attributes can be retrieved by referring to its reference as event reference . attribute name . An example of this is as follows. e1.symbol , refers to the symbol attribute value of the matching event e1 . But when the pattern condition is associated with min count : max count , it is expected to match against on multiple events. Therefore, an event from the matched event collection should be retrieved using the event index from its reference. Here the indexes are specified in square brackets next to event reference, where index 0 referring to the first event, and a special index last referring to the last available event in the collection. Attribute values of all the events in the matching event collection can be accessed a list, by referring to their event reference without an index. Some possible indexes and their behavior is as follows. e1[0].symbol , refers to the symbol attribute value of the 1 st event in reference e1 . e1[3].price , refers to the price attribute value of the 4 th event in reference e1 . e1[last].symbol , refers to the symbol attribute value of the last event in reference e1 . e1[last - 1].symbol , refers to the symbol attribute value of one before the last event in reference e1 . e1.symbol , refers to the list of symbol attribute values of all events in the event collection in reference e1 , as a list object. The system returns null when accessing attribute values, when no matching event is assigned to the event reference (as in when two conditions are combined using or ) or when the provided index is greater than the last event index in the event collection. Example 1 (Every) Query to send alerts when temperature increases at least by one degree between two consecutive temperature events. from every e1=TempStream, e2=TempStream[temp e1.temp + 1] select e1.temp as initialTemp, e2.temp as finalTemp insert into AlertStream; Here, the matching process begins for each event in the TempStream stream (as every is used with e1=TempStream ), and if the immediate next event with a value for temp attribute being greater than e1.temp + 1 of the initial event e1 , then an output is generated via the AlertStream . Example 2 (Every collection) Query to identify temperature peeks by monitoring continuous increases in temp attribute and alerts upon the first drop. define stream TempStream(deviceID long, roomNo int, temp double); @info(name = 'query1') from every e1=TempStream, e2=TempStream[ifThenElse(e2[last].temp is null, e1.temp = temp, e2[last].temp = temp)]+, e3=TempStream[e2[last].temp temp] select e1.temp as initialTemp, e2[last].temp as peekTemp, e3.price as firstDropTemp insert into PeekTempStream ; Here, the matching process begins for each event in the TempStream stream (as every is used with e1=TempStream ). It checks if the temp attribute value of the second event is greater than or equal to the temp attribute value of the first event ( e1.temp ), then for all the following events, their temp attribute value is checked if they are greater than or equal to their previous event's temp attribute value ( e2[last].temp ), and when the temp attribute value becomes less than its previous events temp attribute value value an output is generated via the AlertStream stream. Example 3 (Logical and condition) A query to identify a regulator activation event immediately followed by both temperature sensor and humidity sensor activation events in either order. define stream TempStream(deviceID long, isActive bool); define stream HumidStream(deviceID long, isActive bool); define stream RegulatorStream(deviceID long, isOn bool); from every e1=RegulatorStream[isOn == true], e2=TempStream and e3=HumidStream select e2.isActive as tempSensorActive, e3.isActive as humidSensorActive insert into StateNotificationStream; Here, the matching process begins for each event in the RegulatorStream stream having the isOn attribute true . It generates an output via the AlertStream stream when an event from both TempStream stream and HumidStream stream arrives immediately after the first event in either order.","title":"Sequence"},{"location":"docs/query-guide/#output-rate-limiting","text":"Output rate-limiting limits the number of events emitted by the queries based on a specified criterion such as time, and number of events. Purpose Output rate-limiting helps to reduce the load on the subsequent executions such as query processing, I/O operations, and notifications by reducing the output frequency of the events. Syntax The syntax for output rate limiting is as follows: from input stream ... select attribute name , attribute name , ... output rate limiting configuration insert into output stream Here, the output rate limiting configuration ( rate limiting configuration ) should be defined next to the output keyword and the supported output rate limiting types are explained in the following table: Rate limiting configuration Syntax Description Time based ( output event selection )? every time interval Outputs output event selection every time interval time interval. Number of events based ( output event selection )? every event interval events Outputs output event selection for every event interval number of events. Snapshot based snapshot every time interval Outputs all events currently in the query window (or outputs only the last event if no window is defined in the query) for every given time interval time interval. The output event selection specifies the event(s) that are selected to be outputted from the query, here when no output event selection is defined, all is used by default. The possible values for the output event selection and their behaviors are as follows: * first : The first query output is published as soon as it is generated and the subsequent events are dropped until the specified time interval or the number of events are reached before sending the next event as output. * last : Emits only the last output event generated during the specified time or event interval. * all : Emits all the output events together which are generated during the specified time or event interval. Example 1 (Time based first event) Query to calculate the average temp per roomNo for the events arrived on the last 10 minutes, and send alerts once every 15 minutes of the events having avgTemp more than 30 degrees. define stream TempStream(deviceID long, roomNo int, temp double); from TempStream#window.time(10 min) select roomNo, avg(temp) as avgTemp group by roomNo having avgTemp 30 output first every 15 min insert into AlertStream; Here the first event having avgTemp 30 is emitted immediately and the next event is only emitted after 15 minutes. Example 2 (Event based first event) A query to output the initial event, and from there onwards every 5 th event of TempStream stream. define stream TempStream(deviceID long, roomNo int, temp double); from TempStream output first every 5 events insert into FiveEventBatchStream; Example 3 (Event based all events) Query to collect last 5 TempStream stream events and send them together as a single batch. define stream TempStream(deviceID long, roomNo int, temp double); from TempStream output every 5 events insert into FiveEventBatchStream; As no output event selection is defined, the behavior of all is applied in this case. Example 4 (Time based last event) Query to emit only the last event of TempStream stream for every 10 minute interval. define stream TempStream(deviceID long, roomNo int, temp double); from TempStream output last every 10 min insert into FiveEventBatchStream; Example 5 (Snapshot based) Query to emit the snapshot of events retained by its last 5 minutes window defined on TempStream stream, every second. define stream TempStream(deviceID long, roomNo int, temp double); from TempStream#window.time(5 sec) output snapshot every 1 sec insert into SnapshotTempStream; Here, the query emits all the current events generated which do not have a corresponding expired event at the predefined time interval. Example 6 (Snapshot based) Query to emit the snapshot of events retained every second, when no window is defined on TempStream stream. define stream TempStream(deviceID long, roomNo int, temp double); from TempStream output snapshot every 5 sec insert into SnapshotTempStream; Here, the query outputs the last seen event at the end of each time interval as there are no events stored in no window defined.","title":"Output Rate Limiting"},{"location":"docs/query-guide/#partition","text":"Partition provides data parallelism by categorizing events into various isolated partition instance based on their attribute values and by processing each partition instance in isolation. Here each partition instance is tagged with a partition key, and they only process events that match to the corresponding partition key. Purpose Partition provide ways to segment events into groups and allow them to process the same set of queries in parallel and in isolation without redefining the queries for each segment. Here, events form multiple streams generating the same partition key will result in the same instance of the partition, and executed together. When a stream is used within the partition block without configuring a partition key, all of its events will be executed in all available partition instances. Syntax The syntax for a partition is as follows: @purge(enable='true', interval=' purge interval ', idle.period=' idle period of partition instance ') partition with ( key selection of stream name , key selection of stream name , ... ) begin from stream name ... select attribute name , attribute name , ... insert into (#)? stream name from (#)? stream name ... select attribute name , attribute name , ... insert into stream name ... end; Here, a new instance of a partition will be dynamically created for each unique partition key that is generated based on the key selection applied on the events of their associated streams ( stream name ). These created partition instances will exist in the system forever unless otherwise a purging policy is defined using the @purge annotation. The inner streams denoted by # stream name can be used to chain multiple queries within a partition block without leaving the isolation of the partition instance. The key selection defines the partition key for each event based on the event attribute value or using range expressions as listed below. Key selection type Syntax description Partition by value attribute name Attribute value of the event is used as its partition key. Partition by range compare condition as 'value' or compare condition as 'value' or ... Event is executed against all compare conditions , and the values associated with the matching conditions are used as its partition key. Here, when the event is matched against multiple conditions, it is processed on all the partition instances that are associated with those matching conditions. When there are multiple queries within a partition block, and they can be chained without leaving the isolation of the partition instance using the inner streams denoted by # . More information on inner Streams will be covered in the following sections. Inner Stream Inner stream connects the queries inside a partition instance to one another while preserving partition isolation. These are denoted by a # placed before the stream name, and these streams cannot be accessed outside the partition block. Through this, without repartitioning the streams, the output of a query instance can be used as the input of another query instance that is also in the same partition instance. Using non inner streams to chain queries within a partition block. When the connecting stream is not an inner stream and if it is not configured to generate a partition key, then it outputs events to all available partition instances . However, when the non-inner stream is configured to generate a partition key, it only outputs to the partition instances that are selected based on the repartitioned partition key. Purge Partition Purge partition purges partitions that are not being used for a given period on a regular interval. This is because, by default, when partition instances are created for each unique partition key they exist forever if their queries contain stateful information, and there are use cases (such as partitioning events by date value) where an extremely large number of unique partition keys are used, which generates a large number of partition instances, and this eventually leading to system out of memory. The partition instances that will not be used anymore can purged using the @purge annotation. The elements of the annotation and their behavior is as follows. Purge partition configuration Description enable To enable partition purging. internal Periodic time interval to purge the purgeable partition instances. idle.period The idle period, a particular partition instance (for a given partition key) needs to be idle before it becomes purgeable. Example 1 (Partition by value) Query to calculate the maximum temperature of each deviceID , among its last 10 events. partition with ( deviceID of TempStream ) begin from TempStream#window.length(10) select roomNo, deviceID, max(temp) as maxTemp insert into DeviceTempStream; end; Here, each unique deviceID will create a partition instance which retains the last 10 events arrived for its corresponding partition key and calculates the maximum values without interfering with the events of other partition instances. Example 2 (Partition by range) Query to calculate the average temperature for the last 10 minutes per each office area, where the office areas are identified based on the roomNo attribute ranges from the events of TempStream stream. partition with ( roomNo = 1030 as 'serverRoom' or roomNo 1030 and roomNo = 330 as 'officeRoom' or roomNo 330 as 'lobby' of TempStream) begin from TempStream#window.time(10 min) select roomNo, deviceID, avg(temp) as avgTemp insert into AreaTempStream end; Here, partition instances are created for each office area type such as serverRoom , officeRoom , and lobby . Events are processed only in the partition instances which are associated with matching compare condition values that are satisfied by the event's roomNo attribute, and within each partition instance, the average temp value is calculated based on the events arrived over the last 10 minutes. Example 3 (Inner streams) A partition to calculate the average temperature of every 10 events for each sensor, and send the output via the DeviceTempIncreasingStream stream if consecutive average temperature ( avgTemp ) values increase by more than 5 degrees. partition with ( deviceID of TempStream ) begin from TempStream#window.lengthBatch(10) select roomNo, deviceID, avg(temp) as avgTemp insert into #AvgTempStream from every e1=#AvgTempStream, e2=#AvgTempStream[e1.avgTemp + 5 avgTemp] select e1.deviceID, e1.avgTemp as initialAvgTemp, e2.avgTemp as finalAvgTemp insert into DeviceTempIncreasingStream end; Here, the first query calculates the avgTemp for every 10 events for each unique deviceID and passes the output via the inner stream #AvgTempStream to the second query that is also in the same partition instance. The second query then identifies a pair of consecutive events from #AvgTempStream , where the latter event having 5 degrees more on avgTemp value than its previous event. Example 4 (Purge partition) A partition to identify consecutive three login failure attempts for each session within 1 hour. Here, the number of sessions can be infinite. define stream LoginStream ( sessionID string, loginSuccessful bool); @purge(enable='true', interval='10 sec', idle.period='1 hour') partition with ( sessionID of LoginStream ) begin from every e1=LoginStream[loginSuccessful==false], e2=LoginStream[loginSuccessful==false], e3=LoginStream[loginSuccessful==false] within 1 hour select e1.sessionID as sessionID insert into LoginFailureStream; end; Here, the events in LoginStream is partitioned by their sessionID attribute and matched for consecutive occurrences of events having loginSuccessful==false with 1 hour using a sequence query and inserts the matching pattern's sessionID to LoginFailureStream . As the number of sessions is infinite the @purge annotation is enabled to purge the partition instances. The instances are marked for purging if there are no events from a particular sessionID for the last 1 hour, and the marked instances are periodically purged once every 10 seconds.","title":"Partition"},{"location":"docs/query-guide/#table","text":"A table is a stored collection of events, and its schema is defined via the table definition . A table definition is similar to the stream definition where it contains the table name and a set of attributes having specific types and uniquely identifiable names within the scope of the table. Here, all events associated with the table will have the same schema (i.e., have the same attributes in the same order). The events of the table are stored in-memory , but Siddhi also provides store extensions to mirror the table to external databases such as RDBMS, MongoDB, and others, while allowing the events to be stored on those databases. Table supports primary keys to enforce uniqueness on stored events/recodes, and indexes to improve their searchability. Purpose Tables help to work with stored events. They allow to pick and choose the events that need to be stored by performing insert, update, and delete operations, and help to retrieve necessarily events when by performing read operations. Managing events stored in table The events in the table can be managed using queries that perform join, insert, update, insert or update, and delete operates, which are either initiated by events arriving in Streams or through on-demand queries . Syntax The syntax for defining a table is as follows: @primaryKey( key , key , ... ) @index( key , key , ...) define table table name ( attribute name attribute type , attribute name attribute type , ... ); The following parameters are used to configure the table definition: Parameter Description table nam The name of the table created. (It is recommended to define a table name in PascalCase .) attribute name Uniquely identifiable name of the table attribute. (It is recommended to define attribute names in camelCase .) attribute type The type of each attribute defined in the schema. This can be STRING , INT , LONG , DOUBLE , FLOAT , BOOL or OBJECT . To use and refer table and attribute names that do not follow [a-zA-Z_][a-zA-Z_0-9]* format enclose them in ` . E.g. `$test(0)` . Primary Keys Primary keys help to avoid duplication of data by enforcing nor two events to have the same value for the selected primary key attributes. They also index the table to access the events much faster. Primary keys are optional, and they can be configured using the @primaryKey annotation. Here, each table can only have at most one @primaryKey annotation, which can have one or more attribute name s defined as primary keys (The number of attribute name supported can differ based on the differet store implementations). When more than one attribute is used, the uniqueness of the events stored in the table is determined based on the composite value for those attributes. When more than one events having the same primary keys are inserted to the table, the latter event replaces the event/record that already exists in the table. Indexes Indexes allow events in the tables to be searched/modified much faster, but unlike primary keys, the indexed attributes support duplicate values. Indexes are optional, and they can be configured using the @index annotation. Here, each @index annotation creates an index in the table, and the tables only support one attribute name for each index (The number of @Index annotations and attribute name inside the annotation can differ based on different store implementations). Example 1 (Primary key) define table RoomTypeTable ( roomNo int, type string ); The above table definition defines an in-memory table named RoomTypeTable having the following attributes. roomNo of type int Room type of type string Example 2 (Primary key) @primaryKey('symbol') define table StockTable (symbol string, price float, volume long); The above table definition defines an in-memory table named StockTable having the following attributes. symbol of type string price of type float volume of type long As this table is configured with the primary key symbol , there will be only one record/event exist in the table for a particular value of the symbol attribute. Example 3 (Index) @index('username') @index('salary') define table SalaryTable (username string, salary double); The above table definition defines an in-memory table named SalaryTable having the following attributes. username of type string salary of type double As this table is configured with indexes for username and salary , the search operations on username and/or salary attributes will be much faster than the non-indexed case. Here, the table can contain duplicate events having the same value for username and/or salary. Example 3 (Primary key and index) @primaryKey('username') @index('salary') define table SalaryTable (username string, salary double); The above table definition defines an in-memory table named SalaryTable having the following attributes. username of type string salary of type double As this table is configured with the primary key username and index salary . Hence, there can be only one record/event exist in the table having a particular username value, and the search operations on username and/or salary attributes will be much faster than the non-indexed case.","title":"Table"},{"location":"docs/query-guide/#store","text":"Stores allow creating, reading, updating, and deleting events/recodes stored on external data stores such as RDBMS, MongoDB, and others. They produce these functionalities by using the Siddhi tables as a proxy to external databases. Stores depending on their implementation and the connected external data store, some supports primary keys to enforce uniqueness on stored events/recodes, and indexes to improve their searchability. Since stores work with external data stores, the i/o latency can be quite higher than in-memory tables, the increase in latency can be eliminated by defining a cache, such that recently accessed data will be cached in-memory providing faster data retrievals. Purpose Stores allow searching retrieving and manipulating data stored in external data stores through queries. This is useful for use cases when there is a need to access a common database used by various other systems, to retrieve and transfer data. Syntax The syntax for defining a store along with is associated table is as follows: @store(type=' store type ', common.static.key =' value ', common.static.key =' value ' @cache(size=' cache size ', cache.policy=' cache policy ', retention.period=' retention period ', purge.interval=\" purge interval \")) @primaryKey( key , key , ... ) @index( key , key , ...) define table table name ( attribute name attribute type , attribute name attribute type , ... ); Here the store is defined via the @store annotation, and the schema of the store is defined via the table definition associated with it. In this case the table definition will not create an in-memory table but rather used as a poxy to read, write, and modify data stored in external store. The type parameter of the @store defines the store type to be used to connect to the external data store, and the other parameters of @store annotation other than @cache depend on the store selected, where some of these parameters can be optional. The @primaryKey and @index annotations are optional, and supported by some store implementations. The @primaryKey annotation can be defined at most once, and it can have one or more attribute name s as composed primary keys based on the implementation. At the same time, @index annotation can be defined several times, and it can also have one or more attribute name s as composed indexes if the implementation supports them. Cache The @cache annotation inside @store defines the behavior of the cache. @cache is an optional annotation that can be applied to all store implementations, where when this is not defined, the cache will not be enabled. The parameters defining the cache behavior via the @cache annotation is as follows. Parameter Mandatory/Optional Default Value Description size Mandatory - Maximum number of events/records stored in the cache. cache.policy Optional FIFO Policy to remove elements from the cache when the cache is at its maximum size and new entries need to added due to cache miss. Supported policies are FIFO (First-In First-Out), LRU (Least Recently Used) LFU (Least Frequently Used) retention.period Optional - The period after an event/record will become eligible for removal from the cached irrespective of the case size. This allows the cache to fetch the recent database updates made by other systems. purge.interval Optional Equal to retention period. The periodic time interval the cached events/records that are eligible for removal will be purge. Even though the cache is enabled, its behavior and usage depend on the number of recodes in the external store relative to the maximum cache size defined as follows: Cache size being greater than or equal to the number of recodes in the external store: At startup, all the recodes of the external store data will be preloaded to cached. The cache is used to process all type of data retrieval operations. When retention.period (and purge.interval ) is configured, all records the cache are periodically deleted and reloaded from the external store. Cache size is smaller than the number of recodes in the external store: At startup, the number of recodes equal to the maximum cache size is preloaded from the external store. The cache is used to process only the data retrieval operations that use all defined primary keys in equal ( == ) comparisons, and when there are multiple comparisons, those are combined using and , (For example when customerID and companyID are defined as primary keys then the data retrieval operations with condition customerID == 'John' and companyID == 'Google' and age 28 can be executed in the cache) . All other operations are directly executed in the external data store. If the cache is full and when a cache miss occurs, a record is removed from the cache based on the defined cache expiry policy before adding the missed record from the external data store. When retention.period (and purge.interval ) is configured, the data is cache that are loaded earlier than retention period are periodically deleted. Here, no reloading will be done from the external data store. Supported store types The following is a list of store types supported by Siddhi: Sink mapping type Description RDBMS Optimally stores, retrieves, and manipulates data on RDBMS databases such as MySQL, MS SQL, Postgresql, H2 and Oracle. MongoDB Stores, retrieves, and manipulates data on MongoDB. Redis Stores, retrieves, and manipulates data on Redis. Elasticsearch Supports data access and manipulation operators on Elasticsearch. Example 1 An RDBMS Store configuration to work with MySQL database. @store(type=\"rdbms\", jdbc.url=\"jdbc:mysql://localhost:3306/hotel\", username=\"siddhi\", password=\"123\", jdbc.driver.name=\"com.mysql.jdbc.Driver\") define table RoomTypeTable ( roomNo int, type string ); Here, the store connects to the MySQL table RoomTypeTable in the database hotel hosted on localhost:3306 , and its columns mapped as follows. roomNo of type INTEGER mapped to int type of type VARCHAR(255) mapped to string Example 2 An RDBMS Store configuration to work with an indexed MySQL database using a cache. @store(type=\"rdbms\", jdbc.url=\"jdbc:mysql://localhost:3306/hotel\", username=\"siddhi\", password=\"123\", jdbc.driver.name=\"com.mysql.jdbc.Driver\" @cache(size=\"100\", retention.period=\"5 min\", purge.interval=\"1 min\")) @primaryKey('username') @index('salary') define table SalaryTable (username string, salary double); Here, an RDBMS store is defined with a cache of size 100 that every minute removes the entries added to the cache which are older than 5 minutes. The store connects to the MySQL table named SalaryTable , that is configured with the primary key username and index salary , and located in a MySQL the database hotel hosted on localhost:3306 . Its columns mapped as follows. username of type VARCHAR(255) mapped to string salary of type VARCHAR(255) mapped to string Table (and store) operators The following operations can be performed on tables (and stores).","title":"Store"},{"location":"docs/query-guide/#insert_1","text":"Allows events (records) to be inserted into tables/stores. This is similar to inserting events into streams. Primary Keys If the table is defined with primary keys, and multiple records are inserted with the same primary key, a primary key constrain violations can can occur. In such cases use the update or insert into operation. Syntax Syntax to insert events into a table from a stream is as follows; from input stream select attribute name , attribute name , ... insert into table Similar to streams, the current events , expired events or the all events keyword can be used between insert and into keywords in order to insert only the specific event types. For more information, refer Event Type section. Example Query to inserts all the events from the TempStream stream to the TempTable table. define stream TempStream(tempId string, temp double); define table TempTable(tempId string, temp double); from TempStream select * insert into TempTable;","title":"Insert"},{"location":"docs/query-guide/#join-table","text":"Allows stream or named-window to retrieve events (records) from a table. Other Join Functions Joins can also be performed among two streams , with named-aggregation , or named-window . Syntax The syntax for a stream or a named-window to join with a table is as follows: from ( input stream ( non window handler )*( window )?| named-window ) (as reference )? join type table (as reference )? (on join condition )? select reference . attribute name , reference . attribute name , ... insert into output stream A join with table is similar to the join of two streams , where one of the inputs is a table and other can be either a stream or a named-window . Here, the table and named-window cannot have any optional handlers associated with it. Two tables cannot be joined. A table can only be joint with a stream or named-window. Two tables, or table and named-aggregation cannot be joint because there must be at least one active entity to trigger the join operation. Supported join types Table join supports following join operations. Inner join (join) This is the default behavior of a join operation, and the join keyword is used to join a stream with a table. The output is generated only if there is a matching event in both the stream and the table. Left outer join The left outer join keyword is used to join a stream on the left side with a table on the right side based on a condition. It returns all the events of the left stream even if there are no matching events in the right table by having null values for the attributes of the table on the right. Right outer join This is similar to a left outer join and the right outer join keyword is used to join a stream on right side with a table on the left side based on a condition. It returns all the events of the right stream even if there are no matching events in the left table by having null values for the attributes of the table on the left. Cross join In either of these cases, when the join condition is omitted, the triggering event will successfully match against all the events in the table, producing a cross join behavior. Example A query to join and retrieve the room type from RoomTypeTable table based on equal roomNo attribute of TempStream , and to insert the results into RoomTempStream steam. define table RoomTypeTable (roomNo int, type string); define stream TempStream (deviceID long, roomNo int, temp double); from TempStream as t join RoomTypeTable as r on t.roomNo == r.roomNo select t.deviceID, t.roomNo, r.type as roomType, t.temp insert into RoomTempStream;","title":"Join (Table)"},{"location":"docs/query-guide/#delete","text":"Allows a stream to delete selected events (records) form a table. Syntax Syntax to delete selected events in a table based on the events in a stream is as follows; from input stream select attribute name , attribute name , ... delete table (for event type )? (on condition )? The condition element specifies the basis on which the events in the table are selected to be deleted. When specifying the condition, the table attributes should always be referred with the table name , and and when a condition is not defined, all the events in the table will be deleted. To execute delete, only for specific event types, use the current events , expired events or the all events keyword can be used with for as shown in the syntax. For more information refer Event Type . Note When defining the condition, the table attributes must be always referred with the table name as follows: table name . attribute name Example 1 A query to delete the records in the RoomTypeTable table that has matching values for the roomNo attribute against the values of roomNumber attribute of the events in the DeleteStream stream. define table RoomTypeTable (roomNo int, type string); define stream DeleteStream (roomNumber int); from DeleteStream delete RoomTypeTable on RoomTypeTable.roomNo == roomNumber; Example 2 A query to delete all the records in the BlacklistTable table when an event arrives in the ClearStream stream. define table BlacklistTable (ip string); define stream ClearStream (source string); from ClearStream delete BlacklistTable;","title":"Delete"},{"location":"docs/query-guide/#update","text":"Allows a stream to update selected events (records) form a table. Syntax Syntax to update events on a table is as follows; from input stream select attribute name , attribute name , ... update table (for event type )? (set table . attribute name = ( attribute name | expression ), table . attribute name = ( attribute name | expression ), ...)? (on condition )? The condition element specifies the basis on which the events in the table are selected to be updated. When referring the table attributes in the update clause, they must always be referred to with the table name , and when a condition is not defined, all the events in the table will be updated. The set keyword can be used to update only the selected attributes from the table. Here, for each assignment, the left side should contain the table attribute that is being updated, and the right side can contain a query output attribute, a table attribute, a mathematical operation, or any other. When the set clause is not provided, all attributes in the table will be updated based on the query output. To execute update, only for specific event types, use the current events , expired events or the all events keyword can be used with for as shown in the syntax. For more information refer Event Type . Note In the update clause, the table attributes must be always referred with the table name as follows: table name . attribute name Example 1 A query to update the latestHeartbeatTime on the ServerInfoTable against each serverIP for every event on the HeartbeatStream . define table ServerInfoTable (serverIP string, host string, port int, latestHeartbeatTime long); define stream HeartbeatStream (serverIP string, timestamp long); from HeartbeatStream select * update ServerInfoTable set ServerInfoTable.latestHeartbeatTime = timestamp on ServerInfoTable.serverIP == serverIP; Example 2 A query to update the peoplePresent in the RoomOccupancyTable table for each roomNo based on new people arrival and exit values from events of the UpdateStream stream. define table RoomOccupancyTable (roomNo int, peoplePresent int); define stream UpdateStream (roomNumber int, arrival int, exit int); from UpdateStream select * update RoomOccupancyTable set RoomOccupancyTable.peoplePresent = RoomOccupancyTable.peoplePresent + arrival - exit on RoomOccupancyTable.roomNo == roomNumber; Example 3 A query to update the latestHeartbeatTime on the HeartbeatTable for each event on the HeartbeatStream . define table HeartbeatTable (serverIP string, latestHeartbeatTime long); define stream HeartbeatStream (serverIP string, timestamp long); from HeartbeatStream select serverIP, timestamp as latestHeartbeatTime update HeartbeatTable on ServerInfoTable.serverIP == serverIP;","title":"Update"},{"location":"docs/query-guide/#update-or-insert","text":"Allows a stream to update the events (records) that already exist in the table based on a condition, else inserts the event as a new entry to the table. Syntax Syntax to update or insert events on a table is as follows; from input stream select attribute name , attribute name , ... update or insert into table (for event type )? (set table . attribute name = ( attribute name | expression ), table . attribute name = ( attribute name | expression ), ...)? (on condition )? The condition element specifies the basis on which the events in the table are selected to be updated. When referring the table attributes in the update clause, they must always be referred with the table name , and when the condition does not match with any event in the table, then a new event (a record) is inserted into the table. Here, when a condition is not defined, all the events in the table will be updated. The set clause is only used when an update is performed in the update or insert operation. In this case, the set keyword can be used to update only the selected attributes from the table. Here, for each assignment, the left side should contain the table attribute that is being updated, and the right side can contain a query output attribute, a table attribute, a mathematical operation, or any other. When the set clause is not provided, all attributes in the table will be updated based on the query output. To execute update or insert, only for specific event types, use the current events , expired events or the all events keyword can be used with for as shown in the syntax. For more information refer Event Type . Note In the update or insert clause, the table attributes must be always referred with the table name as follows: table name . attribute name Example A query to update assignee information in the RoomAllocationTable table for the corresponding roomNumber from the RoomAllocationStream stream when at least one matching record is present in the table, and when there are no matching records it inserts a new record to the RoomAllocationTable table based on the query output. define table RoomAllocationTable (roomNo int, type string, assignee string); define stream RoomAllocationStream (roomNumber int, type string, assignee string); from RoomAllocationStream select roomNumber as roomNo, type, assignee update or insert into RoomAllocationTable set RoomAllocationTable.assignee = assignee on RoomAllocationTable.roomNo == roomNo;","title":"Update or Insert"},{"location":"docs/query-guide/#in-table","text":"Allows the query to check whether the expected value exists in the table using a condition operation. Syntax from input stream [ condition in table ] select attribute name , attribute name , ... insert into output stream The condition element specifies the basis on which the events in the table are checked for existence. When referring the table attributes in the condition , they must always be referred with the table name as table name . attribute name . Example 1 A query to filter only the events of server rooms from the TempStream stream using the ServerRoomTable table, and pass them for further processing via ServerRoomTempStream stream. define table ServerRoomTable (roomNo int); define stream TempStream (deviceID long, roomNo int, temp double); from TempStream[ServerRoomTable.roomNo == roomNo in ServerRoomTable] insert into ServerRoomTempStream; Example 2 A query to filter out the blacklisted serverIP s from the RequestStream stream using the BlacklistTable table, and only pass events having IPs that are not blacklisted, for further processing via FilteredRequestStream stream. define table BlacklistTable (serverIP string); define stream RequestStream (ip string, request string); from RequestStream[not (BlacklistTable.serverIP == ip in BlacklistTable)] insert into FilteredRequestStream;","title":"In (Table)"},{"location":"docs/query-guide/#named-aggregation","text":"Named-aggregation aggregates events incrementally for a specified set of time granularities, and allows interactively accessing them to produce reports, dashboards, and to take decisions in realtime with millisecond accuracy. The aggregation logic and schema is defined via the aggregation definition . A aggregation definition contains the aggregation name, input, aggregation logic, the time granularities on which the aggregations are calculated, and the set of aggregated output attributes having specific types and uniquely identifiable names within the scope of the named-aggregation. The aggregated events of the named-aggregation are stored by default in-memory , but Siddhi also provides store extensions to mirror the named-aggregation to external databases such as RDBMS, while allowing the aggregated events to be stored on databases such that allowing it to hold data for longer durations, preserve data at failures, and to aggregate data in a distributed manner. The historical data stored in named-aggregations are purged automatically to limit data growth overtime, and when purging is not configured, system automatically purges the data every 15 minutes, by only retaining the default number of records for each time granularity. Purpose Named-aggregations helps to calculate aggregations over long durations and retrieve the aggregated values over various time ranges, It can perform aggregation using operations such as sum , count , avg , min , max , count and distinctCount on stream attributes for time granularities such as sec , min , hour , day , month , and year . This can be used for in many analytics scenarios as this provides time-series aggregates on calendar time, over long durations, even for out-of-order events, and helps to retrieve historical data for selected time range and time granularity. Syntax The syntax for defining a named-aggregation is as follows: @store(type=' store type ', ...) @purge(enable=\" enable purging \", interval=' purging interval ', @retentionPeriod( granularity = ' retention period ', ...)) @PartitionById(enable=\" enable distributed aggregation \") define aggregation aggregation name from ( stream | named-window ) select attribute name , aggregate function ( attribute name ) as attribute name , ... group by attribute name aggregate by timestamp attribute every time granularities ; The following parameters are used to configure the aggregation definition: Parameter Description @store Annotation to configure the data store to store the aggregated records. This annotation is optional and when not provided, the results are stored in in-memory tables. @purge Annotation to configure data purging on aggregated data. This annotation is optional, and when not provided, the default data purging configuration is enabled. To disable data purging, use @purge(enable='false') configuration, and also make sure to disable when named-aggregation is used only for read-only purposes. Detail information on data purging is explained in the following section. @PartitionById Annotation to enable multiple named-aggregations to process in a distributed manner. Detail information on this is discussed in the distributed named-aggregation section. aggregation name The name of the named-aggregation created. (It is recommended to define an aggregation name in PascalCase .) input stream The stream that feeds the named-aggregation, and this stream must be defined before the aggregation definition . group by attribute name The group by clause to aggregate the events per each unique group by attribute value combination. This is optional, and when not provided, all events are aggregated together. by timestamp attribute Configures a stream attribute to be used as the event timestamp in the aggregation. This is optional, and if not provided, the event time is used by default. When the stream attribute is used as the event timestamp it could be either a long in Unix timestamp in milliseconds (e.g. 1496289950000 ), or a string in the format yyyy - MM - dd HH : mm : ss (if time is in GMT) or yyyy - MM - dd HH : mm : ss Z (if the time is not in GMT) with ISO 8601 UTC offset for Z (e.g., +05:30 , -11:00 ). time granularities Defines the granularity ranges on which the aggregations should be performed using second , minute , hour , day , month , and/or year keywords. Here, the granularity range can be defined with minimum and maximum granularities separating them with three dots (e.g. sec ... year where the aggregation will be performed per each second, minute, hour, day, month, and year), or using comma-separated granularities (e.g. min, hour where the aggregation will be only performed per each minute and hour). The named-aggregation uses calendar time. The named-aggregations aggregate events at calendar start times for each granularity based on GMT timezone. Handles out-of-order event arrival. Named-aggregations aggregates out-of-order event arrivals into their corresponding time range and granularity. Data Purging Data purging on named-aggregations are enabled by default with 15 min purging interval and the following retention periods; Time granularity Default retention period Minimum retention period second 120 seconds 120 seconds minute 24 hours 120 minutes hour 30 days 25 hours day 1 year 32 days month All 13 month year All none This can be modified using the @purge annotation by optionally providing interval parameter to configure the purging interval, and by optionally configuring the @retentionPeriod annotation, the duration the aggregated data needs to be retained when carrying out data purging is defined for each time granularity period using the granularity = ' retention period ' pairs. Here for each granularity, the configured granularity period should be greater than or equal to the respective minimum retention period, and when not defined, the default retention period is applied as specified in the above table. Beware of defining the same named-aggregation in multiple SiddhiApps. The same named-aggregation can be defined in multiple SiddhiApps for data aggregation and data retrieval purposes. In this case, make sure all the named-aggregations to have the same purging configuration or enable purging only in one of the named-aggregations to ensure that data is purged as expected. Further, when these named-aggregations are configured to use the same physical data store using the @stroe annotation while the distributed named-aggregation configuration discussed in the following sections is not used, make sure a named-aggregation in only one of the SiddhiApps performs data aggregation (i.e., the aggregation input stream only feeds events into one of the aggregation definitions) while others are only used for data retrieval either using join, or on-demand select queries. Distributed Named-Aggregations The system will result in an error when more than one named-aggregation, with same aggregation name pointing to the same physical store using the @store annotation, is defined on multiple SiddhiApps unless otherwise Siddhi is configured to perform aggregations in a distributed manner. Distributed named-aggregation configurations allow each SiddhiApp to work as independent shards by partially aggregating the data fed to them. These partial results are combined during data retrieval. Named-aggregations can be configured to process data parallelly and in a distributed manner, by adding the following Siddhi properties to Siddhi configuration. Refer Siddhi configuration guide for detail steps. Siddhi Property Description Possible Values Optional Default Value shardId An ID to uniquely identify the running process (Siddhi Manager/ Siddhi Runner). This helps different instances of the same SiddhiApp running on separate processes to aggregate and store data separately. Any string No \"\" (Empty string) partitionById Enables all named-aggregations on the running process (Siddhi Manager/ Siddhi Runner) to aggregate data in a distributed manner. true , false Yes false The named-aggregations that are enabled to process in a distributed manner using the Siddhi properties can be selectively disabled by adding the @PartitionById annotation to the corresponding aggregation definition and setting its enable property to false as @PartitionById(enable='false') . Once a shardId is introduced it should not be dropped arbitrarily! When a process (Siddhi Manager/ Siddhi Runner) configured with a specific shardId is permanently removed, it will result in unexpected aggregate results unless otherwise the data belonging to that shard is migrated or cleaned in the data store. Example 1 An in-memory named-aggregation with default default purging named as TradeAggregation to calculate the average and sum for price attribute for each unique symbol for all time granularities from second to year using timestamp attribute as the event time, on the events arriving via the TradeStream stream. define stream TradeStream (symbol string, price double, volume long, timestamp long); define aggregation TradeAggregation from TradeStream select symbol, avg(price) as avgPrice, sum(price) as total group by symbol aggregate by timestamp every sec ... year; Example 2 A custom purging enabled RDBMS store based named-aggregation with name TradeAggregation to calculate the min and max price for each unique symbol for time granularities hour, day, and month using Siddhi event timestamp, on the events arriving via the TradeStream stream. define stream TradeStream (symbol string, price double, volume long, timestamp long); @store(type=\"rdbms\", jdbc.url=\"jdbc:mysql://localhost:3306/sweetFactoryDB\", username=\"root\", password=\"root\", jdbc.driver.name=\"com.mysql.jdbc.Driver\") @purge(enable='true', interval='10 min', @retentionPeriod(hour='24 hours', days='1 year', months='all')) define aggregation TradeMinMax from TradeStream select symbol, min(price) as minPrice, max(price) as maxPrice group by symbol aggregate every hour, day, month; Here, the aggregated data is stored in a MySQL store hosted at mysql://localhost:3306/sweetFactoryDB and the data is periodically purged for every 10 min while retaining data for hour, day, and month granularities for 24 hours , 1 year , and forever respectively. Named-aggregation operators The following operation can be performed on named-aggregation.","title":"Named-Aggregation"},{"location":"docs/query-guide/#join-named-aggregation","text":"Allows stream or named-window to retrieve aggregated results from the named-aggregation. Other Join Functions Joins can also be performed among two streams , with table , or named-window . Syntax The syntax for a stream or a named-window to join with a named-aggregation is as follows: from ( input stream ( non window handler )*( window )?| named-window ) (as reference )? join type named-aggregation (as reference )? on join condition within time range per time granularity select reference . attribute name , reference . attribute name , ... insert into output stream ; A join with named-aggregation is similar to the table join with additional within and per clauses, where table is being replaced by a named-aggregation. Here, the named-aggregation cannot have any optional handlers associated with it. Apart from the standard join constructs this supports the within and per clauses as follows. Item Description within time range Specifies the time interval for which the aggregate values should to be retrieved. This can be specified either by providing a start and an end timestamps (in string or long values) separating them by a comma as in \"2014-02-15 00:00:00 +05:30\", \"2014-03-16 00:00:00 +05:30\" and 1496200000000L, 1596434876000L , or by using a wildcard string specifying the data range as in \"2014-02-15 **:**:** +05:30\" . per time granularity Specifies the time granularity by which the data should be grouped and aggregated when data is retrieved. For instance, when days is specified for granularity, the named-aggregation returns aggregated results grouped for each day within the selected time interval. Here, the timestamp of each group can be obtained using the AGG_TIMESTAMP attribute, that is internal to the named-aggregation. Named-aggregations can only be joint with a stream or named-window. Two named-aggregations, or table and named-aggregation cannot be joint because there must be at least one active entity to trigger the join operation. Supported join types Named-aggregation join supports inner join ( join ), left outer join , right outer join , and cross join (when join condition is omitted) similar to the table join . Examples Following aggregation definition is used for all the examples. define stream TradeStream (symbol string, price double, volume long, timestamp long); define aggregation TradeAggregation from TradeStream select symbol, avg(price) as avgPrice, sum(price) as total group by symbol aggregate by timestamp every sec ... year; Example 1 A query to join and retrieve daily aggregations within the time range of \"2014-02-15 00:00:00 +05:30\", \"2014-03-16 00:00:00 +05:30\" from TradeAggregation based on equal symbol attribute of StockStream , and to insert the results into AggregateStockStream steam. Here, +05:30 in time range can be omitted if the timezone is GMT. define stream StockStream (symbol string, value int); from StockStream as s join TradeAggregation as t on s.symbol == t.symbol within \"2014-02-15 00:00:00 +05:30\", \"2014-03-16 00:00:00 +05:30\" per \"days\" select AGG_TIMESTAMP as timestamp, s.symbol, t.total, t.avgPrice insert into AggregateStockStream; Example 2 A query to join and retrieve all the hourly aggregations within the day of 2014-02-15 from TradeAggregation each event in RequestStream stream, order the results by symbol , and to insert the results into AggregateStockStream steam. define stream RequestStream (request string); from RequestStream join TradeAggregation as t within \"2014-02-15 **:**:** +05:30\" per \"hours\" select AGG_TIMESTAMP as timestamp, t.symbol, t.total, t.avgPrice order by symbol insert into AggregateStockStream; Example 3 A query to join and retrieve aggregated results from TradeAggregation for respective granularity and symbol attributes between the start and the end timestamps of events arriving on StockStream , and to insert the results into AggregateStockStream steam. define stream StockStream (symbol string, granularity string, start long, end long); from StockStream as s join TradeAggregation as t on s.symbol == t.symbol within s.start, s.end per s.granularity select AGG_TIMESTAMP as timestamp, s.symbol, t.total, t.avgPrice insert into AggregateStockStream; Here, granularity , start and end can have values such as \"hour\" , 1496200000000 , and 1596434876000 respectively.","title":"Join (Named-Aggregation)"},{"location":"docs/query-guide/#named-window","text":"A named-window is a window that is shared across multiple queries, where multiple queries can insert, join and consume output from the window. Its schema is defined via the window definition . A window definition is similar to the stream definition where it contains the name of named-window, a of attributes having specific types and uniquely identifiable names within the scope of the named-window along with the window type and output event type . Here, all events associated with the named-window will have the same schema (i.e., have the same attributes in the same order). The events of named-window are expired automatically based on the configured window type, and they cannot be explicitly removed by other means. Purpose Named-windows help to use the same instance of a window in multiple queries, this reduces memory consumption, supports calculating various types of aggregations and output them via multiple streams, and allows multiple queries to query on the same window data. Cannot selectively remove events from named-window. The events in the named-window cannot be selectively removed using delete operations, and the only way they are removed is via the automatic expiry operations of the defined window type. Syntax The syntax for defining a named-window is as follows: define window window name ( attribute name attribute type , attribute name attribute type , ... ) window type ( parameter , parameter , \u2026) (output event type )?; The following parameters are used configure the window definition: Parameter Description window name The name of the named-window created. (It is recommended to define a window name in PascalCase .) attribute name Uniquely identifiable name of the named-window attribute. (It is recommended to define attribute names in camelCase .) attribute type The type of each attribute defined in the schema. This can be STRING , INT , LONG , DOUBLE , FLOAT , BOOL or OBJECT . window type ( parameter , ...) The window implementation associated with the named-window and its parameters. event type Defines when the window should be emitting the events, by specifying keywords such as current events , expired events , or all events . Here, when the output is omitted, all events are emitted by default. For more information, refer Event Type section. Example 1 define window SensorWindow (deviceID string, value float, roomNo int) timeBatch(1 second); The above window definition with the name SensorWindow defines a named-window that is configured to retain events for 1 second in timeBatch window, and produce output upon event arrival and expiry to the window. This named-window contains the following attributes. deviceID of type string value of type float roomNo of type int Example 2 define window SensorWindow (deviceID string, value float, roomNo int) time(1 min) output expired events; The above window definition with the name SensorWindow defines a named-window that is configured to retain events for last 1 minute via time window, and produce output upon event expiry form the window. This named-window contains the following attributes. deviceID of type string value of type float roomNo of type int Named-windows operators The following operations can be performed on named-windows.","title":"Named-Window"},{"location":"docs/query-guide/#insert_2","text":"Allows events to be inserted into named-windows. This is similar to inserting events into streams. Syntax Syntax to insert events into a named-window from a stream is as follows; from input stream select attribute name , attribute name , ... insert into window Similar to streams, the current events , expired events or the all events keyword can be used between insert and into keywords in order to insert only the specific event types. For more information, refer Event Type section. Example This query inserts all events from the TempStream stream to the OneMinTempWindow window. define stream TempStream(tempId string, temp double); define window OneMinTempWindow(tempId string, temp double) time(1 min); from TempStream select * insert into OneMinTempWindow;","title":"Insert"},{"location":"docs/query-guide/#join-named-window","text":"Allows stream or named-window to retrieve events from another named-window. Other Join Functions Joins can also be performed among two streams , with named-aggregation , or table . Syntax The syntax for a stream or a named-window to join with another named-window is as follows: from ( input stream ( non window handler )*( window )?| named-window ) (as reference )? join type named-window (as reference )? on condition select reference . attribute name , reference . attribute name , ... insert into output stream A join with named-window is similar to the join of two streams , where either both the inputs are named-windows, or one is a stream and other is a named-window. Here, the named-window cannot have any optional handlers associated with it. Supported join types Named-window join supports inner join ( join ), left outer join , right outer join , full outer join , and cross join (when join condition is omitted) similar to the stream join . Example A query, for each event on CheckStream , to join and calculate the number of temperature events having greater than 40 degrees for the temp attribute value, within the last 2 minutes of the TwoMinTempWindow named-window, and to insert the results into HighTempCountStream steam. define window TwoMinTempWindow (roomNo int, temp double) time(2 min); define stream CheckStream (requestId string); from CheckStream as c join TwoMinTempWindow as t on t.temp 40 select requestId, count(t.temp) as count insert into HighTempCountStream;","title":"Join (Named-Window)"},{"location":"docs/query-guide/#from-named-window","text":"Named-windows can be used as an input to any query, similar to streams. Syntax Syntax for using named-window as an input to a simple query is as follows; from named-window non window handler * ((join ( stream handler *| named-window | table | named-aggregation ))|((,|- )( stream | named-window ) non window handler *)+)? projection output action Named-windows can be used as input for any query type, like how streams are being used . They can be associated with optional non window handlers (such as filters, stream functions, and stream processors) in queries other than when they are used in join query. Example Queries to calculate the max temperature among all rooms, and avg temperature per each room , in the last 5 minutes, using FiveMinTempWindow , and publish the results via MaxTempStream stream, and AvgTempStream stream respectively. define window FiveMinTempWindow (roomNo int, temp double) time(5 min); from FiveMinTempWindow select max(temp) as maxValue insert into MaxTempStream; from FiveMinTempWindow select roomNo, avg(temp) as avgTemp group by roomNo insert into AvgTempStream;","title":"From (Named-Window)"},{"location":"docs/query-guide/#trigger","text":"Trigger produces events periodically based on a given internal with a predefined schema. They can be used in any query, similar to the streams, and defined via the trigger definition . Purpose Triggers help to periodically generate events based on a specified time interval or cron expression, to perform periodic execution of queries. They can also be produced at SiddhiApp startup to perform initialization operations. Syntax The syntax for defining a trigger is as follows: define trigger trigger name at ( 'start'| every time interval | ' cron expression '); Triggers can be used as input to any query, similar to the streams. Because, when defined, they are represented as a stream having one attribute with name triggered_time , and type long as follows. define stream trigger name (triggered_time long); The supported trigger types are as follows. Trigger type Description 'start' Produces a single event when SiddhiApp starts. every time interval Produces events periodically at the given time interval. ' cron expression ' Produces events periodically based on the given cron expression. For configuration details, refer quartz-scheduler . Example 1 A trigger to generate events every 5 minutes. define trigger FiveMinTrigger at every 5 min; Example 2 A trigger to generate events at 10.15 AM on every weekdays. define trigger WorkStartTrigger at '0 15 10 ? * MON-FRI'; Example 3 A trigger to generate an event at SiddhiApp startup. define trigger InitTrigger at 'start';","title":"Trigger"},{"location":"docs/query-guide/#script","text":"The script provides the ability to write custom functions in other programming languages and execute them within Siddhi queries. The custom functions using scripts can be defined via the function definitions and accessed in queries similar to any other inbuilt functions . Purpose Scripts help to define custom functions in other programming languages such as javascript. This can eliminate the need for writing extensions to fulfill the functionalities that are not provided in Siddhi core or by its extension. Syntax The syntax for defining the script is as follows. define function function name [ language name ] return return type { function logic }; The defined function can be used in the queries similar to inbuilt functions as follows. function name ( ( function parameter (, function parameter )*)? ) Here, the function parameter s are passed into the function logic of the definition as an Object[] with the name data . The functions defined via the function definitions have higher precedence compared to inbuilt functions and the functions provided via extensions. The following parameters are used to configure the function definition: Parameter Description function name The name of the function created. (It is recommended to define a function name in camelCase .) language name Name of the programming language used to define the script, such as javascript , r , or scala . return type The return type of the function. This can be int , long , float , double , string , bool , or object . Here, the function implementer is responsible for returning the output according on the defined return type to ensure proper functionality. function logic The execution logic that is written in the language specified under the language name , where it consumes the function parameter s through the data variable and returns the output in the type specified via the return type parameter. Example 1 A function to concatenate three strings into one using JavaScript. define function concatFn[javascript] return string { var str1 = data[0]; var str2 = data[1]; var str3 = data[2]; var response = str1 + str2 + str3; return response; }; define stream TempStream(deviceID long, roomNo int, temp double); from TempStream select concatFn(roomNo,'-',deviceID) as id, temp insert into DeviceTempStream; Here, the defined concatFn function is used in the query by passing three string parameters for concatenation.","title":"Script"},{"location":"docs/query-guide/#on-demand-query","text":"On-demand queries provide a way of performing add hock operations on Siddhi tables ( stores ), named-windows , and named-aggregations . The On-demand query can be submitted to the SiddhiApp using the query() method of the respective Siddhi application runtime as follows. siddhiAppRuntime.query( on-demand query ); To successfully execute an on-demand query, the SiddhiApp of the respective siddhiAppRuntime should have the corresponding table, named-window, or named-aggregation defined, that is being used in the on-demand query. Purpose On-demand queries allow to retrieve, add, delete and update events/data in Siddhi tables ( stores ), named-windows , and named-aggregations without the intervention of streams. This can be used to retrieve the status of the system, extract information for reporting and dashboarding purposes, and many others. The operations supported on tables are: Select Insert Delete Update Update or insert The operation supported on named-windows , and named-aggregations is: Select On-Demand query operators The following operations can be performed via on-demand queries.","title":"On-Demand Query"},{"location":"docs/query-guide/#select-tablenamed-window","text":"On-demand query to retrieve records from the specified table (/ store ) or named-window , based on the given condition. To retrieve data from Named-Aggregation, refer the Named-Aggregation Select section. Syntax Syntax to retrieve events from table or named-window is as follows; from ( table | named-window ) (on condition )? select attribute name , attribute name , ... group by ? having ? order by ? limit ? Here, the input can be either table (/ store ) or named-window , and the other parameters are similar to the standard Siddhi query . Examples The on-demand queries used in the examples are performed on a SiddhiApp that contains a table definition similar to the following. define table RoomTypeTable (roomNo int, type string); Example 1 An on-demand query to retrieve room numbers and their types from the RoomTypeTable table, for all room numbers that are greater than or equal to 10. from RoomTypeTable on roomNo = 10; select roomNo, type Example 2 An on-demand query to calculate the total number of rooms in the RoomTypeTable table. from RoomTypeTable select count(roomNo) as totalRooms","title":"Select (Table/Named-Window)"},{"location":"docs/query-guide/#select-named-aggregation","text":"On-demand query to retrieve records from the specified named-aggregation , based on the time range, time granularity, and the given condition. To retrieve data from table (store), or named-window, refer the Table/Named-Window Select section. Syntax Syntax to retrieve events from named-aggregation is as follows; from aggregation (on condition )? within time range per time granularity select attribute name , attribute name , ... group by ? having ? order by ? limit ? This is similar to the Table/Named-Window Select , but the input should be a named-aggregation , and the within time range and per time granularity should be provided as in named-aggregation join specifying the time interval for which the aggregate values need to be retrieved, and the the time granularity by which the aggregate values must be grouped and returned respectively. Examples The on-demand queries used in the examples are performed on a SiddhiApp that contains an aggregation definition similar to the following. define stream TradeStream (symbol string, price double, volume long, timestamp long); define aggregation TradeAggregation from TradeStream select symbol, avg(price) as avgPrice, sum(price) as total group by symbol aggregate by timestamp every sec ... year; Example 1 An on-demand query to retrieve daily aggregations from the TradeAggregation within the time range of \"2014-02-15 00:00:00 +05:30\", \"2014-03-16 00:00:00 +05:30\" (Here, +05:30 can be omitted if timezone is in GMT). from TradeAggregation within \"2014-02-15 00:00:00 +05:30\", \"2014-03-16 00:00:00 +05:30\" per \"days\" select symbol, total, avgPrice; Example 2 An on-demand query to retrieve the hourly aggregations from the TradeAggregation for \"FB\" symbol within the day of 2014-02-15 . from TradeAggregation on symbol == \"FB\" within \"2014-02-15 **:**:** +05:30\" per \"hours\" select symbol, total, avgPrice;","title":"Select (Named-Aggregation)"},{"location":"docs/query-guide/#insert_3","text":"On-demand query to insert a new record to a table (/ store ), based on the attribute values defined in the select clause. Syntax Syntax to insert events into a table (store) is as follows; select attribute name , attribute name , ... insert into table ; Example An on-demand query to insert a new record into the table RoomOccupancyTable having values for its roomNo and people attributes as 10 and 2 respectively. select 10 as roomNo, 2 as people insert into RoomOccupancyTable Here, the respective SiddhiApp should have a RoomOccupancyTable table something similar to the following. define table RoomOccupancyTable (roomNo int, people int);","title":"Insert"},{"location":"docs/query-guide/#delete_1","text":"On-demand query to delete records from a table (/ store ), based on the specified condition. Syntax Syntax to delete events from a table (store) is as follows; select ? delete table (on condition )? Here, the on condition specifies the basis on which records are selected to be deleted, and when omitted, all recodes in the table will be removed. Note In the delete clause, the table attributes must be always referred with the table name as follows: table name . attribute name . Example On-demand queries to delete records in the RoomTypeTable table that have 10 as the value for their roomNo attribute. select 10 as roomNumber delete RoomTypeTable on RoomTypeTable.roomNo == roomNumber; delete RoomTypeTable on RoomTypeTable.roomNo == 10; Both the above queries result in the same. For the above queries to be performed, the respective SiddhiApp should have a RoomTypeTable table defined something similar to the following. define table RoomTypeTable (roomNo int, type string);","title":"Delete"},{"location":"docs/query-guide/#update_1","text":"On-demand query to update selected attributes of records from a table (/ store ), based on the specified condition. Syntax Syntax to update events on a table (store) is as follows; select attribute name , attribute name , ...? update table (set table . attribute name = ( attribute name | expression ), table . attribute name = ( attribute name | expression ), ...)? (on condition )? The condition element specifies the basis on which the events in the table are selected to be updated. When referring the table attributes in the update clause, they must always be referred to with the table name , and when a condition is not defined, all the events in the table will be updated. The set keyword can be used to update only the selected attributes from the table. Here, for each assignment, the left side should contain the table attribute that is being updated, and the right side can contain a query output attribute, a table attribute, a mathematical operation, or any other. When the set clause is not provided, all attributes in the table will be updated based on the query output. Note In the update clause, the table attributes must be always referred with the table name as follows: table name . attribute name Examples The on-demand queries used in the examples are performed on a SiddhiApp that contains a table definition similar to the following. define table RoomOccupancyTable (roomNo int, people int); Example 1 An on-demand query to increment the number of people by 1 for the roomNo 10 , in the RoomOccupancyTable table. update RoomTypeTable set RoomTypeTable.people = RoomTypeTable.people + 1 on RoomTypeTable.roomNo == 10; Example 2 An on-demand query to increment the number of people by the arrival amount for the given roomNumber , in the RoomOccupancyTable table. select 10 as roomNumber, 1 as arrival update RoomTypeTable set RoomTypeTable.people = RoomTypeTable.people + arrival on RoomTypeTable.roomNo == roomNumber;","title":"Update"},{"location":"docs/query-guide/#update-or-insert_1","text":"On-demand query to update the events (records) that already exist in the table (/ store ) based on a condition, else inserts the event as a new entry to the table. Syntax Syntax to update or insert events on a table (store) is as follows; select attribute name , attribute name , ... update or insert into table (set table . attribute name = ( attribute name | expression ), table . attribute name = ( attribute name | expression ), ...)? (on condition )? The condition element specifies the basis on which the events in the table are selected to be updated. When referring the table attributes in the update clause, they must always be referred with the table name , and when the condition does not match with any event in the table, then a new event (a record) is inserted into the table. Here, when a condition is not defined, all the events in the table will be updated. The set clause is only used when an update is performed in the update or insert operation. In this case, the set keyword can be used to update only the selected attributes from the table. Here, for each assignment, the left side should contain the table attribute that is being updated, and the right side can contain a query output attribute, a table attribute, a mathematical operation, or any other. When the set clause is not provided, all attributes in the table will be updated based on the query output. Note In the update or insert clause, the table attributes must be always referred with the table name as follows: table name . attribute name Example An on-demand query to update the record with assignee \"John\" when there is already and record for roomNo 10 in the RoomAssigneeTable table, else to insert a new record with values 10 , \"single\" and \"John\" for the attributes roomNo , type , and assignee respectively. select 10 as roomNo, \"single\" as type, \"John\" as assignee update or insert into RoomAssigneeTable set RoomAssigneeTable.assignee = assignee on RoomAssigneeTable.roomNo == roomNo; For the above query to be performed, the respective SiddhiApp should have a RoomAssigneeTable table defined something similar to the following. define table RoomAssigneeTable (roomNo int, type string, assignee string);","title":"Update or Insert"},{"location":"docs/query-guide/#siddhiapp-configuration-and-monitoring","text":"","title":"SiddhiApp Configuration and Monitoring"},{"location":"docs/query-guide/#threading-and-synchronization","text":"The threading and synchronization behavior of SiddhiApps can be modified by using the @async annotation on the Streams. By default, SiddhiApp uses the request threads for all the processing. Here, the request threads follow through the streams and process each query in the order they are defined. By using the @async annotation, processing of events can be handed over to a new set of worker threads. Purpose The @async annotation helps to improve the SiddhiApp performance using parallel processing and event chunking, and it can also help to synchronize the execution across multiple operations. Syntax The syntax for configuring threading in Siddhi is as follows. @async(buffer.size=' buffer size ', workers=' workers ', batch.size.max='max batch size ') define stream stream name ( attribute name attribute type , attribute name attribute type , ... ); The following parameters are used to configure the @async definition. Parameter Description Optional Default Value buffer.size The size of the event buffer (in power of 2) that holds the events until they are picked by worker threads for processing. No - workers The number of worker threads that process the buffered events. Yes 1 batch.size.max The maximum number of events that will be fetched from the event buffer to be processed together by a worker thread, at a given time. Yes buffer.size Parallel processing Parallel processing helps to improve the performance by letting multiple threads to process events in parallel. By default, Siddhi does not process events in parallel, unless otherwise, it uses multi-threaded windows, triggers, or the events are sent to Siddhi using multiple input threads either from the sources defined via @source annotation or from the applications calling the Siddhi via InputHander . Parallel processing within a SiddhiApp can be explicitly achieved by defining @async annotations on the appropriate streams with the number of workers being greater than 1 . Here, the whole execution flow beginning from that stream will be executed by multiple workers in parallel. Event chunking Event chunking helps to improve the performance by processing multiple events to together, especially when the operations are I/O bound. By default, Siddhi does not attempt to chunk/group events together. Event chunking in a SiddhiApp can be explicitly achieved, by defining @async annotations on appropriate streams with batch.size.max set to greater than one. Here, the whole execution flow beginning from those streams will execute multiple events together, where each event group will have up to batch.size.max number of events. Use a combination of parallel processing, and event chunking to achieve the best performance. The optimal values for buffer.size , workers and batch.size.max parameters vary depending on the use case and the environment. Therefore, they can be only identified by testing the setup in a staging environment. synchronized execution Synchronized execution eliminates possible concurrent updates and race conditions among queries. By default, Siddhi provides synchronization only within its operators and not across queries. Synchronized execution across multiple queries can be explicitly achieved by defining @async annotation on appropriate streams with workers set to 1 . Here, the whole execution flow beginning from that stream will be executed synchronously by a single thread. Too many async annotations can reduce performance! Having multiple @async annotations will result in many threads being used for processing, this increases the context switching overhead, and thereby reducing the overall performance of the SiddhiApp. Therefore, use @async annotation only when it is necessary .","title":"Threading and Synchronization"},{"location":"docs/query-guide/#statistics","text":"The throughput, latency, and memory consumption of SiddhiApps, and their internal operators can be monitored through Siddhi statistics. SiddhiApps can have preconfigured statistics configurations using the @app:statistics annotation applied on SiddhiApp level, and they can also be dynamically modified at runtime using the setStatisticsLevel() method available on the SiddhiAppRuntime . Purpose Siddhi statistics helps to identify the bottlenecks in the SiddhiApp execution flows, and thereby facilitate to improve SiddhiApp performance by appropriately handling them. The name of the statistics metrics follow the below format. io.siddhi.SiddhiApps. SiddhiApp name .Siddhi. component type . component name . metrics type . The following table lists the component types and their supported of metrics types. Component Type Metrics Type Stream throughput size (The number of buffered events when asynchronous processing is enabled via @async annotation. Trigger throughput (For both trigger and corresponding stream) Source throughput Sink throughput Mapper latency throughput (For both input and output) Table memory throughput (For all operations) Latency (For all operations) Query memory latency Window throughput (For all operations) latency (For all operation) Partition throughput (For all operations) latency (For all operation) Syntax The syntax for defining the statistics for SiddhiApps running on Java or Python modes is as follows. @app:statistics( reporter=' reporter ', interval=' internal ', include=' included metrics for reporting ') The following parameters are used to configure statistics in Java and Python modes. Parameter Description Default Value reporter The implementation of the statistics reporter. Supported values are: console jmx console interval The statistics reporting time interval (in seconds). 60 include Specifies the metricizes that should report statistics using a comma-separated list or via wildcards. *.* (All) The syntax for defining the statistics for SiddhiApps running on Local, Docker, or Kubernetes modes is as follows. @app:statistics(enable = ' is enable ', include = `' included metrics for reporting '`) The following parameters are used to configure statistics in Local, Docker, and Kubernetes modes. Parameter Description Default Value enable Enables statistics reporting. Supported values are: true , false false include Specifies the metricizes that should report statistics using a comma-separated list or via wildcards. *.* (All) Here, other statistics configurations are applied commonly to all SiddhiApps, as specified in the Configuration Guide . Example A SiddhiApp running on Java, to report statistics every minute, by logging its stats on the console. @App:name('TestMetrics') @App:Statistics(reporter = 'console') define stream TestStream (message string); @info(name='logQuery') from TestSream#log(\"Message:\") insert into TempSream; The statistics reported via console log will be as follows. Click to view the extract 11/26/17 8:01:20 PM ============================================================ -- Gauges ---------------------------------------------------------------------- io.siddhi.SiddhiApps.TestMetrics.Siddhi.Queries.logQuery.memory value = 5760 io.siddhi.SiddhiApps.TestMetrics.Siddhi.Streams.TestStream.size value = 0 -- Meters ---------------------------------------------------------------------- io.siddhi.SiddhiApps.TestMetrics.Siddhi.Sources.TestStream.http.throughput count = 0 mean rate = 0.00 events/second 1-minute rate = 0.00 events/second 5-minute rate = 0.00 events/second 15-minute rate = 0.00 events/second io.siddhi.SiddhiApps.TestMetrics.Siddhi.Streams.TempSream.throughput count = 2 mean rate = 0.04 events/second 1-minute rate = 0.03 events/second 5-minute rate = 0.01 events/second 15-minute rate = 0.00 events/second io.siddhi.SiddhiApps.TestMetrics.Siddhi.Streams.TestStream.throughput count = 2 mean rate = 0.04 events/second 1-minute rate = 0.03 events/second 5-minute rate = 0.01 events/second 15-minute rate = 0.00 events/second -- Timers ---------------------------------------------------------------------- io.siddhi.SiddhiApps.TestMetrics.Siddhi.Queries.logQuery.latency count = 2 mean rate = 0.11 calls/second 1-minute rate = 0.34 calls/second 5-minute rate = 0.39 calls/second 15-minute rate = 0.40 calls/second min = 0.61 milliseconds max = 1.08 milliseconds mean = 0.84 milliseconds stddev = 0.23 milliseconds median = 0.61 milliseconds 75% < = 1.08 milliseconds 95% < = 1.08 milliseconds 98% < = 1.08 milliseconds 99% < = 1.08 milliseconds 99.9% < = 1.08 milliseconds","title":"Statistics"},{"location":"docs/query-guide/#event-playback","text":"The speed of time within the SiddhiApp can be altered based on the actual event timestamp, using the @app:playback SiddhiApp annotation. Here, the event playback updates the current SiddhiApp time to the largest event time seen so far. Purpose Event playback helps to reprocess previously consumed and stored events in much faster speed, without losing the time-based properties of Siddhi queries, by rapidly replaying the events. Syntax The syntax for defining event playback is as follows. @app:playback(idle.time = ' idle time before incrementing timestamp ', increment = ' incremented time interval ') The following parameters are used to configure this annotation. Parameter Description idle.time The time duration (in milliseconds), within which when no events arrive, the current SiddhiApp time will be incremented by the value specified under the increment parameter. increment The number of seconds, by which, the current SiddhiApp time must be incremented when no events receive during the idle.time period. Here, both the parameters are optional, and when omitted, the current SiddhiApp time will not be automatically incremented when events do not arrive. Example 1 SiddhiApp to perform playback while incrementing the current SiddhiApp time by 2 seconds when no events arrive for every 100 milliseconds . @app:playback(idle.time = '100 millisecond', increment = '2 sec') Example 2 SiddhiApp to perform playback while not incrementing the current SiddhiApp time when no events arrive. @app:playback()","title":"Event Playback"},{"location":"docs/siddhi-as-a-docker-microservice/","text":"Siddhi 5.1 as a Docker Microservice This section provides information on running Siddhi Apps on Docker. Siddhi Microservice can run one or more Siddhi Applications with required system configurations. Here, the Siddhi application ( .siddhi file) contains stream processing logic and the necessary system configurations can be passed via the Siddhi configuration .yaml file. Steps to Run Siddhi Docker Microservice is as follows. Pull the the latest Siddhi Runner image from Siddhiio Docker Hub . docker pull siddhiio/siddhi-runner-alpine:latest Start SiddhiApps with the runner config by executing the following docker command. docker run -it -v local-siddhi-file-path : siddhi-file-mount-path -v local-conf-file-path : conf-file-mount-path siddhiio/siddhi-runner-alpine:latest -Dapps= siddhi-file-mount-path -Dconfig= conf-file-mount-path E.g., docker run -it -v /home/me/siddhi-apps:/apps -v /home/me/siddhi-configs:/configs siddhiio/siddhi-runner-alpine:latest -Dapps=/apps/Foo.siddhi -Dconfig=/configs/siddhi-config.yaml Running multiple SiddhiApps in one runner instance. To run multiple SiddhiApps in one runtime instance, have all SiddhiApps in a directory, mount the directory and pass its location through -Dapps parameter as follows, -Dapps= siddhi-apps-directory Always use absolute path for SiddhiApps and runner configs. Providing absolute path of SiddhiApp file, or directory in -Dapps parameter, and when providing the Siddhi runner config yaml on -Dconfig parameter while starting Siddhi runner. Siddhi Tooling You can also use the powerful Siddhi Editor to implement and test steam processing applications. Configuring Siddhi To configure databases, extensions, authentication, periodic state persistence, and statistics for Siddhi as Docker Microservice refer Siddhi Config Guide . Samples Running Siddhi App Following SiddhiApp collects events via HTTP and logs the number of events arrived during last 15 seconds. @App:name('CountOverTime') @App:description('Receive events via HTTP, and logs the number of events received during last 15 seconds') @source(type = 'http', receiver.url = \"http://0.0.0.0:8006/production\", @map(type = 'json')) define stream ProductionStream (name string, amount double); @sink(type = 'log') define stream TotalCountStream (totalCount long); -- Count the incoming events @info(name = 'query1') from ProductionStream#window.time(15 sec) select count() as totalCount insert into TotalCountStream; Always listen on 0.0.0.0 with the Siddhi Application running inside a docker container. If you listen on localhost inside the container, nothing outside the container can connect to your application. That includes blocking port forwarding from the docker host and container to container networking. Copy the above SiddhiApp, and create the SiddhiApp file CountOverTime.siddhi . Run the SiddhiApp by executing following commands from the distribution directory docker run -it -p 8006:8006 -v local-absolute-siddhi-file-path /CountOverTime.siddhi:/apps/CountOverTime.siddhi siddhiio/siddhi-runner-alpine -Dapps=/apps/CountOverTime.siddhi Test the SiddhiApp by calling the HTTP endpoint using curl or Postman as follows Publish events with curl command: Publish few json to the http endpoint as follows, curl -X POST http://localhost:8006/production \\ --header \"Content-Type:application/json\" \\ -d '{\"event\":{\"name\":\"Cake\",\"amount\":20.12}}' Publish events with Postman: Install 'Postman' application from Chrome web store Launch the application Make a 'Post' request to 'http://localhost:8006/production' endpoint. Set the Content-Type to 'application/json' and set the request body in json format as follows, { \"event\": { \"name\": \"Cake\", \"amount\": 20.12 } } Runner logs the total count on the console. Note, how the count increments with every event sent. [2019-04-11 13:36:03,517] INFO {io.siddhi.core.stream.output.sink.LogSink} - CountOverTime : TotalCountStream : Event{timestamp=1554969963512, data=[1], isExpired=false} [2019-04-11 13:36:10,267] INFO {io.siddhi.core.stream.output.sink.LogSink} - CountOverTime : TotalCountStream : Event{timestamp=1554969970267, data=[2], isExpired=false} [2019-04-11 13:36:41,694] INFO {io.siddhi.core.stream.output.sink.LogSink} - CountOverTime : TotalCountStream : Event{timestamp=1554970001694, data=[1], isExpired=false} Running with runner config When running SiddhiApps users can optionally provide a config yaml to Siddhi runner to manage configurations such as state persistence, databases connections and secure vault. Following SiddhiApp collects events via HTTP and store them in H2 Database. @App:name(\"ConsumeAndStore\") @App:description(\"Consume events from HTTP and write to TEST_DB\") @source(type = 'http', receiver.url = \"http://0.0.0.0:8006/production\", @map(type = 'json')) define stream ProductionStream (name string, amount double); @store(type='rdbms', datasource='TEST_DB') define table ProductionTable (name string, amount double); -- Store all events to the table @info(name = 'query1') from ProductionStream insert into ProductionTable; The runner config can be configured with the relevant datasource information and passed when starting the runner dataSources: - name: TEST_DB description: The datasource used for testing definition: type: RDBMS configuration: jdbcUrl: 'jdbc:h2:${sys:carbon.home}/wso2/${sys:wso2.runtime}/database/TEST_DB;DB_CLOSE_ON_EXIT=FALSE;LOCK_TIMEOUT=60000' username: admin password: admin driverClassName: org.h2.Driver maxPoolSize: 10 idleTimeout: 60000 connectionTestQuery: SELECT 1 validationTimeout: 30000 isAutoCommit: false Copy the above SiddhiApp, & config yaml, and create corresponding the SiddhiApp file ConsumeAndStore.siddhi and TestDb.yaml files. Run the SiddhiApp by executing following command docker run -it -p 8006:8006 -p 9443:9443 -v local-absolute-siddhi-file-path /ConsumeAndStore.siddhi:/apps/ConsumeAndStore.siddhi -v local-absolute-config-yaml-path /TestDb.yaml:/conf/TestDb.yaml siddhiio/siddhi-runner-alpine -Dapps=/apps/ConsumeAndStore.siddhi -Dconfig=/conf/TestDb.yaml Test the SiddhiApp by calling the HTTP endpoint using curl or Postman as follows Publish events with curl command: Publish few json to the http endpoint as follows, curl -X POST http://localhost:8006/production \\ --header \"Content-Type:application/json\" \\ -d '{\"event\":{\"name\":\"Cake\",\"amount\":20.12}}' Publish events with Postman: Install 'Postman' application from Chrome web store Launch the application Make a 'Post' request to 'http://localhost:8006/production' endpoint. Set the Content-Type to 'application/json' and set the request body in json format as follows, { \"event\": { \"name\": \"Cake\", \"amount\": 20.12 } } Query Siddhi Store APIs to retrieve 10 records from the table. Query stored events with curl command: Publish few json to the http endpoint as follows, curl -X POST https://localhost:9443/stores/query \\ -H \"content-type: application/json\" \\ -u \"admin:admin\" \\ -d '{\"appName\" : \"ConsumeAndStore\", \"query\" : \"from ProductionTable select * limit 10;\" }' -k Query stored events with Postman: Install 'Postman' application from Chrome web store Launch the application Make a 'Post' request to 'https://localhost:9443/stores/query' endpoint. Set the Content-Type to 'application/json' and set the request body in json format as follows, { \"appName\" : \"ConsumeAndStore\", \"query\" : \"from ProductionTable select * limit 10;\" } The results of the query will be as follows, { \"records\":[ [\"Cake\",20.12] ] } Running with environmental/system variables Templating SiddhiApps allows users to provide environment/system variables to siddhiApps at runtime. This can help users to migrate SiddhiApps from one environment to another (E.g from dev, test and to prod). Following templated SiddhiApp collects events via HTTP, filters them based on amount greater than a given threshold value, and only sends the filtered events via email. Here the THRESHOLD value, and TO_EMAIL are templated in the TemplatedFilterAndEmail.siddhi SiddhiApp. @App:name(\"TemplatedFilterAndEmail\") @App:description(\"Consumes events from HTTP, filters them based on amount greater than a templated threshold value, and sends filtered events via email.\") @source(type = 'http', receiver.url = \"http://0.0.0.0:8006/production\", @map(type = 'json')) define stream ProductionStream (name string, amount double); @sink(ref = 'email-sink', subject = 'High {{name}} production!', to = '${TO_EMAIL}', content.type = 'text/html', @map(type = 'text', @payload(\"\"\" Hi, br/ br/ High production of b {{name}}, /b with amount b {{amount}} /b identified. br/ br/ For more information please contact production department. br/ br/ Thank you\"\"\"))) define stream FilteredProductionStream (name string, amount double); -- Filters the events based on threshold @info(name = 'query1') from ProductionStream[amount ${THRESHOLD}] insert into FilteredProductionStream; The runner config is configured with a gmail account to send email messages in EmailConfig.yaml by templating sending EMAIL_ADDRESS , EMAIL_USERNAME and EMAIL_PASSWORD . refs: - ref: name: 'email-sink' type: 'email' properties: port: '465' host: 'smtp.gmail.com' ssl.enable: 'true' auth: 'true' # User your gmail configurations here address: ' EMAIL_ADDRESS ' #E.g. test@gmail.com username: ' EMAIL_USERNAME ' #E.g. test password: ' EMAIL_PASSWORD ' #E.g. password Copy the above SiddhiApp, & config yaml, and create corresponding the SiddhiApp file TemplatedFilterAndEmail.siddhi and EmailConfig.yaml files. Set the following variables in the EmailConfig.yaml file: EMAIL_ADDRESS= gmail address EMAIL_USERNAME= gmail username EMAIL_PASSWORD= gmail password Set the below environment variables by passing them during the docker run command: THRESHOLD=20 TO_EMAIL= to email address Or they can also be passed as system variables by adding them to the end of the docker run command . -DTHRESHOLD=20 -DTO_EMAIL= to email address Run the SiddhiApp by executing following command. docker run -it -p 8006:8006 -v local-absolute-siddhi-file-path /TemplatedFilterAndEmail.siddhi:/apps/TemplatedFilterAndEmail.siddhi -v local-absolute-config-yaml-path /EmailConfig.yaml:/conf/EmailConfig.yaml -e THRESHOLD=20 -e TO_EMAIL= to email address siddhiio/siddhi-runner-alpine -Dapps=/apps/TemplatedFilterAndEmail.siddhi -Dconfig=/conf/EmailConfig.yaml Test the SiddhiApp by calling the HTTP endpoint using curl or Postman as follows Publish events with curl command: Publish few json to the http endpoint as follows, curl -X POST http://localhost:8006/production \\ --header \"Content-Type:application/json\" \\ -d '{\"event\":{\"name\":\"Cake\",\"amount\":2000.0}}' Publish events with Postman: Install 'Postman' application from Chrome web store Launch the application Make a 'Post' request to 'http://localhost:8006/production' endpoint. Set the Content-Type to 'application/json' and set the request body in json format as follows, { \"event\": { \"name\": \"Cake\", \"amount\": 2000.0 } } Check the to.email for the published email message, which will look as follows, Subject : High Cake production! Hi, High production of Cake, with amount 2000.0 identified. For more information please contact production department. Thank you","title":"Siddhi Docker Microservice"},{"location":"docs/siddhi-as-a-docker-microservice/#siddhi-51-as-a-docker-microservice","text":"This section provides information on running Siddhi Apps on Docker. Siddhi Microservice can run one or more Siddhi Applications with required system configurations. Here, the Siddhi application ( .siddhi file) contains stream processing logic and the necessary system configurations can be passed via the Siddhi configuration .yaml file. Steps to Run Siddhi Docker Microservice is as follows. Pull the the latest Siddhi Runner image from Siddhiio Docker Hub . docker pull siddhiio/siddhi-runner-alpine:latest Start SiddhiApps with the runner config by executing the following docker command. docker run -it -v local-siddhi-file-path : siddhi-file-mount-path -v local-conf-file-path : conf-file-mount-path siddhiio/siddhi-runner-alpine:latest -Dapps= siddhi-file-mount-path -Dconfig= conf-file-mount-path E.g., docker run -it -v /home/me/siddhi-apps:/apps -v /home/me/siddhi-configs:/configs siddhiio/siddhi-runner-alpine:latest -Dapps=/apps/Foo.siddhi -Dconfig=/configs/siddhi-config.yaml Running multiple SiddhiApps in one runner instance. To run multiple SiddhiApps in one runtime instance, have all SiddhiApps in a directory, mount the directory and pass its location through -Dapps parameter as follows, -Dapps= siddhi-apps-directory Always use absolute path for SiddhiApps and runner configs. Providing absolute path of SiddhiApp file, or directory in -Dapps parameter, and when providing the Siddhi runner config yaml on -Dconfig parameter while starting Siddhi runner. Siddhi Tooling You can also use the powerful Siddhi Editor to implement and test steam processing applications. Configuring Siddhi To configure databases, extensions, authentication, periodic state persistence, and statistics for Siddhi as Docker Microservice refer Siddhi Config Guide .","title":"Siddhi 5.1 as a Docker Microservice"},{"location":"docs/siddhi-as-a-docker-microservice/#samples","text":"","title":"Samples"},{"location":"docs/siddhi-as-a-docker-microservice/#running-siddhi-app","text":"Following SiddhiApp collects events via HTTP and logs the number of events arrived during last 15 seconds. @App:name('CountOverTime') @App:description('Receive events via HTTP, and logs the number of events received during last 15 seconds') @source(type = 'http', receiver.url = \"http://0.0.0.0:8006/production\", @map(type = 'json')) define stream ProductionStream (name string, amount double); @sink(type = 'log') define stream TotalCountStream (totalCount long); -- Count the incoming events @info(name = 'query1') from ProductionStream#window.time(15 sec) select count() as totalCount insert into TotalCountStream; Always listen on 0.0.0.0 with the Siddhi Application running inside a docker container. If you listen on localhost inside the container, nothing outside the container can connect to your application. That includes blocking port forwarding from the docker host and container to container networking. Copy the above SiddhiApp, and create the SiddhiApp file CountOverTime.siddhi . Run the SiddhiApp by executing following commands from the distribution directory docker run -it -p 8006:8006 -v local-absolute-siddhi-file-path /CountOverTime.siddhi:/apps/CountOverTime.siddhi siddhiio/siddhi-runner-alpine -Dapps=/apps/CountOverTime.siddhi Test the SiddhiApp by calling the HTTP endpoint using curl or Postman as follows Publish events with curl command: Publish few json to the http endpoint as follows, curl -X POST http://localhost:8006/production \\ --header \"Content-Type:application/json\" \\ -d '{\"event\":{\"name\":\"Cake\",\"amount\":20.12}}' Publish events with Postman: Install 'Postman' application from Chrome web store Launch the application Make a 'Post' request to 'http://localhost:8006/production' endpoint. Set the Content-Type to 'application/json' and set the request body in json format as follows, { \"event\": { \"name\": \"Cake\", \"amount\": 20.12 } } Runner logs the total count on the console. Note, how the count increments with every event sent. [2019-04-11 13:36:03,517] INFO {io.siddhi.core.stream.output.sink.LogSink} - CountOverTime : TotalCountStream : Event{timestamp=1554969963512, data=[1], isExpired=false} [2019-04-11 13:36:10,267] INFO {io.siddhi.core.stream.output.sink.LogSink} - CountOverTime : TotalCountStream : Event{timestamp=1554969970267, data=[2], isExpired=false} [2019-04-11 13:36:41,694] INFO {io.siddhi.core.stream.output.sink.LogSink} - CountOverTime : TotalCountStream : Event{timestamp=1554970001694, data=[1], isExpired=false}","title":"Running Siddhi App"},{"location":"docs/siddhi-as-a-docker-microservice/#running-with-runner-config","text":"When running SiddhiApps users can optionally provide a config yaml to Siddhi runner to manage configurations such as state persistence, databases connections and secure vault. Following SiddhiApp collects events via HTTP and store them in H2 Database. @App:name(\"ConsumeAndStore\") @App:description(\"Consume events from HTTP and write to TEST_DB\") @source(type = 'http', receiver.url = \"http://0.0.0.0:8006/production\", @map(type = 'json')) define stream ProductionStream (name string, amount double); @store(type='rdbms', datasource='TEST_DB') define table ProductionTable (name string, amount double); -- Store all events to the table @info(name = 'query1') from ProductionStream insert into ProductionTable; The runner config can be configured with the relevant datasource information and passed when starting the runner dataSources: - name: TEST_DB description: The datasource used for testing definition: type: RDBMS configuration: jdbcUrl: 'jdbc:h2:${sys:carbon.home}/wso2/${sys:wso2.runtime}/database/TEST_DB;DB_CLOSE_ON_EXIT=FALSE;LOCK_TIMEOUT=60000' username: admin password: admin driverClassName: org.h2.Driver maxPoolSize: 10 idleTimeout: 60000 connectionTestQuery: SELECT 1 validationTimeout: 30000 isAutoCommit: false Copy the above SiddhiApp, & config yaml, and create corresponding the SiddhiApp file ConsumeAndStore.siddhi and TestDb.yaml files. Run the SiddhiApp by executing following command docker run -it -p 8006:8006 -p 9443:9443 -v local-absolute-siddhi-file-path /ConsumeAndStore.siddhi:/apps/ConsumeAndStore.siddhi -v local-absolute-config-yaml-path /TestDb.yaml:/conf/TestDb.yaml siddhiio/siddhi-runner-alpine -Dapps=/apps/ConsumeAndStore.siddhi -Dconfig=/conf/TestDb.yaml Test the SiddhiApp by calling the HTTP endpoint using curl or Postman as follows Publish events with curl command: Publish few json to the http endpoint as follows, curl -X POST http://localhost:8006/production \\ --header \"Content-Type:application/json\" \\ -d '{\"event\":{\"name\":\"Cake\",\"amount\":20.12}}' Publish events with Postman: Install 'Postman' application from Chrome web store Launch the application Make a 'Post' request to 'http://localhost:8006/production' endpoint. Set the Content-Type to 'application/json' and set the request body in json format as follows, { \"event\": { \"name\": \"Cake\", \"amount\": 20.12 } } Query Siddhi Store APIs to retrieve 10 records from the table. Query stored events with curl command: Publish few json to the http endpoint as follows, curl -X POST https://localhost:9443/stores/query \\ -H \"content-type: application/json\" \\ -u \"admin:admin\" \\ -d '{\"appName\" : \"ConsumeAndStore\", \"query\" : \"from ProductionTable select * limit 10;\" }' -k Query stored events with Postman: Install 'Postman' application from Chrome web store Launch the application Make a 'Post' request to 'https://localhost:9443/stores/query' endpoint. Set the Content-Type to 'application/json' and set the request body in json format as follows, { \"appName\" : \"ConsumeAndStore\", \"query\" : \"from ProductionTable select * limit 10;\" } The results of the query will be as follows, { \"records\":[ [\"Cake\",20.12] ] }","title":"Running with runner config"},{"location":"docs/siddhi-as-a-docker-microservice/#running-with-environmentalsystem-variables","text":"Templating SiddhiApps allows users to provide environment/system variables to siddhiApps at runtime. This can help users to migrate SiddhiApps from one environment to another (E.g from dev, test and to prod). Following templated SiddhiApp collects events via HTTP, filters them based on amount greater than a given threshold value, and only sends the filtered events via email. Here the THRESHOLD value, and TO_EMAIL are templated in the TemplatedFilterAndEmail.siddhi SiddhiApp. @App:name(\"TemplatedFilterAndEmail\") @App:description(\"Consumes events from HTTP, filters them based on amount greater than a templated threshold value, and sends filtered events via email.\") @source(type = 'http', receiver.url = \"http://0.0.0.0:8006/production\", @map(type = 'json')) define stream ProductionStream (name string, amount double); @sink(ref = 'email-sink', subject = 'High {{name}} production!', to = '${TO_EMAIL}', content.type = 'text/html', @map(type = 'text', @payload(\"\"\" Hi, br/ br/ High production of b {{name}}, /b with amount b {{amount}} /b identified. br/ br/ For more information please contact production department. br/ br/ Thank you\"\"\"))) define stream FilteredProductionStream (name string, amount double); -- Filters the events based on threshold @info(name = 'query1') from ProductionStream[amount ${THRESHOLD}] insert into FilteredProductionStream; The runner config is configured with a gmail account to send email messages in EmailConfig.yaml by templating sending EMAIL_ADDRESS , EMAIL_USERNAME and EMAIL_PASSWORD . refs: - ref: name: 'email-sink' type: 'email' properties: port: '465' host: 'smtp.gmail.com' ssl.enable: 'true' auth: 'true' # User your gmail configurations here address: ' EMAIL_ADDRESS ' #E.g. test@gmail.com username: ' EMAIL_USERNAME ' #E.g. test password: ' EMAIL_PASSWORD ' #E.g. password Copy the above SiddhiApp, & config yaml, and create corresponding the SiddhiApp file TemplatedFilterAndEmail.siddhi and EmailConfig.yaml files. Set the following variables in the EmailConfig.yaml file: EMAIL_ADDRESS= gmail address EMAIL_USERNAME= gmail username EMAIL_PASSWORD= gmail password Set the below environment variables by passing them during the docker run command: THRESHOLD=20 TO_EMAIL= to email address Or they can also be passed as system variables by adding them to the end of the docker run command . -DTHRESHOLD=20 -DTO_EMAIL= to email address Run the SiddhiApp by executing following command. docker run -it -p 8006:8006 -v local-absolute-siddhi-file-path /TemplatedFilterAndEmail.siddhi:/apps/TemplatedFilterAndEmail.siddhi -v local-absolute-config-yaml-path /EmailConfig.yaml:/conf/EmailConfig.yaml -e THRESHOLD=20 -e TO_EMAIL= to email address siddhiio/siddhi-runner-alpine -Dapps=/apps/TemplatedFilterAndEmail.siddhi -Dconfig=/conf/EmailConfig.yaml Test the SiddhiApp by calling the HTTP endpoint using curl or Postman as follows Publish events with curl command: Publish few json to the http endpoint as follows, curl -X POST http://localhost:8006/production \\ --header \"Content-Type:application/json\" \\ -d '{\"event\":{\"name\":\"Cake\",\"amount\":2000.0}}' Publish events with Postman: Install 'Postman' application from Chrome web store Launch the application Make a 'Post' request to 'http://localhost:8006/production' endpoint. Set the Content-Type to 'application/json' and set the request body in json format as follows, { \"event\": { \"name\": \"Cake\", \"amount\": 2000.0 } } Check the to.email for the published email message, which will look as follows, Subject : High Cake production! Hi, High production of Cake, with amount 2000.0 identified. For more information please contact production department. Thank you","title":"Running with environmental/system variables"},{"location":"docs/siddhi-as-a-java-library/","text":"Siddhi 5.1 as a Java library Siddhi can be used as a library in any Java program (including in OSGi runtimes) just by adding Siddhi and its extension jars as dependencies. Find a sample Siddhi project that's implemented as a Java program using Maven here , this can be used as a reference for any based implementation. Following are the mandatory dependencies that need to be added to the Maven pom.xml file (or to the program classpath). dependency groupId io.siddhi /groupId artifactId siddhi-core /artifactId version 5.x.x /version /dependency dependency groupId io.siddhi /groupId artifactId siddhi-query-api /artifactId version 5.x.x /version /dependency dependency groupId io.siddhi /groupId artifactId siddhi-query-compiler /artifactId version 5.x.x /version /dependency dependency groupId io.siddhi /groupId artifactId siddhi-annotations /artifactId version 5.x.x /version /dependency Sample Sample Java class using Siddhi is as follows. package io.siddhi.sample; import io.siddhi.core.SiddhiAppRuntime; import io.siddhi.core.SiddhiManager; import io.siddhi.core.event.Event; import io.siddhi.core.stream.input.InputHandler; import io.siddhi.core.stream.output.StreamCallback; import io.siddhi.core.util.EventPrinter; /** * The sample demonstrate how to use Siddhi within another Java program. * This sample contains a simple filter query. */ public class SimpleFilterSample { public static void main(String[] args) throws InterruptedException { // Create Siddhi Manager SiddhiManager siddhiManager = new SiddhiManager(); //Siddhi Application String siddhiApp = \"\" + \"define stream StockStream (symbol string, price float, volume long); \" + \"\" + \"@info(name = 'query1') \" + \"from StockStream[volume 150] \" + \"select symbol, price \" + \"insert into OutputStream;\"; //Generate runtime SiddhiAppRuntime siddhiAppRuntime = siddhiManager.createSiddhiAppRuntime(siddhiApp); //Adding callback to retrieve output events from stream siddhiAppRuntime.addCallback(\"OutputStream\", new StreamCallback() { @Override public void receive(Event[] events) { EventPrinter.print(events); //To convert and print event as a map //EventPrinter.print(toMap(events)); } }); //Get InputHandler to push events into Siddhi InputHandler inputHandler = siddhiAppRuntime.getInputHandler(\"StockStream\"); //Start processing siddhiAppRuntime.start(); //Sending events to Siddhi inputHandler.send(new Object[]{\"IBM\", 700f, 100L}); inputHandler.send(new Object[]{\"WSO2\", 60.5f, 200L}); inputHandler.send(new Object[]{\"GOOG\", 50f, 30L}); inputHandler.send(new Object[]{\"IBM\", 76.6f, 400L}); inputHandler.send(new Object[]{\"WSO2\", 45.6f, 50L}); Thread.sleep(500); //Shutdown runtime siddhiAppRuntime.shutdown(); //Shutdown Siddhi Manager siddhiManager.shutdown(); } } Configuration Siddhi running as a embedded Java library can be configured with environment specific properties using Extension System Parameters , References , and Siddhi Properties using either InMemoryConfigManager or YAMLConfigManager . Please refer the blog for details.","title":"Siddhi Java Library"},{"location":"docs/siddhi-as-a-java-library/#siddhi-51-as-a-java-library","text":"Siddhi can be used as a library in any Java program (including in OSGi runtimes) just by adding Siddhi and its extension jars as dependencies. Find a sample Siddhi project that's implemented as a Java program using Maven here , this can be used as a reference for any based implementation. Following are the mandatory dependencies that need to be added to the Maven pom.xml file (or to the program classpath). dependency groupId io.siddhi /groupId artifactId siddhi-core /artifactId version 5.x.x /version /dependency dependency groupId io.siddhi /groupId artifactId siddhi-query-api /artifactId version 5.x.x /version /dependency dependency groupId io.siddhi /groupId artifactId siddhi-query-compiler /artifactId version 5.x.x /version /dependency dependency groupId io.siddhi /groupId artifactId siddhi-annotations /artifactId version 5.x.x /version /dependency","title":"Siddhi 5.1 as a Java library"},{"location":"docs/siddhi-as-a-java-library/#sample","text":"Sample Java class using Siddhi is as follows. package io.siddhi.sample; import io.siddhi.core.SiddhiAppRuntime; import io.siddhi.core.SiddhiManager; import io.siddhi.core.event.Event; import io.siddhi.core.stream.input.InputHandler; import io.siddhi.core.stream.output.StreamCallback; import io.siddhi.core.util.EventPrinter; /** * The sample demonstrate how to use Siddhi within another Java program. * This sample contains a simple filter query. */ public class SimpleFilterSample { public static void main(String[] args) throws InterruptedException { // Create Siddhi Manager SiddhiManager siddhiManager = new SiddhiManager(); //Siddhi Application String siddhiApp = \"\" + \"define stream StockStream (symbol string, price float, volume long); \" + \"\" + \"@info(name = 'query1') \" + \"from StockStream[volume 150] \" + \"select symbol, price \" + \"insert into OutputStream;\"; //Generate runtime SiddhiAppRuntime siddhiAppRuntime = siddhiManager.createSiddhiAppRuntime(siddhiApp); //Adding callback to retrieve output events from stream siddhiAppRuntime.addCallback(\"OutputStream\", new StreamCallback() { @Override public void receive(Event[] events) { EventPrinter.print(events); //To convert and print event as a map //EventPrinter.print(toMap(events)); } }); //Get InputHandler to push events into Siddhi InputHandler inputHandler = siddhiAppRuntime.getInputHandler(\"StockStream\"); //Start processing siddhiAppRuntime.start(); //Sending events to Siddhi inputHandler.send(new Object[]{\"IBM\", 700f, 100L}); inputHandler.send(new Object[]{\"WSO2\", 60.5f, 200L}); inputHandler.send(new Object[]{\"GOOG\", 50f, 30L}); inputHandler.send(new Object[]{\"IBM\", 76.6f, 400L}); inputHandler.send(new Object[]{\"WSO2\", 45.6f, 50L}); Thread.sleep(500); //Shutdown runtime siddhiAppRuntime.shutdown(); //Shutdown Siddhi Manager siddhiManager.shutdown(); } }","title":"Sample"},{"location":"docs/siddhi-as-a-java-library/#configuration","text":"Siddhi running as a embedded Java library can be configured with environment specific properties using Extension System Parameters , References , and Siddhi Properties using either InMemoryConfigManager or YAMLConfigManager . Please refer the blog for details.","title":"Configuration"},{"location":"docs/siddhi-as-a-kubernetes-microservice/","text":"Siddhi 5.1 as a Kubernetes Microservice This section provides information on running Siddhi Apps natively in Kubernetes via Siddhi Kubernetes Operator. Siddhi can be configured using SiddhiProcess kind and passed to the Siddhi operator for deployment. Here, the Siddhi applications containing stream processing logic can be written inline in SiddhiProcess yaml or passed as .siddhi files via contig maps. SiddhiProcess yaml can also be configured with the necessary system configurations. Prerequisites A Kubernetes cluster v1.10.11 or higher. Minikube Google Kubernetes Engine(GKE) Cluster Docker for Mac Or any other Kubernetes cluster Distributed deployment of Siddhi apps need NATS operator and NATS streaming operator . Admin privileges to install Siddhi operator Minikube Siddhi operator automatically creates NGINX ingress. Therefore it to work we can either enable ingress on Minikube using the following command. minikube addons enable ingress or disable Siddhi operator's automatically ingress creation . Google Kubernetes Engine (GKE) Cluster To install Siddhi operator, you have to give cluster admin permission to your account. In order to do that execute the following command (by replacing \"your-address@email.com\" with your account email address). kubectl create clusterrolebinding user-cluster-admin-binding --clusterrole=cluster-admin --user=your-address@email.com Docker for Mac Siddhi operator automatically creates NGINX ingress. Therefore it to work we can either enable ingress on Docker for mac following the official documentation or disable Siddhi operator's automatically ingress creation . Port Forwarding for Testing Debugging Purposes Instead of creating ingress you can enable port forwarding ( kubectl port-forward ) to access the application in the Kubernetes cluster. This will help a lot for TCP connections as well. kubectl port-forward svc/mysql-db 13306:3306 For more details please refer this Kubernetes official documentation Install Siddhi Operator To install the Siddhi Kubernetes operator run the following commands. kubectl apply -f https://github.com/siddhi-io/siddhi-operator/releases/download/v0.2.0/00-prereqs.yaml kubectl apply -f https://github.com/siddhi-io/siddhi-operator/releases/download/v0.2.0/01-siddhi-operator.yaml You can verify the installation by making sure the following deployments are running in your Kubernetes cluster. $ kubectl get deployment NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE siddhi-operator 1 1 1 1 1m Using a custom-built Siddhi runner image If you need to use a custom-built siddhi-runner image for all the SiddhiProcess deployments, you have to configure siddhiRunnerImage entry in siddhi-operator-config config map. Refer the documentation on creating custom Siddhi runner images bundling additional JARs here . If you are pulling the custom-built image from a private Docker registry/repository, specify the corresponding kubernetes secret as siddhiRunnerImageSecret entry in siddhi-operator-config config map. For more details on using docker images from private registries/repositories refer this documentation . Deploy and run Siddhi App Siddhi applications can be deployed on Kubernetes using the Siddhi operator. Here we will be creating a very simple Siddhi stream processing application that receives power consumption from several devices in a house. If the power consumption of dryer exceeds the consumption limit of 6000W then that Siddhi app sends an alert from printing a log. This can be created using a SiddhiProcess YAML file as given below. apiVersion: siddhi.io/v1alpha2 kind: SiddhiProcess metadata: name: power-surge-app spec: apps: - script: | @App:name(\"PowerSurgeDetection\") @App:description(\"App consume events from HTTP as a JSON message of { 'deviceType': 'dryer', 'power': 6000 } format and inserts the events into DevicePowerStream, and alerts the user if the power level is greater than or equal to 600 by printing a message in the log.\") /* Input: deviceType string and powerConsuption int(Watt) Output: Alert user from printing a log, if there is a power surge in the dryer. In other words, notify when power is greater than or equal to 600W. */ @source( type='http', receiver.url='${RECEIVER_URL}', basic.auth.enabled='false', @map(type='json') ) define stream DevicePowerStream(deviceType string, power int); @sink(type='log', prefix='LOGGER') define stream PowerSurgeAlertStream(deviceType string, power int); @info(name='power-filter') from DevicePowerStream[deviceType == 'dryer' and power = 600] select deviceType, power insert into PowerSurgeAlertStream; container: env: - name: RECEIVER_URL value: \"http://0.0.0.0:8080/checkPower\" image: \"siddhiio/siddhi-runner-ubuntu:5.1.0\" Always listen on 0.0.0.0 with the Siddhi Application running inside a container environment. If you listen on localhost inside the container, nothing outside the container can connect to your application. Siddhi Tooling You can also use the powerful Siddhi Editor to implement and test steam processing applications. Configuring Siddhi To configure databases, extensions, authentication, periodic state persistence, and statistics for Siddhi as Kubernetes Microservice refer Siddhi Config Guide . To deploy the above Siddhi app in your Kubernetes cluster, copy above YAML to a file with name power-surge-app.yaml and execute the following command. kubectl create -f absolute-yaml-file-path /power-surge-app.yaml TLS secret Within the SiddhiProcess, a TLS secret named siddhi-tls is configured. If a Kubernetes secret with the same name does not exist in the Kubernetes cluster, the NGINX will ignore it and use a self-generated certificate. Configuring a secret will be necessary for calling HTTPS endpoints, refer deploy and run Siddhi apps with HTTPS section for more details. If the power-surge-app is deployed successfully, it should create SiddhiProcess, deployment, service, and ingress as following. $ kubectl get SiddhiProcesses NAME STATUS READY AGE power-surge-app Running 1/1 2m $ kubectl get deployment NAME READY UP-TO-DATE AVAILABLE AGE power-surge-app-0 1/1 1 1 2m siddhi-operator 1/1 1 1 2m $ kubectl get service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 none 443/TCP 2d power-surge-app-0 ClusterIP 10.96.44.182 none 8080/TCP 2m siddhi-operator ClusterIP 10.98.78.238 none 8383/TCP 2m $ kubectl get ingress NAME HOSTS ADDRESS PORTS AGE siddhi siddhi 10.0.2.15 80 2m Invoke Siddhi Applications To invoke the Siddhi App, obtain the external IP of the ingress load balancer using kubectl get ingress command as following. $ kubectl get ingress NAME HOSTS ADDRESS PORTS AGE siddhi siddhi 10.0.2.15 80 2m Then, add the host siddhi and related external IP ( ADDRESS ) to the /etc/hosts file in your machine. Minikube For Minikube, you have to use Minikube IP as the external IP. Hence, run minikube ip command to get the IP of the Minikube cluster. Docker for Mac For Docker for Mac, you have to use 0.0.0.0 as the external IP. Use the following CURL command to send events to power-surge-app deployed in Kubernetes. curl -X POST \\ http://siddhi/power-surge-app-0/8080/checkPower \\ -H 'Accept: */*' \\ -H 'Content-Type: application/json' \\ -H 'Host: siddhi' \\ -d '{ \"deviceType\": \"dryer\", \"power\": 60000 }' View Siddhi Process Logs Since the output of power-surge-app is logged, you can see the output by monitoring the associated pod's logs. To find the power-surge-app pod use the kubectl get pods command. This will list down all the deployed pods. $ kubectl get pods NAME READY STATUS RESTARTS AGE power-surge-app-0-646c4f9dd5-rxzkq 1/1 Running 0 4m siddhi-operator-6698d8f69d-6rfb6 1/1 Running 0 4m Here, the pod starting with the SiddhiProcess name (in this case power-surge-app- ) is the pod we need to monitor. To view the logs, run the kubectl logs pod name command. This will show all the Siddhi process logs, along with the filtered output events as given below. $ kubectl logs power-surge-app-0-646c4f9dd5-rxzkq ... [2019-07-12 07:12:48,925] INFO {org.wso2.transport.http.netty.contractimpl.listener.ServerConnectorBootstrap$HttpServerConnector} - HTTP(S) Interface starting on host 0.0.0.0 and port 9443 [2019-07-12 07:12:48,927] INFO {org.wso2.transport.http.netty.contractimpl.listener.ServerConnectorBootstrap$HttpServerConnector} - HTTP(S) Interface starting on host 0.0.0.0 and port 9090 [2019-07-12 07:12:48,941] INFO {org.wso2.carbon.kernel.internal.CarbonStartupHandler} - Siddhi Runner Distribution started in 6.853 sec [2019-07-12 07:17:22,219] INFO {io.siddhi.core.stream.output.sink.LogSink} - LOGGER : Event{timestamp=1562915842182, data=[dryer, 60000], isExpired=false} Get Siddhi process status List Siddhi processes List the Siddhi process using the kubectl get sps or kubectl get SiddhiProcesses commands as follows. $ kubectl get sps NAME STATUS READY AGE power-surge-app Running 1/1 5m $ kubectl get SiddhiProcesses NAME STATUS READY AGE power-surge-app Running 1/1 5m View Siddhi process configs Describe the Siddhi process configuration details using kubectl describe sp command as follows. $ kubectl describe sp power-surge-app Name: power-surge-app Namespace: default Labels: none Annotations: kubectl.kubernetes.io/last-applied-configuration={\"apiVersion\":\"siddhi.io/v1alpha2\",\"kind\":\"SiddhiProcess\",\"metadata\":{\"annotations\":{},\"name\":\"power-surge-app\",\"namespace\":\"default\"},\"spec\":{\"apps\":[... API Version: siddhi.io/v1alpha2 Kind: SiddhiProcess Metadata: Creation Timestamp: 2019-07-12T07:12:35Z Generation: 1 Resource Version: 148205 Self Link: /apis/siddhi.io/v1alpha2/namespaces/default/siddhiprocesses/power-surge-app UID: 6c6d90a4-a474-11e9-a05b-080027f4eb25 Spec: Apps: Script: @App:name(\"PowerSurgeDetection\") @App:description(\"App consume events from HTTP as a JSON message of { 'deviceType': 'dryer', 'power': 6000 } format and inserts the events into DevicePowerStream, and alerts the user if the power level is greater than or equal to 600 by printing a message in the log.\") /* Input: deviceType string and powerConsuption int(Watt) Output: Alert user from printing a log, if there is a power surge in the dryer. In other words, notify when power is greater than or equal to 600W. */ @source( type='http', receiver.url='${RECEIVER_URL}', basic.auth.enabled='false', @map(type='json') ) define stream DevicePowerStream(deviceType string, power int); @sink(type='log', prefix='LOGGER') define stream PowerSurgeAlertStream(deviceType string, power int); @info(name='power-filter') from DevicePowerStream[deviceType == 'dryer' and power = 600] select deviceType, power insert into PowerSurgeAlertStream; Container: Env: Name: RECEIVER_URL Value: http://0.0.0.0:8080/checkPower Name: BASIC_AUTH_ENABLED Value: false Image: siddhiio/siddhi-runner-ubuntu:5.1.0 Status: Nodes: nil Ready: 1/1 Status: Running Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal DeploymentCreated 11m siddhiprocess-controller power-surge-app-0 deployment created successfully Normal ServiceCreated 11m siddhiprocess-controller power-surge-app-0 service created successfully View Siddhi process logs To view the Siddhi process logs, first get the Siddhi process pods using the kubectl get pods command as follows. $ kubectl get pods NAME READY STATUS RESTARTS AGE power-surge-app-0-646c4f9dd5-rxzkq 1/1 Running 0 4m siddhi-operator-6698d8f69d-6rfb6 1/1 Running 0 4m Then to retrieve the Siddhi process logs, run kubectl logs pod name command. Here pod name should be replaced with the name of the pod that starts with the relevant SiddhiProcess's name. A sample output logs are of this command is as follows. $ kubectl logs power-surge-app-0-646c4f9dd5-rxzkq ... [2019-07-12 07:12:48,925] INFO {org.wso2.transport.http.netty.contractimpl.listener.ServerConnectorBootstrap$HttpServerConnector} - HTTP(S) Interface starting on host 0.0.0.0 and port 9443 [2019-07-12 07:12:48,927] INFO {org.wso2.transport.http.netty.contractimpl.listener.ServerConnectorBootstrap$HttpServerConnector} - HTTP(S) Interface starting on host 0.0.0.0 and port 9090 [2019-07-12 07:12:48,941] INFO {org.wso2.carbon.kernel.internal.CarbonStartupHandler} - Siddhi Runner Distribution started in 6.853 sec [2019-07-12 07:17:22,219] INFO {io.siddhi.core.stream.output.sink.LogSink} - LOGGER : Event{timestamp=1562915842182, data=[dryer, 60000], isExpired=false} Change the Default Configurations of Siddhi Runner Siddhi runner use SIDDHI_RUNNER_HOME /conf/runner/deployment.yaml file as the default configuration file. In the deployment.yaml the file you can configure data sources that you planned to use, add refs, and enable state persistence, etc. To change the configurations of the deployment.yaml , you can add runner YAML spec like below to your SiddhiProcess YAML file. For example, the following config change will enable file system state persistence. runner: | state.persistence: enabled: true intervalInMin: 1 revisionsToKeep: 2 persistenceStore: io.siddhi.distribution.core.persistence.FileSystemPersistenceStore config: location: siddhi-app-persistence Using a custom-built Siddhi runner image You can change the custom built siddhi-runner image in two ways. Change the image to all the SiddhiProcess deployments. Change the image only for a particular SiddhiProcess deployment. To change the siddhi-runner image for all the SiddhiProcess deployments you can use the siddhi-operator-config config map. You can update siddhiImage to change the image and if you are using a private docker registry/repository, then you can create a Kubernetes secret that contains credentials to the registry and specify that secret name siddhiImageSecret spec. Refer the documentation on creating custom Siddhi runner images bundling additional JARs here . For more details on using docker images from private registries/repositories refer this documentation . apiVersion: v1 kind: ConfigMap metadata: name: siddhi-operator-config data: siddhiHome: /home/siddhi_user/siddhi-runner/ siddhiProfile: runner siddhiImage: YOUR-CUSTOM-IMAGE autoIngressCreation: \"true\" siddhiImageSecret: SECRET If you need to use a custom-built siddhi-runner image for a specific SiddhiProcess deployment, you have to configure container.image spec in the SiddhiProcess . If you are pulling the custom-built image from a private Docker registry/repository, specify the corresponding Kubernetes secret as imagePullSecret argument in the SiddhiProcess YAML file. container: image: YOUR-CUSTOM-IMAGE imagePullSecret: SECRET Deploy and run Siddhi App using config maps Siddhi operator allows you to deploy Siddhi app configurations via config maps instead of just adding them inline. Through this, you can also run multiple Siddhi Apps in a single SiddhiProcess. This can be done by passing the config maps containing Siddhi app files to the SiddhiProcess's apps configuration as follows. apps: - configMap: power-surge-cm1 - configMap: power-surge-cm2 Sample on deploying and running Siddhi Apps via config maps Here we will be creating a very simple Siddhi stream processing application that receives power consumption from several devices in a house. If the power consumption of dryer exceeds the consumption limit of 6000W then that Siddhi app sends an alert from printing a log. @App:name(\"PowerSurgeDetection\") @App:description(\"App consume events from HTTP as a JSON message of { 'deviceType': 'dryer', 'power': 6000 } format and inserts the events into DevicePowerStream, and alerts the user if the power level is greater than or equal to 600 by printing a message in the log.\") /* Input: deviceType string and powerConsuption int(Watt) Output: Alert user from printing a log, if there is a power surge in the dryer. In other words, notify when power is greater than or equal to 600W. */ @source( type='http', receiver.url='${RECEIVER_URL}', basic.auth.enabled='false', @map(type='json') ) define stream DevicePowerStream(deviceType string, power int); @sink(type='log', prefix='LOGGER') define stream PowerSurgeAlertStream(deviceType string, power int); @info(name='power-filter') from DevicePowerStream[deviceType == 'dryer' and power = 600] select deviceType, power insert into PowerSurgeAlertStream; Siddhi Tooling You can also use the powerful Siddhi Editor to implement and test steam processing applications. Save the above Siddhi App file as PowerSurgeDetection.siddhi , and use this file to create a Kubernetes config map with the name power-surge-cm . This can be achieved by running the following command. kubectl create configmap power-surge-cm --from-file= absolute-file-path /PowerSurgeDetection.siddhi The created config map can be added to SiddhiProcess YAML under the apps entry as follows. apiVersion: siddhi.io/v1alpha2 kind: SiddhiProcess metadata: name: power-surge-app spec: apps: - configMap: power-surge-cm container: env: - name: RECEIVER_URL value: \"http://0.0.0.0:8080/checkPower\" image: \"siddhiio/siddhi-runner-ubuntu:5.1.0\" Save the YAML file as power-surge-app.yaml , and use the following command to deploy the SiddhiProcess. kubectl create -f absolute-yaml-file-path /power-surge-app.yaml Using a config, created from a directory containing multiple Siddhi files SiddhiProcess's apps.configMap configuration also supports a config map that is created from a directory containing multiple Siddhi files. Use kubectl create configmap siddhi-apps --from-file= DIRECTORY_PATH command to create a config map from a directory. Invoke Siddhi Applications To invoke the Siddhi App, first obtain the external IP of the ingress load balancer using kubectl get ingress command as follows. $ kubectl get ingress NAME HOSTS ADDRESS PORTS AGE siddhi siddhi 10.0.2.15 80 2m Then, add the host siddhi and related external IP ( ADDRESS ) to the /etc/hosts file in your machine. Minikube For Minikube, you have to use Minikube IP as the external IP. Hence, run minikube ip command to get the IP of the Minikube cluster. Use the following CURL command to send events to power-surge-app deployed in Kubernetes. curl -X POST \\ http://siddhi/power-surge-app-0/8080/checkPower \\ -H 'Accept: */*' \\ -H 'Content-Type: application/json' \\ -H 'Host: siddhi' \\ -H 'cache-control: no-cache' \\ -d '{ \"deviceType\": \"dryer\", \"power\": 60000 }' View Siddhi Process Logs Since the output of power-surge-app is logged, you can see the output by monitoring the associated pod's logs. To find the power-surge-app pod use the kubectl get pods command. This will list down all the deployed pods. $ kubectl get pods NAME READY STATUS RESTARTS AGE power-surge-app-0-646c4f9dd5-tns7l 1/1 Running 0 2m siddhi-operator-6698d8f69d-6rfb6 1/1 Running 0 8m Here, the pod starting with the SiddhiProcess name (in this case power-surge-app- ) is the pod we need to monitor. To view the logs, run the kubectl logs pod name command. This will show all the Siddhi process logs, along with the filtered output events as given below. $ kubectl logs power-surge-app-0-646c4f9dd5-tns7l ... [2019-07-12 07:50:32,861] INFO {org.wso2.carbon.kernel.internal.CarbonStartupHandler} - Siddhi Runner Distribution started in 8.048 sec [2019-07-12 07:50:32,864] INFO {org.wso2.transport.http.netty.contractimpl.listener.ServerConnectorBootstrap$HttpServerConnector} - HTTP(S) Interface starting on host 0.0.0.0 and port 9443 [2019-07-12 07:50:32,866] INFO {org.wso2.transport.http.netty.contractimpl.listener.ServerConnectorBootstrap$HttpServerConnector} - HTTP(S) Interface starting on host 0.0.0.0 and port 9090 [2019-07-12 07:51:42,488] INFO {io.siddhi.core.stream.output.sink.LogSink} - LOGGER : Event{timestamp=1562917902484, data=[dryer, 60000], isExpired=false} Deploy Siddhi Apps without Ingress creation By default, Siddhi operator creates an NGINX ingress and exposes your HTTP/HTTPS through that ingress. If you need to disable automatic ingress creation, you have to change the autoIngressCreation value in the Siddhi siddhi-operator-config config map to false or null as below. # This config map used to parse configurations to the Siddhi operator. apiVersion: v1 kind: ConfigMap metadata: name: siddhi-operator-config data: siddhiHome: /home/siddhi_user/siddhi-runner/ siddhiProfile: runner siddhiImage: siddhiio/siddhi-runner-alpine:5.1.0 autoIngressCreation: \"false\" Deploy and run Siddhi App with HTTPS Configuring TLS will allow Siddhi ingress NGINX to expose HTTPS endpoints of your Siddhi Apps. To do so, create a Kubernetes secret( siddhi-tls ) and add that to the TLS configuration in siddhi-operator-config config map as given below. ingressTLS: siddhi-tls Sample on deploying and running Siddhi App with HTTPS First, you need to create a certificate using the following commands. For more details about the certificate creation refers this . openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout siddhi.key -out siddhi.crt -subj \"/CN=siddhi/O=siddhi\" After that, create a kubernetes secret called siddhi-tls , which we intended to add to the TLS configurations using the following command. kubectl create secret tls siddhi-tls --key siddhi.key --cert siddhi.crt The created secret then need to be added to the siddhi-operator-config config map as follow. apiVersion: v1 kind: ConfigMap metadata: name: siddhi-operator-config data: siddhiHome: /home/siddhi_user/siddhi-runner/ siddhiProfile: runner siddhiImage: siddhiio/siddhi-runner-ubuntu:5.1.0 autoIngressCreation: \"true\" ingressTLS: siddhi-tls When this is done Siddhi operator will now enable TLS support via the NGINX ingress, and you will be able to access all the HTTPS endpoints. Invoke Siddhi Applications You can use now send the events to following HTTPS endpoint. https://siddhi/power-surge-app-0/8080/checkPower Further, you can use the following CURL command to send a request to the deployed Siddhi applications via HTTPS. curl --cacert siddhi.crt -X POST \\ https://siddhi/power-surge-app-0/8080/checkPower \\ -H 'Accept: */*' \\ -H 'Content-Type: application/json' \\ -H 'Host: siddhi' \\ -H 'cache-control: no-cache' \\ -d '{ \"deviceType\": \"dryer\", \"power\": 60000 }' View Siddhi Process Logs The output logs show the event that you sent using the previous CURL command. $ kubectl get pods NAME READY STATUS RESTARTS AGE power-surge-app-0-646c4f9dd5-kk5md 1/1 Running 0 2m siddhi-operator-6698d8f69d-6rfb6 1/1 Running 0 10m $ kubectl logs monitor-app-667c97c898-rrtfs ... [2019-07-12 09:06:15,173] INFO {org.wso2.transport.http.netty.contractimpl.listener.ServerConnectorBootstrap$HttpServerConnector} - HTTP(S) Interface starting on host 0.0.0.0 and port 9443 [2019-07-12 09:06:15,184] INFO {org.wso2.transport.http.netty.contractimpl.listener.ServerConnectorBootstrap$HttpServerConnector} - HTTP(S) Interface starting on host 0.0.0.0 and port 9090 [2019-07-12 09:06:15,187] INFO {org.wso2.carbon.kernel.internal.CarbonStartupHandler} - Siddhi Runner Distribution started in 10.819 sec [2019-07-12 09:07:50,098] INFO {io.siddhi.core.stream.output.sink.LogSink} - LOGGER : Event{timestamp=1562922470093, data=[dryer, 60000], isExpired=false} Externally publish data to NATS from Siddhi The default ingress creation of the Siddhi operator allows accessing HTTP/HTTPS endpoints externally. By default, it will not support TCP endpoints. Sometimes you may have some TCP endpoints to configure like NATS and Kafka sources and access those endpoints externally. @source(type='nats', @map(type='text'), destination='SP_NATS_INPUT_TEST', bootstrap.servers='nats://localhost:4222',client.id='nats_client',server.id='test-cluster',queue.group.name = 'group_nats',durable.name = 'nats-durable',subscription.sequence = '100') define stream inputStream (name string, age int, country string); To access these TCP connections externally you can do it as in the following example. First, you have to disable automatic ingress creation in the Siddhi operator . Then you have to manually create ingress and enable the TCP configurations. To enable TCP configurations in NGINX ingress please refer to this documentation . To create NATS cluster you will need a NATS spec like below. apiVersion: nats.io/v1alpha2 kind: NatsCluster metadata: name: nats-siddhi spec: size: 1 Save this yaml as nats-cluster.yaml and deploy it using kubeclt . $ kubeclt apply -f nats-cluster.yaml Likewise, create a nats streaming cluster as below. apiVersion: streaming.nats.io/v1alpha1 kind: NatsStreamingCluster metadata: name: stan-siddhi spec: size: 1 natsSvc: nats-siddhi Save this yaml as stan-cluster.yaml and deploy it using kubeclt . $ kubeclt apply -f stan-cluster.yaml Now you can deploy the following Siddhi app that contained a NATS source. apiVersion: siddhi.io/v1alpha2 kind: SiddhiProcess metadata: name: power-consume-app spec: apps: - script: | @App:name(\"PowerConsumptionSurgeDetection\") @App:description(\"App consumes events from NATS as a text message of { 'deviceType': 'dryer', 'power': 6000 } format and inserts the events into DevicePowerStream, and alerts the user if the power consumption in 1 minute is greater than or equal to 10000W by printing a message in the log for every 30 seconds.\") /* Input: deviceType string and powerConsuption int(Joules) Output: Alert user from printing a log, if there is a power surge in the dryer within 1 minute period. Notify the user in every 30 seconds when total power consumption is greater than or equal to 10000W in 1 minute time period. */ @source( type='nats', cluster.id='siddhi-stan', destination = 'PowerStream', bootstrap.servers='nats://siddhi-nats:4222', @map(type='text') ) define stream DevicePowerStream(deviceType string, power int); @sink(type='log', prefix='LOGGER') define stream PowerSurgeAlertStream(deviceType string, powerConsumed long); @info(name='surge-detector') from DevicePowerStream#window.time(1 min) select deviceType, sum(power) as powerConsumed group by deviceType having powerConsumed 10000 output every 30 sec insert into PowerSurgeAlertStream; container: image: \"siddhiio/siddhi-runner-ubuntu:5.1.0\" persistentVolumeClaim: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi storageClassName: standard volumeMode: Filesystem runner: | state.persistence: enabled: true intervalInMin: 1 revisionsToKeep: 2 persistenceStore: io.siddhi.distribution.core.persistence.FileSystemPersistenceStore config: location: siddhi-app-persistence Save this yaml as power-consume-app.yaml and deploy it using kubeclt . $ kubeclt apply -f power-consume-app.yaml This commands will create Kubernetes artifacts like below. $ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 none 443/TCP 12d power-consume-app-0 ClusterIP 10.99.148.217 none 4222/TCP 5m siddhi-nats ClusterIP 10.105.250.215 none 4222/TCP 5m siddhi-nats-mgmt ClusterIP None none 6222/TCP,8222/TCP,7777/TCP 5m siddhi-operator ClusterIP 10.102.251.237 none 8383/TCP 5m $ kubectl get pods NAME READY STATUS RESTARTS AGE nats-operator-b8f4977fc-8gnjd 1/1 Running 0 5m nats-streaming-operator-64b565bcc7-r9rpw 1/1 Running 0 5m power-consume-app-0-84f6774bd8-jl95w 1/1 Running 0 5m siddhi-nats-1 1/1 Running 0 5m siddhi-operator-6c6c5d8fcc-hvl7j 1/1 Running 0 5m siddhi-stan-1 1/1 Running 0 5m Now you have to create an ingress for the siddhi-nats service. apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: siddhi-nats annotations: nginx.ingress.kubernetes.io/rewrite-target: / spec: rules: - http: paths: - path: /nats backend: serviceName: siddhi-nats servicePort: 4222 Save this yaml as siddhi-nats.yaml and deploy it using kubeclt . $ kubeclt apply -f siddhi-nats.yaml Now you can send messages directly to the NATS streaming server that running on your Kubernetes cluster. You have to send those messages to nats:// KUBERNETES_CLUSTER_IP :4222 URI. To send messages to this NATS streaming cluster you can use a Siddhi app that has NATS sink or samples provided by NATS. Minikube External TCP Access The TCP configuration change that described in the ingress NGINX documentation occurred connection refused problems in Minikube. To configure TCP external access properly in Minikube please refer to the steps described in this comment . Deploy and run Siddhi App in Distributed Mode Siddhi apps can be in two different types. Stateless Siddhi apps Stateful Siddhi apps The deployment of the stateful Siddhi apps follows distributed architecture to ensure high availability. The fully distributed deployment of Siddhi apps will be handle using Siddhi distributed annotations. Without Messaging System With Messaging System Without Distributed Annotations Case 1 : The given Siddhi app will be deployed in a stateless mode in a single kubernetes deployment. Case 2 : If given Siddhi app contains stateful queries then the Siddhi app divided into two partial Siddhi apps (passthrough and process) and deployed in two kubernetes deployments. Use the configured messaging system to communicate between two apps. With Distributed Annotations Case 3 : WIP(Work In Progress) Case 4 : WIP(Work In Progress) The previously described Siddhi app deployments fall under this Case 1 category. The following sample will cover the Siddhi app deployments which fall under Case 2. Sample on deploying and running Siddhi App with a Messaging System The Siddhi operator currently supports NATS as the messaging system. Therefore it is prerequisite to deploying NATS operator and NATS streaming operator in your kubernetes cluster before you install the Siddhi app. Refer this documentation to install NATS operator and NATS streaming operator. Install the Siddhi operator . Create a persistence volume in your cluster. Now we need a NATS cluster and NATS streaming cluster to run the Siddhi app deployment. For this, there are two cases handled by the operator. User can create NATS cluster and NATS streaming cluster as described in this documentation . Specify cluster details in the YAML file like following. messagingSystem: type: nats config: bootstrapServers: - \"nats://example-nats:4222\" clusterId: example-stan If the user only specifies messaging system as NATS like below then Siddhi operator will automatically create NATS cluster( siddhi-nats ) and NATS streaming cluster( siddhi-stan ), and connect two partial apps. messagingSystem: type: nats Before installing a Siddhi app you have to check that all prerequisites(Siddhi-operator, nats-operator, and nats-streaming-operator) up and running perfectly like below. $ kubectl get deployments NAME READY UP-TO-DATE AVAILABLE AGE nats-operator 1/1 1 1 5m nats-streaming-operator 1/1 1 1 5m siddhi-operator 1/1 1 1 5m Now you need to specify a YAML file like below to create stateful Siddhi app deployment. apiVersion: siddhi.io/v1alpha2 kind: SiddhiProcess metadata: name: power-consume-app spec: apps: - script: | @App:name(\"PowerConsumptionSurgeDetection\") @App:description(\"App consumes events from HTTP as a JSON message of { 'deviceType': 'dryer', 'power': 6000 } format and inserts the events into DevicePowerStream, and alerts the user if the power consumption in 1 minute is greater than or equal to 10000W by printing a message in the log for every 30 seconds.\") /* Input: deviceType string and powerConsuption int(Joules) Output: Alert user from printing a log, if there is a power surge in the dryer within 1 minute period. Notify the user in every 30 seconds when total power consumption is greater than or equal to 10000W in 1 minute time period. */ @source( type='http', receiver.url='${RECEIVER_URL}', basic.auth.enabled='false', @map(type='json') ) define stream DevicePowerStream(deviceType string, power int); @sink(type='log', prefix='LOGGER') define stream PowerSurgeAlertStream(deviceType string, powerConsumed long); @info(name='power-consumption-window') from DevicePowerStream#window.time(1 min) select deviceType, sum(power) as powerConsumed group by deviceType having powerConsumed 10000 output every 30 sec insert into PowerSurgeAlertStream; container: env: - name: RECEIVER_URL value: \"http://0.0.0.0:8080/checkPower\" - name: BASIC_AUTH_ENABLED value: \"false\" image: \"siddhiio/siddhi-runner-ubuntu:5.1.0\" messagingSystem: type: nats persistentVolumeClaim: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi storageClassName: standard volumeMode: Filesystem runner: | state.persistence: enabled: true intervalInMin: 1 revisionsToKeep: 2 persistenceStore: io.siddhi.distribution.core.persistence.FileSystemPersistenceStore config: location: siddhi-app-persistence Save this YAML as power-consume-app.yaml as use kubectl to deploy the app. kubectl apply -f power-consume-app.yaml This kubectl execution in the Siddhi operator will do the following tasks. Create a NATS cluster and streaming cluster since the user did not specify it. Parse the given Siddhi app and create two partial Siddhi apps(passthrough and process). Then deploy both apps in separate deployments to distribute I/O time. Check health of the Siddhi runner and make deployments up and running. Create a service for passthrough app. Create an ingress rule that maps to passthrough service. After a successful deployment, your kubernetes cluster should have these artifacts. $ kubectl get SiddhiProcesses NAME STATUS READY AGE power-consume-app Running 2/2 5m $ kubectl get deployments NAME READY UP-TO-DATE AVAILABLE AGE nats-operator 1/1 1 1 10m nats-streaming-operator 1/1 1 1 10m power-consume-app-0 1/1 1 1 5m power-consume-app-1 1/1 1 1 5m siddhi-operator 1/1 1 1 10m $ kubectl get service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 none 443/TCP 2d7h power-consume-app-0 ClusterIP 10.105.67.227 none 8080/TCP 5m siddhi-nats ClusterIP 10.100.205.21 none 4222/TCP 10m siddhi-nats-mgmt ClusterIP None none 6222/TCP,8222/TCP,7777/TCP 10m siddhi-operator ClusterIP 10.103.229.109 none 8383/TCP 10m $ kubectl get ingress NAME HOSTS ADDRESS PORTS AGE siddhi siddhi 10.0.2.15 80 10m $ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE siddhi-pv 1Gi RWO Recycle Bound default/power-consume-app-1-pvc standard 10m $ kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE power-consume-app-1-pvc Bound siddhi-pv 1Gi RWO standard 5m Here power-consume-app-0 is the passthrough deployment and power-consume-app-1 is the process deployment. Now you can send an HTTP request to the passthrough app. curl -X POST \\ http://siddhi/power-consume-app-0/8080/checkPower \\ -H 'Accept: */*' \\ -H 'Content-Type: application/json' \\ -H 'Host: siddhi' \\ -d '{ \"deviceType\": \"dryer\", \"power\": 60000 }' The process app logs will show that event. $ kubectl get pods NAME READY STATUS RESTARTS AGE nats-operator-dd7f4945f-x4vf8 1/1 Running 0 10m nats-streaming-operator-6fbb6695ff-9rmlx 1/1 Running 0 10m power-consume-app-0-7486b87979-6tccx 1/1 Running 0 5m power-consume-app-1-588996fcfb-prncj 1/1 Running 0 5m siddhi-nats-1 1/1 Running 0 5m siddhi-operator-6698d8f69d-w2kvj 1/1 Running 0 10m siddhi-stan-1 1/1 Running 1 5m $ kubectl logs power-consume-app-1-588996fcfb-prncj JAVA_HOME environment variable is set to /opt/java/openjdk CARBON_HOME environment variable is set to /home/siddhi_user/siddhi-runner RUNTIME_HOME environment variable is set to /home/siddhi_user/siddhi-runner/wso2/runner Picked up JAVA_TOOL_OPTIONS: -XX:+UnlockExperimentalVMOptions -XX:+UseCGroupMemoryLimitForHeap [2019-07-12 14:09:16,648] INFO {org.wso2.carbon.launcher.extensions.OSGiLibBundleDeployerUtils updateOSGiLib} - Successfully updated the OSGi bundle information of Carbon Runtime: runner ... [2019-07-12 14:12:04,969] INFO {io.siddhi.core.stream.output.sink.LogSink} - LOGGER : Event{timestamp=1562940716559, data=[dryer, 60000], isExpired=false}","title":"Siddhi Kubernetes Microservice"},{"location":"docs/siddhi-as-a-kubernetes-microservice/#siddhi-51-as-a-kubernetes-microservice","text":"This section provides information on running Siddhi Apps natively in Kubernetes via Siddhi Kubernetes Operator. Siddhi can be configured using SiddhiProcess kind and passed to the Siddhi operator for deployment. Here, the Siddhi applications containing stream processing logic can be written inline in SiddhiProcess yaml or passed as .siddhi files via contig maps. SiddhiProcess yaml can also be configured with the necessary system configurations.","title":"Siddhi 5.1 as a Kubernetes Microservice"},{"location":"docs/siddhi-as-a-kubernetes-microservice/#prerequisites","text":"A Kubernetes cluster v1.10.11 or higher. Minikube Google Kubernetes Engine(GKE) Cluster Docker for Mac Or any other Kubernetes cluster Distributed deployment of Siddhi apps need NATS operator and NATS streaming operator . Admin privileges to install Siddhi operator Minikube Siddhi operator automatically creates NGINX ingress. Therefore it to work we can either enable ingress on Minikube using the following command. minikube addons enable ingress or disable Siddhi operator's automatically ingress creation . Google Kubernetes Engine (GKE) Cluster To install Siddhi operator, you have to give cluster admin permission to your account. In order to do that execute the following command (by replacing \"your-address@email.com\" with your account email address). kubectl create clusterrolebinding user-cluster-admin-binding --clusterrole=cluster-admin --user=your-address@email.com Docker for Mac Siddhi operator automatically creates NGINX ingress. Therefore it to work we can either enable ingress on Docker for mac following the official documentation or disable Siddhi operator's automatically ingress creation . Port Forwarding for Testing Debugging Purposes Instead of creating ingress you can enable port forwarding ( kubectl port-forward ) to access the application in the Kubernetes cluster. This will help a lot for TCP connections as well. kubectl port-forward svc/mysql-db 13306:3306 For more details please refer this Kubernetes official documentation","title":"Prerequisites"},{"location":"docs/siddhi-as-a-kubernetes-microservice/#install-siddhi-operator","text":"To install the Siddhi Kubernetes operator run the following commands. kubectl apply -f https://github.com/siddhi-io/siddhi-operator/releases/download/v0.2.0/00-prereqs.yaml kubectl apply -f https://github.com/siddhi-io/siddhi-operator/releases/download/v0.2.0/01-siddhi-operator.yaml You can verify the installation by making sure the following deployments are running in your Kubernetes cluster. $ kubectl get deployment NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE siddhi-operator 1 1 1 1 1m Using a custom-built Siddhi runner image If you need to use a custom-built siddhi-runner image for all the SiddhiProcess deployments, you have to configure siddhiRunnerImage entry in siddhi-operator-config config map. Refer the documentation on creating custom Siddhi runner images bundling additional JARs here . If you are pulling the custom-built image from a private Docker registry/repository, specify the corresponding kubernetes secret as siddhiRunnerImageSecret entry in siddhi-operator-config config map. For more details on using docker images from private registries/repositories refer this documentation .","title":"Install Siddhi Operator"},{"location":"docs/siddhi-as-a-kubernetes-microservice/#deploy-and-run-siddhi-app","text":"Siddhi applications can be deployed on Kubernetes using the Siddhi operator. Here we will be creating a very simple Siddhi stream processing application that receives power consumption from several devices in a house. If the power consumption of dryer exceeds the consumption limit of 6000W then that Siddhi app sends an alert from printing a log. This can be created using a SiddhiProcess YAML file as given below. apiVersion: siddhi.io/v1alpha2 kind: SiddhiProcess metadata: name: power-surge-app spec: apps: - script: | @App:name(\"PowerSurgeDetection\") @App:description(\"App consume events from HTTP as a JSON message of { 'deviceType': 'dryer', 'power': 6000 } format and inserts the events into DevicePowerStream, and alerts the user if the power level is greater than or equal to 600 by printing a message in the log.\") /* Input: deviceType string and powerConsuption int(Watt) Output: Alert user from printing a log, if there is a power surge in the dryer. In other words, notify when power is greater than or equal to 600W. */ @source( type='http', receiver.url='${RECEIVER_URL}', basic.auth.enabled='false', @map(type='json') ) define stream DevicePowerStream(deviceType string, power int); @sink(type='log', prefix='LOGGER') define stream PowerSurgeAlertStream(deviceType string, power int); @info(name='power-filter') from DevicePowerStream[deviceType == 'dryer' and power = 600] select deviceType, power insert into PowerSurgeAlertStream; container: env: - name: RECEIVER_URL value: \"http://0.0.0.0:8080/checkPower\" image: \"siddhiio/siddhi-runner-ubuntu:5.1.0\" Always listen on 0.0.0.0 with the Siddhi Application running inside a container environment. If you listen on localhost inside the container, nothing outside the container can connect to your application. Siddhi Tooling You can also use the powerful Siddhi Editor to implement and test steam processing applications. Configuring Siddhi To configure databases, extensions, authentication, periodic state persistence, and statistics for Siddhi as Kubernetes Microservice refer Siddhi Config Guide . To deploy the above Siddhi app in your Kubernetes cluster, copy above YAML to a file with name power-surge-app.yaml and execute the following command. kubectl create -f absolute-yaml-file-path /power-surge-app.yaml TLS secret Within the SiddhiProcess, a TLS secret named siddhi-tls is configured. If a Kubernetes secret with the same name does not exist in the Kubernetes cluster, the NGINX will ignore it and use a self-generated certificate. Configuring a secret will be necessary for calling HTTPS endpoints, refer deploy and run Siddhi apps with HTTPS section for more details. If the power-surge-app is deployed successfully, it should create SiddhiProcess, deployment, service, and ingress as following. $ kubectl get SiddhiProcesses NAME STATUS READY AGE power-surge-app Running 1/1 2m $ kubectl get deployment NAME READY UP-TO-DATE AVAILABLE AGE power-surge-app-0 1/1 1 1 2m siddhi-operator 1/1 1 1 2m $ kubectl get service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 none 443/TCP 2d power-surge-app-0 ClusterIP 10.96.44.182 none 8080/TCP 2m siddhi-operator ClusterIP 10.98.78.238 none 8383/TCP 2m $ kubectl get ingress NAME HOSTS ADDRESS PORTS AGE siddhi siddhi 10.0.2.15 80 2m Invoke Siddhi Applications To invoke the Siddhi App, obtain the external IP of the ingress load balancer using kubectl get ingress command as following. $ kubectl get ingress NAME HOSTS ADDRESS PORTS AGE siddhi siddhi 10.0.2.15 80 2m Then, add the host siddhi and related external IP ( ADDRESS ) to the /etc/hosts file in your machine. Minikube For Minikube, you have to use Minikube IP as the external IP. Hence, run minikube ip command to get the IP of the Minikube cluster. Docker for Mac For Docker for Mac, you have to use 0.0.0.0 as the external IP. Use the following CURL command to send events to power-surge-app deployed in Kubernetes. curl -X POST \\ http://siddhi/power-surge-app-0/8080/checkPower \\ -H 'Accept: */*' \\ -H 'Content-Type: application/json' \\ -H 'Host: siddhi' \\ -d '{ \"deviceType\": \"dryer\", \"power\": 60000 }' View Siddhi Process Logs Since the output of power-surge-app is logged, you can see the output by monitoring the associated pod's logs. To find the power-surge-app pod use the kubectl get pods command. This will list down all the deployed pods. $ kubectl get pods NAME READY STATUS RESTARTS AGE power-surge-app-0-646c4f9dd5-rxzkq 1/1 Running 0 4m siddhi-operator-6698d8f69d-6rfb6 1/1 Running 0 4m Here, the pod starting with the SiddhiProcess name (in this case power-surge-app- ) is the pod we need to monitor. To view the logs, run the kubectl logs pod name command. This will show all the Siddhi process logs, along with the filtered output events as given below. $ kubectl logs power-surge-app-0-646c4f9dd5-rxzkq ... [2019-07-12 07:12:48,925] INFO {org.wso2.transport.http.netty.contractimpl.listener.ServerConnectorBootstrap$HttpServerConnector} - HTTP(S) Interface starting on host 0.0.0.0 and port 9443 [2019-07-12 07:12:48,927] INFO {org.wso2.transport.http.netty.contractimpl.listener.ServerConnectorBootstrap$HttpServerConnector} - HTTP(S) Interface starting on host 0.0.0.0 and port 9090 [2019-07-12 07:12:48,941] INFO {org.wso2.carbon.kernel.internal.CarbonStartupHandler} - Siddhi Runner Distribution started in 6.853 sec [2019-07-12 07:17:22,219] INFO {io.siddhi.core.stream.output.sink.LogSink} - LOGGER : Event{timestamp=1562915842182, data=[dryer, 60000], isExpired=false}","title":"Deploy and run Siddhi App"},{"location":"docs/siddhi-as-a-kubernetes-microservice/#get-siddhi-process-status","text":"","title":"Get Siddhi process status"},{"location":"docs/siddhi-as-a-kubernetes-microservice/#list-siddhi-processes","text":"List the Siddhi process using the kubectl get sps or kubectl get SiddhiProcesses commands as follows. $ kubectl get sps NAME STATUS READY AGE power-surge-app Running 1/1 5m $ kubectl get SiddhiProcesses NAME STATUS READY AGE power-surge-app Running 1/1 5m","title":"List Siddhi processes"},{"location":"docs/siddhi-as-a-kubernetes-microservice/#view-siddhi-process-configs","text":"Describe the Siddhi process configuration details using kubectl describe sp command as follows. $ kubectl describe sp power-surge-app Name: power-surge-app Namespace: default Labels: none Annotations: kubectl.kubernetes.io/last-applied-configuration={\"apiVersion\":\"siddhi.io/v1alpha2\",\"kind\":\"SiddhiProcess\",\"metadata\":{\"annotations\":{},\"name\":\"power-surge-app\",\"namespace\":\"default\"},\"spec\":{\"apps\":[... API Version: siddhi.io/v1alpha2 Kind: SiddhiProcess Metadata: Creation Timestamp: 2019-07-12T07:12:35Z Generation: 1 Resource Version: 148205 Self Link: /apis/siddhi.io/v1alpha2/namespaces/default/siddhiprocesses/power-surge-app UID: 6c6d90a4-a474-11e9-a05b-080027f4eb25 Spec: Apps: Script: @App:name(\"PowerSurgeDetection\") @App:description(\"App consume events from HTTP as a JSON message of { 'deviceType': 'dryer', 'power': 6000 } format and inserts the events into DevicePowerStream, and alerts the user if the power level is greater than or equal to 600 by printing a message in the log.\") /* Input: deviceType string and powerConsuption int(Watt) Output: Alert user from printing a log, if there is a power surge in the dryer. In other words, notify when power is greater than or equal to 600W. */ @source( type='http', receiver.url='${RECEIVER_URL}', basic.auth.enabled='false', @map(type='json') ) define stream DevicePowerStream(deviceType string, power int); @sink(type='log', prefix='LOGGER') define stream PowerSurgeAlertStream(deviceType string, power int); @info(name='power-filter') from DevicePowerStream[deviceType == 'dryer' and power = 600] select deviceType, power insert into PowerSurgeAlertStream; Container: Env: Name: RECEIVER_URL Value: http://0.0.0.0:8080/checkPower Name: BASIC_AUTH_ENABLED Value: false Image: siddhiio/siddhi-runner-ubuntu:5.1.0 Status: Nodes: nil Ready: 1/1 Status: Running Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal DeploymentCreated 11m siddhiprocess-controller power-surge-app-0 deployment created successfully Normal ServiceCreated 11m siddhiprocess-controller power-surge-app-0 service created successfully","title":"View Siddhi process configs"},{"location":"docs/siddhi-as-a-kubernetes-microservice/#view-siddhi-process-logs","text":"To view the Siddhi process logs, first get the Siddhi process pods using the kubectl get pods command as follows. $ kubectl get pods NAME READY STATUS RESTARTS AGE power-surge-app-0-646c4f9dd5-rxzkq 1/1 Running 0 4m siddhi-operator-6698d8f69d-6rfb6 1/1 Running 0 4m Then to retrieve the Siddhi process logs, run kubectl logs pod name command. Here pod name should be replaced with the name of the pod that starts with the relevant SiddhiProcess's name. A sample output logs are of this command is as follows. $ kubectl logs power-surge-app-0-646c4f9dd5-rxzkq ... [2019-07-12 07:12:48,925] INFO {org.wso2.transport.http.netty.contractimpl.listener.ServerConnectorBootstrap$HttpServerConnector} - HTTP(S) Interface starting on host 0.0.0.0 and port 9443 [2019-07-12 07:12:48,927] INFO {org.wso2.transport.http.netty.contractimpl.listener.ServerConnectorBootstrap$HttpServerConnector} - HTTP(S) Interface starting on host 0.0.0.0 and port 9090 [2019-07-12 07:12:48,941] INFO {org.wso2.carbon.kernel.internal.CarbonStartupHandler} - Siddhi Runner Distribution started in 6.853 sec [2019-07-12 07:17:22,219] INFO {io.siddhi.core.stream.output.sink.LogSink} - LOGGER : Event{timestamp=1562915842182, data=[dryer, 60000], isExpired=false}","title":"View Siddhi process logs"},{"location":"docs/siddhi-as-a-kubernetes-microservice/#change-the-default-configurations-of-siddhi-runner","text":"Siddhi runner use SIDDHI_RUNNER_HOME /conf/runner/deployment.yaml file as the default configuration file. In the deployment.yaml the file you can configure data sources that you planned to use, add refs, and enable state persistence, etc. To change the configurations of the deployment.yaml , you can add runner YAML spec like below to your SiddhiProcess YAML file. For example, the following config change will enable file system state persistence. runner: | state.persistence: enabled: true intervalInMin: 1 revisionsToKeep: 2 persistenceStore: io.siddhi.distribution.core.persistence.FileSystemPersistenceStore config: location: siddhi-app-persistence","title":"Change the Default Configurations of Siddhi Runner"},{"location":"docs/siddhi-as-a-kubernetes-microservice/#using-a-custom-built-siddhi-runner-image","text":"You can change the custom built siddhi-runner image in two ways. Change the image to all the SiddhiProcess deployments. Change the image only for a particular SiddhiProcess deployment. To change the siddhi-runner image for all the SiddhiProcess deployments you can use the siddhi-operator-config config map. You can update siddhiImage to change the image and if you are using a private docker registry/repository, then you can create a Kubernetes secret that contains credentials to the registry and specify that secret name siddhiImageSecret spec. Refer the documentation on creating custom Siddhi runner images bundling additional JARs here . For more details on using docker images from private registries/repositories refer this documentation . apiVersion: v1 kind: ConfigMap metadata: name: siddhi-operator-config data: siddhiHome: /home/siddhi_user/siddhi-runner/ siddhiProfile: runner siddhiImage: YOUR-CUSTOM-IMAGE autoIngressCreation: \"true\" siddhiImageSecret: SECRET If you need to use a custom-built siddhi-runner image for a specific SiddhiProcess deployment, you have to configure container.image spec in the SiddhiProcess . If you are pulling the custom-built image from a private Docker registry/repository, specify the corresponding Kubernetes secret as imagePullSecret argument in the SiddhiProcess YAML file. container: image: YOUR-CUSTOM-IMAGE imagePullSecret: SECRET","title":"Using a custom-built Siddhi runner image"},{"location":"docs/siddhi-as-a-kubernetes-microservice/#deploy-and-run-siddhi-app-using-config-maps","text":"Siddhi operator allows you to deploy Siddhi app configurations via config maps instead of just adding them inline. Through this, you can also run multiple Siddhi Apps in a single SiddhiProcess. This can be done by passing the config maps containing Siddhi app files to the SiddhiProcess's apps configuration as follows. apps: - configMap: power-surge-cm1 - configMap: power-surge-cm2 Sample on deploying and running Siddhi Apps via config maps Here we will be creating a very simple Siddhi stream processing application that receives power consumption from several devices in a house. If the power consumption of dryer exceeds the consumption limit of 6000W then that Siddhi app sends an alert from printing a log. @App:name(\"PowerSurgeDetection\") @App:description(\"App consume events from HTTP as a JSON message of { 'deviceType': 'dryer', 'power': 6000 } format and inserts the events into DevicePowerStream, and alerts the user if the power level is greater than or equal to 600 by printing a message in the log.\") /* Input: deviceType string and powerConsuption int(Watt) Output: Alert user from printing a log, if there is a power surge in the dryer. In other words, notify when power is greater than or equal to 600W. */ @source( type='http', receiver.url='${RECEIVER_URL}', basic.auth.enabled='false', @map(type='json') ) define stream DevicePowerStream(deviceType string, power int); @sink(type='log', prefix='LOGGER') define stream PowerSurgeAlertStream(deviceType string, power int); @info(name='power-filter') from DevicePowerStream[deviceType == 'dryer' and power = 600] select deviceType, power insert into PowerSurgeAlertStream; Siddhi Tooling You can also use the powerful Siddhi Editor to implement and test steam processing applications. Save the above Siddhi App file as PowerSurgeDetection.siddhi , and use this file to create a Kubernetes config map with the name power-surge-cm . This can be achieved by running the following command. kubectl create configmap power-surge-cm --from-file= absolute-file-path /PowerSurgeDetection.siddhi The created config map can be added to SiddhiProcess YAML under the apps entry as follows. apiVersion: siddhi.io/v1alpha2 kind: SiddhiProcess metadata: name: power-surge-app spec: apps: - configMap: power-surge-cm container: env: - name: RECEIVER_URL value: \"http://0.0.0.0:8080/checkPower\" image: \"siddhiio/siddhi-runner-ubuntu:5.1.0\" Save the YAML file as power-surge-app.yaml , and use the following command to deploy the SiddhiProcess. kubectl create -f absolute-yaml-file-path /power-surge-app.yaml Using a config, created from a directory containing multiple Siddhi files SiddhiProcess's apps.configMap configuration also supports a config map that is created from a directory containing multiple Siddhi files. Use kubectl create configmap siddhi-apps --from-file= DIRECTORY_PATH command to create a config map from a directory. Invoke Siddhi Applications To invoke the Siddhi App, first obtain the external IP of the ingress load balancer using kubectl get ingress command as follows. $ kubectl get ingress NAME HOSTS ADDRESS PORTS AGE siddhi siddhi 10.0.2.15 80 2m Then, add the host siddhi and related external IP ( ADDRESS ) to the /etc/hosts file in your machine. Minikube For Minikube, you have to use Minikube IP as the external IP. Hence, run minikube ip command to get the IP of the Minikube cluster. Use the following CURL command to send events to power-surge-app deployed in Kubernetes. curl -X POST \\ http://siddhi/power-surge-app-0/8080/checkPower \\ -H 'Accept: */*' \\ -H 'Content-Type: application/json' \\ -H 'Host: siddhi' \\ -H 'cache-control: no-cache' \\ -d '{ \"deviceType\": \"dryer\", \"power\": 60000 }' View Siddhi Process Logs Since the output of power-surge-app is logged, you can see the output by monitoring the associated pod's logs. To find the power-surge-app pod use the kubectl get pods command. This will list down all the deployed pods. $ kubectl get pods NAME READY STATUS RESTARTS AGE power-surge-app-0-646c4f9dd5-tns7l 1/1 Running 0 2m siddhi-operator-6698d8f69d-6rfb6 1/1 Running 0 8m Here, the pod starting with the SiddhiProcess name (in this case power-surge-app- ) is the pod we need to monitor. To view the logs, run the kubectl logs pod name command. This will show all the Siddhi process logs, along with the filtered output events as given below. $ kubectl logs power-surge-app-0-646c4f9dd5-tns7l ... [2019-07-12 07:50:32,861] INFO {org.wso2.carbon.kernel.internal.CarbonStartupHandler} - Siddhi Runner Distribution started in 8.048 sec [2019-07-12 07:50:32,864] INFO {org.wso2.transport.http.netty.contractimpl.listener.ServerConnectorBootstrap$HttpServerConnector} - HTTP(S) Interface starting on host 0.0.0.0 and port 9443 [2019-07-12 07:50:32,866] INFO {org.wso2.transport.http.netty.contractimpl.listener.ServerConnectorBootstrap$HttpServerConnector} - HTTP(S) Interface starting on host 0.0.0.0 and port 9090 [2019-07-12 07:51:42,488] INFO {io.siddhi.core.stream.output.sink.LogSink} - LOGGER : Event{timestamp=1562917902484, data=[dryer, 60000], isExpired=false}","title":"Deploy and run Siddhi App using config maps"},{"location":"docs/siddhi-as-a-kubernetes-microservice/#deploy-siddhi-apps-without-ingress-creation","text":"By default, Siddhi operator creates an NGINX ingress and exposes your HTTP/HTTPS through that ingress. If you need to disable automatic ingress creation, you have to change the autoIngressCreation value in the Siddhi siddhi-operator-config config map to false or null as below. # This config map used to parse configurations to the Siddhi operator. apiVersion: v1 kind: ConfigMap metadata: name: siddhi-operator-config data: siddhiHome: /home/siddhi_user/siddhi-runner/ siddhiProfile: runner siddhiImage: siddhiio/siddhi-runner-alpine:5.1.0 autoIngressCreation: \"false\"","title":"Deploy Siddhi Apps without Ingress creation"},{"location":"docs/siddhi-as-a-kubernetes-microservice/#deploy-and-run-siddhi-app-with-https","text":"Configuring TLS will allow Siddhi ingress NGINX to expose HTTPS endpoints of your Siddhi Apps. To do so, create a Kubernetes secret( siddhi-tls ) and add that to the TLS configuration in siddhi-operator-config config map as given below. ingressTLS: siddhi-tls Sample on deploying and running Siddhi App with HTTPS First, you need to create a certificate using the following commands. For more details about the certificate creation refers this . openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout siddhi.key -out siddhi.crt -subj \"/CN=siddhi/O=siddhi\" After that, create a kubernetes secret called siddhi-tls , which we intended to add to the TLS configurations using the following command. kubectl create secret tls siddhi-tls --key siddhi.key --cert siddhi.crt The created secret then need to be added to the siddhi-operator-config config map as follow. apiVersion: v1 kind: ConfigMap metadata: name: siddhi-operator-config data: siddhiHome: /home/siddhi_user/siddhi-runner/ siddhiProfile: runner siddhiImage: siddhiio/siddhi-runner-ubuntu:5.1.0 autoIngressCreation: \"true\" ingressTLS: siddhi-tls When this is done Siddhi operator will now enable TLS support via the NGINX ingress, and you will be able to access all the HTTPS endpoints. Invoke Siddhi Applications You can use now send the events to following HTTPS endpoint. https://siddhi/power-surge-app-0/8080/checkPower Further, you can use the following CURL command to send a request to the deployed Siddhi applications via HTTPS. curl --cacert siddhi.crt -X POST \\ https://siddhi/power-surge-app-0/8080/checkPower \\ -H 'Accept: */*' \\ -H 'Content-Type: application/json' \\ -H 'Host: siddhi' \\ -H 'cache-control: no-cache' \\ -d '{ \"deviceType\": \"dryer\", \"power\": 60000 }' View Siddhi Process Logs The output logs show the event that you sent using the previous CURL command. $ kubectl get pods NAME READY STATUS RESTARTS AGE power-surge-app-0-646c4f9dd5-kk5md 1/1 Running 0 2m siddhi-operator-6698d8f69d-6rfb6 1/1 Running 0 10m $ kubectl logs monitor-app-667c97c898-rrtfs ... [2019-07-12 09:06:15,173] INFO {org.wso2.transport.http.netty.contractimpl.listener.ServerConnectorBootstrap$HttpServerConnector} - HTTP(S) Interface starting on host 0.0.0.0 and port 9443 [2019-07-12 09:06:15,184] INFO {org.wso2.transport.http.netty.contractimpl.listener.ServerConnectorBootstrap$HttpServerConnector} - HTTP(S) Interface starting on host 0.0.0.0 and port 9090 [2019-07-12 09:06:15,187] INFO {org.wso2.carbon.kernel.internal.CarbonStartupHandler} - Siddhi Runner Distribution started in 10.819 sec [2019-07-12 09:07:50,098] INFO {io.siddhi.core.stream.output.sink.LogSink} - LOGGER : Event{timestamp=1562922470093, data=[dryer, 60000], isExpired=false}","title":"Deploy and run Siddhi App with HTTPS"},{"location":"docs/siddhi-as-a-kubernetes-microservice/#externally-publish-data-to-nats-from-siddhi","text":"The default ingress creation of the Siddhi operator allows accessing HTTP/HTTPS endpoints externally. By default, it will not support TCP endpoints. Sometimes you may have some TCP endpoints to configure like NATS and Kafka sources and access those endpoints externally. @source(type='nats', @map(type='text'), destination='SP_NATS_INPUT_TEST', bootstrap.servers='nats://localhost:4222',client.id='nats_client',server.id='test-cluster',queue.group.name = 'group_nats',durable.name = 'nats-durable',subscription.sequence = '100') define stream inputStream (name string, age int, country string); To access these TCP connections externally you can do it as in the following example. First, you have to disable automatic ingress creation in the Siddhi operator . Then you have to manually create ingress and enable the TCP configurations. To enable TCP configurations in NGINX ingress please refer to this documentation . To create NATS cluster you will need a NATS spec like below. apiVersion: nats.io/v1alpha2 kind: NatsCluster metadata: name: nats-siddhi spec: size: 1 Save this yaml as nats-cluster.yaml and deploy it using kubeclt . $ kubeclt apply -f nats-cluster.yaml Likewise, create a nats streaming cluster as below. apiVersion: streaming.nats.io/v1alpha1 kind: NatsStreamingCluster metadata: name: stan-siddhi spec: size: 1 natsSvc: nats-siddhi Save this yaml as stan-cluster.yaml and deploy it using kubeclt . $ kubeclt apply -f stan-cluster.yaml Now you can deploy the following Siddhi app that contained a NATS source. apiVersion: siddhi.io/v1alpha2 kind: SiddhiProcess metadata: name: power-consume-app spec: apps: - script: | @App:name(\"PowerConsumptionSurgeDetection\") @App:description(\"App consumes events from NATS as a text message of { 'deviceType': 'dryer', 'power': 6000 } format and inserts the events into DevicePowerStream, and alerts the user if the power consumption in 1 minute is greater than or equal to 10000W by printing a message in the log for every 30 seconds.\") /* Input: deviceType string and powerConsuption int(Joules) Output: Alert user from printing a log, if there is a power surge in the dryer within 1 minute period. Notify the user in every 30 seconds when total power consumption is greater than or equal to 10000W in 1 minute time period. */ @source( type='nats', cluster.id='siddhi-stan', destination = 'PowerStream', bootstrap.servers='nats://siddhi-nats:4222', @map(type='text') ) define stream DevicePowerStream(deviceType string, power int); @sink(type='log', prefix='LOGGER') define stream PowerSurgeAlertStream(deviceType string, powerConsumed long); @info(name='surge-detector') from DevicePowerStream#window.time(1 min) select deviceType, sum(power) as powerConsumed group by deviceType having powerConsumed 10000 output every 30 sec insert into PowerSurgeAlertStream; container: image: \"siddhiio/siddhi-runner-ubuntu:5.1.0\" persistentVolumeClaim: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi storageClassName: standard volumeMode: Filesystem runner: | state.persistence: enabled: true intervalInMin: 1 revisionsToKeep: 2 persistenceStore: io.siddhi.distribution.core.persistence.FileSystemPersistenceStore config: location: siddhi-app-persistence Save this yaml as power-consume-app.yaml and deploy it using kubeclt . $ kubeclt apply -f power-consume-app.yaml This commands will create Kubernetes artifacts like below. $ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 none 443/TCP 12d power-consume-app-0 ClusterIP 10.99.148.217 none 4222/TCP 5m siddhi-nats ClusterIP 10.105.250.215 none 4222/TCP 5m siddhi-nats-mgmt ClusterIP None none 6222/TCP,8222/TCP,7777/TCP 5m siddhi-operator ClusterIP 10.102.251.237 none 8383/TCP 5m $ kubectl get pods NAME READY STATUS RESTARTS AGE nats-operator-b8f4977fc-8gnjd 1/1 Running 0 5m nats-streaming-operator-64b565bcc7-r9rpw 1/1 Running 0 5m power-consume-app-0-84f6774bd8-jl95w 1/1 Running 0 5m siddhi-nats-1 1/1 Running 0 5m siddhi-operator-6c6c5d8fcc-hvl7j 1/1 Running 0 5m siddhi-stan-1 1/1 Running 0 5m Now you have to create an ingress for the siddhi-nats service. apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: siddhi-nats annotations: nginx.ingress.kubernetes.io/rewrite-target: / spec: rules: - http: paths: - path: /nats backend: serviceName: siddhi-nats servicePort: 4222 Save this yaml as siddhi-nats.yaml and deploy it using kubeclt . $ kubeclt apply -f siddhi-nats.yaml Now you can send messages directly to the NATS streaming server that running on your Kubernetes cluster. You have to send those messages to nats:// KUBERNETES_CLUSTER_IP :4222 URI. To send messages to this NATS streaming cluster you can use a Siddhi app that has NATS sink or samples provided by NATS. Minikube External TCP Access The TCP configuration change that described in the ingress NGINX documentation occurred connection refused problems in Minikube. To configure TCP external access properly in Minikube please refer to the steps described in this comment .","title":"Externally publish data to NATS from Siddhi"},{"location":"docs/siddhi-as-a-kubernetes-microservice/#deploy-and-run-siddhi-app-in-distributed-mode","text":"Siddhi apps can be in two different types. Stateless Siddhi apps Stateful Siddhi apps The deployment of the stateful Siddhi apps follows distributed architecture to ensure high availability. The fully distributed deployment of Siddhi apps will be handle using Siddhi distributed annotations. Without Messaging System With Messaging System Without Distributed Annotations Case 1 : The given Siddhi app will be deployed in a stateless mode in a single kubernetes deployment. Case 2 : If given Siddhi app contains stateful queries then the Siddhi app divided into two partial Siddhi apps (passthrough and process) and deployed in two kubernetes deployments. Use the configured messaging system to communicate between two apps. With Distributed Annotations Case 3 : WIP(Work In Progress) Case 4 : WIP(Work In Progress) The previously described Siddhi app deployments fall under this Case 1 category. The following sample will cover the Siddhi app deployments which fall under Case 2. Sample on deploying and running Siddhi App with a Messaging System The Siddhi operator currently supports NATS as the messaging system. Therefore it is prerequisite to deploying NATS operator and NATS streaming operator in your kubernetes cluster before you install the Siddhi app. Refer this documentation to install NATS operator and NATS streaming operator. Install the Siddhi operator . Create a persistence volume in your cluster. Now we need a NATS cluster and NATS streaming cluster to run the Siddhi app deployment. For this, there are two cases handled by the operator. User can create NATS cluster and NATS streaming cluster as described in this documentation . Specify cluster details in the YAML file like following. messagingSystem: type: nats config: bootstrapServers: - \"nats://example-nats:4222\" clusterId: example-stan If the user only specifies messaging system as NATS like below then Siddhi operator will automatically create NATS cluster( siddhi-nats ) and NATS streaming cluster( siddhi-stan ), and connect two partial apps. messagingSystem: type: nats Before installing a Siddhi app you have to check that all prerequisites(Siddhi-operator, nats-operator, and nats-streaming-operator) up and running perfectly like below. $ kubectl get deployments NAME READY UP-TO-DATE AVAILABLE AGE nats-operator 1/1 1 1 5m nats-streaming-operator 1/1 1 1 5m siddhi-operator 1/1 1 1 5m Now you need to specify a YAML file like below to create stateful Siddhi app deployment. apiVersion: siddhi.io/v1alpha2 kind: SiddhiProcess metadata: name: power-consume-app spec: apps: - script: | @App:name(\"PowerConsumptionSurgeDetection\") @App:description(\"App consumes events from HTTP as a JSON message of { 'deviceType': 'dryer', 'power': 6000 } format and inserts the events into DevicePowerStream, and alerts the user if the power consumption in 1 minute is greater than or equal to 10000W by printing a message in the log for every 30 seconds.\") /* Input: deviceType string and powerConsuption int(Joules) Output: Alert user from printing a log, if there is a power surge in the dryer within 1 minute period. Notify the user in every 30 seconds when total power consumption is greater than or equal to 10000W in 1 minute time period. */ @source( type='http', receiver.url='${RECEIVER_URL}', basic.auth.enabled='false', @map(type='json') ) define stream DevicePowerStream(deviceType string, power int); @sink(type='log', prefix='LOGGER') define stream PowerSurgeAlertStream(deviceType string, powerConsumed long); @info(name='power-consumption-window') from DevicePowerStream#window.time(1 min) select deviceType, sum(power) as powerConsumed group by deviceType having powerConsumed 10000 output every 30 sec insert into PowerSurgeAlertStream; container: env: - name: RECEIVER_URL value: \"http://0.0.0.0:8080/checkPower\" - name: BASIC_AUTH_ENABLED value: \"false\" image: \"siddhiio/siddhi-runner-ubuntu:5.1.0\" messagingSystem: type: nats persistentVolumeClaim: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi storageClassName: standard volumeMode: Filesystem runner: | state.persistence: enabled: true intervalInMin: 1 revisionsToKeep: 2 persistenceStore: io.siddhi.distribution.core.persistence.FileSystemPersistenceStore config: location: siddhi-app-persistence Save this YAML as power-consume-app.yaml as use kubectl to deploy the app. kubectl apply -f power-consume-app.yaml This kubectl execution in the Siddhi operator will do the following tasks. Create a NATS cluster and streaming cluster since the user did not specify it. Parse the given Siddhi app and create two partial Siddhi apps(passthrough and process). Then deploy both apps in separate deployments to distribute I/O time. Check health of the Siddhi runner and make deployments up and running. Create a service for passthrough app. Create an ingress rule that maps to passthrough service. After a successful deployment, your kubernetes cluster should have these artifacts. $ kubectl get SiddhiProcesses NAME STATUS READY AGE power-consume-app Running 2/2 5m $ kubectl get deployments NAME READY UP-TO-DATE AVAILABLE AGE nats-operator 1/1 1 1 10m nats-streaming-operator 1/1 1 1 10m power-consume-app-0 1/1 1 1 5m power-consume-app-1 1/1 1 1 5m siddhi-operator 1/1 1 1 10m $ kubectl get service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 none 443/TCP 2d7h power-consume-app-0 ClusterIP 10.105.67.227 none 8080/TCP 5m siddhi-nats ClusterIP 10.100.205.21 none 4222/TCP 10m siddhi-nats-mgmt ClusterIP None none 6222/TCP,8222/TCP,7777/TCP 10m siddhi-operator ClusterIP 10.103.229.109 none 8383/TCP 10m $ kubectl get ingress NAME HOSTS ADDRESS PORTS AGE siddhi siddhi 10.0.2.15 80 10m $ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE siddhi-pv 1Gi RWO Recycle Bound default/power-consume-app-1-pvc standard 10m $ kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE power-consume-app-1-pvc Bound siddhi-pv 1Gi RWO standard 5m Here power-consume-app-0 is the passthrough deployment and power-consume-app-1 is the process deployment. Now you can send an HTTP request to the passthrough app. curl -X POST \\ http://siddhi/power-consume-app-0/8080/checkPower \\ -H 'Accept: */*' \\ -H 'Content-Type: application/json' \\ -H 'Host: siddhi' \\ -d '{ \"deviceType\": \"dryer\", \"power\": 60000 }' The process app logs will show that event. $ kubectl get pods NAME READY STATUS RESTARTS AGE nats-operator-dd7f4945f-x4vf8 1/1 Running 0 10m nats-streaming-operator-6fbb6695ff-9rmlx 1/1 Running 0 10m power-consume-app-0-7486b87979-6tccx 1/1 Running 0 5m power-consume-app-1-588996fcfb-prncj 1/1 Running 0 5m siddhi-nats-1 1/1 Running 0 5m siddhi-operator-6698d8f69d-w2kvj 1/1 Running 0 10m siddhi-stan-1 1/1 Running 1 5m $ kubectl logs power-consume-app-1-588996fcfb-prncj JAVA_HOME environment variable is set to /opt/java/openjdk CARBON_HOME environment variable is set to /home/siddhi_user/siddhi-runner RUNTIME_HOME environment variable is set to /home/siddhi_user/siddhi-runner/wso2/runner Picked up JAVA_TOOL_OPTIONS: -XX:+UnlockExperimentalVMOptions -XX:+UseCGroupMemoryLimitForHeap [2019-07-12 14:09:16,648] INFO {org.wso2.carbon.launcher.extensions.OSGiLibBundleDeployerUtils updateOSGiLib} - Successfully updated the OSGi bundle information of Carbon Runtime: runner ... [2019-07-12 14:12:04,969] INFO {io.siddhi.core.stream.output.sink.LogSink} - LOGGER : Event{timestamp=1562940716559, data=[dryer, 60000], isExpired=false}","title":"Deploy and run Siddhi App in Distributed Mode"},{"location":"docs/siddhi-as-a-local-microservice/","text":"Siddhi 5.1 as a Local Microservice This section provides information on running Siddhi Apps on Bare Metal or VM. Siddhi Microservice can run one or more Siddhi Applications with required system configurations. Here, the Siddhi application ( .siddhi file) contains stream processing logic and the necessary system configurations can be passed via the Siddhi configuration .yaml file. Steps to Run Siddhi Local Microservice is as follows. Download the latest Siddhi Runner distribution Unzip the siddhi-runner-x.x.x.zip Start SiddhiApps with the runner config by executing the following commands from the distribution directory Linux/Mac : ./bin/runner.sh -Dapps= siddhi-file -Dconfig= config-yaml-file Windows : bin\\runner.bat -Dapps= siddhi-file -Dconfig= config-yaml-file Running Multiple SiddhiApps in one runner. To run multiple SiddhiApps in one runtime, have all SiddhiApps in a directory and pass its location through -Dapps parameter as follows, -Dapps= siddhi-apps-directory Always use absolute path for SiddhiApps and runner configs. Providing absolute path of SiddhiApp file, or directory in -Dapps parameter, and when providing the Siddhi runner config yaml on -Dconfig parameter while starting Siddhi runner. Siddhi Tooling You can also use the powerful Siddhi Editor to implement and test steam processing applications. Configuring Siddhi To configure databases, extensions, authentication, periodic state persistence, and statistics for Siddhi as Local Microservice refer Siddhi Config Guide . Samples Running Siddhi App Following SiddhiApp collects events via HTTP and logs the number of events arrived during last 15 seconds. @App:name('CountOverTime') @App:description('Receive events via HTTP, and logs the number of events received during last 15 seconds') @source(type = 'http', receiver.url = \"http://0.0.0.0:8006/production\", @map(type = 'json')) define stream ProductionStream (name string, amount double); @sink(type = 'log') define stream TotalCountStream (totalCount long); -- Count the incoming events @info(name = 'query1') from ProductionStream#window.time(15 sec) select count() as totalCount insert into TotalCountStream; Copy the above SiddhiApp, and create the SiddhiApp file CountOverTime.siddhi Run the SiddhiApp by executing following commands from the distribution directory Linux/Mac : ./bin/runner.sh -Dapps= absolute-siddhi-file-path /CountOverTime.siddhi Windows : bin\\runner.bat -Dapps= absolute-siddhi-file-path \\CountOverTime.siddhi Test the SiddhiApp by calling the HTTP endpoint using curl or Postman as follows, Publish events with curl command: curl -X POST http://localhost:8006/production \\ -header \"Content-Type:application/json\" \\ -d '{\"event\":{\"name\":\"Cake\",\"amount\":20.12}}' Publish events with Postman: Install 'Postman' application from Chrome web store Launch the application Make a 'Post' request to 'http://localhost:8006/production' endpoint. Set the Content-Type to 'application/json' and set the request body in json format as follows, { \"event\": { \"name\": \"Cake\", \"amount\": 20.12 } } Runner logs the total count on the console. Note, how the count increments with every event sent. [2019-04-11 13:36:03,517 ] INFO {io.siddhi.core.stream.output.sink.LogSink} - CountOverTime : TotalCountStream : Event{timestamp=1554969963512, data=[1], isExpired=false} [2019-04-11 13:36:10,267] INFO {io.siddhi.core.stream.output.sink.LogSink} - CountOverTime : TotalCountStream : Event{timestamp=1554969970267, data=[2], isExpired=false} [2019-04-11 13:36:41,694] INFO {io.siddhi.core.stream.output.sink.LogSink} - CountOverTime : TotalCountStream : Event{timestamp=1554970001694, data=[1], isExpired=false} Running with runner config When running SiddhiApps users can optionally provide a config yaml to Siddhi runner to manage configurations such as state persistence, databases connections and secure vault. Following SiddhiApp collects events via HTTP and store them in H2 Database. @App:name(\"ConsumeAndStore\") @App:description(\"Consume events from HTTP and write to TEST_DB\") @source(type = 'http', receiver.url = \"http://0.0.0.0:8006/production\", @map(type = 'json')) define stream ProductionStream (name string, amount double); @store(type='rdbms', datasource='TEST_DB') define table ProductionTable (name string, amount double); -- Store all events to the table @info(name = 'query1') from ProductionStream insert into ProductionTable; The runner config can by configured with the relevant datasource information and passed when starting the runner dataSources: - name: TEST_DB description: The datasource used for testing definition: type: RDBMS configuration: jdbcUrl: 'jdbc:h2:${sys:carbon.home}/wso2/${sys:wso2.runtime}/database/TEST_DB;DB_CLOSE_ON_EXIT=FALSE;LOCK_TIMEOUT=60000' username: admin password: admin driverClassName: org.h2.Driver maxPoolSize: 10 idleTimeout: 60000 connectionTestQuery: SELECT 1 validationTimeout: 30000 isAutoCommit: false Copy the above SiddhiApp, & config yaml, and create corresponding the SiddhiApp file ConsumeAndStore.siddhi and TestDb.yaml files. Run the SiddhiApp by executing following commands from the distribution directory Linux/Mac : ./bin/runner.sh -Dapps= absolute-siddhi-file-path /ConsumeAndStore.siddhi \\ -Dconfig= absolute-config-yaml-path /TestDb.yaml Windows : bin\\runner.sh -Dapps= absolute-siddhi-file-path \\ConsumeAndStore.siddhi ^ -Dconfig= absolute-config-yaml-path \\TestDb.yaml Add data to H2 Database by calling the Siddhi Input stream HTTP endpoint Publish events with curl command: Publish few json to the http endpoint as follows, curl -X POST http://localhost:8006/production \\ --header \"Content-Type:application/json\" \\ -d '{\"event\":{\"name\":\"Cake\",\"amount\":20.12}}' Publish events with Postman: Install 'Postman' application from Chrome web store Launch the application Make a 'Post' request to 'http://localhost:8006/production' endpoint. Set the Content-Type to 'application/json' and set the request body in json format as follows, { \"event\": { \"name\": \"Cake\", \"amount\": 20.12 } } Query Siddhi Store APIs to retrieve 10 records from the table. Query stored events with curl command: Publish few json to the http endpoint as follows, curl -X POST https://localhost:9443/stores/query \\ -H \"content-type: application/json\" \\ -u \"admin:admin\" \\ -d '{\"appName\" : \"ConsumeAndStore\", \"query\" : \"from ProductionTable select * limit 10;\" }' -k Query stored events with Postman: Install 'Postman' application from Chrome web store Launch the application Make a 'Post' request to 'https://localhost:9443/stores/query' endpoint. Set the Content-Type to 'application/json' and set the request body in json format as follows, { \"appName\" : \"ConsumeAndStore\", \"query\" : \"from ProductionTable select * limit 10;\" } The results of the query will be as follows, { \"records\":[ [\"Cake\",20.12] ] } Running with environmental/system variables Templating SiddhiApps allows users to provide environment/system variables to siddhiApps at runtime. This can help users to migrate SiddhiApps from one environment to another (E.g from dev, test and to prod). Following templated SiddhiApp collects events via HTTP, filters them based on amount greater than a given threshold value, and only sends the filtered events via email. Here the THRESHOLD value, and TO_EMAIL are templated in the TemplatedFilterAndEmail.siddhi SiddhiApp. @App:name(\"TemplatedFilterAndEmail\") @App:description(\"Consumes events from HTTP, filters them based on amount greater than a templated threshold value, and sends filtered events via email.\") @source(type = 'http', receiver.url = \"http://0.0.0.0:8006/production\", @map(type = 'json')) define stream ProductionStream (name string, amount double); @sink(ref = 'email-sink', subject = 'High {{name}} production!', to = '${TO_EMAIL}', content.type = 'text/html', @map(type = 'text', @payload(\"\"\" Hi, br/ br/ High production of b {{name}}, /b with amount b {{amount}} /b identified. br/ br/ For more information please contact production department. br/ br/ Thank you\"\"\"))) define stream FilteredProductionStream (name string, amount double); -- Filters the events based on threshold @info(name = 'query1') from ProductionStream[amount ${THRESHOLD}] insert into FilteredProductionStream; The runner config is configured with a gmail account to send email messages in EmailConfig.yaml by templating sending EMAIL_ADDRESS , EMAIL_USERNAME and EMAIL_PASSWORD . refs: - ref: name: 'email-sink' type: 'email' properties: port: '465' host: 'smtp.gmail.com' ssl.enable: 'true' auth: 'true' ssl.enable: 'true' # User your gmail configurations here address: '${EMAIL_ADDRESS}' #E.g. test@gmail.com username: '${EMAIL_USERNAME}' #E.g. test password: '${EMAIL_PASSWORD}' #E.g. password Copy the above SiddhiApp, & config yaml, and create corresponding the SiddhiApp file TemplatedFilterAndEmail.siddhi and EmailConfig.yaml files. Set environment variables by running following in the termial Siddhi is about to run: export THRESHOLD=20 export TO_EMAIL= to email address export EMAIL_ADDRESS= gmail address export EMAIL_USERNAME= gmail username export EMAIL_PASSWORD= gmail password Or they can also be passed as system variables by adding -DTHRESHOLD=20 -DTO_EMAIL= to email address -DEMAIL_ADDRESS= gmail address -DEMAIL_USERNAME= gmail username -DEMAIL_PASSWORD= gmail password to the end of the runner startup script. Run the SiddhiApp by executing following commands from the distribution directory Linux/Mac : ./bin/runner.sh -Dapps= absolute-file-path /TemplatedFilterAndEmail.siddhi \\ -Dconfig= absolute-config-yaml-path /EmailConfig.yaml Windows : bin\\runner.bat -Dapps= absolute-file-path \\TemplatedFilterAndEmail.siddhi ^ -Dconfig= absolute-config-yaml-path \\EmailConfig.yaml Test the SiddhiApp by calling the HTTP endpoint using curl or Postman as follows Publish events with curl command: Publish few json to the http endpoint as follows, curl -X POST http://localhost:8006/production \\ --header \"Content-Type:application/json\" \\ -d '{\"event\":{\"name\":\"Cake\",\"amount\":2000.0}}' Publish events with Postman: Install 'Postman' application from Chrome web store Launch the application Make a 'Post' request to 'http://localhost:8006/production' endpoint. Set the Content-Type to 'application/json' and set the request body in json format as follows, { \"event\": { \"name\": \"Cake\", \"amount\": 2000.0 } } Check the to.email for the published email message, which will look as follows, Subject : High Cake production! Hi, High production of Cake, with amount 2000.0 identified. For more information please contact production department. Thank you","title":"Siddhi Local Microservice"},{"location":"docs/siddhi-as-a-local-microservice/#siddhi-51-as-a-local-microservice","text":"This section provides information on running Siddhi Apps on Bare Metal or VM. Siddhi Microservice can run one or more Siddhi Applications with required system configurations. Here, the Siddhi application ( .siddhi file) contains stream processing logic and the necessary system configurations can be passed via the Siddhi configuration .yaml file. Steps to Run Siddhi Local Microservice is as follows. Download the latest Siddhi Runner distribution Unzip the siddhi-runner-x.x.x.zip Start SiddhiApps with the runner config by executing the following commands from the distribution directory Linux/Mac : ./bin/runner.sh -Dapps= siddhi-file -Dconfig= config-yaml-file Windows : bin\\runner.bat -Dapps= siddhi-file -Dconfig= config-yaml-file Running Multiple SiddhiApps in one runner. To run multiple SiddhiApps in one runtime, have all SiddhiApps in a directory and pass its location through -Dapps parameter as follows, -Dapps= siddhi-apps-directory Always use absolute path for SiddhiApps and runner configs. Providing absolute path of SiddhiApp file, or directory in -Dapps parameter, and when providing the Siddhi runner config yaml on -Dconfig parameter while starting Siddhi runner. Siddhi Tooling You can also use the powerful Siddhi Editor to implement and test steam processing applications. Configuring Siddhi To configure databases, extensions, authentication, periodic state persistence, and statistics for Siddhi as Local Microservice refer Siddhi Config Guide .","title":"Siddhi 5.1 as a Local Microservice"},{"location":"docs/siddhi-as-a-local-microservice/#samples","text":"","title":"Samples"},{"location":"docs/siddhi-as-a-local-microservice/#running-siddhi-app","text":"Following SiddhiApp collects events via HTTP and logs the number of events arrived during last 15 seconds. @App:name('CountOverTime') @App:description('Receive events via HTTP, and logs the number of events received during last 15 seconds') @source(type = 'http', receiver.url = \"http://0.0.0.0:8006/production\", @map(type = 'json')) define stream ProductionStream (name string, amount double); @sink(type = 'log') define stream TotalCountStream (totalCount long); -- Count the incoming events @info(name = 'query1') from ProductionStream#window.time(15 sec) select count() as totalCount insert into TotalCountStream; Copy the above SiddhiApp, and create the SiddhiApp file CountOverTime.siddhi Run the SiddhiApp by executing following commands from the distribution directory Linux/Mac : ./bin/runner.sh -Dapps= absolute-siddhi-file-path /CountOverTime.siddhi Windows : bin\\runner.bat -Dapps= absolute-siddhi-file-path \\CountOverTime.siddhi Test the SiddhiApp by calling the HTTP endpoint using curl or Postman as follows, Publish events with curl command: curl -X POST http://localhost:8006/production \\ -header \"Content-Type:application/json\" \\ -d '{\"event\":{\"name\":\"Cake\",\"amount\":20.12}}' Publish events with Postman: Install 'Postman' application from Chrome web store Launch the application Make a 'Post' request to 'http://localhost:8006/production' endpoint. Set the Content-Type to 'application/json' and set the request body in json format as follows, { \"event\": { \"name\": \"Cake\", \"amount\": 20.12 } } Runner logs the total count on the console. Note, how the count increments with every event sent. [2019-04-11 13:36:03,517 ] INFO {io.siddhi.core.stream.output.sink.LogSink} - CountOverTime : TotalCountStream : Event{timestamp=1554969963512, data=[1], isExpired=false} [2019-04-11 13:36:10,267] INFO {io.siddhi.core.stream.output.sink.LogSink} - CountOverTime : TotalCountStream : Event{timestamp=1554969970267, data=[2], isExpired=false} [2019-04-11 13:36:41,694] INFO {io.siddhi.core.stream.output.sink.LogSink} - CountOverTime : TotalCountStream : Event{timestamp=1554970001694, data=[1], isExpired=false}","title":"Running Siddhi App"},{"location":"docs/siddhi-as-a-local-microservice/#running-with-runner-config","text":"When running SiddhiApps users can optionally provide a config yaml to Siddhi runner to manage configurations such as state persistence, databases connections and secure vault. Following SiddhiApp collects events via HTTP and store them in H2 Database. @App:name(\"ConsumeAndStore\") @App:description(\"Consume events from HTTP and write to TEST_DB\") @source(type = 'http', receiver.url = \"http://0.0.0.0:8006/production\", @map(type = 'json')) define stream ProductionStream (name string, amount double); @store(type='rdbms', datasource='TEST_DB') define table ProductionTable (name string, amount double); -- Store all events to the table @info(name = 'query1') from ProductionStream insert into ProductionTable; The runner config can by configured with the relevant datasource information and passed when starting the runner dataSources: - name: TEST_DB description: The datasource used for testing definition: type: RDBMS configuration: jdbcUrl: 'jdbc:h2:${sys:carbon.home}/wso2/${sys:wso2.runtime}/database/TEST_DB;DB_CLOSE_ON_EXIT=FALSE;LOCK_TIMEOUT=60000' username: admin password: admin driverClassName: org.h2.Driver maxPoolSize: 10 idleTimeout: 60000 connectionTestQuery: SELECT 1 validationTimeout: 30000 isAutoCommit: false Copy the above SiddhiApp, & config yaml, and create corresponding the SiddhiApp file ConsumeAndStore.siddhi and TestDb.yaml files. Run the SiddhiApp by executing following commands from the distribution directory Linux/Mac : ./bin/runner.sh -Dapps= absolute-siddhi-file-path /ConsumeAndStore.siddhi \\ -Dconfig= absolute-config-yaml-path /TestDb.yaml Windows : bin\\runner.sh -Dapps= absolute-siddhi-file-path \\ConsumeAndStore.siddhi ^ -Dconfig= absolute-config-yaml-path \\TestDb.yaml Add data to H2 Database by calling the Siddhi Input stream HTTP endpoint Publish events with curl command: Publish few json to the http endpoint as follows, curl -X POST http://localhost:8006/production \\ --header \"Content-Type:application/json\" \\ -d '{\"event\":{\"name\":\"Cake\",\"amount\":20.12}}' Publish events with Postman: Install 'Postman' application from Chrome web store Launch the application Make a 'Post' request to 'http://localhost:8006/production' endpoint. Set the Content-Type to 'application/json' and set the request body in json format as follows, { \"event\": { \"name\": \"Cake\", \"amount\": 20.12 } } Query Siddhi Store APIs to retrieve 10 records from the table. Query stored events with curl command: Publish few json to the http endpoint as follows, curl -X POST https://localhost:9443/stores/query \\ -H \"content-type: application/json\" \\ -u \"admin:admin\" \\ -d '{\"appName\" : \"ConsumeAndStore\", \"query\" : \"from ProductionTable select * limit 10;\" }' -k Query stored events with Postman: Install 'Postman' application from Chrome web store Launch the application Make a 'Post' request to 'https://localhost:9443/stores/query' endpoint. Set the Content-Type to 'application/json' and set the request body in json format as follows, { \"appName\" : \"ConsumeAndStore\", \"query\" : \"from ProductionTable select * limit 10;\" } The results of the query will be as follows, { \"records\":[ [\"Cake\",20.12] ] }","title":"Running with runner config"},{"location":"docs/siddhi-as-a-local-microservice/#running-with-environmentalsystem-variables","text":"Templating SiddhiApps allows users to provide environment/system variables to siddhiApps at runtime. This can help users to migrate SiddhiApps from one environment to another (E.g from dev, test and to prod). Following templated SiddhiApp collects events via HTTP, filters them based on amount greater than a given threshold value, and only sends the filtered events via email. Here the THRESHOLD value, and TO_EMAIL are templated in the TemplatedFilterAndEmail.siddhi SiddhiApp. @App:name(\"TemplatedFilterAndEmail\") @App:description(\"Consumes events from HTTP, filters them based on amount greater than a templated threshold value, and sends filtered events via email.\") @source(type = 'http', receiver.url = \"http://0.0.0.0:8006/production\", @map(type = 'json')) define stream ProductionStream (name string, amount double); @sink(ref = 'email-sink', subject = 'High {{name}} production!', to = '${TO_EMAIL}', content.type = 'text/html', @map(type = 'text', @payload(\"\"\" Hi, br/ br/ High production of b {{name}}, /b with amount b {{amount}} /b identified. br/ br/ For more information please contact production department. br/ br/ Thank you\"\"\"))) define stream FilteredProductionStream (name string, amount double); -- Filters the events based on threshold @info(name = 'query1') from ProductionStream[amount ${THRESHOLD}] insert into FilteredProductionStream; The runner config is configured with a gmail account to send email messages in EmailConfig.yaml by templating sending EMAIL_ADDRESS , EMAIL_USERNAME and EMAIL_PASSWORD . refs: - ref: name: 'email-sink' type: 'email' properties: port: '465' host: 'smtp.gmail.com' ssl.enable: 'true' auth: 'true' ssl.enable: 'true' # User your gmail configurations here address: '${EMAIL_ADDRESS}' #E.g. test@gmail.com username: '${EMAIL_USERNAME}' #E.g. test password: '${EMAIL_PASSWORD}' #E.g. password Copy the above SiddhiApp, & config yaml, and create corresponding the SiddhiApp file TemplatedFilterAndEmail.siddhi and EmailConfig.yaml files. Set environment variables by running following in the termial Siddhi is about to run: export THRESHOLD=20 export TO_EMAIL= to email address export EMAIL_ADDRESS= gmail address export EMAIL_USERNAME= gmail username export EMAIL_PASSWORD= gmail password Or they can also be passed as system variables by adding -DTHRESHOLD=20 -DTO_EMAIL= to email address -DEMAIL_ADDRESS= gmail address -DEMAIL_USERNAME= gmail username -DEMAIL_PASSWORD= gmail password to the end of the runner startup script. Run the SiddhiApp by executing following commands from the distribution directory Linux/Mac : ./bin/runner.sh -Dapps= absolute-file-path /TemplatedFilterAndEmail.siddhi \\ -Dconfig= absolute-config-yaml-path /EmailConfig.yaml Windows : bin\\runner.bat -Dapps= absolute-file-path \\TemplatedFilterAndEmail.siddhi ^ -Dconfig= absolute-config-yaml-path \\EmailConfig.yaml Test the SiddhiApp by calling the HTTP endpoint using curl or Postman as follows Publish events with curl command: Publish few json to the http endpoint as follows, curl -X POST http://localhost:8006/production \\ --header \"Content-Type:application/json\" \\ -d '{\"event\":{\"name\":\"Cake\",\"amount\":2000.0}}' Publish events with Postman: Install 'Postman' application from Chrome web store Launch the application Make a 'Post' request to 'http://localhost:8006/production' endpoint. Set the Content-Type to 'application/json' and set the request body in json format as follows, { \"event\": { \"name\": \"Cake\", \"amount\": 2000.0 } } Check the to.email for the published email message, which will look as follows, Subject : High Cake production! Hi, High production of Cake, with amount 2000.0 identified. For more information please contact production department. Thank you","title":"Running with environmental/system variables"},{"location":"docs/siddhi-as-a-python-library/","text":"Siddhi 5.1 as a Python library PySiddhi is a Python wrapper for Siddhi which can listens to events from data streams, detects complex conditions described via a Streaming SQL language, and triggers actions. To use Siddhi in Python via PySiddhi and to get a working sample follow the below steps: There are some prerequisites should be met before installing PySiddhi with above command. Prerequisites The following dependencies should be installed prior to installation of library. Linux Python 2.7 or 3.x Cython sudo pip install cython Python Developer Package sudo apt-get install python-dev python3-dev python-dev Oracle Java 8 and set JAVA_HOME path libboost for Python (Only to build from Source) sudo apt-get install libboost-python-dev Maven (Only to build from Source) g++ and other development tools (Only to build from Source) sudo apt-get install build-essential g++ autotools-dev libicu-dev build-essential libbz2-dev libboost-all-dev macOS Install brew Install python using brew Cython sudo pip install cython Oracle Java 8 and set JAVA_HOME path boost for python (Only to build from Source) brew install boost Maven (Only to build from Source) Windows Install Python Cython sudo pip install cython Oracle Java 8 and set JAVA_HOME path Install Visual Studio Build tools (Only to build from Source) Maven (Only to build from Source) Download siddhi-sdk release from here and set the SIDDHISDK_HOME as an environment variable. export SIDDHISDK_HOME=\" path-to-siddhi-sdk \" Download siddhi-python-api-proxy-5.1.0.jar from here and copy to SIDDHISDK_HOME /lib directory Installation The current version is tested with Unix/Linux based operating systems. PySiddhi can be installed using one of the following methods. Install PySiddhi via Python Package Management PySiddhi can be installed using pip. pip install pysiddhi Install PySiddhi from Online Code Using the following PIP command, PySiddhi can be directly installed from online code available in GitHub. pip install git+https://github.com/siddhi-io/PySiddhi.git Note: In case of permission errors, use sudo Install from Downloaded Code Switch to the branch master of PySiddhi. Navigate to source code root directory and execute the following PIP command. pip install . Note the period (.) at end of command. In case of permission errors, use sudo Sample Let's tryout the below sample with PySiddhi to get better understanding about the usage. This sample demonstrating how to write a streaming query to detect stock records having volume less than 150. Step 1 - Initialize libraries and imports Add this file to working directory in order to enable log4j logging. Log4j is used by PrintEvent to generate output. from PySiddhi.DataTypes.LongType import LongType from PySiddhi.core.SiddhiManager import SiddhiManager from PySiddhi.core.query.output.callback.QueryCallback import QueryCallback from PySiddhi.core.util.EventPrinter import PrintEvent from time import sleep Step 2 - Define filter using Siddhi Query siddhiManager = SiddhiManager() # Siddhi Query to filter events with volume less than 150 as output siddhiApp = \"define stream cseEventStream (symbol string, price float, volume long);\" + \\ \"@info(name = 'query1') \" + \\ \"from cseEventStream[volume 150] \" + \\ \"select symbol, price \" + \\ \"insert into outputStream;\" # Generate runtime siddhiAppRuntime = siddhiManager.createSiddhiAppRuntime(siddhiApp) For more details on Siddhi Query Language, refer Siddhi Query Language Guide . Step 3 - Define a listener for filtered events # Add listener to capture output events class QueryCallbackImpl(QueryCallback): def receive(self, timestamp, inEvents, outEvents): PrintEvent(timestamp, inEvents, outEvents) siddhiAppRuntime.addCallback(\"query1\",QueryCallbackImpl()) Step 4 - Test filter query using sample input events # Retrieving input handler to push events into Siddhi inputHandler = siddhiAppRuntime.getInputHandler(\"cseEventStream\") # Starting event processing siddhiAppRuntime.start() # Sending events to Siddhi inputHandler.send([\"IBM\",700.0,LongType(100)]) inputHandler.send([\"WSO2\", 60.5, LongType(200)]) inputHandler.send([\"GOOG\", 50, LongType(30)]) inputHandler.send([\"IBM\", 76.6, LongType(400)]) inputHandler.send([\"WSO2\", 45.6, LongType(50)]) # Wait for response sleep(0.1) Clean Up - Remember to shutdown the Siddhi Manager when your done. siddhiManager.shutdown() Complete Siddhi App Please find the complete sample Siddhi app in below from PySiddhi.DataTypes.LongType import LongType from PySiddhi.core.SiddhiManager import SiddhiManager from PySiddhi.core.query.output.callback.QueryCallback import QueryCallback from PySiddhi.core.util.EventPrinter import PrintEvent from time import sleep siddhiManager = SiddhiManager() # Siddhi Query to filter events with volume less than 150 as output siddhiApp = \"define stream cseEventStream (symbol string, price float, volume long); \" + \\ \"@info(name = 'query1') from cseEventStream[volume 150] select symbol,price insert into outputStream;\" # Generate runtime siddhiAppRuntime = siddhiManager.createSiddhiAppRuntime(siddhiApp) # Add listener to capture output events class QueryCallbackImpl(QueryCallback): def receive(self, timestamp, inEvents, outEvents): PrintEvent(timestamp, inEvents, outEvents) siddhiAppRuntime.addCallback(\"query1\",QueryCallbackImpl()) # Retrieving input handler to push events into Siddhi inputHandler = siddhiAppRuntime.getInputHandler(\"cseEventStream\") # Starting event processing siddhiAppRuntime.start() # Sending events to Siddhi inputHandler.send([\"IBM\",700.0,LongType(100)]) inputHandler.send([\"WSO2\", 60.5, LongType(200)]) inputHandler.send([\"GOOG\", 50, LongType(30)]) inputHandler.send([\"IBM\", 76.6, LongType(400)]) inputHandler.send([\"WSO2\", 45.6, LongType(50)]) # Wait for response sleep(10) siddhiManager.shutdown() Expected Output The 3 events with volume less than 150 are printed in log. INFO EventPrinter - Events{ @timestamp = 1497708406678, inEvents = [Event{timestamp=1497708406678, id=-1, data=[IBM, 700.0], isExpired=false}], RemoveEvents = null } INFO EventPrinter - Events{ @timestamp = 1497708406685, inEvents = [Event{timestamp=1497708406685, id=-1, data=[GOOG, 50], isExpired=false}], RemoveEvents = null } INFO EventPrinter - Events{ @timestamp = 1497708406687, inEvents = [Event{timestamp=1497708406687, id=-1, data=[WSO2, 45.6], isExpired=false}], RemoveEvents = null } Uninstall PySiddhi If the library has been installed as explained above, it could be uninstalled using the following pip command. pip uninstall pysiddhi Refer here to get more details about running Siddhi on Python.","title":"Siddhi Python Library"},{"location":"docs/siddhi-as-a-python-library/#siddhi-51-as-a-python-library","text":"PySiddhi is a Python wrapper for Siddhi which can listens to events from data streams, detects complex conditions described via a Streaming SQL language, and triggers actions. To use Siddhi in Python via PySiddhi and to get a working sample follow the below steps: There are some prerequisites should be met before installing PySiddhi with above command.","title":"Siddhi 5.1 as a Python library"},{"location":"docs/siddhi-as-a-python-library/#prerequisites","text":"The following dependencies should be installed prior to installation of library. Linux Python 2.7 or 3.x Cython sudo pip install cython Python Developer Package sudo apt-get install python-dev python3-dev python-dev Oracle Java 8 and set JAVA_HOME path libboost for Python (Only to build from Source) sudo apt-get install libboost-python-dev Maven (Only to build from Source) g++ and other development tools (Only to build from Source) sudo apt-get install build-essential g++ autotools-dev libicu-dev build-essential libbz2-dev libboost-all-dev macOS Install brew Install python using brew Cython sudo pip install cython Oracle Java 8 and set JAVA_HOME path boost for python (Only to build from Source) brew install boost Maven (Only to build from Source) Windows Install Python Cython sudo pip install cython Oracle Java 8 and set JAVA_HOME path Install Visual Studio Build tools (Only to build from Source) Maven (Only to build from Source) Download siddhi-sdk release from here and set the SIDDHISDK_HOME as an environment variable. export SIDDHISDK_HOME=\" path-to-siddhi-sdk \" Download siddhi-python-api-proxy-5.1.0.jar from here and copy to SIDDHISDK_HOME /lib directory","title":"Prerequisites"},{"location":"docs/siddhi-as-a-python-library/#installation","text":"The current version is tested with Unix/Linux based operating systems. PySiddhi can be installed using one of the following methods.","title":"Installation"},{"location":"docs/siddhi-as-a-python-library/#install-pysiddhi-via-python-package-management","text":"PySiddhi can be installed using pip. pip install pysiddhi","title":"Install PySiddhi via Python Package Management"},{"location":"docs/siddhi-as-a-python-library/#install-pysiddhi-from-online-code","text":"Using the following PIP command, PySiddhi can be directly installed from online code available in GitHub. pip install git+https://github.com/siddhi-io/PySiddhi.git Note: In case of permission errors, use sudo","title":"Install PySiddhi from Online Code"},{"location":"docs/siddhi-as-a-python-library/#install-from-downloaded-code","text":"Switch to the branch master of PySiddhi. Navigate to source code root directory and execute the following PIP command. pip install . Note the period (.) at end of command. In case of permission errors, use sudo","title":"Install from Downloaded Code"},{"location":"docs/siddhi-as-a-python-library/#sample","text":"Let's tryout the below sample with PySiddhi to get better understanding about the usage. This sample demonstrating how to write a streaming query to detect stock records having volume less than 150.","title":"Sample"},{"location":"docs/siddhi-as-a-python-library/#step-1-initialize-libraries-and-imports","text":"Add this file to working directory in order to enable log4j logging. Log4j is used by PrintEvent to generate output. from PySiddhi.DataTypes.LongType import LongType from PySiddhi.core.SiddhiManager import SiddhiManager from PySiddhi.core.query.output.callback.QueryCallback import QueryCallback from PySiddhi.core.util.EventPrinter import PrintEvent from time import sleep","title":"Step 1 - Initialize libraries and imports"},{"location":"docs/siddhi-as-a-python-library/#step-2-define-filter-using-siddhi-query","text":"siddhiManager = SiddhiManager() # Siddhi Query to filter events with volume less than 150 as output siddhiApp = \"define stream cseEventStream (symbol string, price float, volume long);\" + \\ \"@info(name = 'query1') \" + \\ \"from cseEventStream[volume 150] \" + \\ \"select symbol, price \" + \\ \"insert into outputStream;\" # Generate runtime siddhiAppRuntime = siddhiManager.createSiddhiAppRuntime(siddhiApp) For more details on Siddhi Query Language, refer Siddhi Query Language Guide .","title":"Step 2 - Define filter using Siddhi Query"},{"location":"docs/siddhi-as-a-python-library/#step-3-define-a-listener-for-filtered-events","text":"# Add listener to capture output events class QueryCallbackImpl(QueryCallback): def receive(self, timestamp, inEvents, outEvents): PrintEvent(timestamp, inEvents, outEvents) siddhiAppRuntime.addCallback(\"query1\",QueryCallbackImpl())","title":"Step 3 - Define a listener for filtered events"},{"location":"docs/siddhi-as-a-python-library/#step-4-test-filter-query-using-sample-input-events","text":"# Retrieving input handler to push events into Siddhi inputHandler = siddhiAppRuntime.getInputHandler(\"cseEventStream\") # Starting event processing siddhiAppRuntime.start() # Sending events to Siddhi inputHandler.send([\"IBM\",700.0,LongType(100)]) inputHandler.send([\"WSO2\", 60.5, LongType(200)]) inputHandler.send([\"GOOG\", 50, LongType(30)]) inputHandler.send([\"IBM\", 76.6, LongType(400)]) inputHandler.send([\"WSO2\", 45.6, LongType(50)]) # Wait for response sleep(0.1)","title":"Step 4 - Test filter query using sample input events"},{"location":"docs/siddhi-as-a-python-library/#clean-up-remember-to-shutdown-the-siddhi-manager-when-your-done","text":"siddhiManager.shutdown()","title":"Clean Up - Remember to shutdown the Siddhi Manager when your done."},{"location":"docs/siddhi-as-a-python-library/#complete-siddhi-app","text":"Please find the complete sample Siddhi app in below from PySiddhi.DataTypes.LongType import LongType from PySiddhi.core.SiddhiManager import SiddhiManager from PySiddhi.core.query.output.callback.QueryCallback import QueryCallback from PySiddhi.core.util.EventPrinter import PrintEvent from time import sleep siddhiManager = SiddhiManager() # Siddhi Query to filter events with volume less than 150 as output siddhiApp = \"define stream cseEventStream (symbol string, price float, volume long); \" + \\ \"@info(name = 'query1') from cseEventStream[volume 150] select symbol,price insert into outputStream;\" # Generate runtime siddhiAppRuntime = siddhiManager.createSiddhiAppRuntime(siddhiApp) # Add listener to capture output events class QueryCallbackImpl(QueryCallback): def receive(self, timestamp, inEvents, outEvents): PrintEvent(timestamp, inEvents, outEvents) siddhiAppRuntime.addCallback(\"query1\",QueryCallbackImpl()) # Retrieving input handler to push events into Siddhi inputHandler = siddhiAppRuntime.getInputHandler(\"cseEventStream\") # Starting event processing siddhiAppRuntime.start() # Sending events to Siddhi inputHandler.send([\"IBM\",700.0,LongType(100)]) inputHandler.send([\"WSO2\", 60.5, LongType(200)]) inputHandler.send([\"GOOG\", 50, LongType(30)]) inputHandler.send([\"IBM\", 76.6, LongType(400)]) inputHandler.send([\"WSO2\", 45.6, LongType(50)]) # Wait for response sleep(10) siddhiManager.shutdown()","title":"Complete Siddhi App"},{"location":"docs/siddhi-as-a-python-library/#expected-output","text":"The 3 events with volume less than 150 are printed in log. INFO EventPrinter - Events{ @timestamp = 1497708406678, inEvents = [Event{timestamp=1497708406678, id=-1, data=[IBM, 700.0], isExpired=false}], RemoveEvents = null } INFO EventPrinter - Events{ @timestamp = 1497708406685, inEvents = [Event{timestamp=1497708406685, id=-1, data=[GOOG, 50], isExpired=false}], RemoveEvents = null } INFO EventPrinter - Events{ @timestamp = 1497708406687, inEvents = [Event{timestamp=1497708406687, id=-1, data=[WSO2, 45.6], isExpired=false}], RemoveEvents = null }","title":"Expected Output"},{"location":"docs/siddhi-as-a-python-library/#uninstall-pysiddhi","text":"If the library has been installed as explained above, it could be uninstalled using the following pip command. pip uninstall pysiddhi Refer here to get more details about running Siddhi on Python.","title":"Uninstall PySiddhi"},{"location":"docs/tooling/","text":"Siddhi 5.1 Tooling Overview Siddhi provides tooling that supports following features to develop and test stream processing applications: Text Query Editor with syntax highlighting and advanced auto completion support. Graphical Query Editor with drag and drop query building support. Event Simulator to test Siddhi Applications. Graphical Query Editor Text Query Editor Introduction to Siddhi Editor Siddhi App Export Tool for Kubernetes Tooling Configuration Tooling in Local Machine Install Oracle Java SE Development Kit (JDK) version 1.8. Set the JAVA_HOME environment variable. Download the latest tooling distribution from here . Extract the downloaded zip and navigate to TOOLING_HOME /bin . ( TOOLING_HOME refers to the extracted folder). Issue the following command in the command prompt (Windows) / terminal (Linux/Mac) For Windows: tooling.bat For Linux/Mac: ./tooling.sh Tooling in Docker There is a docker image for Siddhi tooling with all the dependencies that required for the Siddhi development. If you are familiar with Docker then you could use it. You find the Siddhi tooling docker images in the docker hub You can issue the below command to run Siddhi docker container. Make sure, you already have a docker installation (Docker for Mac or Docker for Windows or Docker CE or any other docker engines) locally. docker run -it -p 9390:9390 siddhiio/siddhi-tooling:5.1.0 After successfully starting the Siddhi Editor, the terminal should look like as shown below: As you can see, we have exposed the port 9390 to the host machine since you have to access the Tooling web editor through that. After starting the Siddhi Editor, access the Editor GUI by visiting the following link in your browser (Google Chrome is the recommended). This takes you to the Siddhi Editor landing page. http://localhost:9390/editor More info, There is a situation that you wanted to add any external dependencies (such as MySQL client jars), change the configurations and etc. Then, it would be ideal to create few mounting paths for the docker tooling as give below. Mounting path for the editor workspace (eg: workspace ) - To avoid losing Siddhi apps if there are any failures with the docker container. Mounting path to add jars (eg: jars ) - To add any external jars to the Siddhi tooling (for validation and testing purposes). Mounting path to add bundles (eg: bundles ) - To add any external OSGI bundles to the Siddhi tooling (for validation and testing purposes). Mounting path to add configuration files (eg: configs ) - To set any custom configurations or any other resources for the testing purposes. In that case, you can directories locally as per your requirement and issue a similar command as shown below, docker run -it -p 9390:9390 \\ -v absolute path /configs:/artifacts \\ -v absolute path /workspace:/home/siddhi_user/siddhi-tooling/wso2/tooling/deployment/workspace \\ -v absolute path /jars:/home/siddhi_user/siddhi-tooling/jars \\ -v absolute path /bundles:/home/siddhi_user/siddhi-tooling/bundles \\ siddhiio/siddhi-tooling:5.1.0 For example, let's assume that your email configuration as a file that you wanted to use within the Siddhi editor. In that case, you can add the email configuration file (eg: EmailConfig.yaml ) into the configs mounting directory that you have created locally and issue below command to apply the configuration in Siddhi tooling. docker run -it -p 9390:9390 \\ -v absolute path /configs:/artifacts \\ -v absolute path /workspace:/home/siddhi_user/siddhi-tooling/wso2/tooling/deployment/workspace \\ -v absolute path /jars:/home/siddhi_user/siddhi-tooling/jars \\ -v absolute path /bundles:/home/siddhi_user/siddhi-tooling/bundles \\ siddhiio/siddhi-tooling:5.1.0 -Dconfig=/artifacts/EmailConfig.yaml","title":"Tooling"},{"location":"docs/tooling/#siddhi-51-tooling","text":"","title":"Siddhi 5.1 Tooling"},{"location":"docs/tooling/#overview","text":"Siddhi provides tooling that supports following features to develop and test stream processing applications: Text Query Editor with syntax highlighting and advanced auto completion support. Graphical Query Editor with drag and drop query building support. Event Simulator to test Siddhi Applications. Graphical Query Editor Text Query Editor","title":"Overview"},{"location":"docs/tooling/#introduction-to-siddhi-editor","text":"","title":"Introduction to Siddhi Editor"},{"location":"docs/tooling/#siddhi-app-export-tool-for-kubernetes","text":"","title":"Siddhi App Export Tool for Kubernetes"},{"location":"docs/tooling/#tooling-configuration","text":"","title":"Tooling Configuration"},{"location":"docs/tooling/#tooling-in-local-machine","text":"Install Oracle Java SE Development Kit (JDK) version 1.8. Set the JAVA_HOME environment variable. Download the latest tooling distribution from here . Extract the downloaded zip and navigate to TOOLING_HOME /bin . ( TOOLING_HOME refers to the extracted folder). Issue the following command in the command prompt (Windows) / terminal (Linux/Mac) For Windows: tooling.bat For Linux/Mac: ./tooling.sh","title":"Tooling in Local Machine"},{"location":"docs/tooling/#tooling-in-docker","text":"There is a docker image for Siddhi tooling with all the dependencies that required for the Siddhi development. If you are familiar with Docker then you could use it. You find the Siddhi tooling docker images in the docker hub You can issue the below command to run Siddhi docker container. Make sure, you already have a docker installation (Docker for Mac or Docker for Windows or Docker CE or any other docker engines) locally. docker run -it -p 9390:9390 siddhiio/siddhi-tooling:5.1.0 After successfully starting the Siddhi Editor, the terminal should look like as shown below: As you can see, we have exposed the port 9390 to the host machine since you have to access the Tooling web editor through that. After starting the Siddhi Editor, access the Editor GUI by visiting the following link in your browser (Google Chrome is the recommended). This takes you to the Siddhi Editor landing page. http://localhost:9390/editor More info, There is a situation that you wanted to add any external dependencies (such as MySQL client jars), change the configurations and etc. Then, it would be ideal to create few mounting paths for the docker tooling as give below. Mounting path for the editor workspace (eg: workspace ) - To avoid losing Siddhi apps if there are any failures with the docker container. Mounting path to add jars (eg: jars ) - To add any external jars to the Siddhi tooling (for validation and testing purposes). Mounting path to add bundles (eg: bundles ) - To add any external OSGI bundles to the Siddhi tooling (for validation and testing purposes). Mounting path to add configuration files (eg: configs ) - To set any custom configurations or any other resources for the testing purposes. In that case, you can directories locally as per your requirement and issue a similar command as shown below, docker run -it -p 9390:9390 \\ -v absolute path /configs:/artifacts \\ -v absolute path /workspace:/home/siddhi_user/siddhi-tooling/wso2/tooling/deployment/workspace \\ -v absolute path /jars:/home/siddhi_user/siddhi-tooling/jars \\ -v absolute path /bundles:/home/siddhi_user/siddhi-tooling/bundles \\ siddhiio/siddhi-tooling:5.1.0 For example, let's assume that your email configuration as a file that you wanted to use within the Siddhi editor. In that case, you can add the email configuration file (eg: EmailConfig.yaml ) into the configs mounting directory that you have created locally and issue below command to apply the configuration in Siddhi tooling. docker run -it -p 9390:9390 \\ -v absolute path /configs:/artifacts \\ -v absolute path /workspace:/home/siddhi_user/siddhi-tooling/wso2/tooling/deployment/workspace \\ -v absolute path /jars:/home/siddhi_user/siddhi-tooling/jars \\ -v absolute path /bundles:/home/siddhi_user/siddhi-tooling/bundles \\ siddhiio/siddhi-tooling:5.1.0 -Dconfig=/artifacts/EmailConfig.yaml","title":"Tooling in Docker"},{"location":"docs/api/5.1.0/","text":"API Docs - v5.1.0 Core and (Aggregate Function) Returns the results of AND operation for all the events. Origin: siddhi-core:5.1.7 Syntax BOOL and( BOOL arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be AND operation. BOOL No Yes Examples EXAMPLE 1 from cscStream#window.lengthBatch(10) select and(isFraud) as isFraudTransaction insert into alertStream; This will returns the result for AND operation of isFraud values as a boolean value for event chunk expiry by window length batch. avg (Aggregate Function) Calculates the average for all the events. Origin: siddhi-core:5.1.7 Syntax DOUBLE avg( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that need to be averaged. INT LONG DOUBLE FLOAT No Yes Examples EXAMPLE 1 from fooStream#window.timeBatch select avg(temp) as avgTemp insert into barStream; avg(temp) returns the average temp value for all the events based on their arrival and expiry. count (Aggregate Function) Returns the count of all the events. Origin: siddhi-core:5.1.7 Syntax LONG count() LONG count( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg This function accepts one parameter. It can belong to any one of the available types. INT LONG DOUBLE FLOAT STRING BOOL OBJECT Yes Yes Examples EXAMPLE 1 from fooStream#window.timeBatch(10 sec) select count() as count insert into barStream; This will return the count of all the events for time batch in 10 seconds. distinctCount (Aggregate Function) This returns the count of distinct occurrences for a given arg. Origin: siddhi-core:5.1.7 Syntax LONG distinctCount( INT|LONG|DOUBLE|FLOAT|STRING arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The object for which the number of distinct occurences needs to be counted. INT LONG DOUBLE FLOAT STRING No Yes Examples EXAMPLE 1 from fooStream select distinctcount(pageID) as count insert into barStream; distinctcount(pageID) for the following output returns '3' when the available values are as follows. \"WEB_PAGE_1\" \"WEB_PAGE_1\" \"WEB_PAGE_2\" \"WEB_PAGE_3\" \"WEB_PAGE_1\" \"WEB_PAGE_2\" The three distinct occurences identified are 'WEB_PAGE_1', 'WEB_PAGE_2', and 'WEB_PAGE_3'. max (Aggregate Function) Returns the maximum value for all the events. Origin: siddhi-core:5.1.7 Syntax INT|LONG|DOUBLE|FLOAT max( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be compared to find the maximum value. INT LONG DOUBLE FLOAT No Yes Examples EXAMPLE 1 from fooStream#window.timeBatch(10 sec) select max(temp) as maxTemp insert into barStream; max(temp) returns the maximum temp value recorded for all the events based on their arrival and expiry. maxForever (Aggregate Function) This is the attribute aggregator to store the maximum value for a given attribute throughout the lifetime of the query regardless of any windows in-front. Origin: siddhi-core:5.1.7 Syntax INT|LONG|DOUBLE|FLOAT maxForever( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be compared to find the maximum value. INT LONG DOUBLE FLOAT No Yes Examples EXAMPLE 1 from inputStream select maxForever(temp) as max insert into outputStream; maxForever(temp) returns the maximum temp value recorded for all the events throughout the lifetime of the query. min (Aggregate Function) Returns the minimum value for all the events. Origin: siddhi-core:5.1.7 Syntax INT|LONG|DOUBLE|FLOAT min( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be compared to find the minimum value. INT LONG DOUBLE FLOAT No Yes Examples EXAMPLE 1 from inputStream select min(temp) as minTemp insert into outputStream; min(temp) returns the minimum temp value recorded for all the events based on their arrival and expiry. minForever (Aggregate Function) This is the attribute aggregator to store the minimum value for a given attribute throughout the lifetime of the query regardless of any windows in-front. Origin: siddhi-core:5.1.7 Syntax INT|LONG|DOUBLE|FLOAT minForever( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be compared to find the minimum value. INT LONG DOUBLE FLOAT No Yes Examples EXAMPLE 1 from inputStream select minForever(temp) as max insert into outputStream; minForever(temp) returns the minimum temp value recorded for all the events throughoutthe lifetime of the query. or (Aggregate Function) Returns the results of OR operation for all the events. Origin: siddhi-core:5.1.7 Syntax BOOL or( BOOL arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be OR operation. BOOL No Yes Examples EXAMPLE 1 from cscStream#window.lengthBatch(10) select or(isFraud) as isFraudTransaction insert into alertStream; This will returns the result for OR operation of isFraud values as a boolean value for event chunk expiry by window length batch. stdDev (Aggregate Function) Returns the calculated standard deviation for all the events. Origin: siddhi-core:5.1.7 Syntax DOUBLE stdDev( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that should be used to calculate the standard deviation. INT LONG DOUBLE FLOAT No Yes Examples EXAMPLE 1 from inputStream select stddev(temp) as stdTemp insert into outputStream; stddev(temp) returns the calculated standard deviation of temp for all the events based on their arrival and expiry. sum (Aggregate Function) Returns the sum for all the events. Origin: siddhi-core:5.1.7 Syntax LONG|DOUBLE sum( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be summed. INT LONG DOUBLE FLOAT No Yes Examples EXAMPLE 1 from inputStream select sum(volume) as sumOfVolume insert into outputStream; This will returns the sum of volume values as a long value for each event arrival and expiry. unionSet (Aggregate Function) Union multiple sets. This attribute aggregator maintains a union of sets. The given input set is put into the union set and the union set is returned. Origin: siddhi-core:5.1.7 Syntax OBJECT unionSet( OBJECT set) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic set The java.util.Set object that needs to be added into the union set. OBJECT No Yes Examples EXAMPLE 1 from stockStream select createSet(symbol) as initialSet insert into initStream from initStream#window.timeBatch(10 sec) select unionSet(initialSet) as distinctSymbols insert into distinctStockStream; distinctStockStream will return the set object which contains the distinct set of stock symbols received during a sliding window of 10 seconds. UUID (Function) Generates a UUID (Universally Unique Identifier). Origin: siddhi-core:5.1.7 Syntax STRING UUID() Examples EXAMPLE 1 from TempStream select convert(roomNo, 'string') as roomNo, temp, UUID() as messageID insert into RoomTempStream; This will converts a room number to string, introducing a message ID to each event asUUID() returns a34eec40-32c2-44fe-8075-7f4fde2e2dd8 from TempStream select convert(roomNo, 'string') as roomNo, temp, UUID() as messageID insert into RoomTempStream; cast (Function) Converts the first parameter according to the cast.to parameter. Incompatible arguments cause Class Cast exceptions if further processed. This function is used with map extension that returns attributes of the object type. You can use this function to cast the object to an accurate and concrete type. Origin: siddhi-core:5.1.7 Syntax INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT cast( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT to.be.caster, STRING cast.to) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic to.be.caster This specifies the attribute to be casted. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No Yes cast.to A string constant parameter expressing the cast to type using one of the following strings values: int, long, float, double, string, bool. STRING No Yes Examples EXAMPLE 1 from fooStream select symbol as name, cast(temp, 'double') as temp insert into barStream; This will cast the fooStream temp field value into 'double' format. coalesce (Function) Returns the value of the first input parameter that is not null, and all input parameters have to be on the same type. Origin: siddhi-core:5.1.7 Syntax INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT coalesce( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg, INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT ...) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg This function accepts one or more parameters. They can belong to any one of the available types. All the specified parameters should be of the same type. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No Yes Examples EXAMPLE 1 from fooStream select coalesce('123', null, '789') as value insert into barStream; This will returns first null value 123. EXAMPLE 2 from fooStream select coalesce(null, 76, 567) as value insert into barStream; This will returns first null value 76. EXAMPLE 3 from fooStream select coalesce(null, null, null) as value insert into barStream; This will returns null as there are no notnull values. convert (Function) Converts the first input parameter according to the convertedTo parameter. Origin: siddhi-core:5.1.7 Syntax INT|LONG|DOUBLE|FLOAT|STRING|BOOL convert( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT to.be.converted, STRING converted.to) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic to.be.converted This specifies the value to be converted. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No Yes converted.to A string constant parameter to which type the attribute need to be converted using one of the following strings values: 'int', 'long', 'float', 'double', 'string', 'bool'. STRING No Yes Examples EXAMPLE 1 from fooStream select convert(temp, 'double') as temp insert into barStream; This will convert fooStream temp value into 'double'. EXAMPLE 2 from fooStream select convert(temp, 'int') as temp insert into barStream; This will convert fooStream temp value into 'int' (value = \"convert(45.9, 'int') returns 46\"). createSet (Function) Includes the given input parameter in a java.util.HashSet and returns the set. Origin: siddhi-core:5.1.7 Syntax OBJECT createSet( INT|LONG|DOUBLE|FLOAT|STRING|BOOL input) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input The input that needs to be added into the set. INT LONG DOUBLE FLOAT STRING BOOL No Yes Examples EXAMPLE 1 from stockStream select createSet(symbol) as initialSet insert into initStream; For every incoming stockStream event, the initStream stream will produce a set object having only one element: the symbol in the incoming stockStream. currentTimeMillis (Function) Returns the current timestamp of siddhi application in milliseconds. Origin: siddhi-core:5.1.7 Syntax LONG currentTimeMillis() Examples EXAMPLE 1 from fooStream select symbol as name, currentTimeMillis() as eventTimestamp insert into barStream; This will extract current siddhi application timestamp. default (Function) Checks if the 'attribute' parameter is null and if so returns the value of the 'default' parameter Origin: siddhi-core:5.1.7 Syntax INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT default( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT attribute, INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT default) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic attribute The attribute that could be null. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No Yes default The default value that will be used when 'attribute' parameter is null INT LONG DOUBLE FLOAT STRING BOOL OBJECT No Yes Examples EXAMPLE 1 from TempStream select default(temp, 0.0) as temp, roomNum insert into StandardTempStream; This will replace TempStream's temp attribute with default value if the temp is null. eventTimestamp (Function) Returns the timestamp of the processed event. Origin: siddhi-core:5.1.7 Syntax LONG eventTimestamp() Examples EXAMPLE 1 from fooStream select symbol as name, eventTimestamp() as eventTimestamp insert into barStream; This will extract current events timestamp. ifThenElse (Function) Evaluates the 'condition' parameter and returns value of the 'if.expression' parameter if the condition is true, or returns value of the 'else.expression' parameter if the condition is false. Here both 'if.expression' and 'else.expression' should be of the same type. Origin: siddhi-core:5.1.7 Syntax INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT ifThenElse( BOOL condition, INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT if.expression, INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT else.expression) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic condition This specifies the if then else condition value. BOOL No Yes if.expression This specifies the value to be returned if the value of the condition parameter is true. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No Yes else.expression This specifies the value to be returned if the value of the condition parameter is false. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No Yes Examples EXAMPLE 1 @info(name = 'query1') from sensorEventStream select sensorValue, ifThenElse(sensorValue 35,'High','Low') as status insert into outputStream; This will returns High if sensorValue = 50. EXAMPLE 2 @info(name = 'query1') from sensorEventStream select sensorValue, ifThenElse(voltage 5, 0, 1) as status insert into outputStream; This will returns 1 if voltage= 12. EXAMPLE 3 @info(name = 'query1') from userEventStream select userName, ifThenElse(password == 'admin', true, false) as passwordState insert into outputStream; This will returns passwordState as true if password = admin. instanceOfBoolean (Function) Checks whether the parameter is an instance of Boolean or not. Origin: siddhi-core:5.1.7 Syntax BOOL instanceOfBoolean( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No Yes Examples EXAMPLE 1 from fooStream select instanceOfBoolean(switchState) as state insert into barStream; This will return true if the value of switchState is true. EXAMPLE 2 from fooStream select instanceOfBoolean(value) as state insert into barStream; if the value = 32 then this will returns false as the value is not an instance of the boolean. instanceOfDouble (Function) Checks whether the parameter is an instance of Double or not. Origin: siddhi-core:5.1.7 Syntax BOOL instanceOfDouble( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No Yes Examples EXAMPLE 1 from fooStream select instanceOfDouble(value) as state insert into barStream; This will return true if the value field format is double ex : 56.45. EXAMPLE 2 from fooStream select instanceOfDouble(switchState) as state insert into barStream; if the switchState = true then this will returns false as the value is not an instance of the double. instanceOfFloat (Function) Checks whether the parameter is an instance of Float or not. Origin: siddhi-core:5.1.7 Syntax BOOL instanceOfFloat( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No Yes Examples EXAMPLE 1 from fooStream select instanceOfFloat(value) as state insert into barStream; This will return true if the value field format is float ex : 56.45f. EXAMPLE 2 from fooStream select instanceOfFloat(switchState) as state insert into barStream; if the switchState = true then this will returns false as the value is an instance of the boolean not a float. instanceOfInteger (Function) Checks whether the parameter is an instance of Integer or not. Origin: siddhi-core:5.1.7 Syntax BOOL instanceOfInteger( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No Yes Examples EXAMPLE 1 from fooStream select instanceOfInteger(value) as state insert into barStream; This will return true if the value field format is integer. EXAMPLE 2 from fooStream select instanceOfInteger(switchState) as state insert into barStream; if the switchState = true then this will returns false as the value is an instance of the boolean not a long. instanceOfLong (Function) Checks whether the parameter is an instance of Long or not. Origin: siddhi-core:5.1.7 Syntax BOOL instanceOfLong( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No Yes Examples EXAMPLE 1 from fooStream select instanceOfLong(value) as state insert into barStream; This will return true if the value field format is long ex : 56456l. EXAMPLE 2 from fooStream select instanceOfLong(switchState) as state insert into barStream; if the switchState = true then this will returns false as the value is an instance of the boolean not a long. instanceOfString (Function) Checks whether the parameter is an instance of String or not. Origin: siddhi-core:5.1.7 Syntax BOOL instanceOfString( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No Yes Examples EXAMPLE 1 from fooStream select instanceOfString(value) as state insert into barStream; This will return true if the value field format is string ex : 'test'. EXAMPLE 2 from fooStream select instanceOfString(switchState) as state insert into barStream; if the switchState = true then this will returns false as the value is an instance of the boolean not a string. maximum (Function) Returns the maximum value of the input parameters. Origin: siddhi-core:5.1.7 Syntax INT|LONG|DOUBLE|FLOAT maximum( INT|LONG|DOUBLE|FLOAT arg, INT|LONG|DOUBLE|FLOAT ...) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg This function accepts one or more parameters. They can belong to any one of the available types. All the specified parameters should be of the same type. INT LONG DOUBLE FLOAT No Yes Examples EXAMPLE 1 @info(name = 'query1') from inputStream select maximum(price1, price2, price3) as max insert into outputStream; This will returns the maximum value of the input parameters price1, price2, price3. minimum (Function) Returns the minimum value of the input parameters. Origin: siddhi-core:5.1.7 Syntax INT|LONG|DOUBLE|FLOAT minimum( INT|LONG|DOUBLE|FLOAT arg, INT|LONG|DOUBLE|FLOAT ...) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg This function accepts one or more parameters. They can belong to any one of the available types. All the specified parameters should be of the same type. INT LONG DOUBLE FLOAT No Yes Examples EXAMPLE 1 @info(name = 'query1') from inputStream select maximum(price1, price2, price3) as max insert into outputStream; This will returns the minimum value of the input parameters price1, price2, price3. sizeOfSet (Function) Returns the size of an object of type java.util.Set. Origin: siddhi-core:5.1.7 Syntax INT sizeOfSet( OBJECT set) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic set The set object. This parameter should be of type java.util.Set. A set object may be created by the 'set' attribute aggregator in Siddhi. OBJECT No Yes Examples EXAMPLE 1 from stockStream select initSet(symbol) as initialSet insert into initStream; ;from initStream#window.timeBatch(10 sec) select union(initialSet) as distinctSymbols insert into distinctStockStream; from distinctStockStream select sizeOfSet(distinctSymbols) sizeOfSymbolSet insert into sizeStream; The sizeStream stream will output the number of distinct stock symbols received during a sliding window of 10 seconds. pol2Cart (Stream Function) The pol2Cart function calculating the cartesian coordinates x y for the given theta, rho coordinates and adding them as new attributes to the existing events. Origin: siddhi-core:5.1.7 Syntax pol2Cart( DOUBLE theta, DOUBLE rho) pol2Cart( DOUBLE theta, DOUBLE rho, DOUBLE z) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic theta The theta value of the coordinates. DOUBLE No Yes rho The rho value of the coordinates. DOUBLE No Yes z z value of the cartesian coordinates. If z value is not given, drop the third parameter of the output. DOUBLE Yes Yes Examples EXAMPLE 1 from PolarStream#pol2Cart(theta, rho) select x, y insert into outputStream ; This will return cartesian coordinates (4.99953024681082, 0.06853693328228748) for theta: 0.7854 and rho: 5. EXAMPLE 2 from PolarStream#pol2Cart(theta, rho, 3.4) select x, y, z insert into outputStream ; This will return cartesian coordinates (4.99953024681082, 0.06853693328228748, 3.4)for theta: 0.7854 and rho: 5 and z: 3.4. log (Stream Processor) Logs the message on the given priority with or without the processed event. Origin: siddhi-core:5.1.7 Syntax log() log( STRING log.message) log( BOOL is.event.logged) log( STRING log.message, BOOL is.event.logged) log( STRING priority, STRING log.message) log( STRING priority, STRING log.message, BOOL is.event.logged) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic priority The priority/type of this log message (INFO, DEBUG, WARN, FATAL, ERROR, OFF, TRACE). INFO STRING Yes No log.message This message will be logged. : STRING Yes Yes is.event.logged To log the processed event. true BOOL Yes No Examples EXAMPLE 1 from FooStream#log() select * insert into BarStream; Logs events with SiddhiApp name message prefix on default log level INFO. EXAMPLE 2 from FooStream#log(\"Sample Event :\") select * insert into BarStream; Logs events with the message prefix \"Sample Event :\" on default log level INFO. EXAMPLE 3 from FooStream#log(\"DEBUG\", \"Sample Event :\", true) select * insert into BarStream; Logs events with the message prefix \"Sample Event :\" on log level DEBUG. EXAMPLE 4 from FooStream#log(\"Event Arrived\", false) select * insert into BarStream; For each event logs a message \"Event Arrived\" on default log level INFO. EXAMPLE 5 from FooStream#log(\"Sample Event :\", true) select * insert into BarStream; Logs events with the message prefix \"Sample Event :\" on default log level INFO. EXAMPLE 6 from FooStream#log(true) select * insert into BarStream; Logs events with on default log level INFO. batch (Window) A window that holds an incoming events batch. When a new set of events arrives, the previously arrived old events will be expired. Batch window can be used to aggregate events that comes in batches. If it has the parameter length specified, then batch window process the batch as several chunks. Origin: siddhi-core:5.1.7 Syntax batch() batch( INT window.length) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.length The length of a chunk If length value was not given it assign 0 as length and process the whole batch as once INT Yes No Examples EXAMPLE 1 define stream consumerItemStream (itemId string, price float) from consumerItemStream#window.batch() select price, str:groupConcat(itemId) as itemIds group by price insert into outputStream; This will output comma separated items IDs that have the same price for each incoming batch of events. cron (Window) This window outputs the arriving events as and when they arrive, and resets (expires) the window periodically based on the given cron expression. Origin: siddhi-core:5.1.7 Syntax cron( STRING cron.expression) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic cron.expression The cron expression that resets the window. STRING No No Examples EXAMPLE 1 define stream InputEventStream (symbol string, price float, volume int); @info(name = 'query1') from InputEventStream#cron('*/5 * * * * ?') select symbol, sum(price) as totalPrice insert into OutputStream; This let the totalPrice to gradually increase and resets to zero as a batch every 5 seconds. EXAMPLE 2 define stream StockEventStream (symbol string, price float, volume int) define window StockEventWindow (symbol string, price float, volume int) cron('*/5 * * * * ?'); @info(name = 'query0') from StockEventStream insert into StockEventWindow; @info(name = 'query1') from StockEventWindow select symbol, sum(price) as totalPrice insert into OutputStream ; The defined window will let the totalPrice to gradually increase and resets to zero as a batch every 5 seconds. delay (Window) A delay window holds events for a specific time period that is regarded as a delay period before processing them. Origin: siddhi-core:5.1.7 Syntax delay( INT|LONG|TIME window.delay) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.delay The time period (specified in sec, min, ms) for which the window should delay the events. INT LONG TIME No No Examples EXAMPLE 1 define window delayWindow(symbol string, volume int) delay(1 hour); define stream PurchaseStream(symbol string, volume int); define stream DeliveryStream(symbol string); define stream OutputStream(symbol string); @info(name='query1') from PurchaseStream select symbol, volume insert into delayWindow; @info(name='query2') from delayWindow join DeliveryStream on delayWindow.symbol == DeliveryStream.symbol select delayWindow.symbol insert into OutputStream; In this example, purchase events that arrive in the 'PurchaseStream' stream are directed to a delay window. At any given time, this delay window holds purchase events that have arrived within the last hour. These purchase events in the window are matched by the 'symbol' attribute, with delivery events that arrive in the 'DeliveryStream' stream. This monitors whether the delivery of products is done with a minimum delay of one hour after the purchase. externalTime (Window) A sliding time window based on external time. It holds events that arrived during the last windowTime period from the external timestamp, and gets updated on every monotonically increasing timestamp. Origin: siddhi-core:5.1.7 Syntax externalTime( LONG timestamp, INT|LONG|TIME window.time) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic timestamp The time which the window determines as current time and will act upon. The value of this parameter should be monotonically increasing. LONG No Yes window.time The sliding time period for which the window should hold events. INT LONG TIME No No Examples EXAMPLE 1 define window cseEventWindow (symbol string, price float, volume int) externalTime(eventTime, 20 sec) output expired events; @info(name = 'query0') from cseEventStream insert into cseEventWindow; @info(name = 'query1') from cseEventWindow select symbol, sum(price) as price insert expired events into outputStream ; processing events arrived within the last 20 seconds from the eventTime and output expired events. externalTimeBatch (Window) A batch (tumbling) time window based on external time, that holds events arrived during windowTime periods, and gets updated for every windowTime. Origin: siddhi-core:5.1.7 Syntax externalTimeBatch( LONG timestamp, INT|LONG|TIME window.time) externalTimeBatch( LONG timestamp, INT|LONG|TIME window.time, INT|LONG|TIME start.time) externalTimeBatch( LONG timestamp, INT|LONG|TIME window.time, INT|LONG|TIME start.time, INT|LONG|TIME timeout) externalTimeBatch( LONG timestamp, INT|LONG|TIME window.time, INT|LONG|TIME start.time, INT|LONG|TIME timeout, BOOL replace.with.batchtime) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic timestamp The time which the window determines as current time and will act upon. The value of this parameter should be monotonically increasing. LONG No Yes window.time The batch time period for which the window should hold events. INT LONG TIME No No start.time User defined start time. This could either be a constant (of type int, long or time) or an attribute of the corresponding stream (of type long). If an attribute is provided, initial value of attribute would be considered as startTime. Timestamp of first event INT LONG TIME Yes Yes timeout Time to wait for arrival of new event, before flushing and giving output for events belonging to a specific batch. System waits till an event from next batch arrives to flush current batch INT LONG TIME Yes No replace.with.batchtime This indicates to replace the expired event timeStamp as the batch end timeStamp System waits till an event from next batch arrives to flush current batch BOOL Yes No Examples EXAMPLE 1 define window cseEventWindow (symbol string, price float, volume int) externalTimeBatch(eventTime, 1 sec) output expired events; @info(name = 'query0') from cseEventStream insert into cseEventWindow; @info(name = 'query1') from cseEventWindow select symbol, sum(price) as price insert expired events into outputStream ; This will processing events that arrive every 1 seconds from the eventTime. EXAMPLE 2 define window cseEventWindow (symbol string, price float, volume int) externalTimeBatch(eventTime, 20 sec, 0) output expired events; This will processing events that arrive every 1 seconds from the eventTime. Starts on 0 th millisecond of an hour. EXAMPLE 3 define window cseEventWindow (symbol string, price float, volume int) externalTimeBatch(eventTime, 2 sec, eventTimestamp, 100) output expired events; This will processing events that arrive every 2 seconds from the eventTim. Considers the first event's eventTimestamp value as startTime. Waits 100 milliseconds for the arrival of a new event before flushing current batch. frequent (Window) Deprecated This window returns the latest events with the most frequently occurred value for a given attribute(s). Frequency calculation for this window processor is based on Misra-Gries counting algorithm. Origin: siddhi-core:5.1.7 Syntax frequent( INT event.count) frequent( INT event.count, STRING attribute) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic event.count The number of most frequent events to be emitted to the stream. INT No No attribute The attributes to group the events. If no attributes are given, the concatenation of all the attributes of the event is considered. The concatenation of all the attributes of the event is considered. STRING Yes Yes Examples EXAMPLE 1 @info(name = 'query1') from purchase[price = 30]#window.frequent(2) select cardNo, price insert all events into PotentialFraud; This will returns the 2 most frequent events. EXAMPLE 2 @info(name = 'query1') from purchase[price = 30]#window.frequent(2, cardNo) select cardNo, price insert all events into PotentialFraud; This will returns the 2 latest events with the most frequently appeared card numbers. length (Window) A sliding length window that holds the last 'window.length' events at a given time, and gets updated for each arrival and expiry. Origin: siddhi-core:5.1.7 Syntax length( INT window.length) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.length The number of events that should be included in a sliding length window. INT No No Examples EXAMPLE 1 define window StockEventWindow (symbol string, price float, volume int) length(10) output all events; @info(name = 'query0') from StockEventStream insert into StockEventWindow; @info(name = 'query1') from StockEventWindow select symbol, sum(price) as price insert all events into outputStream ; This will process last 10 events in a sliding manner. lengthBatch (Window) A batch (tumbling) length window that holds and process a number of events as specified in the window.length. Origin: siddhi-core:5.1.7 Syntax lengthBatch( INT window.length) lengthBatch( INT window.length, BOOL stream.current.event) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.length The number of events the window should tumble. INT No No stream.current.event Let the window stream the current events out as and when they arrive to the window while expiring them in batches. false BOOL Yes No Examples EXAMPLE 1 define stream InputEventStream (symbol string, price float, volume int); @info(name = 'query1') from InputEventStream#lengthBatch(10) select symbol, sum(price) as price insert into OutputStream; This collect and process 10 events as a batch and output them. EXAMPLE 2 define stream InputEventStream (symbol string, price float, volume int); @info(name = 'query1') from InputEventStream#lengthBatch(10, true) select symbol, sum(price) as sumPrice insert into OutputStream; This window sends the arriving events directly to the output letting the sumPrice to increase gradually, after every 10 events it clears the window as a batch and resets the sumPrice to zero. EXAMPLE 3 define stream InputEventStream (symbol string, price float, volume int); define window StockEventWindow (symbol string, price float, volume int) lengthBatch(10) output all events; @info(name = 'query0') from InputEventStream insert into StockEventWindow; @info(name = 'query1') from StockEventWindow select symbol, sum(price) as price insert all events into OutputStream ; This uses an defined window to process 10 events as a batch and output all events. lossyFrequent (Window) Deprecated This window identifies and returns all the events of which the current frequency exceeds the value specified for the supportThreshold parameter. Origin: siddhi-core:5.1.7 Syntax lossyFrequent( DOUBLE support.threshold) lossyFrequent( DOUBLE support.threshold, DOUBLE error.bound) lossyFrequent( DOUBLE support.threshold, DOUBLE error.bound, STRING attribute) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic support.threshold The support threshold value. DOUBLE No No error.bound The error bound value. support.threshold /10 DOUBLE Yes No attribute The attributes to group the events. If no attributes are given, the concatenation of all the attributes of the event is considered. The concatenation of all the attributes of the event is considered. STRING Yes Yes Examples EXAMPLE 1 define stream purchase (cardNo string, price float); define window purchaseWindow (cardNo string, price float) lossyFrequent(0.1, 0.01); @info(name = 'query0') from purchase[price = 30] insert into purchaseWindow; @info(name = 'query1') from purchaseWindow select cardNo, price insert all events into PotentialFraud; lossyFrequent(0.1, 0.01) returns all the events of which the current frequency exceeds 0.1, with an error bound of 0.01. EXAMPLE 2 define stream purchase (cardNo string, price float); define window purchaseWindow (cardNo string, price float) lossyFrequent(0.3, 0.05, cardNo); @info(name = 'query0') from purchase[price = 30] insert into purchaseWindow; @info(name = 'query1') from purchaseWindow select cardNo, price insert all events into PotentialFraud; lossyFrequent(0.3, 0.05, cardNo) returns all the events of which the cardNo attributes frequency exceeds 0.3, with an error bound of 0.05. session (Window) This is a session window that holds events that belong to a specific session. The events that belong to a specific session are identified by a grouping attribute (i.e., a session key). A session gap period is specified to determine the time period after which the session is considered to be expired. A new event that arrives with a specific value for the session key is matched with the session window with the same session key. There can be out of order and late arrival of events, these events can arrive after the session is expired, to include those events to the matching session key specify a latency time period that is less than the session gap period.To have aggregate functions with session windows, the events need to be grouped by the session key via a 'group by' clause. Origin: siddhi-core:5.1.7 Syntax session( INT|LONG|TIME window.session) session( INT|LONG|TIME window.session, STRING window.key) session( INT|LONG|TIME window.session, STRING window.key, INT|LONG|TIME window.allowed.latency) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.session The time period for which the session considered is valid. This is specified in seconds, minutes, or milliseconds (i.e., 'min', 'sec', or 'ms'. INT LONG TIME No No window.key The grouping attribute for events. default-key STRING Yes Yes window.allowed.latency This specifies the time period for which the session window is valid after the expiration of the session. The time period specified here should be less than the session time gap (which is specified via the 'window.session' parameter). 0 INT LONG TIME Yes No Examples EXAMPLE 1 define stream PurchaseEventStream (user string, item_number int, price float, quantity int); @info(name='query0) from PurchaseEventStream#window.session(5 sec, user, 2 sec) select * insert all events into OutputStream; This query processes events that arrive at the PurchaseEvent input stream. The 'user' attribute is the session key, and the session gap is 5 seconds. '2 sec' is specified as the allowed latency. Therefore, events with the matching user name that arrive 2 seconds after the expiration of the session are also considered when performing aggregations for the session identified by the given user name. sort (Window) This window holds a batch of events that equal the number specified as the windowLength and sorts them in the given order. Origin: siddhi-core:5.1.7 Syntax sort( INT window.length, STRING|DOUBLE|INT|LONG|FLOAT|LONG attribute) sort( INT window.length, STRING|DOUBLE|INT|LONG|FLOAT|LONG attribute, STRING order, STRING ...) sort( INT window.length, STRING|DOUBLE|INT|LONG|FLOAT|LONG attribute, STRING order, STRING|DOUBLE|INT|LONG|FLOAT|LONG attribute, STRING|DOUBLE|INT|LONG|FLOAT|LONG ...) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.length The size of the window length. INT No No attribute The attribute that should be checked for the order. The concatenation of all the attributes of the event is considered. STRING DOUBLE INT LONG FLOAT LONG No Yes order The order define as \"asc\" or \"desc\". asc STRING Yes No Examples EXAMPLE 1 define stream cseEventStream (symbol string, price float, volume long); define window cseEventWindow (symbol string, price float, volume long) sort(2,volume, 'asc'); @info(name = 'query0') from cseEventStream insert into cseEventWindow; @info(name = 'query1') from cseEventWindow select volume insert all events into outputStream ; sort(5, price, 'asc') keeps the events sorted by price in the ascending order. Therefore, at any given time, the window contains the 5 lowest prices. time (Window) A sliding time window that holds events that arrived during the last windowTime period at a given time, and gets updated for each event arrival and expiry. Origin: siddhi-core:5.1.7 Syntax time( INT|LONG|TIME window.time) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.time The sliding time period for which the window should hold events. INT LONG TIME No No Examples EXAMPLE 1 define window cseEventWindow (symbol string, price float, volume int) time(20) output all events; @info(name = 'query0') from cseEventStream insert into cseEventWindow; @info(name = 'query1') from cseEventWindow select symbol, sum(price) as price insert all events into outputStream ; This will processing events that arrived within the last 20 milliseconds. timeBatch (Window) A batch (tumbling) time window that holds and process events that arrive during 'window.time' period as a batch. Origin: siddhi-core:5.1.7 Syntax timeBatch( INT|LONG|TIME window.time) timeBatch( INT|LONG|TIME window.time, INT|LONG start.time) timeBatch( INT|LONG|TIME window.time, BOOL stream.current.event) timeBatch( INT|LONG|TIME window.time, INT|LONG start.time, BOOL stream.current.event) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.time The batch time period in which the window process the events. INT LONG TIME No No start.time This specifies an offset in milliseconds in order to start the window at a time different to the standard time. Timestamp of first event INT LONG Yes No stream.current.event Let the window stream the current events out as and when they arrive to the window while expiring them in batches. false BOOL Yes No Examples EXAMPLE 1 define stream InputEventStream (symbol string, price float, volume int); @info(name = 'query1') from InputEventStream#timeBatch(20 sec) select symbol, sum(price) as price insert into OutputStream; This collect and process incoming events as a batch every 20 seconds and output them. EXAMPLE 2 define stream InputEventStream (symbol string, price float, volume int); @info(name = 'query1') from InputEventStream#timeBatch(20 sec, true) select symbol, sum(price) as sumPrice insert into OutputStream; This window sends the arriving events directly to the output letting the sumPrice to increase gradually and on every 20 second interval it clears the window as a batch resetting the sumPrice to zero. EXAMPLE 3 define stream InputEventStream (symbol string, price float, volume int); define window StockEventWindow (symbol string, price float, volume int) timeBatch(20 sec) output all events; @info(name = 'query0') from InputEventStream insert into StockEventWindow; @info(name = 'query1') from StockEventWindow select symbol, sum(price) as price insert all events into OutputStream ; This uses an defined window to process events arrived every 20 seconds as a batch and output all events. timeLength (Window) A sliding time window that, at a given time holds the last window.length events that arrived during last window.time period, and gets updated for every event arrival and expiry. Origin: siddhi-core:5.1.7 Syntax timeLength( INT|LONG|TIME window.time, INT window.length) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.time The sliding time period for which the window should hold events. INT LONG TIME No No window.length The number of events that should be be included in a sliding length window.. INT No No Examples EXAMPLE 1 define stream cseEventStream (symbol string, price float, volume int); define window cseEventWindow (symbol string, price float, volume int) timeLength(2 sec, 10); @info(name = 'query0') from cseEventStream insert into cseEventWindow; @info(name = 'query1') from cseEventWindow select symbol, price, volume insert all events into outputStream; window.timeLength(2 sec, 10) holds the last 10 events that arrived during last 2 seconds and gets updated for every event arrival and expiry. Js eval (Function) This extension evaluates a given string and return the output according to the user specified data type. Origin: siddhi-script-js:5.0.2 Syntax INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT js:eval( STRING expression, STRING return.type) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic expression Any single line js expression or function. STRING No Yes return.type The return type of the evaluated expression. Supported types are int|long|float|double|bool|string. STRING No No Examples EXAMPLE 1 js:eval(\"700 800\", 'bool') In this example, the expression 700 800 will be evaluated and return result as false because user specified return type as bool. Json group (Aggregate Function) This function aggregates the JSON elements and returns a JSON object by adding enclosing.element if it is provided. If enclosing.element is not provided it aggregate the JSON elements returns a JSON array. Origin: siddhi-execution-json:2.0.4 Syntax OBJECT json:group( STRING|OBJECT json) OBJECT json:group( STRING|OBJECT json, BOOL distinct) OBJECT json:group( STRING|OBJECT json, STRING enclosing.element) OBJECT json:group( STRING|OBJECT json, STRING enclosing.element, BOOL distinct) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json The JSON element that needs to be aggregated. STRING OBJECT No Yes enclosing.element The JSON element used to enclose the aggregated JSON elements. EMPTY_STRING STRING Yes Yes distinct This is used to only have distinct JSON elements in the concatenated JSON object/array that is returned. false BOOL Yes Yes Examples EXAMPLE 1 from InputStream#window.length(5) select json:group(\"json\") as groupedJSONArray input OutputStream; When we input events having values for the json as {\"date\":\"2013-11-19\",\"time\":\"10:30\"} and {\"date\":\"2013-11-19\",\"time\":\"12:20\"} , it returns [{\"date\":\"2013-11-19\",\"time\":\"10:30\"}{\"date\":\"2013-11-19\",\"time\":\"12:20\"}] to the 'OutputStream'. EXAMPLE 2 from InputStream#window.length(5) select json:group(\"json\", true) as groupedJSONArray input OutputStream; When we input events having values for the json as {\"date\":\"2013-11-19\",\"time\":\"10:30\"} and {\"date\":\"2013-11-19\",\"time\":\"10:30\"} , it returns [{\"date\":\"2013-11-19\",\"time\":\"10:30\"}] to the 'OutputStream'. EXAMPLE 3 from InputStream#window.length(5) select json:group(\"json\", \"result\") as groupedJSONArray input OutputStream; When we input events having values for the json as {\"date\":\"2013-11-19\",\"time\":\"10:30\"} and {\"date\":\"2013-11-19\",\"time\":\"12:20\"} , it returns {\"result\":[{\"date\":\"2013-11-19\",\"time\":\"10:30\"},{\"date\":\"2013-11-19\",\"time\":\"12:20\"}} to the 'OutputStream'. EXAMPLE 4 from InputStream#window.length(5) select json:group(\"json\", \"result\", true) as groupedJSONArray input OutputStream; When we input events having values for the json as {\"date\":\"2013-11-19\",\"time\":\"10:30\"} and {\"date\":\"2013-11-19\",\"time\":\"10:30\"} , it returns {\"result\":[{\"date\":\"2013-11-19\",\"time\":\"10:30\"}]} to the 'OutputStream'. groupAsObject (Aggregate Function) This function aggregates the JSON elements and returns a JSON object by adding enclosing.element if it is provided. If enclosing.element is not provided it aggregate the JSON elements returns a JSON array. Origin: siddhi-execution-json:2.0.4 Syntax OBJECT json:groupAsObject( STRING|OBJECT json) OBJECT json:groupAsObject( STRING|OBJECT json, BOOL distinct) OBJECT json:groupAsObject( STRING|OBJECT json, STRING enclosing.element) OBJECT json:groupAsObject( STRING|OBJECT json, STRING enclosing.element, BOOL distinct) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json The JSON element that needs to be aggregated. STRING OBJECT No Yes enclosing.element The JSON element used to enclose the aggregated JSON elements. EMPTY_STRING STRING Yes Yes distinct This is used to only have distinct JSON elements in the concatenated JSON object/array that is returned. false BOOL Yes Yes Examples EXAMPLE 1 from InputStream#window.length(5) select json:groupAsObject(\"json\") as groupedJSONArray input OutputStream; When we input events having values for the json as {\"date\":\"2013-11-19\",\"time\":\"10:30\"} and {\"date\":\"2013-11-19\",\"time\":\"12:20\"} , it returns [{\"date\":\"2013-11-19\",\"time\":\"10:30\"}{\"date\":\"2013-11-19\",\"time\":\"12:20\"}] to the 'OutputStream'. EXAMPLE 2 from InputStream#window.length(5) select json:groupAsObject(\"json\", true) as groupedJSONArray input OutputStream; When we input events having values for the json as {\"date\":\"2013-11-19\",\"time\":\"10:30\"} and {\"date\":\"2013-11-19\",\"time\":\"10:30\"} , it returns [{\"date\":\"2013-11-19\",\"time\":\"10:30\"}] to the 'OutputStream'. EXAMPLE 3 from InputStream#window.length(5) select json:groupAsObject(\"json\", \"result\") as groupedJSONArray input OutputStream; When we input events having values for the json as {\"date\":\"2013-11-19\",\"time\":\"10:30\"} and {\"date\":\"2013-11-19\",\"time\":\"12:20\"} , it returns {\"result\":[{\"date\":\"2013-11-19\",\"time\":\"10:30\"},{\"date\":\"2013-11-19\",\"time\":\"12:20\"}} to the 'OutputStream'. EXAMPLE 4 from InputStream#window.length(5) select json:groupAsObject(\"json\", \"result\", true) as groupedJSONArray input OutputStream; When we input events having values for the json as {\"date\":\"2013-11-19\",\"time\":\"10:30\"} and {\"date\":\"2013-11-19\",\"time\":\"10:30\"} , it returns {\"result\":[{\"date\":\"2013-11-19\",\"time\":\"10:30\"}]} to the 'OutputStream'. getBool (Function) Function retrieves the 'boolean' value specified in the given path of the JSON element. Origin: siddhi-execution-json:2.0.4 Syntax BOOL json:getBool( STRING|OBJECT json, STRING path) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json The JSON input containing boolean value. STRING OBJECT No Yes path The JSON path to fetch the boolean value. STRING No Yes Examples EXAMPLE 1 json:getBool(json,'$.married') If the json is the format {'name' : 'John', 'married' : true} , the function returns true as there is a matching boolean at .married /code . /p p /p span id=\"example-2\" class=\"md-typeset\" style=\"display: block; color: rgba(0, 0, 0, 0.54); font-size: 12.8px; font-weight: bold;\" EXAMPLE 2 /span json:getBool(json,'$.name') p /p p style=\"word-wrap: break-word;margin: 0;\" If the code json /code is the format code {'name' : 'John', 'married' : true} /code , the function returns code null /code as there is no matching boolean at code .married</code>.</p> <p></p> <span id=\"example-2\" class=\"md-typeset\" style=\"display: block; color: rgba(0, 0, 0, 0.54); font-size: 12.8px; font-weight: bold;\">EXAMPLE 2</span> <pre class=\"codehilite\"><code>json:getBool(json,'$.name')</code></pre> <p></p> <p style=\"word-wrap: break-word;margin: 0;\">If the <code>json</code> is the format <code>{'name' : 'John', 'married' : true}</code>, the function returns <code>null</code> as there is no matching boolean at <code> .name . EXAMPLE 3 json:getBool(json,'$.foo') If the json is the format {'name' : 'John', 'married' : true} , the function returns null as there is no matching element at $.foo . getDouble (Function) Function retrieves the 'double' value specified in the given path of the JSON element. Origin: siddhi-execution-json:2.0.4 Syntax DOUBLE json:getDouble( STRING|OBJECT json, STRING path) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json The JSON input containing double value. STRING OBJECT No Yes path The JSON path to fetch the double value. STRING No Yes Examples EXAMPLE 1 json:getDouble(json,'$.salary') If the json is the format {'name' : 'John', 'salary' : 12000.0} , the function returns 12000.0 as there is a matching double at .salary /code . /p p /p span id=\"example-2\" class=\"md-typeset\" style=\"display: block; color: rgba(0, 0, 0, 0.54); font-size: 12.8px; font-weight: bold;\" EXAMPLE 2 /span json:getDouble(json,'$.salary') p /p p style=\"word-wrap: break-word;margin: 0;\" If the code json /code is the format code {'name' : 'John', 'age' : 23} /code , the function returns code null /code as there are no matching element at code .salary</code>.</p> <p></p> <span id=\"example-2\" class=\"md-typeset\" style=\"display: block; color: rgba(0, 0, 0, 0.54); font-size: 12.8px; font-weight: bold;\">EXAMPLE 2</span> <pre class=\"codehilite\"><code>json:getDouble(json,'$.salary')</code></pre> <p></p> <p style=\"word-wrap: break-word;margin: 0;\">If the <code>json</code> is the format <code>{'name' : 'John', 'age' : 23}</code>, the function returns <code>null</code> as there are no matching element at <code> .salary . EXAMPLE 3 json:getDouble(json,'$.name') If the json is the format {'name' : 'John', 'age' : 23} , the function returns null as there are no matching double at $.name . getFloat (Function) Function retrieves the 'float' value specified in the given path of the JSON element. Origin: siddhi-execution-json:2.0.4 Syntax FLOAT json:getFloat( STRING|OBJECT json, STRING path) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json The JSON input containing float value. STRING OBJECT No Yes path The JSON path to fetch the float value. STRING No Yes Examples EXAMPLE 1 json:getFloat(json,'$.salary') If the json is the format {'name' : 'John', 'salary' : 12000.0} , the function returns 12000 as there is a matching float at .salary /code . /p p /p span id=\"example-2\" class=\"md-typeset\" style=\"display: block; color: rgba(0, 0, 0, 0.54); font-size: 12.8px; font-weight: bold;\" EXAMPLE 2 /span json:getFloat(json,'$.salary') p /p p style=\"word-wrap: break-word;margin: 0;\" If the code json /code is the format code {'name' : 'John', 'age' : 23} /code , the function returns code null /code as there are no matching element at code .salary</code>.</p> <p></p> <span id=\"example-2\" class=\"md-typeset\" style=\"display: block; color: rgba(0, 0, 0, 0.54); font-size: 12.8px; font-weight: bold;\">EXAMPLE 2</span> <pre class=\"codehilite\"><code>json:getFloat(json,'$.salary')</code></pre> <p></p> <p style=\"word-wrap: break-word;margin: 0;\">If the <code>json</code> is the format <code>{'name' : 'John', 'age' : 23}</code>, the function returns <code>null</code> as there are no matching element at <code> .salary . EXAMPLE 3 json:getFloat(json,'$.name') If the json is the format {'name' : 'John', 'age' : 23} , the function returns null as there are no matching float at $.name . getInt (Function) Function retrieves the 'int' value specified in the given path of the JSON element. Origin: siddhi-execution-json:2.0.4 Syntax INT json:getInt( STRING|OBJECT json, STRING path) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json The JSON input containing int value. STRING OBJECT No Yes path The JSON path to fetch the int value. STRING No Yes Examples EXAMPLE 1 json:getInt(json,'$.age') If the json is the format {'name' : 'John', 'age' : 23} , the function returns 23 as there is a matching int at .age /code . /p p /p span id=\"example-2\" class=\"md-typeset\" style=\"display: block; color: rgba(0, 0, 0, 0.54); font-size: 12.8px; font-weight: bold;\" EXAMPLE 2 /span json:getInt(json,'$.salary') p /p p style=\"word-wrap: break-word;margin: 0;\" If the code json /code is the format code {'name' : 'John', 'age' : 23} /code , the function returns code null /code as there are no matching element at code .age</code>.</p> <p></p> <span id=\"example-2\" class=\"md-typeset\" style=\"display: block; color: rgba(0, 0, 0, 0.54); font-size: 12.8px; font-weight: bold;\">EXAMPLE 2</span> <pre class=\"codehilite\"><code>json:getInt(json,'$.salary')</code></pre> <p></p> <p style=\"word-wrap: break-word;margin: 0;\">If the <code>json</code> is the format <code>{'name' : 'John', 'age' : 23}</code>, the function returns <code>null</code> as there are no matching element at <code> .salary . EXAMPLE 3 json:getInt(json,'$.name') If the json is the format {'name' : 'John', 'age' : 23} , the function returns null as there are no matching int at $.name . getLong (Function) Function retrieves the 'long' value specified in the given path of the JSON element. Origin: siddhi-execution-json:2.0.4 Syntax LONG json:getLong( STRING|OBJECT json, STRING path) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json The JSON input containing long value. STRING OBJECT No Yes path The JSON path to fetch the long value. STRING No Yes Examples EXAMPLE 1 json:getLong(json,'$.age') If the json is the format {'name' : 'John', 'age' : 23} , the function returns 23 as there is a matching long at .age /code . /p p /p span id=\"example-2\" class=\"md-typeset\" style=\"display: block; color: rgba(0, 0, 0, 0.54); font-size: 12.8px; font-weight: bold;\" EXAMPLE 2 /span json:getLong(json,'$.salary') p /p p style=\"word-wrap: break-word;margin: 0;\" If the code json /code is the format code {'name' : 'John', 'age' : 23} /code , the function returns code null /code as there are no matching element at code .age</code>.</p> <p></p> <span id=\"example-2\" class=\"md-typeset\" style=\"display: block; color: rgba(0, 0, 0, 0.54); font-size: 12.8px; font-weight: bold;\">EXAMPLE 2</span> <pre class=\"codehilite\"><code>json:getLong(json,'$.salary')</code></pre> <p></p> <p style=\"word-wrap: break-word;margin: 0;\">If the <code>json</code> is the format <code>{'name' : 'John', 'age' : 23}</code>, the function returns <code>null</code> as there are no matching element at <code> .salary . EXAMPLE 3 json:getLong(json,'$.name') If the json is the format {'name' : 'John', 'age' : 23} , the function returns null as there are no matching long at $.name . getObject (Function) Function retrieves the object specified in the given path of the JSON element. Origin: siddhi-execution-json:2.0.4 Syntax OBJECT json:getObject( STRING|OBJECT json, STRING path) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json The JSON input containing the object. STRING OBJECT No Yes path The JSON path to fetch the object. STRING No Yes Examples EXAMPLE 1 json:getObject(json,'$.address') If the json is the format {'name' : 'John', 'address' : {'city' : 'NY', 'country' : 'USA'}} , the function returns {'city' : 'NY', 'country' : 'USA'} as there is a matching object at .address /code . /p p /p span id=\"example-2\" class=\"md-typeset\" style=\"display: block; color: rgba(0, 0, 0, 0.54); font-size: 12.8px; font-weight: bold;\" EXAMPLE 2 /span json:getObject(json,'$.age') p /p p style=\"word-wrap: break-word;margin: 0;\" If the code json /code is the format code {'name' : 'John', 'age' : 23} /code , the function returns code 23 /code as there is a matching object at code .address</code>.</p> <p></p> <span id=\"example-2\" class=\"md-typeset\" style=\"display: block; color: rgba(0, 0, 0, 0.54); font-size: 12.8px; font-weight: bold;\">EXAMPLE 2</span> <pre class=\"codehilite\"><code>json:getObject(json,'$.age')</code></pre> <p></p> <p style=\"word-wrap: break-word;margin: 0;\">If the <code>json</code> is the format <code>{'name' : 'John', 'age' : 23}</code>, the function returns <code>23</code> as there is a matching object at <code> .age . EXAMPLE 3 json:getObject(json,'$.salary') If the json is the format {'name' : 'John', 'age' : 23} , the function returns null as there are no matching element at $.salary . getString (Function) Function retrieves value specified in the given path of the JSON element as a string. Origin: siddhi-execution-json:2.0.4 Syntax STRING json:getString( STRING|OBJECT json, STRING path) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json The JSON input containing value. STRING OBJECT No Yes path The JSON path to fetch the value. STRING No Yes Examples EXAMPLE 1 json:getString(json,'$.name') If the json is the format {'name' : 'John', 'age' : 23} , the function returns John as there is a matching string at .name /code . /p p /p span id=\"example-2\" class=\"md-typeset\" style=\"display: block; color: rgba(0, 0, 0, 0.54); font-size: 12.8px; font-weight: bold;\" EXAMPLE 2 /span json:getString(json,'$.salary') p /p p style=\"word-wrap: break-word;margin: 0;\" If the code json /code is the format code {'name' : 'John', 'age' : 23} /code , the function returns code null /code as there are no matching element at code .name</code>.</p> <p></p> <span id=\"example-2\" class=\"md-typeset\" style=\"display: block; color: rgba(0, 0, 0, 0.54); font-size: 12.8px; font-weight: bold;\">EXAMPLE 2</span> <pre class=\"codehilite\"><code>json:getString(json,'$.salary')</code></pre> <p></p> <p style=\"word-wrap: break-word;margin: 0;\">If the <code>json</code> is the format <code>{'name' : 'John', 'age' : 23}</code>, the function returns <code>null</code> as there are no matching element at <code> .salary . EXAMPLE 3 json:getString(json,'$.age') If the json is the format {'name' : 'John', 'age' : 23} , the function returns 23 as a string as there is a matching element at .age /code . /p p /p span id=\"example-4\" class=\"md-typeset\" style=\"display: block; color: rgba(0, 0, 0, 0.54); font-size: 12.8px; font-weight: bold;\" EXAMPLE 4 /span json:getString(json,'$.address') p /p p style=\"word-wrap: break-word;margin: 0;\" If the code json /code is the format code {'name' : 'John', 'address' : {'city' : 'NY', 'country' : 'USA'}} /code , the function returns code {'city' : 'NY', 'country' : 'USA'} /code as a string as there is a matching element at code .age</code>.</p> <p></p> <span id=\"example-4\" class=\"md-typeset\" style=\"display: block; color: rgba(0, 0, 0, 0.54); font-size: 12.8px; font-weight: bold;\">EXAMPLE 4</span> <pre class=\"codehilite\"><code>json:getString(json,'$.address')</code></pre> <p></p> <p style=\"word-wrap: break-word;margin: 0;\">If the <code>json</code> is the format <code>{'name' : 'John', 'address' : {'city' : 'NY', 'country' : 'USA'}}</code>, the function returns <code>{'city' : 'NY', 'country' : 'USA'}</code> as a string as there is a matching element at <code> .address . isExists (Function) Function checks whether there is a JSON element present in the given path or not. Origin: siddhi-execution-json:2.0.4 Syntax BOOL json:isExists( STRING|OBJECT json, STRING path) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json The JSON input that needs to be searched for an elements. STRING OBJECT No Yes path The JSON path to check for the element. STRING No Yes Examples EXAMPLE 1 json:isExists(json, '$.name') If the json is the format {'name' : 'John', 'age' : 23} , the function returns true as there is an element in the given path. EXAMPLE 2 json:isExists(json, '$.salary') If the json is the format {'name' : 'John', 'age' : 23} , the function returns false as there is no element in the given path. setElement (Function) Function sets JSON element into a given JSON at the specific path. Origin: siddhi-execution-json:2.0.4 Syntax OBJECT json:setElement( STRING|OBJECT json, STRING path, STRING|BOOL|DOUBLE|FLOAT|INT|LONG|OBJECT json.element) OBJECT json:setElement( STRING|OBJECT json, STRING path, STRING|BOOL|DOUBLE|FLOAT|INT|LONG|OBJECT json.element, STRING key) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json The JSON to which a JSON element needs to be added/replaced. STRING OBJECT No Yes path The JSON path where the JSON element should be added/replaced. STRING No Yes json.element The JSON element being added. STRING BOOL DOUBLE FLOAT INT LONG OBJECT No Yes key The key to be used to refer the newly added element in the input JSON. Assumes the element is added to a JSON array, or the element selected by the JSON path will be updated. STRING Yes Yes Examples EXAMPLE 1 json:setElement(json, '$', \"{'country' : 'USA'}\", 'address') If the json is the format {'name' : 'John', 'married' : true} ,the function updates the json as {'name' : 'John', 'married' : true, 'address' : {'country' : 'USA'}} by adding 'address' element and returns the updated JSON. EXAMPLE 2 json:setElement(json, '$', 40, 'age') If the json is the format {'name' : 'John', 'married' : true} ,the function updates the json as {'name' : 'John', 'married' : true, 'age' : 40} by adding 'age' element and returns the updated JSON. EXAMPLE 3 json:setElement(json, '$', 45, 'age') If the json is the format {'name' : 'John', 'married' : true, 'age' : 40} , the function updates the json as {'name' : 'John', 'married' : true, 'age' : 45} by replacing 'age' element and returns the updated JSON. EXAMPLE 4 json:setElement(json, '$.items', 'book') If the json is the format {'name' : 'Stationary', 'items' : ['pen', 'pencil']} , the function updates the json as {'name' : 'John', 'items' : ['pen', 'pencil', 'book']} by adding 'book' in the items array and returns the updated JSON. EXAMPLE 5 json:setElement(json, '$.item', 'book') If the json is the format {'name' : 'Stationary', 'item' : 'pen'} , the function updates the json as {'name' : 'John', 'item' : 'book'} by replacing 'item' element and returns the updated JSON. EXAMPLE 6 json:setElement(json, '$.address', 'city', 'SF') If the json is the format {'name' : 'John', 'married' : true} ,the function will not update, but returns the original JSON as there are no valid path for $.address . toObject (Function) Function generate JSON object from the given JSON string. Origin: siddhi-execution-json:2.0.4 Syntax OBJECT json:toObject( STRING json) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json A valid JSON string that needs to be converted to a JSON object. STRING No Yes Examples EXAMPLE 1 json:toJson(json) This returns the JSON object corresponding to the given JSON string. toString (Function) Function generates a JSON string corresponding to a given JSON object. Origin: siddhi-execution-json:2.0.4 Syntax STRING json:toString( OBJECT json) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json A valid JSON object to generates a JSON string. OBJECT No Yes Examples EXAMPLE 1 json:toString(json) This returns the JSON string corresponding to a given JSON object. tokenize (Stream Processor) Stream processor tokenizes the given JSON into to multiple JSON string elements and sends them as separate events. Origin: siddhi-execution-json:2.0.4 Syntax json:tokenize( STRING|OBJECT json, STRING path) json:tokenize( STRING|OBJECT json, STRING path, BOOL fail.on.missing.attribute) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json The input JSON that needs to be tokenized. STRING OBJECT No Yes path The path of the set of elements that will be tokenized. STRING No Yes fail.on.missing.attribute If there are no element on the given path, when set to true the system will drop the event, and when set to false the system will pass 'null' value to the jsonElement output attribute. true BOOL Yes No Extra Return Attributes Name Description Possible Types jsonElement The JSON element retrieved based on the given path will be returned as a JSON string. If the 'path' selects a JSON array then the system returns each element in the array as a JSON string via a separate events. STRING Examples EXAMPLE 1 define stream InputStream (json string, path string); @info(name = 'query1') from InputStream#json:tokenizeAsObject(json, path) select path, jsonElement insert into OutputStream; If the input 'json' is {name:'John', enrolledSubjects:['Mathematics', 'Physics']} , and the 'path' is passed as .enrolledSubjects /code then for both the elements in the selected JSON array, it generates it generates events as code (' .enrolledSubjects</code> then for both the elements in the selected JSON array, it generates it generates events as <code>(' .enrolledSubjects', 'Mathematics') , and (' .enrolledSubjects', 'Physics') /code . br For the same input JSON, if the 'path' is passed as code .enrolledSubjects', 'Physics')</code>.<br>For the same input JSON, if the 'path' is passed as <code> .name then it will only produce one event (' .name', 'John') /code as the 'path' provided a single JSON element. /p p /p span id=\"example-2\" class=\"md-typeset\" style=\"display: block; color: rgba(0, 0, 0, 0.54); font-size: 12.8px; font-weight: bold;\" EXAMPLE 2 /span define stream InputStream (json string, path string); @info(name = 'query1') from InputStream#json:tokenizeAsObject(json, path, true) select path, jsonElement insert into OutputStream; p /p p style=\"word-wrap: break-word;margin: 0;\" If the input 'json' is code {name:'John', age:25} /code ,and the 'path' is passed as code .name', 'John')</code> as the 'path' provided a single JSON element.</p> <p></p> <span id=\"example-2\" class=\"md-typeset\" style=\"display: block; color: rgba(0, 0, 0, 0.54); font-size: 12.8px; font-weight: bold;\">EXAMPLE 2</span> <pre class=\"codehilite\"><code>define stream InputStream (json string, path string); @info(name = 'query1') from InputStream#json:tokenizeAsObject(json, path, true) select path, jsonElement insert into OutputStream;</code></pre> <p></p> <p style=\"word-wrap: break-word;margin: 0;\">If the input 'json' is <code>{name:'John', age:25}</code>,and the 'path' is passed as <code> .salary then the system will produce (' .salary', null) /code , as the 'fail.on.missing.attribute' is code true /code and there are no matching element for code .salary', null)</code>, as the 'fail.on.missing.attribute' is <code>true</code> and there are no matching element for <code> .salary . tokenizeAsObject (Stream Processor) Stream processor tokenizes the given JSON into to multiple JSON object elements and sends them as separate events. Origin: siddhi-execution-json:2.0.4 Syntax json:tokenizeAsObject( STRING|OBJECT json, STRING path) json:tokenizeAsObject( STRING|OBJECT json, STRING path, BOOL fail.on.missing.attribute) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json The input JSON that needs to be tokenized. STRING OBJECT No Yes path The path of the set of elements that will be tokenized. STRING No Yes fail.on.missing.attribute If there are no element on the given path, when set to true the system will drop the event, and when set to false the system will pass 'null' value to the jsonElement output attribute. true BOOL Yes No Extra Return Attributes Name Description Possible Types jsonElement The JSON element retrieved based on the given path will be returned as a JSON object. If the 'path' selects a JSON array then the system returns each element in the array as a JSON object via a separate events. OBJECT Examples EXAMPLE 1 define stream InputStream (json string, path string); @info(name = 'query1') from InputStream#json:tokenizeAsObject(json, path) select path, jsonElement insert into OutputStream; If the input 'json' is {name:'John', enrolledSubjects:['Mathematics', 'Physics']} , and the 'path' is passed as .enrolledSubjects /code then for both the elements in the selected JSON array, it generates it generates events as code (' .enrolledSubjects</code> then for both the elements in the selected JSON array, it generates it generates events as <code>(' .enrolledSubjects', 'Mathematics') , and (' .enrolledSubjects', 'Physics') /code . br For the same input JSON, if the 'path' is passed as code .enrolledSubjects', 'Physics')</code>.<br>For the same input JSON, if the 'path' is passed as <code> .name then it will only produce one event (' .name', 'John') /code as the 'path' provided a single JSON element. /p p /p span id=\"example-2\" class=\"md-typeset\" style=\"display: block; color: rgba(0, 0, 0, 0.54); font-size: 12.8px; font-weight: bold;\" EXAMPLE 2 /span define stream InputStream (json string, path string); @info(name = 'query1') from InputStream#json:tokenizeAsObject(json, path, true) select path, jsonElement insert into OutputStream; p /p p style=\"word-wrap: break-word;margin: 0;\" If the input 'json' is code {name:'John', age:25} /code ,and the 'path' is passed as code .name', 'John')</code> as the 'path' provided a single JSON element.</p> <p></p> <span id=\"example-2\" class=\"md-typeset\" style=\"display: block; color: rgba(0, 0, 0, 0.54); font-size: 12.8px; font-weight: bold;\">EXAMPLE 2</span> <pre class=\"codehilite\"><code>define stream InputStream (json string, path string); @info(name = 'query1') from InputStream#json:tokenizeAsObject(json, path, true) select path, jsonElement insert into OutputStream;</code></pre> <p></p> <p style=\"word-wrap: break-word;margin: 0;\">If the input 'json' is <code>{name:'John', age:25}</code>,and the 'path' is passed as <code> .salary then the system will produce (' .salary', null) /code , as the 'fail.on.missing.attribute' is code true /code and there are no matching element for code .salary', null)</code>, as the 'fail.on.missing.attribute' is <code>true</code> and there are no matching element for <code> .salary . List collect (Aggregate Function) Collects multiple values to construct a list. Origin: siddhi-execution-list:1.0.0 Syntax OBJECT list:collect( OBJECT|INT|LONG|FLOAT|DOUBLE|BOOL|STRING value) OBJECT list:collect( OBJECT|INT|LONG|FLOAT|DOUBLE|BOOL|STRING value, BOOL is.distinct) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic value Value of the list element OBJECT INT LONG FLOAT DOUBLE BOOL STRING No Yes is.distinct If true only distinct elements are collected false BOOL Yes Yes Examples EXAMPLE 1 from StockStream#window.lengthBatch(10) select list:collect(symbol) as stockSymbols insert into OutputStream; For the window expiry of 10 events, the collect() function will collect attributes of symbol to a single list and return as stockSymbols. merge (Aggregate Function) Collects multiple lists to merge as a single list. Origin: siddhi-execution-list:1.0.0 Syntax OBJECT list:merge( OBJECT list) OBJECT list:merge( OBJECT list, BOOL is.distinct) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic list List to be merged OBJECT No Yes is.distinct Whether to return list with distinct values false BOOL Yes Yes Examples EXAMPLE 1 from StockStream#window.lengthBatch(2) select list:merge(list) as stockSymbols insert into OutputStream; For the window expiry of 2 events, the merge() function will collect attributes of list and merge them to a single list, returned as stockSymbols. add (Function) Function returns the updated list after adding the given value. Origin: siddhi-execution-list:1.0.0 Syntax OBJECT list:add( OBJECT list, OBJECT|INT|LONG|FLOAT|DOUBLE|BOOL|STRING value) OBJECT list:add( OBJECT list, OBJECT|INT|LONG|FLOAT|DOUBLE|BOOL|STRING value, INT index) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic list The list to which the value should be added. OBJECT No Yes value The value to be added. OBJECT INT LONG FLOAT DOUBLE BOOL STRING No Yes index The index in which the value should to be added. last INT Yes Yes Examples EXAMPLE 1 list:add(stockSymbols, 'IBM') Function returns the updated list after adding the value IBM in the last index. EXAMPLE 2 list:add(stockSymbols, 'IBM', 0) Function returns the updated list after adding the value IBM in the 0 th index`. addAll (Function) Function returns the updated list after adding all the values from the given list. Origin: siddhi-execution-list:1.0.0 Syntax OBJECT list:addAll( OBJECT to.list, OBJECT from.list) OBJECT list:addAll( OBJECT to.list, OBJECT from.list, BOOL is.distinct) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic to.list The list into which the values need to copied. OBJECT No Yes from.list The list from which the values are copied. OBJECT No Yes is.distinct If true returns list with distinct values false BOOL Yes Yes Examples EXAMPLE 1 list:putAll(toList, fromList) If toList contains values ('IBM', 'WSO2), and if fromList contains values ('IBM', 'XYZ') then the function returns updated toList with values ('IBM', 'WSO2', 'IBM', 'XYZ'). EXAMPLE 2 list:putAll(toList, fromList, true) If toList contains values ('IBM', 'WSO2), and if fromList contains values ('IBM', 'XYZ') then the function returns updated toList with values ('IBM', 'WSO2', 'XYZ'). clear (Function) Function returns the cleared list. Origin: siddhi-execution-list:1.0.0 Syntax OBJECT list:clear( OBJECT list) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic list The list which needs to be cleared OBJECT No Yes Examples EXAMPLE 1 list:clear(stockDetails) Returns an empty list. clone (Function) Function returns the cloned list. Origin: siddhi-execution-list:1.0.0 Syntax OBJECT list:clone( OBJECT list) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic list The list to which needs to be cloned. OBJECT No Yes Examples EXAMPLE 1 list:clone(stockSymbols) Function returns cloned list of stockSymbols. contains (Function) Function checks whether the list contains the specific value. Origin: siddhi-execution-list:1.0.0 Syntax BOOL list:contains( OBJECT list, OBJECT|INT|LONG|FLOAT|DOUBLE|BOOL|STRING value) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic list The list that needs to be checked on whether it contains the value or not. OBJECT No Yes value The value that needs to be checked. OBJECT INT LONG FLOAT DOUBLE BOOL STRING No Yes Examples EXAMPLE 1 list:contains(stockSymbols, 'IBM') Returns 'true' if the stockSymbols list contains value IBM else it returns false . containsAll (Function) Function checks whether the list contains all the values in the given list. Origin: siddhi-execution-list:1.0.0 Syntax BOOL list:containsAll( OBJECT list, OBJECT given.list) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic list The list that needs to be checked on whether it contains all the values or not. OBJECT No Yes given.list The list which contains all the values to be checked. OBJECT No Yes Examples EXAMPLE 1 list:containsAll(stockSymbols, latestStockSymbols) Returns 'true' if the stockSymbols list contains values in latestStockSymbols else it returns false . create (Function) Function creates a list containing all values provided. Origin: siddhi-execution-list:1.0.0 Syntax OBJECT list:create() OBJECT list:create( OBJECT|INT|LONG|FLOAT|DOUBLE|BOOL|STRING value1) OBJECT list:create( OBJECT|INT|LONG|FLOAT|DOUBLE|BOOL|STRING value1, OBJECT|INT|LONG|FLOAT|DOUBLE|BOOL|STRING ...) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic value1 Value 1 OBJECT INT LONG FLOAT DOUBLE BOOL STRING Yes Yes Examples EXAMPLE 1 list:create(1, 2, 3, 4, 5, 6) This returns a list with values 1 , 2 , 3 , 4 , 5 and 6 . EXAMPLE 2 list:create() This returns an empty list. get (Function) Function returns the value at the specific index, null if index is out of range. Origin: siddhi-execution-list:1.0.0 Syntax OBJECT|INT|LONG|FLOAT|DOUBLE|BOOL|STRING list:get( OBJECT list, INT index) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic list Attribute containing the list OBJECT No Yes index Index of the element INT No Yes Examples EXAMPLE 1 list:get(stockSymbols, 1) This returns the element in the 1 st index in the stockSymbols list. indexOf (Function) Function returns the last index of the given element. Origin: siddhi-execution-list:1.0.0 Syntax INT list:indexOf( OBJECT list, OBJECT|INT|LONG|FLOAT|DOUBLE|BOOL|STRING value) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic list The list to be checked to get index of an element. OBJECT No Yes value Value for which last index needs to be identified. OBJECT INT LONG FLOAT DOUBLE BOOL STRING No Yes Examples EXAMPLE 1 list:indexOf(stockSymbols. `IBM`) Returns the last index of the element IBM if present else it returns -1. isEmpty (Function) Function checks if the list is empty. Origin: siddhi-execution-list:1.0.0 Syntax BOOL list:isEmpty( OBJECT list) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic list The list that needs to be checked whether it's empty or not. OBJECT No Yes Examples EXAMPLE 1 list:isEmpty(stockSymbols) Returns 'true' if the stockSymbols list is empty else it returns false . isList (Function) Function checks if the object is type of a list. Origin: siddhi-execution-list:1.0.0 Syntax BOOL list:isList( OBJECT|INT|LONG|FLOAT|DOUBLE|BOOL|STRING arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The argument the need to be determined whether it's a list or not. OBJECT INT LONG FLOAT DOUBLE BOOL STRING No Yes Examples EXAMPLE 1 list:isList(stockSymbols) Returns 'true' if the stockSymbols is and an instance of java.util.List else it returns false . lastIndexOf (Function) Function returns the index of the given value. Origin: siddhi-execution-list:1.0.0 Syntax INT list:lastIndexOf( OBJECT list, OBJECT|INT|LONG|FLOAT|DOUBLE|BOOL|STRING value) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic list The list to be checked to get index of an element. OBJECT No Yes value Value for which last index needs to be identified. OBJECT INT LONG FLOAT DOUBLE BOOL STRING No Yes Examples EXAMPLE 1 list:lastIndexOf(stockSymbols. `IBM`) Returns the last index of the element IBM if present else it returns -1. remove (Function) Function returns the updated list after removing the element with the specified value. Origin: siddhi-execution-list:1.0.0 Syntax OBJECT list:remove( OBJECT list, OBJECT|INT|LONG|FLOAT|DOUBLE|BOOL|STRING value) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic list The list that needs to be updated. OBJECT No Yes value The value of the element that needs to removed. OBJECT INT LONG FLOAT DOUBLE BOOL STRING No Yes Examples EXAMPLE 1 list:remove(stockSymbols, 'IBM') This returns the updated list, stockSymbols after stockSymbols the value IBM . removeAll (Function) Function returns the updated list after removing all the element with the specified list. Origin: siddhi-execution-list:1.0.0 Syntax OBJECT list:removeAll( OBJECT list, OBJECT given.list) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic list The list that needs to be updated. OBJECT No Yes given.list The list with all the elements that needs to removed. OBJECT No Yes Examples EXAMPLE 1 list:removeAll(stockSymbols, latestStockSymbols) This returns the updated list, stockSymbols after removing all the values in latestStockSymbols. removeByIndex (Function) Function returns the updated list after removing the element with the specified index. Origin: siddhi-execution-list:1.0.0 Syntax OBJECT list:removeByIndex( OBJECT list, INT index) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic list The list that needs to be updated. OBJECT No Yes index The index of the element that needs to removed. INT No Yes Examples EXAMPLE 1 list:removeByIndex(stockSymbols, 0) This returns the updated list, stockSymbols after removing value at 0 th index. retainAll (Function) Function returns the updated list after retaining all the elements in the specified list. Origin: siddhi-execution-list:1.0.0 Syntax OBJECT list:retainAll( OBJECT list, OBJECT given.list) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic list The list that needs to be updated. OBJECT No Yes given.list The list with all the elements that needs to reatined. OBJECT No Yes Examples EXAMPLE 1 list:retainAll(stockSymbols, latestStockSymbols) This returns the updated list, stockSymbols after retaining all the values in latestStockSymbols. setValue (Function) Function returns the updated list after replacing the element in the given index by the given value. Origin: siddhi-execution-list:1.0.0 Syntax OBJECT list:setValue( OBJECT list, INT index, OBJECT|INT|LONG|FLOAT|DOUBLE|BOOL|STRING value) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic list The list to which the value should be updated. OBJECT No Yes index The index in which the value should to be updated. INT No Yes value The value to be updated with. OBJECT INT LONG FLOAT DOUBLE BOOL STRING No Yes Examples EXAMPLE 1 list:set(stockSymbols, 0, 'IBM') Function returns the updated list after replacing the value at 0 th index with the value IBM size (Function) Function to return the size of the list. Origin: siddhi-execution-list:1.0.0 Syntax INT list:size( OBJECT list) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic list The list for which size should be returned. OBJECT No Yes Examples EXAMPLE 1 list:size(stockSymbols) Returns size of the stockSymbols list. sort (Function) Function returns lists sorted in ascending or descending order. Origin: siddhi-execution-list:1.0.0 Syntax OBJECT list:sort( OBJECT list) OBJECT list:sort( OBJECT list, STRING order) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic list The list which should be sorted. OBJECT No Yes order Order in which the list needs to be sorted (ASC/DESC/REV). REV STRING Yes No Examples EXAMPLE 1 list:sort(stockSymbols) Function returns the sorted list in ascending order. EXAMPLE 2 list:sort(stockSymbols, 'DESC') Function returns the sorted list in descending order. tokenize (Stream Processor) Tokenize the list and return each key, value as new attributes in events Origin: siddhi-execution-list:1.0.0 Syntax list:tokenize( OBJECT list) list:tokenize( OBJECT list, OBJECT ...) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic list Array list which needs to be tokenized OBJECT No Yes Extra Return Attributes Name Description Possible Types index Index of an entry consisted in the list INT value Value of an entry consisted in the list OBJECT Examples EXAMPLE 1 list:tokenize(customList) If custom list contains ('WSO2', 'IBM', 'XYZ') elements, then tokenize function will return 3 events with value attributes WSO2, IBM and XYZ respectively. Map create (Function) Function creates a map pairing the keys and their corresponding values. Origin: siddhi-execution-map:5.0.4 Syntax OBJECT map:create() OBJECT map:create( OBJECT|INT|LONG|FLOAT|DOUBLE|FLOAT|BOOL|STRING key1, OBJECT|INT|LONG|FLOAT|DOUBLE|FLOAT|BOOL|STRING value1) OBJECT map:create( OBJECT|INT|LONG|FLOAT|DOUBLE|FLOAT|BOOL|STRING key1, OBJECT|INT|LONG|FLOAT|DOUBLE|FLOAT|BOOL|STRING value1, OBJECT|INT|LONG|FLOAT|DOUBLE|FLOAT|BOOL|STRING ...) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic key1 Key 1 - OBJECT INT LONG FLOAT DOUBLE FLOAT BOOL STRING Yes Yes value1 Value 1 - OBJECT INT LONG FLOAT DOUBLE FLOAT BOOL STRING Yes Yes Examples EXAMPLE 1 map:create(1, 'one', 2, 'two', 3, 'three') This returns a map with keys 1 , 2 , 3 mapped with their corresponding values, one , two , three . EXAMPLE 2 map:create() This returns an empty map. createFromJSON (Function) Function returns the map created by pairing the keys with their corresponding values given in the JSON string. Origin: siddhi-execution-map:5.0.4 Syntax OBJECT map:createFromJSON( STRING json.string) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json.string JSON as a string, which is used to create the map. STRING No Yes Examples EXAMPLE 1 map:createFromJSON(\"{\u2018symbol' : 'IBM', 'price' : 200, 'volume' : 100}\") This returns a map with the keys symbol , price , and volume , and their values, IBM , 200 and 100 respectively. createFromXML (Function) Function returns the map created by pairing the keys with their corresponding values,given as an XML string. Origin: siddhi-execution-map:5.0.4 Syntax OBJECT map:createFromXML( STRING xml.string) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic xml.string The XML string, which is used to create the map. STRING No Yes Examples EXAMPLE 1 map:createFromXML(\" stock symbol IBM /symbol price 200 /price volume 100 /volume /stock \") This returns a map with the keys symbol , price , volume , and with their values IBM , 200 and 100 respectively. get (Function) Function returns the value corresponding to the given key from the map. Origin: siddhi-execution-map:5.0.4 Syntax OBJECT map:get( OBJECT map, OBJECT|INT|LONG|FLOAT|DOUBLE|FLOAT|BOOL|STRING key) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic map The map from where the value should be obtained. OBJECT No Yes key The key to fetch the value. OBJECT INT LONG FLOAT DOUBLE FLOAT BOOL STRING No Yes Examples EXAMPLE 1 map:get(companyMap, 1) If the companyMap has key 1 and value ABC in it's set of key value pairs. The function returns ABC . EXAMPLE 2 map:get(companyMap, 2) If the companyMap does not have any value for key 2 then the function returns null . isMap (Function) Function checks if the object is type of a map. Origin: siddhi-execution-map:5.0.4 Syntax BOOL map:isMap( OBJECT|INT|LONG|FLOAT|DOUBLE|FLOAT|BOOL|STRING arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The argument the need to be determined whether it's a map or not. OBJECT INT LONG FLOAT DOUBLE FLOAT BOOL STRING No Yes Examples EXAMPLE 1 map:isMap(students) Returns 'true' if the students is and an instance of java.util.Map else it returns false . put (Function) Function returns the updated map after adding the given key-value pair. If the key already exist in the map the key is updated with the new value. Origin: siddhi-execution-map:5.0.4 Syntax OBJECT map:put( OBJECT map, OBJECT|INT|LONG|FLOAT|DOUBLE|FLOAT|BOOL|STRING key, OBJECT|INT|LONG|FLOAT|DOUBLE|FLOAT|BOOL|STRING value) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic map The map to which the value should be added. OBJECT No Yes key The key to be added. OBJECT INT LONG FLOAT DOUBLE FLOAT BOOL STRING No Yes value The value to be added. OBJECT INT LONG FLOAT DOUBLE FLOAT BOOL STRING No Yes Examples EXAMPLE 1 map:put(students , 1234 , 'sam') Function returns the updated map named students after adding the value sam with the key 1234 . putAll (Function) Function returns the updated map after adding all the key-value pairs from another map. If there are duplicate keys, the key will be assigned new values from the map that's being copied. Origin: siddhi-execution-map:5.0.4 Syntax OBJECT map:putAll( OBJECT to.map, OBJECT from.map) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic to.map The map into which the key-values need to copied. OBJECT No Yes from.map The map from which the key-values are copied. OBJECT No Yes Examples EXAMPLE 1 map:putAll(toMap, fromMap) If toMap contains key-value pairs ('symbol': 'wso2'), ('volume' : 100), and if fromMap contains key-value pairs ('symbol': 'IBM'), ('price' : 12), then the function returns updated toMap with key-value pairs ('symbol': 'IBM'), ('price' : 12), ('volume' : 100). remove (Function) Function returns the updated map after removing the element with the specified key. Origin: siddhi-execution-map:5.0.4 Syntax OBJECT map:remove( OBJECT map, OBJECT|INT|LONG|FLOAT|DOUBLE|FLOAT|BOOL|STRING key) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic map The map that needs to be updated. OBJECT No Yes key The key of the element that needs to removed. OBJECT INT LONG FLOAT DOUBLE FLOAT BOOL STRING No Yes Examples EXAMPLE 1 map:remove(students, 1234) This returns the updated map, students after removing the key-value pair corresponding to the key 1234 . toJSON (Function) Function converts a map into a JSON object and returns the JSON as a string. Origin: siddhi-execution-map:5.0.4 Syntax STRING map:toJSON( OBJECT map) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic map The map that needs to be converted to JSON OBJECT No Yes Examples EXAMPLE 1 map:toJSON(company) If company is a map with key-value pairs, ('symbol': 'wso2'),('volume' : 100), and ('price', 200), it returns the JSON string {\"symbol\" : \"wso2\", \"volume\" : 100 , \"price\" : 200} . toXML (Function) Function returns the map as an XML string. Origin: siddhi-execution-map:5.0.4 Syntax STRING map:toXML( OBJECT map) STRING map:toXML( OBJECT map, OBJECT|STRING root.element.name) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic map The map that needs to be converted to XML. OBJECT No Yes root.element.name The root element of the map. The XML root element will be ignored OBJECT STRING Yes Yes Examples EXAMPLE 1 toXML(company, 'abcCompany') If company is a map with key-value pairs, ('symbol' : 'wso2'), ('volume' : 100), and ('price' : 200), this function returns XML as a string, abcCompany symbol wso2 /symbol volume 100 /volume price 200 /price /abcCompany . EXAMPLE 2 toXML(company) If company is a map with key-value pairs, ('symbol' : 'wso2'), ('volume' : 100), and ('price' : 200), this function returns XML without root element as a string, symbol wso2 /symbol volume 100 /volume price 200 /price . Math percentile (Aggregate Function) This functions returns the pth percentile value of a given argument. Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:percentile( INT|LONG|FLOAT|DOUBLE arg, DOUBLE p) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value of the parameter whose percentile should be found. INT LONG FLOAT DOUBLE No Yes p Estimate of the percentile to be found (pth percentile) where p is any number greater than 0 or lesser than or equal to 100. DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (sensorId int, temperature double); from InValueStream select math:percentile(temperature, 97.0) as percentile insert into OutMediationStream; This function returns the percentile value based on the argument given. For example, math:percentile(temperature, 97.0) returns the 97 th percentile value of all the temperature events. abs (Function) This function returns the absolute value of the given parameter. It wraps the java.lang.Math.abs() function. Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:abs( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The parameter whose absolute value is found. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:abs(inValue) as absValue insert into OutMediationStream; Irrespective of whether the 'invalue' in the input stream holds a value of abs(3) or abs(-3),the function returns 3 since the absolute value of both 3 and -3 is 3. The result directed to OutMediationStream stream. acos (Function) If -1 = p1 = 1, this function returns the arc-cosine (inverse cosine) value of p1.If the domain is invalid, it returns NULL. The value returned is in radian scale. This function wraps the java.lang.Math.acos() function. Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:acos( FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose arc-cosine (inverse cosine) value is found. FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:acos(inValue) as acosValue insert into OutMediationStream; If the 'inValue' in the input stream is given, the function calculates the arc-cosine value of it and returns the arc-cosine value to the output stream, OutMediationStream. For example, acos(0.5) returns 1.0471975511965979. asin (Function) If -1 = p1 = 1, this function returns the arc-sin (inverse sine) value of p1. If the domain is invalid, it returns NULL. The value returned is in radian scale. This function wraps the java.lang.Math.asin() function. Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:asin( FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose arc-sin (inverse sine) value is found. FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:asin(inValue) as asinValue insert into OutMediationStream; If the 'inValue' in the input stream is given, the function calculates the arc-sin value of it and returns the arc-sin value to the output stream, OutMediationStream. For example, asin(0.5) returns 0.5235987755982989. atan (Function) 1. If a single p1 is received, this function returns the arc-tangent (inverse tangent) value of p1 . 2. If p1 is received along with an optional p1 , it considers them as x and y coordinates and returns the arc-tangent (inverse tangent) value. The returned value is in radian scale. This function wraps the java.lang.Math.atan() function. Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:atan( INT|LONG|FLOAT|DOUBLE p1) DOUBLE math:atan( INT|LONG|FLOAT|DOUBLE p1, INT|LONG|FLOAT|DOUBLE p2) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose arc-tangent (inverse tangent) is found. If the optional second parameter is given this represents the x coordinate of the (x,y) coordinate pair. INT LONG FLOAT DOUBLE No Yes p2 This optional parameter represents the y coordinate of the (x,y) coordinate pair. 0D INT LONG FLOAT DOUBLE Yes Yes Examples EXAMPLE 1 define stream InValueStream (inValue1 double, inValue2 double); from InValueStream select math:atan(inValue1, inValue2) as convertedValue insert into OutMediationStream; If the 'inValue1' in the input stream is given, the function calculates the arc-tangent value of it and returns the arc-tangent value to the output stream, OutMediationStream. If both the 'inValue1' and 'inValue2' are given, then the function considers them to be x and y coordinates respectively and returns the calculated arc-tangent value to the output stream, OutMediationStream. For example, atan(12d, 5d) returns 1.1760052070951352. bin (Function) This function returns a string representation of the p1 argument, that is of either 'integer' or 'long' data type, as an unsigned integer in base 2. It wraps the java.lang.Integer.toBinaryString and java.lang.Long.toBinaryString` methods. Origin: siddhi-execution-math:5.0.4 Syntax STRING math:bin( INT|LONG p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value in either 'integer' or 'long', that should be converted into an unsigned integer of base 2. INT LONG No Yes Examples EXAMPLE 1 define stream InValueStream (inValue long); from InValueStream select math:bin(inValue) as binValue insert into OutMediationStream; If the 'inValue' in the input stream is given, the function converts it into an unsigned integer in base 2 and directs the output to the output stream, OutMediationStream. For example, bin(9) returns '1001'. cbrt (Function) This function returns the cube-root of 'p1' which is in radians. It wraps the java.lang.Math.cbrt() function. Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:cbrt( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose cube-root should be found. Input is required to be in radians. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:cbrt(inValue) as cbrtValue insert into OutMediationStream; If the 'inValue' is given, the function calculates the cube-root value for the same and directs the output to the output stream, OutMediationStream. For example, cbrt(17d) returns 2.5712815906582356. ceil (Function) This function returns the smallest double value, i.e., the closest to the negative infinity, that is greater than or equal to the p1 argument, and is equal to a mathematical integer. It wraps the java.lang.Math.ceil() method. Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:ceil( FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose ceiling value is found. FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:ceil(inValue) as ceilingValue insert into OutMediationStream; This function calculates the ceiling value of the given 'inValue' and directs the result to 'OutMediationStream' output stream. For example, ceil(423.187d) returns 424.0. conv (Function) This function converts a from the fromBase base to the toBase base. Origin: siddhi-execution-math:5.0.4 Syntax STRING math:conv( STRING a, INT from.base, INT to.base) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic a The value whose base should be changed. Input should be given as a 'String'. STRING No Yes from.base The source base of the input parameter 'a'. INT No Yes to.base The target base that the input parameter 'a' should be converted into. INT No Yes Examples EXAMPLE 1 define stream InValueStream (inValue string,fromBase int,toBase int); from InValueStream select math:conv(inValue,fromBase,toBase) as convertedValue insert into OutMediationStream; If the 'inValue' in the input stream is given, and the base in which it currently resides in and the base to which it should be converted to is specified, the function converts it into a string in the target base and directs it to the output stream, OutMediationStream. For example, conv(\"7f\", 16, 10) returns \"127\". copySign (Function) This function returns a value of an input with the received magnitude and sign of another input. It wraps the java.lang.Math.copySign() function. Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:copySign( INT|LONG|FLOAT|DOUBLE magnitude, INT|LONG|FLOAT|DOUBLE sign) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic magnitude The magnitude of this parameter is used in the output attribute. INT LONG FLOAT DOUBLE No Yes sign The sign of this parameter is used in the output attribute. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue1 double, inValue2 double); from InValueStream select math:copySign(inValue1,inValue2) as copysignValue insert into OutMediationStream; If two values are provided as 'inValue1' and 'inValue2', the function copies the magnitude and sign of the second argument into the first one and directs the result to the output stream, OutMediatonStream. For example, copySign(5.6d, -3.0d) returns -5.6. cos (Function) This function returns the cosine of p1 which is in radians. It wraps the java.lang.Math.cos() function. Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:cos( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose cosine value should be found.The input is required to be in radians. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:cos(inValue) as cosValue insert into OutMediationStream; If the 'inValue' is given, the function calculates the cosine value for the same and directs the output to the output stream, OutMediationStream. For example, cos(6d) returns 0.9601702866503661. cosh (Function) This function returns the hyperbolic cosine of p1 which is in radians. It wraps the java.lang.Math.cosh() function. Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:cosh( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose hyperbolic cosine should be found. The input is required to be in radians. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:cosh(inValue) as cosValue insert into OutMediationStream; If the 'inValue' is given, the function calculates the hyperbolic cosine value for the same and directs the output to the output stream, OutMediationStream. For example, cosh (6d) returns 201.7156361224559. e (Function) This function returns the java.lang.Math.E constant, which is the closest double value to e, where e is the base of the natural logarithms. Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:e() Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:e() as eValue insert into OutMediationStream; This function returns the constant, 2.7182818284590452354 which is the closest double value to e and directs the output to 'OutMediationStream' output stream. exp (Function) This function returns the Euler's number e raised to the power of p1 . It wraps the java.lang.Math.exp() function. Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:exp( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The power that the Euler's number e is raised to. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:exp(inValue) as expValue insert into OutMediationStream; If the 'inValue' in the inputstream holds a value, this function calculates the corresponding Euler's number 'e' and directs it to the output stream, OutMediationStream. For example, exp(10.23) returns 27722.51006805505. floor (Function) This function wraps the java.lang.Math.floor() function and returns the largest value, i.e., closest to the positive infinity, that is less than or equal to p1 , and is equal to a mathematical integer. Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:floor( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose floor value should be found. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:floor(inValue) as floorValue insert into OutMediationStream; This function calculates the floor value of the given 'inValue' input and directs the output to the 'OutMediationStream' output stream. For example, (10.23) returns 10.0. getExponent (Function) This function returns the unbiased exponent that is used in the representation of p1 . This function wraps the java.lang.Math.getExponent() function. Origin: siddhi-execution-math:5.0.4 Syntax INT math:getExponent( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of whose unbiased exponent representation should be found. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:getExponent(inValue) as expValue insert into OutMediationStream; This function calculates the unbiased exponent of a given input, 'inValue' and directs the result to the 'OutMediationStream' output stream. For example, getExponent(60984.1) returns 15. hex (Function) This function wraps the java.lang.Double.toHexString() function. It returns a hexadecimal string representation of the input, p1`. Origin: siddhi-execution-math:5.0.4 Syntax STRING math:hex( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose hexadecimal value should be found. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue int); from InValueStream select math:hex(inValue) as hexString insert into OutMediationStream; If the 'inValue' in the input stream is provided, the function converts this into its corresponding hexadecimal format and directs the output to the output stream, OutMediationStream. For example, hex(200) returns \"c8\". isInfinite (Function) This function wraps the java.lang.Float.isInfinite() and java.lang.Double.isInfinite() and returns true if p1 is infinitely large in magnitude and false if otherwise. Origin: siddhi-execution-math:5.0.4 Syntax BOOL math:isInfinite( FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 This is the value of the parameter that the function determines to be either infinite or finite. FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue1 double,inValue2 int); from InValueStream select math:isInfinite(inValue1) as isInfinite insert into OutMediationStream; If the value given in the 'inValue' in the input stream is of infinitely large magnitude, the function returns the value, 'true' and directs the result to the output stream, OutMediationStream'. For example, isInfinite(java.lang.Double.POSITIVE_INFINITY) returns true. isNan (Function) This function wraps the java.lang.Float.isNaN() and java.lang.Double.isNaN() functions and returns true if p1 is NaN (Not-a-Number), and returns false if otherwise. Origin: siddhi-execution-math:5.0.4 Syntax BOOL math:isNan( FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter which the function determines to be either NaN or a number. FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue1 double,inValue2 int); from InValueStream select math:isNan(inValue1) as isNaN insert into OutMediationStream; If the 'inValue1' in the input stream has a value that is undefined, then the function considers it as an 'NaN' value and directs 'True' to the output stream, OutMediationStream. For example, isNan(java.lang.Math.log(-12d)) returns true. ln (Function) This function returns the natural logarithm (base e) of p1 . Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:ln( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose natural logarithm (base e) should be found. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:ln(inValue) as lnValue insert into OutMediationStream; If the 'inValue' in the input stream is given, the function calculates its natural logarithm (base e) and directs the results to the output stream, 'OutMeditionStream'. For example, ln(11.453) returns 2.438251704415579. log (Function) This function returns the logarithm of the received number as per the given base . Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:log( INT|LONG|FLOAT|DOUBLE number, INT|LONG|FLOAT|DOUBLE base) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic number The value of the parameter whose base should be changed. INT LONG FLOAT DOUBLE No Yes base The base value of the ouput. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (number double, base double); from InValueStream select math:log(number, base) as logValue insert into OutMediationStream; If the number and the base to which it has to be converted into is given in the input stream, the function calculates the number to the base specified and directs the result to the output stream, OutMediationStream. For example, log(34, 2f) returns 5.08746284125034. log10 (Function) This function returns the base 10 logarithm of p1 . Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:log10( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose base 10 logarithm should be found. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:log10(inValue) as lnValue insert into OutMediationStream; If the 'inValue' in the input stream is given, the function calculates the base 10 logarithm of the same and directs the result to the output stream, OutMediatioStream. For example, log10(19.234) returns 1.2840696117100832. log2 (Function) This function returns the base 2 logarithm of p1 . Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:log2( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose base 2 logarithm should be found. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:log2(inValue) as lnValue insert into OutMediationStream; If the 'inValue' in the input stream is given, the function calculates the base 2 logarithm of the same and returns the value to the output stream, OutMediationStream. For example log2(91d) returns 6.507794640198696. max (Function) This function returns the greater value of p1 and p2 . Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:max( INT|LONG|FLOAT|DOUBLE p1, INT|LONG|FLOAT|DOUBLE p2) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 One of the input values to be compared in order to find the larger value of the two INT LONG FLOAT DOUBLE No Yes p2 The input value to be compared with 'p1' in order to find the larger value of the two. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue1 double,inValue2 int); from InValueStream select math:max(inValue1,inValue2) as maxValue insert into OutMediationStream; If two input values 'inValue1, and 'inValue2' are given, the function compares them and directs the larger value to the output stream, OutMediationStream. For example, max(123.67d, 91) returns 123.67. min (Function) This function returns the smaller value of p1 and p2 . Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:min( INT|LONG|FLOAT|DOUBLE p1, INT|LONG|FLOAT|DOUBLE p2) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 One of the input values that are to be compared in order to find the smaller value. INT LONG FLOAT DOUBLE No Yes p2 The input value that is to be compared with 'p1' in order to find the smaller value. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue1 double,inValue2 int); from InValueStream select math:min(inValue1,inValue2) as minValue insert into OutMediationStream; If two input values, 'inValue1' and 'inValue2' are given, the function compares them and directs the smaller value of the two to the output stream, OutMediationStream. For example, min(123.67d, 91) returns 91. oct (Function) This function converts the input parameter p1 to octal. Origin: siddhi-execution-math:5.0.4 Syntax STRING math:oct( INT|LONG p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose octal representation should be found. INT LONG No Yes Examples EXAMPLE 1 define stream InValueStream (inValue long); from InValueStream select math:oct(inValue) as octValue insert into OutMediationStream; If the 'inValue' in the input stream is given, this function calculates the octal value corresponding to the same and directs it to the output stream, OutMediationStream. For example, oct(99l) returns \"143\". parseDouble (Function) This function returns the double value of the string received. Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:parseDouble( STRING p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that should be converted into a double value. STRING No Yes Examples EXAMPLE 1 define stream InValueStream (inValue string); from InValueStream select math:parseDouble(inValue) as output insert into OutMediationStream; If the 'inValue' in the input stream holds a value, this function converts it into the corresponding double value and directs it to the output stream, OutMediationStream. For example, parseDouble(\"123\") returns 123.0. parseFloat (Function) This function returns the float value of the received string. Origin: siddhi-execution-math:5.0.4 Syntax FLOAT math:parseFloat( STRING p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that should be converted into a float value. STRING No Yes Examples EXAMPLE 1 define stream InValueStream (inValue string); from InValueStream select math:parseFloat(inValue) as output insert into OutMediationStream; The function converts the input value given in 'inValue',into its corresponding float value and directs the result into the output stream, OutMediationStream. For example, parseFloat(\"123\") returns 123.0. parseInt (Function) This function returns the integer value of the received string. Origin: siddhi-execution-math:5.0.4 Syntax INT math:parseInt( STRING p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that should be converted to an integer. STRING No Yes Examples EXAMPLE 1 define stream InValueStream (inValue string); from InValueStream select math:parseInt(inValue) as output insert into OutMediationStream; The function converts the 'inValue' into its corresponding integer value and directs the output to the output stream, OutMediationStream. For example, parseInt(\"123\") returns 123. parseLong (Function) This function returns the long value of the string received. Origin: siddhi-execution-math:5.0.4 Syntax LONG math:parseLong( STRING p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that should be converted to a long value. STRING No Yes Examples EXAMPLE 1 define stream InValueStream (inValue string); from InValueStream select math:parseLong(inValue) as output insert into OutMediationStream; The function converts the 'inValue' to its corresponding long value and directs the result to the output stream, OutMediationStream. For example, parseLong(\"123\") returns 123. pi (Function) This function returns the java.lang.Math.PI constant, which is the closest value to pi, i.e., the ratio of the circumference of a circle to its diameter. Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:pi() Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:pi() as piValue insert into OutMediationStream; pi() always returns 3.141592653589793. power (Function) This function raises the given value to a given power. Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:power( INT|LONG|FLOAT|DOUBLE value, INT|LONG|FLOAT|DOUBLE to.power) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic value The value that should be raised to the power of 'to.power' input parameter. INT LONG FLOAT DOUBLE No Yes to.power The power to which the 'value' input parameter should be raised. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue1 double, inValue2 double); from InValueStream select math:power(inValue1,inValue2) as powerValue insert into OutMediationStream; This function raises the 'inValue1' to the power of 'inValue2' and directs the output to the output stream, 'OutMediationStream. For example, (5.6d, 3.0d) returns 175.61599999999996. rand (Function) This returns a stream of pseudo-random numbers when a sequence of calls are sent to the rand() . Optionally, it is possible to define a seed, i.e., rand(seed) using which the pseudo-random numbers are generated. These functions internally use the java.util.Random class. Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:rand() DOUBLE math:rand( INT|LONG seed) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic seed An optional seed value that will be used to generate the random number sequence. defaultSeed INT LONG Yes Yes Examples EXAMPLE 1 define stream InValueStream (symbol string, price long, volume long); from InValueStream select symbol, math:rand() as randNumber select math:oct(inValue) as octValue insert into OutMediationStream; In the example given above, a random double value between 0 and 1 will be generated using math:rand(). round (Function) This function returns the value of the input argument rounded off to the closest integer/long value. Origin: siddhi-execution-math:5.0.4 Syntax INT|LONG math:round( FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that should be rounded off to the closest integer/long value. FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:round(inValue) as roundValue insert into OutMediationStream; The function rounds off 'inValue1' to the closest int/long value and directs the output to the output stream, 'OutMediationStream'. For example, round(3252.353) returns 3252. signum (Function) This returns +1, 0, or -1 for the given positive, zero and negative values respectively. This function wraps the java.lang.Math.signum() function. Origin: siddhi-execution-math:5.0.4 Syntax INT math:signum( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that should be checked to be positive, negative or zero. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:signum(inValue) as sign insert into OutMediationStream; The function evaluates the 'inValue' given to be positive, negative or zero and directs the result to the output stream, 'OutMediationStream'. For example, signum(-6.32d) returns -1. sin (Function) This returns the sine of the value given in radians. This function wraps the java.lang.Math.sin() function. Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:sin( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose sine value should be found. Input is required to be in radians. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:sin(inValue) as sinValue insert into OutMediationStream; The function calculates the sine value of the given 'inValue' and directs the output to the output stream, 'OutMediationStream. For example, sin(6d) returns -0.27941549819892586. sinh (Function) This returns the hyperbolic sine of the value given in radians. This function wraps the java.lang.Math.sinh() function. Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:sinh( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose hyperbolic sine value should be found. Input is required to be in radians. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:sinh(inValue) as sinhValue insert into OutMediationStream; This function calculates the hyperbolic sine value of 'inValue' and directs the output to the output stream, 'OutMediationStream'. For example, sinh(6d) returns 201.71315737027922. sqrt (Function) This function returns the square-root of the given value. It wraps the java.lang.Math.sqrt() s function. Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:sqrt( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose square-root value should be found. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:sqrt(inValue) as sqrtValue insert into OutMediationStream; The function calculates the square-root value of the 'inValue' and directs the output to the output stream, 'OutMediationStream'. For example, sqrt(4d) returns 2. tan (Function) This function returns the tan of the given value in radians. It wraps the java.lang.Math.tan() function. Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:tan( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose tan value should be found. Input is required to be in radians. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:tan(inValue) as tanValue insert into OutMediationStream; This function calculates the tan value of the 'inValue' given and directs the output to the output stream, 'OutMediationStream'. For example, tan(6d) returns -0.29100619138474915. tanh (Function) This function returns the hyperbolic tangent of the value given in radians. It wraps the java.lang.Math.tanh() function. Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:tanh( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose hyperbolic tangent value should be found. Input is required to be in radians. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:tanh(inValue) as tanhValue insert into OutMediationStream; If the 'inVaue' in the input stream is given, this function calculates the hyperbolic tangent value of the same and directs the output to 'OutMediationStream' stream. For example, tanh(6d) returns 0.9999877116507956. toDegrees (Function) This function converts the value given in radians to degrees. It wraps the java.lang.Math.toDegrees() function. Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:toDegrees( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The input value in radians that should be converted to degrees. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:toDegrees(inValue) as degreesValue insert into OutMediationStream; The function converts the 'inValue' in the input stream from radians to degrees and directs the output to 'OutMediationStream' output stream. For example, toDegrees(6d) returns 343.77467707849394. toRadians (Function) This function converts the value given in degrees to radians. It wraps the java.lang.Math.toRadians() function. Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:toRadians( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The input value in degrees that should be converted to radians. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:toRadians(inValue) as radiansValue insert into OutMediationStream; This function converts the input, from degrees to radians and directs the result to 'OutMediationStream' output stream. For example, toRadians(6d) returns 0.10471975511965977. Rdbms cud (Stream Processor) This function performs SQL CUD (INSERT, UPDATE, DELETE) queries on data sources. Note: This function to work data sources should be set at the Siddhi Manager level. Origin: siddhi-store-rdbms:7.0.1 Syntax rdbms:cud( STRING datasource.name, STRING query) rdbms:cud( STRING datasource.name, STRING query, STRING|BOOL|INT|DOUBLE|FLOAT|LONG parameter) rdbms:cud( STRING datasource.name, STRING query, STRING|BOOL|INT|DOUBLE|FLOAT|LONG parameter, STRING|BOOL|INT|DOUBLE|FLOAT|LONG ...) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic datasource.name The name of the datasource for which the query should be performed. If Siddhi is used as a Java/Python library the datasource should be explicitly set in the siddhi manager in order for the function to work. STRING No No query The update, delete, or insert query(formatted according to the relevant database type) that needs to be performed. STRING No Yes parameter If the second parameter is a parametrised SQL query, then siddhi attributes can be passed to set the values of the parameters STRING BOOL INT DOUBLE FLOAT LONG Yes Yes System Parameters Name Description Default Value Possible Parameters perform.CUD.operations If this parameter is set to 'true', the RDBMS CUD function is enabled to perform CUD operations. false true false Extra Return Attributes Name Description Possible Types numRecords The number of records manipulated by the query. INT Examples EXAMPLE 1 from TriggerStream#rdbms:cud(\"SAMPLE_DB\", \"UPDATE Customers_Table SET customerName='abc' where customerName='xyz'\") select numRecords insert into RecordStream; This query updates the events from the input stream named 'TriggerStream' with an additional attribute named 'numRecords', of which the value indicates the number of records manipulated. The updated events are inserted into an output stream named 'RecordStream'. EXAMPLE 2 from TriggerStream#rdbms:cud(\"SAMPLE_DB\", \"UPDATE Customers_Table SET customerName=? where customerName=?\", changedName, previousName) select numRecords insert into RecordStream; This query updates the events from the input stream named 'TriggerStream' with an additional attribute named 'numRecords', of which the value indicates the number of records manipulated. The updated events are inserted into an output stream named 'RecordStream'. Here the values of attributes changedName and previousName in the event will be set to the query. query (Stream Processor) This function performs SQL retrieval queries on data sources. Note: This function to work data sources should be set at the Siddhi Manager level. Origin: siddhi-store-rdbms:7.0.1 Syntax rdbms:query( STRING datasource.name, STRING attribute.definition.list, STRING query) rdbms:query( STRING datasource.name, STRING attribute.definition.list, STRING query, STRING|BOOL|INT|DOUBLE|FLOAT|LONG parameter) rdbms:query( STRING datasource.name, STRING attribute.definition.list, STRING query, STRING|BOOL|INT|DOUBLE|FLOAT|LONG parameter, STRING|BOOL|INT|DOUBLE|FLOAT|LONG ...) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic datasource.name The name of the datasource for which the query should be performed. If Siddhi is used as a Java/Python library the datasource should be explicitly set in the siddhi manager in order for the function to work. STRING No No attribute.definition.list This is provided as a comma-separated list in the ' AttributeName AttributeType ' format. The SQL query is expected to return the attributes in the given order. e.g., If one attribute is defined here, the SQL query should return one column result set. If more than one column is returned, then the first column is processed. The Siddhi data types supported are 'STRING', 'INT', 'LONG', 'DOUBLE', 'FLOAT', and 'BOOL'. Mapping of the Siddhi data type to the database data type can be done as follows, Siddhi Datatype - Datasource Datatype STRING - CHAR , VARCHAR , LONGVARCHAR INT - INTEGER LONG - BIGINT DOUBLE - DOUBLE FLOAT - REAL BOOL - BIT STRING No No query The select query(formatted according to the relevant database type) that needs to be performed STRING No Yes parameter If the second parameter is a parametrised SQL query, then siddhi attributes can be passed to set the values of the parameters STRING BOOL INT DOUBLE FLOAT LONG Yes Yes Extra Return Attributes Name Description Possible Types attributeName The return attributes will be the ones defined in the parameter attribute.definition.list . STRING INT LONG DOUBLE FLOAT BOOL Examples EXAMPLE 1 from TriggerStream#rdbms:query('SAMPLE_DB', 'creditcardno string, country string, transaction string, amount int', 'select * from Transactions_Table') select creditcardno, country, transaction, amount insert into recordStream; Events inserted into recordStream includes all records matched for the query i.e an event will be generated for each record retrieved from the datasource. The event will include as additional attributes, the attributes defined in the attribute.definition.list (creditcardno, country, transaction, amount). EXAMPLE 2 from TriggerStream#rdbms:query('SAMPLE_DB', 'creditcardno string, country string,transaction string, amount int', 'select * from where country=?', countrySearchWord) select creditcardno, country, transaction, amount insert into recordStream; Events inserted into recordStream includes all records matched for the query i.e an event will be generated for each record retrieved from the datasource. The event will include as additional attributes, the attributes defined in the attribute.definition.list (creditcardno, country, transaction, amount). countrySearchWord value from the event will be set in the query when querying the datasource. Regex find (Function) Finds the subsequence that matches the given regex pattern. Origin: siddhi-execution-regex:5.0.5 Syntax BOOL regex:find( STRING regex, STRING input.sequence) BOOL regex:find( STRING regex, STRING input.sequence, INT starting.index) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic regex A regular expression that is matched to a sequence in order to find the subsequence of the same. For example, \\d\\d(.*)WSO2 . STRING No Yes input.sequence The input sequence to be matched with the regular expression. For example, 21 products are produced by WSO2 . STRING No Yes starting.index The starting index of the input sequence from where the input sequence ismatched with the given regex pattern.For example, 10 . 0 INT Yes Yes Examples EXAMPLE 1 regex:find('\\d\\d(.*)WSO2', '21 products are produced by WSO2 currently') This method attempts to find the subsequence of the input.sequence that matches the regex pattern, \\d\\d(. )WSO2 . It returns true as a subsequence exists. EXAMPLE 2 regex:find('\\d\\d(.*)WSO2', '21 products are produced by WSO2.', 4) This method attempts to find the subsequence of the input.sequence that matches the regex pattern, \\d\\d(. )WSO2 starting from index 4 . It returns 'false' as subsequence does not exists. group (Function) Returns the subsequence captured by the given group during the regex match operation. Origin: siddhi-execution-regex:5.0.5 Syntax STRING regex:group( STRING regex, STRING input.sequence, INT group.id) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic regex A regular expression. For example, \\d\\d(.*)WSO2. STRING No Yes input.sequence The input sequence to be matched with the regular expression. For example, 2 1 products are produced by WSO2 . STRING No Yes group.id The given group id of the regex expression. For example, 2 . INT No Yes Examples EXAMPLE 1 regex:group('\\d\\d(.*)(WSO2.*)(WSO2.*)', '21 products are produced within 10 years by WSO2 currently by WSO2 employees', 3) Function returns 'WSO2 employees', the subsequence captured by the groupID 3 according to the regex pattern, \\d\\d(. )(WSO2. )(WSO2.*) . lookingAt (Function) Matches the input.sequence from the beginning against the regex pattern, and unlike regex:matches() it does not require that the entire input.sequence be matched. Origin: siddhi-execution-regex:5.0.5 Syntax BOOL regex:lookingAt( STRING regex, STRING input.sequence) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic regex A regular expression. For example, \\d\\d(.*)WSO2 . STRING No Yes input.sequence The input sequence to be matched with the regular expression. For example, 21 products are produced by WSO2 . STRING No Yes Examples EXAMPLE 1 regex:lookingAt('\\d\\d(.*)(WSO2.*)', '21 products are produced by WSO2 currently in Sri Lanka') Function matches the input.sequence against the regex pattern, \\d\\d(. )(WSO2. ) from the beginning, and as it matches it returns true . EXAMPLE 2 regex:lookingAt('WSO2(.*)middleware(.*)', 'sample test string and WSO2 is situated in trace and it's a middleware company') Function matches the input.sequence against the regex pattern, WSO2(. )middleware(. ) from the beginning, and as it does not match it returns false . matches (Function) Matches the entire input.sequence against the regex pattern. Origin: siddhi-execution-regex:5.0.5 Syntax BOOL regex:matches( STRING regex, STRING input.sequence) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic regex A regular expression. For example, \\d\\d(.*)WSO2 . STRING No Yes input.sequence The input sequence to be matched with the regular expression. For example, 21 products are produced by WSO2 . STRING No Yes Examples EXAMPLE 1 regex:matches('WSO2(.*)middleware(.*)', 'WSO2 is situated in trace and its a middleware company') Function matches the entire input.sequence against WSO2(. )middleware(. ) regex pattern, and as it matches it returns true . EXAMPLE 2 regex:matches('WSO2(.*)middleware', 'WSO2 is situated in trace and its a middleware company') Function matches the entire input.sequence against WSO2(.*)middleware regex pattern. As it does not match it returns false . Reorder akslack (Stream Processor) Stream processor performs reordering of out-of-order events optimized for a givenparameter using AQ-K-Slack algorithm . This is best for reordering events on attributes those are used for aggregations.data . Origin: siddhi-execution-reorder:5.0.3 Syntax reorder:akslack( LONG timestamp, INT|FLOAT|LONG|DOUBLE correlation.field) reorder:akslack( LONG timestamp, INT|FLOAT|LONG|DOUBLE correlation.field, LONG batch.size) reorder:akslack( LONG timestamp, INT|FLOAT|LONG|DOUBLE correlation.field, LONG batch.size, LONG timeout) reorder:akslack( LONG timestamp, INT|FLOAT|LONG|DOUBLE correlation.field, LONG batch.size, LONG timeout, LONG max.k) reorder:akslack( LONG timestamp, INT|FLOAT|LONG|DOUBLE correlation.field, LONG batch.size, LONG timeout, LONG max.k, BOOL discard.late.arrival) reorder:akslack( LONG timestamp, INT|FLOAT|LONG|DOUBLE correlation.field, LONG batch.size, LONG timeout, LONG max.k, BOOL discard.late.arrival, DOUBLE error.threshold, DOUBLE confidence.level) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic timestamp The event timestamp on which the events should be ordered. LONG No Yes correlation.field By monitoring the changes in this field Alpha K-Slack dynamically optimises its behavior. This field is used to calculate the runtime window coverage threshold, which represents the upper limit set for unsuccessfully handled late arrivals. INT FLOAT LONG DOUBLE No Yes batch.size The parameter 'batch.size' denotes the number of events that should be considered in the calculation of an alpha value. This should be greater than or equal to 15. 10,000 LONG Yes No timeout A timeout value in milliseconds, where the buffered events who are older than the given timeout period get flushed every second. -1 (timeout is infinite) LONG Yes No max.k The maximum K-Slack window threshold ('K' parameter). 9,223,372,036,854,775,807 (The maximum Long value) LONG Yes No discard.late.arrival If set to true the processor would discarded the out-of-order events arriving later than the K-Slack window, and in otherwise it allows the late arrivals to proceed. false BOOL Yes No error.threshold The error threshold to be applied in Alpha K-Slack algorithm. 0.03 (3%) DOUBLE Yes No confidence.level The confidence level to be applied in Alpha K-Slack algorithm. 0.95 (95%) DOUBLE Yes No Examples EXAMPLE 1 define stream StockStream (eventTime long, symbol string, volume long); @info(name = 'query1') from StockStream#reorder:akslack(eventTime, volume, 20)#window.time(5 min) select eventTime, symbol, sum(volume) as total insert into OutputStream; The query reorders events based on the 'eventTime' attribute value and optimises for aggregating 'volume' attribute considering last 20 events. kslack (Stream Processor) Stream processor performs reordering of out-of-order events using K-Slack algorithm . Origin: siddhi-execution-reorder:5.0.3 Syntax reorder:kslack( LONG timestamp) reorder:kslack( LONG timestamp, LONG timeout) reorder:kslack( LONG timestamp, BOOL discard.late.arrival) reorder:kslack( LONG timestamp, LONG timeout, LONG max.k) reorder:kslack( LONG timestamp, LONG timeout, BOOL discard.late.arrival) reorder:kslack( LONG timestamp, LONG timeout, LONG max.k, BOOL discard.late.arrival) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic timestamp The event timestamp on which the events should be ordered. LONG No Yes timeout A timeout value in milliseconds, where the buffered events who are older than the given timeout period get flushed every second. -1 (timeout is infinite) LONG Yes No max.k The maximum K-Slack window threshold ('K' parameter). 9,223,372,036,854,775,807 (The maximum Long value) LONG Yes No discard.late.arrival If set to true the processor would discarded the out-of-order events arriving later than the K-Slack window, and in otherwise it allows the late arrivals to proceed. false BOOL Yes No Examples EXAMPLE 1 define stream StockStream (eventTime long, symbol string, volume long); @info(name = 'query1') from StockStream#reorder:kslack(eventTime, 5000) select eventTime, symbol, volume insert into OutputStream; The query reorders events based on the 'eventTime' attribute value, and it forcefully flushes all the events who have arrived older than the given 'timeout' value ( 5000 milliseconds) every second. Script javascript (Script) This extension allows you to include JavaScript functions within the Siddhi Query Language. Origin: siddhi-script-js:5.0.2 Syntax define function FunctionName [javascript] return type { // Script code }; Examples EXAMPLE 1 define function concatJ[JavaScript] return string {\" var str1 = data[0]; var str2 = data[1]; var str3 = data[2]; var res = str1.concat(str2,str3); return res; }; This JS function will consume 3 var variables, concatenate them and will return as a string Sink email (Sink) The email sink uses the 'smtp' server to publish events via emails. The events can be published in 'text', 'xml' or 'json' formats. The user can define email sink parameters in either the ' SP_HOME /conf/ PROFILE /deployment yaml' file or in the stream definition. The email sink first checks the stream definition for parameters, and if they are no configured there, it checks the 'deployment.yaml' file. If the parameters are not configured in either place, default values are considered for optional parameters. If you need to configure server system parameters that are not provided as options in the stream definition, then those parameters need to be defined them in the 'deployment.yaml' file under 'email sink properties'. For more information about the SMTP server parameters, see https://javaee.github.io/javamail/SMTP-Transport. Further, some email accounts are required to enable the 'access to less secure apps' option. For gmail accounts, you can enable this option via https://myaccount.google.com/lesssecureapps. Origin: siddhi-io-email:2.0.4 Syntax @sink(type=\"email\", username=\" STRING \", address=\" STRING \", password=\" STRING \", host=\" STRING \", port=\" INT \", ssl.enable=\" BOOL \", auth=\" BOOL \", content.type=\" STRING \", subject=\" STRING \", to=\" STRING \", cc=\" STRING \", bcc=\" STRING \", attachments=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic username The username of the email account that is used to send emails. e.g., 'abc' is the username of the 'abc@gmail.com' account. STRING No No address The address of the email account that is used to send emails. STRING No No password The password of the email account. STRING No No host The host name of the SMTP server. e.g., 'smtp.gmail.com' is a host name for a gmail account. The default value 'smtp.gmail.com' is only valid if the email account is a gmail account. smtp.gmail.com STRING Yes No port The port that is used to create the connection. '465' the default value is only valid is SSL is enabled. INT Yes No ssl.enable This parameter specifies whether the connection should be established via a secure connection or not. The value can be either 'true' or 'false'. If it is 'true', then the connection is establish via the 493 port which is a secure connection. true BOOL Yes No auth This parameter specifies whether to use the 'AUTH' command when authenticating or not. If the parameter is set to 'true', an attempt is made to authenticate the user using the 'AUTH' command. true BOOL Yes No content.type The content type can be either 'text/plain' or 'text/html'. text/plain STRING Yes No subject The subject of the mail to be send. STRING No Yes to The address of the 'to' recipient. If there are more than one 'to' recipients, then all the required addresses can be given as a comma-separated list. STRING No Yes cc The address of the 'cc' recipient. If there are more than one 'cc' recipients, then all the required addresses can be given as a comma-separated list. None STRING Yes No bcc The address of the 'bcc' recipient. If there are more than one 'bcc' recipients, then all the required addresses can be given as a comma-separated list. None STRING Yes No attachments File paths of the files that need to be attached to the email. These paths should be absolute paths. They can be either directories or files . If the path is to a directory, all the files located at the first level (i.e., not within another sub directory) are attached. None STRING Yes Yes System Parameters Name Description Default Value Possible Parameters mail.smtp.ssl.trust If this parameter is se, and a socket factory has not been specified, it enables the use of a MailSSLSocketFactory. If this parameter is set to \" \", all the hosts are trusted. If it is set to a whitespace-separated list of hosts, only those specified hosts are trusted. If not, the hosts trusted depends on the certificate presented by the server. String mail.smtp.connectiontimeout The socket connection timeout value in milliseconds. infinite timeout Any Integer mail.smtp.timeout The socket I/O timeout value in milliseconds. infinite timeout Any Integer mail.smtp.from The email address to use for the SMTP MAIL command. This sets the envelope return address. Defaults to msg.getFrom() or InternetAddress.getLocalAddress(). Any valid email address mail.smtp.localport The local port number to bind to when creating the SMTP socket. Defaults to the port number picked by the Socket class. Any Integer mail.smtp.ehlo If this parameter is set to 'false', you must not attempt to sign in with the EHLO command. true true or false mail.smtp.auth.login.disable If this is set to 'true', it is not allowed to use the 'AUTH LOGIN' command. false true or false mail.smtp.auth.plain.disable If this parameter is set to 'true', it is not allowed to use the 'AUTH PLAIN' command. false true or false mail.smtp.auth.digest-md5.disable If this parameter is set to 'true', it is not allowed to use the 'AUTH DIGEST-MD5' command. false true or false mail.smtp.auth.ntlm.disable If this parameter is set to 'true', it is not allowed to use the 'AUTH NTLM' command false true or false mail.smtp.auth.ntlm.domain The NTLM authentication domain. None The valid NTLM authentication domain name. mail.smtp.auth.ntlm.flags NTLM protocol-specific flags. For more details, see http://curl.haxx.se/rfc/ntlm.html#theNtlmFlags. None Valid NTLM protocol-specific flags. mail.smtp.dsn.notify The NOTIFY option to the RCPT command. None Either 'NEVER', or a combination of 'SUCCESS', 'FAILURE', and 'DELAY' (separated by commas). mail.smtp.dsn.ret The 'RET' option to the 'MAIL' command. None Either 'FULL' or 'HDRS'. mail.smtp.sendpartial If this parameter is set to 'true' and a message is addressed to both valid and invalid addresses, the message is sent with a log that reports the partial failure with a 'SendFailedException' error. If this parameter is set to 'false' (which is default), the message is not sent to any of the recipients when the recipient lists contain one or more invalid addresses. false true or false mail.smtp.sasl.enable If this parameter is set to 'true', the system attempts to use the 'javax.security.sasl' package to choose an authentication mechanism for the login. false true or false mail.smtp.sasl.mechanisms Enter a space or a comma-separated list of SASL mechanism names that the system shouldt try to use. None mail.smtp.sasl.authorizationid The authorization ID to be used in the SASL authentication. If no value is specified, the authentication ID (i.e., username) is used. username Valid ID mail.smtp.sasl.realm The realm to be used with the 'DIGEST-MD5' authentication. None mail.smtp.quitwait If this parameter is set to 'false', the 'QUIT' command is issued and the connection is immediately closed. If this parameter is set to 'true' (which is default), the transport waits for the response to the QUIT command. false true or false mail.smtp.reportsuccess If this parameter is set to 'true', the transport to includes an 'SMTPAddressSucceededException' for each address to which the message is successfully delivered. false true or false mail.smtp.socketFactory If this parameter is set to a class that implements the 'javax.net.SocketFactory' interface, this class is used to create SMTP sockets. None Socket Factory mail.smtp.socketFactory.class If this parameter is set, it specifies the name of a class that implements the 'javax.net.SocketFactory interface'. This class is used to create SMTP sockets. None mail.smtp.socketFactory.fallback If this parameter is set to 'true', the failure to create a socket using the specified socket factory class causes the socket to be created using the 'java.net.Socket' class. true true or false mail.smtp.socketFactory.port This specifies the port to connect to when using the specified socket factory. 25 Valid port number mail.smtp.ssl.protocols This specifies the SSL protocols that need to be enabled for the SSL connections. None This parameter specifies a whitespace separated list of tokens that are acceptable to the 'javax.net.ssl.SSLSocket.setEnabledProtocols' method. mail.smtp.starttls.enable If this parameter is set to 'true', it is possible to issue the 'STARTTLS' command (if supported by the server) to switch the connection to a TLS-protected connection before issuing any login commands. false true or false mail.smtp.starttls.required If this parameter is set to 'true', it is required to use the 'STARTTLS' command. If the server does not support the 'STARTTLS' command, or if the command fails, the connection method will fail. false true or false mail.smtp.socks.host This specifies the host name of a SOCKS5 proxy server to be used for the connections to the mail server. None mail.smtp.socks.port This specifies the port number for the SOCKS5 proxy server. This needs to be used only if the proxy server is not using the standard port number 1080. 1080 valid port number mail.smtp.auth.ntlm.disable If this parameter is set to 'true', the AUTH NTLM command cannot be issued. false true or false mail.smtp.mailextension The extension string to be appended to the MAIL command. None mail.smtp.userset If this parameter is set to 'true', you should use the 'RSET' command instead of the 'NOOP' command in the 'isConnected' method. In some scenarios, 'sendmail' responds slowly after many 'NOOP' commands. This is avoided by using 'RSET' instead. false true or false Examples EXAMPLE 1 @sink(type='email', @map(type ='json'), username='sender.account', address='sender.account@gmail.com',password='account.password',subject='Alerts from Wso2 Stream Processor',to='{{email}}',)define stream FooStream (email string, loginId int, name string); This example illustrates how to publish events via an email sink based on the values provided for the mandatory parameters. As shown in the example, it publishes events from the 'FooStream' in 'json' format as emails to the specified 'to' recipients via the email sink. The email is sent from the 'sender.account@gmail.com' email address via a secure connection. EXAMPLE 2 @sink(type='email', @map(type ='json'), subject='Alerts from Wso2 Stream Processor',to='{{email}}',)define stream FooStream (email string, loginId int, name string); This example illustrates how to configure the query parameters and the system parameters in the 'deployment.yaml' file. Corresponding parameters need to be configured under 'email', and namespace:'sink' as follows: siddhi: extensions: - extension: name:'email' namespace:'sink' properties: username: sender's email username address: sender's email address password: sender's email password As shown in the example, events from the FooStream are published in 'json' format via the email sink as emails to the given 'to' recipients. The email is sent from the 'sender.account@gmail.com' address via a secure connection. EXAMPLE 3 @sink(type='email', @map(type ='json'), username='sender.account', address='sender.account@gmail.com',password='account.password',host='smtp.gmail.com',port='465',ssl.enable='true',auth='true',content.type='text/html',subject='Alerts from Wso2 Stream Processor-{{name}}',to='to1.account@gmail.com, to2.account@gmail.com',cc='cc1.account@gmail.com, cc2.account@gmail.com',bcc='bcc1.account@gmail.com)define stream FooStream (name string, age int, country string); This example illustrates how to publish events via the email sink. Events from the 'FooStream' stream are published in 'xml' format via the email sink as a text/html message and sent to the specified 'to', 'cc', and 'bcc' recipients via a secure connection. The 'name' namespace in the 'subject' attribute is the value of the 'name' parameter in the corresponding output event. EXAMPLE 4 @sink(type='email', @map(type ='json'), username='sender.account', address='sender.account@gmail.com',password='account.password',host='smtp.gmail.com',port='465',ssl.enable='true',auth='true',content.type='text/html',subject='Alerts from Wso2 Stream Processor-{{name}}',to='to1.account@gmail.com, to2.account@gmail.com',cc='cc1.account@gmail.com, cc2.account@gmail.com',bcc='bcc1.account@gmail.comattachments= '{{attachments}}')define stream FooStream (name string, age int, country string, attachments string); This example illustrates how to publish events via the email sink. Here, the email also contains attachments. Events from the FooStream are published in 'xml' format via the email sink as a 'text/html' message to the specified 'to','cc', and 'bcc' recipients via a secure connection. The 'name' namespace in the 'subject' attribute is the value for the 'name' parameter in the corresponding output event. The attachments included in the email message are the local files available in the path specified as the value for the 'attachments' attribute. file (Sink) File Sink can be used to publish (write) event data which is processed within siddhi to files. Siddhi-io-file sink provides support to write both textual and binary data into files Origin: siddhi-io-file:2.0.3 Syntax @sink(type=\"file\", file.uri=\" STRING \", append=\" BOOL \", add.line.separator=\" BOOL \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic file.uri Used to specify the file for data to be written. STRING No Yes append This parameter is used to specify whether the data should be append to the file or not. If append = 'true', data will be write at the end of the file without changing the existing content. If file does not exist, a new fill will be crated and then data will be written. If append append = 'false', If given file exists, existing content will be deleted and then data will be written back to the file. If given file does not exist, a new file will be created and then data will be written on it. true BOOL Yes No add.line.separator This parameter is used to specify whether events added to the file should be separated by a newline. If add.event.separator= 'true',then a newline will be added after data is added to the file. true. (However, if csv mapper is used, it is false) BOOL Yes No Examples EXAMPLE 1 @sink(type='file', @map(type='json'), append='false', file.uri='/abc/{{symbol}}.txt') define stream BarStream (symbol string, price float, volume long); Under above configuration, for each event, a file will be generated if there's no such a file,and then data will be written to that file as json messagesoutput will looks like below. { \"event\":{ \"symbol\":\"WSO2\", \"price\":55.6, \"volume\":100 } } grpc (Sink) This extension publishes event data encoded into GRPC Classes as defined in the user input jar. This extension has a default gRPC service classes added. The default service is called \"EventService\". Please find the protobuf definition here . If we want to use our custom gRPC services, we have to pack auto-generated gRPC service classes and protobuf classes into a jar file and add it into the project classpath (or to the jars folder in the siddhi-tooling folder if we use it with siddhi-tooling ). Please find the custom protobuf definition that uses in examples here . This grpc sink is used for scenarios where we send a request and don't expect a response back. I.e getting a google.protobuf.Empty response back. Origin: siddhi-io-grpc:1.0.2 Syntax @sink(type=\"grpc\", publisher.url=\" STRING \", headers=\" STRING \", idle.timeout=\" LONG \", keep.alive.time=\" LONG \", keep.alive.timeout=\" LONG \", keep.alive.without.calls=\" BOOL \", enable.retry=\" BOOL \", max.retry.attempts=\" INT \", retry.buffer.size=\" LONG \", per.rpc.buffer.size=\" LONG \", channel.termination.waiting.time=\" LONG \", truststore.file=\" STRING \", truststore.password=\" STRING \", truststore.algorithm=\" STRING \", tls.store.type=\" STRING \", keystore.file=\" STRING \", keystore.password=\" STRING \", keystore.algorithm=\" STRING \", enable.ssl=\" BOOL \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic publisher.url The url to which the outgoing events should be published via this extension. This url should consist the host hostPort, port, fully qualified service name, method name in the following format. grpc://0.0.0.0:9763/ serviceName / methodName For example: grpc://0.0.0.0:9763/org.wso2.grpc.EventService/consume STRING No No headers GRPC Request headers in format \"' key : value ',' key : value '\" . If header parameter is not provided just the payload is sent - STRING Yes No idle.timeout Set the duration in seconds without ongoing RPCs before going to idle mode. 1800 LONG Yes No keep.alive.time Sets the time in seconds without read activity before sending a keepalive ping. Keepalives can increase the load on services so must be used with caution. By default set to Long.MAX_VALUE which disables keep alive pinging. Long.MAX_VALUE LONG Yes No keep.alive.timeout Sets the time in seconds waiting for read activity after sending a keepalive ping. 20 LONG Yes No keep.alive.without.calls Sets whether keepalive will be performed when there are no outstanding RPC on a connection. false BOOL Yes No enable.retry Enables the retry mechanism provided by the gRPC library. false BOOL Yes No max.retry.attempts Sets max number of retry attempts. The total number of retry attempts for each RPC will not exceed this number even if service config may allow a higher number. 5 INT Yes No retry.buffer.size Sets the retry buffer size in bytes. If the buffer limit is exceeded, no RPC could retry at the moment, and in hedging case all hedges but one of the same RPC will cancel. 16777216 LONG Yes No per.rpc.buffer.size Sets the per RPC buffer limit in bytes used for retry. The RPC is not retriable if its buffer limit is exceeded. 1048576 LONG Yes No channel.termination.waiting.time The time in seconds to wait for the channel to become terminated, giving up if the timeout is reached. 5 LONG Yes No truststore.file the file path of truststore. If this is provided then server authentication is enabled - STRING Yes No truststore.password the password of truststore. If this is provided then the integrity of the keystore is checked - STRING Yes No truststore.algorithm the encryption algorithm to be used for server authentication - STRING Yes No tls.store.type TLS store type - STRING Yes No keystore.file the file path of keystore. If this is provided then client authentication is enabled - STRING Yes No keystore.password the password of keystore - STRING Yes No keystore.algorithm the encryption algorithm to be used for client authentication - STRING Yes No enable.ssl to enable ssl. If set to true and truststore.file is not given then it will be set to default carbon jks by default FALSE BOOL Yes No Examples EXAMPLE 1 @sink(type='grpc', publisher.url = 'grpc://134.23.43.35:8080/org.wso2.grpc.EventService/consume', @map(type='json')) define stream FooStream (message String); Here a stream named FooStream is defined with grpc sink. A grpc server should be running at 194.23.98.100 listening to port 8080. sink.id is set to 1 here. So we can write a source with sink.id 1 so that it will listen to responses for requests published from this stream. Note that since we are using EventService/consume the sink will be operating in default mode EXAMPLE 2 @sink(type='grpc', publisher.url = 'grpc://134.23.43.35:8080/org.wso2.grpc.EventService/consume', headers='{{headers}}', @map(type='json'), @payload('{{message}}')) define stream FooStream (message String, headers String); A similar example to above but with headers. Headers are also send into the stream as a data. In the sink headers dynamic property reads the value and sends it as MetaData with the request EXAMPLE 3 @sink(type='grpc', publisher.url = 'grpc://134.23.43.35:8080/org.wso2.grpc.MyService/send', @map(type='protobuf'), define stream FooStream (stringValue string, intValue int,longValue long,booleanValue bool,floatValue float,doubleValue double); Here a stream named FooStream is defined with grpc sink. A grpc server should be running at 134.23.43.35 listening to port 8080 since there is no mapper provided, attributes of stream definition should be as same as the attributes of protobuf message definition. EXAMPLE 4 @sink(type='grpc', publisher.url = 'grpc://134.23.43.35:8080/org.wso2.grpc.MyService/testMap', @map(type='protobuf'), define stream FooStream (stringValue string, intValue int,map object); Here a stream named FooStream is defined with grpc sink. A grpc server should be running at 134.23.43.35 listening to port 8080. The 'map object' in the stream definition defines that this stream is going to use Map object with grpc service. We can use any map object that extends 'java.util.AbstractMap' class. EXAMPLE 5 @sink(type='grpc', publisher.url = 'grpc://134.23.43.35:8080/org.wso2.grpc.MyService/testMap', @map(type='protobuf', @payload(stringValue='a',longValue='b',intValue='c',booleanValue='d',floatValue = 'e', doubleValue = 'f'))) define stream FooStream (a string, b long, c int,d bool,e float,f double); Here a stream named FooStream is defined with grpc sink. A grpc server should be running at 194.23.98.100 listening to port 8080. @payload is provided in this stream, therefore we can use any name for the attributes in the stream definition, but we should correctly map those names with protobuf message attributes. If we are planning to send metadata within a stream we should use @payload to map attributes to identify the metadata attribute and the protobuf attributes separately. EXAMPLE 6 @sink(type='grpc', publisher.url = 'grpc://194.23.98.100:8888/org.wso2.grpc.test.StreamService/clientStream', @map(type='protobuf')) define stream FooStream (stringValue string, intValue int,longValue long,booleanValue bool,floatValue float,doubleValue double); Here in the grpc sink, we are sending a stream of requests to the server that runs on 194.23.98.100 and port 8888. When we need to send a stream of requests from the grpc sink we have to define a client stream RPC method.Then the siddhi will identify whether it's a unary method or a stream method and send requests according to the method type. grpc-call (Sink) This extension publishes event data encoded into GRPC Classes as defined in the user input jar. This extension has a default gRPC service classes jar added. The default service is called \"EventService\". Please find the protobuf definition here . If we want to use our custom gRPC services, we have to pack auto-generated gRPC service classes and protobuf classes into a jar file and add it into the project classpath (or to the jars folder in the siddhi-tooling folder if we use it with siddhi-tooling ). Please find the custom protobuf definition that uses in examples here . This grpc-call sink is used for scenarios where we send a request out and expect a response back. In default mode this will use EventService process method. grpc-call-response source is used to receive the responses. A unique sink.id is used to correlate between the sink and its corresponding source. Origin: siddhi-io-grpc:1.0.2 Syntax @sink(type=\"grpc-call\", publisher.url=\" STRING \", sink.id=\" INT \", headers=\" STRING \", idle.timeout=\" LONG \", keep.alive.time=\" LONG \", keep.alive.timeout=\" LONG \", keep.alive.without.calls=\" BOOL \", enable.retry=\" BOOL \", max.retry.attempts=\" INT \", retry.buffer.size=\" LONG \", per.rpc.buffer.size=\" LONG \", channel.termination.waiting.time=\" LONG \", max.inbound.message.size=\" LONG \", max.inbound.metadata.size=\" LONG \", truststore.file=\" STRING \", truststore.password=\" STRING \", truststore.algorithm=\" STRING \", tls.store.type=\" STRING \", keystore.file=\" STRING \", keystore.password=\" STRING \", keystore.algorithm=\" STRING \", enable.ssl=\" BOOL \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic publisher.url The url to which the outgoing events should be published via this extension. This url should consist the host hostPort, port, fully qualified service name, method name in the following format. grpc://0.0.0.0:9763/ serviceName / methodName For example: grpc://0.0.0.0:9763/org.wso2.grpc.EventService/consume STRING No No sink.id a unique ID that should be set for each grpc-call-sink. There is a 1:1 mapping between grpc-call sinks and grpc-call-response sources. Each sink has one particular source listening to the responses to requests published from that sink. So the same sink.id should be given when writing the source also. INT No No headers GRPC Request headers in format \"' key : value ',' key : value '\" . If header parameter is not provided just the payload is sent - STRING Yes No idle.timeout Set the duration in seconds without ongoing RPCs before going to idle mode. 1800 LONG Yes No keep.alive.time Sets the time in seconds without read activity before sending a keepalive ping. Keepalives can increase the load on services so must be used with caution. By default set to Long.MAX_VALUE which disables keep alive pinging. Long.MAX_VALUE LONG Yes No keep.alive.timeout Sets the time in seconds waiting for read activity after sending a keepalive ping. 20 LONG Yes No keep.alive.without.calls Sets whether keepalive will be performed when there are no outstanding RPC on a connection. false BOOL Yes No enable.retry Enables the retry and hedging mechanism provided by the gRPC library. false BOOL Yes No max.retry.attempts Sets max number of retry attempts. The total number of retry attempts for each RPC will not exceed this number even if service config may allow a higher number. 5 INT Yes No retry.buffer.size Sets the retry buffer size in bytes. If the buffer limit is exceeded, no RPC could retry at the moment, and in hedging case all hedges but one of the same RPC will cancel. 16777216 LONG Yes No per.rpc.buffer.size Sets the per RPC buffer limit in bytes used for retry. The RPC is not retriable if its buffer limit is exceeded. 1048576 LONG Yes No channel.termination.waiting.time The time in seconds to wait for the channel to become terminated, giving up if the timeout is reached. 5 LONG Yes No max.inbound.message.size Sets the maximum message size allowed to be received on the channel in bytes 4194304 LONG Yes No max.inbound.metadata.size Sets the maximum size of metadata allowed to be received in bytes 8192 LONG Yes No truststore.file the file path of truststore. If this is provided then server authentication is enabled - STRING Yes No truststore.password the password of truststore. If this is provided then the integrity of the keystore is checked - STRING Yes No truststore.algorithm the encryption algorithm to be used for server authentication - STRING Yes No tls.store.type TLS store type - STRING Yes No keystore.file the file path of keystore. If this is provided then client authentication is enabled - STRING Yes No keystore.password the password of keystore - STRING Yes No keystore.algorithm the encryption algorithm to be used for client authentication - STRING Yes No enable.ssl to enable ssl. If set to true and truststore.file is not given then it will be set to default carbon jks by default FALSE BOOL Yes No Examples EXAMPLE 1 @sink(type='grpc-call', publisher.url = 'grpc://194.23.98.100:8080/EventService/process', sink.id= '1', @map(type='json')) define stream FooStream (message String); @source(type='grpc-call-response', sink.id= '1') define stream BarStream (message String); Here a stream named FooStream is defined with grpc sink. A grpc server should be running at 194.23.98.100 listening to port 8080. sink.id is set to 1 here. So we can write a source with sink.id 1 so that it will listen to responses for requests published from this stream. Note that since we are using EventService/process the sink will be operating in default mode EXAMPLE 2 @sink(type='grpc-call', publisher.url = 'grpc://194.23.98.100:8080/EventService/process', sink.id= '1', @map(type='json')) define stream FooStream (message String); @source(type='grpc-call-response', sink.id= '1') define stream BarStream (message String); Here with the same FooStream definition we have added a BarStream which has a grpc-call-response source with the same sink.id 1. So the responses for calls sent from the FooStream will be added to BarStream. EXAMPLE 3 @sink(type='grpc-call', publisher.url = 'grpc://194.23.98.100:8888/org.wso2.grpc.test.MyService/process', sink.id= '1', @map(type='protobuf')) define stream FooStream (stringValue string, intValue int,longValue long,booleanValue bool,floatValue float,doubleValue double); @source(type='grpc-call-response', receiver.url = 'grpc://localhost:8888/org.wso2.grpc.MyService/process', sink.id= '1', @map(type='protobuf'))define stream FooStream (stringValue string, intValue int,longValue long,booleanValue bool,floatValue float,doubleValue double); Here a stream named FooStream is defined with grpc sink. A grpc server should be running at 194.23.98.100 listening to port 8080. We have added another stream called BarStream which is a grpc-call-response source with the same sink.id 1 and as same as FooStream definition. So the responses for calls sent from the FooStream will be added to BarStream. Since there is no mapping available in the stream definition attributes names should be as same as the attributes of the protobuf message definition. (Here the only reason we provide receiver.url in the grpc-call-response source is for protobuf mapper to map Response into a siddhi event, we can give any address and any port number in the URL, but we should provide the service name and the method name correctly) EXAMPLE 4 @sink(type='grpc-call', publisher.url = 'grpc://194.23.98.100:8888/org.wso2.grpc.test.MyService/process', sink.id= '1', @map(type='protobuf', @payload(stringValue='a',longValue='c',intValue='b',booleanValue='d',floatValue = 'e', doubleValue = 'f')))define stream FooStream (a string, b int,c long,d bool,e float,f double); @source(type='grpc-call-response', receiver.url = 'grpc://localhost:8888/org.wso2.grpc.test.MyService/process', sink.id= '1', @map(type='protobuf',@attributes(a = 'stringValue', b = 'intValue', c = 'longValue',d = 'booleanValue', e ='floatValue', f ='doubleValue')))define stream FooStream (a string, b int,c long,d bool,e float,f double); Here with the same FooStream definition we have added a BarStream which has a grpc-call-response source with the same sink.id 1. So the responses for calls sent from the FooStream will be added to BarStream. In this stream we provided mapping for both the sink and the source. so we can use any name for the attributes in the stream definition, but we have to map those attributes with correct protobuf attributes. As same as the grpc-sink, if we are planning to use metadata we should map the attributes. grpc-service-response (Sink) This extension is used to send responses back to a gRPC client after receiving requests through grpc-service source. This correlates with the particular source using a unique source.id Origin: siddhi-io-grpc:1.0.2 Syntax @sink(type=\"grpc-service-response\", source.id=\" INT \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic source.id A unique id to identify the correct source to which this sink is mapped. There is a 1:1 mapping between source and sink INT No No Examples EXAMPLE 1 @sink(type='grpc-service-response', source.id='1', @map(type='json')) define stream BarStream (messageId String, message String); @source(type='grpc-service', url='grpc://134.23.43.35:8080/org.wso2.grpc.EventService/process', source.id='1', @map(type='json', @attributes(messageId='trp:messageId', message='message'))) define stream FooStream (messageId String, message String); from FooStream select * insert into BarStream; The grpc requests are received through the grpc-service sink. Each received event is sent back through grpc-service-source. This is just a passthrough through Siddhi as we are selecting everything from FooStream and inserting into BarStream. http (Sink) HTTP sink publishes messages via HTTP or HTTPS protocols using methods such as POST, GET, PUT, and DELETE on formats text , XML and JSON . It can also publish to endpoints protected by basic authentication or OAuth 2.0. Origin: siddhi-io-http:2.1.2 Syntax @sink(type=\"http\", publisher.url=\" STRING \", basic.auth.username=\" STRING \", basic.auth.password=\" STRING \", https.truststore.file=\" STRING \", https.truststore.password=\" STRING \", oauth.username=\" STRING \", oauth.password=\" STRING \", consumer.key=\" STRING \", consumer.secret=\" STRING \", token.url=\" STRING \", refresh.token=\" STRING \", headers=\" STRING \", method=\" STRING \", socket.idle.timeout=\" INT \", chunk.disabled=\" BOOL \", ssl.protocol=\" STRING \", ssl.verification.disabled=\" BOOL \", tls.store.type=\" STRING \", ssl.configurations=\" STRING \", proxy.host=\" STRING \", proxy.port=\" STRING \", proxy.username=\" STRING \", proxy.password=\" STRING \", client.bootstrap.configurations=\" STRING \", max.pool.active.connections=\" INT \", min.pool.idle.connections=\" INT \", max.pool.idle.connections=\" INT \", min.evictable.idle.time=\" STRING \", time.between.eviction.runs=\" STRING \", max.wait.time=\" STRING \", test.on.borrow=\" BOOL \", test.while.idle=\" BOOL \", exhausted.action=\" INT \", hostname.verification.enabled=\" BOOL \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic publisher.url The URL to which the outgoing events should be published. Examples: http://localhost:8080/endpoint , https://localhost:8080/endpoint STRING No No basic.auth.username The username to be included in the authentication header when calling endpoints protected by basic authentication. basic.auth.password property should be also set when using this property. - STRING Yes No basic.auth.password The password to be included in the authentication header when calling endpoints protected by basic authentication. basic.auth.username property should be also set when using this property. - STRING Yes No https.truststore.file The file path of the client truststore when sending messages through https protocol. ${carbon.home}/resources/security/client-truststore.jks STRING Yes No https.truststore.password The password for the client-truststore. wso2carbon STRING Yes No oauth.username The username to be included in the authentication header when calling endpoints protected by OAuth 2.0. oauth.password property should be also set when using this property. - STRING Yes No oauth.password The password to be included in the authentication header when calling endpoints protected by OAuth 2.0. oauth.username property should be also set when using this property. - STRING Yes No consumer.key Consumer key used for calling endpoints protected by OAuth 2.0 - STRING Yes No consumer.secret Consumer secret used for calling endpoints protected by OAuth 2.0 - STRING Yes No token.url Token URL to generate a new access tokens when calling endpoints protected by OAuth 2.0 - STRING Yes No refresh.token Refresh token used for generating new access tokens when calling endpoints protected by OAuth 2.0 - STRING Yes No headers HTTP request headers in format \"' key : value ',' key : value '\" . When Content-Type header is not provided the system derives the Content-Type based on the provided sink mapper as following: - @map(type='xml') : application/xml - @map(type='json') : application/json - @map(type='text') : plain/text - @map(type='keyvalue') : application/x-www-form-urlencoded - For all other cases system defaults to plain/text Also the Content-Length header need not to be provided, as the system automatically defines it by calculating the size of the payload. Content-Type and Content-Length headers STRING Yes No method The HTTP method used for calling the endpoint. POST STRING Yes No socket.idle.timeout Socket timeout in millis. 6000 INT Yes No chunk.disabled Disable chunked transfer encoding. false BOOL Yes No ssl.protocol SSL/TLS protocol. TLS STRING Yes No ssl.verification.disabled Disable SSL verification. false BOOL Yes No tls.store.type TLS store type. JKS STRING Yes No ssl.configurations SSL/TSL configurations in format \"' key : value ',' key : value '\" . Some supported parameters: - SSL/TLS protocols: 'sslEnabledProtocols:TLSv1.1,TLSv1.2' - List of ciphers: 'ciphers:TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256' - Enable session creation: 'client.enable.session.creation:true' - Supported server names: 'server.suported.server.names:server' - Add HTTP SNIMatcher: 'server.supported.snimatchers:SNIMatcher' - STRING Yes No proxy.host Proxy server host - STRING Yes No proxy.port Proxy server port - STRING Yes No proxy.username Proxy server username - STRING Yes No proxy.password Proxy server password - STRING Yes No client.bootstrap.configurations Client bootstrap configurations in format \"' key : value ',' key : value '\" . Some supported configurations : - Client connect timeout in millis: 'client.bootstrap.connect.timeout:15000' - Client socket timeout in seconds: 'client.bootstrap.socket.timeout:15' - Client socket reuse: 'client.bootstrap.socket.reuse:true' - Enable TCP no delay: 'client.bootstrap.nodelay:true' - Enable client keep alive: 'client.bootstrap.keepalive:true' - Send buffer size: 'client.bootstrap.sendbuffersize:1048576' - Receive buffer size: 'client.bootstrap.recievebuffersize:1048576' - STRING Yes No max.pool.active.connections Maximum possible number of active connection per client pool. -1 INT Yes No min.pool.idle.connections Minimum number of idle connections that can exist per client pool. 0 INT Yes No max.pool.idle.connections Maximum number of idle connections that can exist per client pool. 100 INT Yes No min.evictable.idle.time Minimum time (in millis) a connection may sit idle in the client pool before it become eligible for eviction. 300000 STRING Yes No time.between.eviction.runs Time between two eviction operations (in millis) on the client pool. 30000 STRING Yes No max.wait.time The maximum time (in millis) the pool will wait (when there are no available connections) for a connection to be returned to the pool. 60000 STRING Yes No test.on.borrow Enable connections to be validated before being borrowed from the client pool. true BOOL Yes No test.while.idle Enable connections to be validated during the eviction operation (if any). true BOOL Yes No exhausted.action Action that should be taken when the maximum number of active connections are being used. This action should be indicated as an int and possible action values are following. 0 - Fail the request. 1 - Block the request, until a connection returns to the pool. 2 - Grow the connection pool size. 1 (Block when exhausted) INT Yes No hostname.verification.enabled Enable hostname verification. true BOOL Yes No System Parameters Name Description Default Value Possible Parameters clientBootstrapClientGroupSize Number of client threads to perform non-blocking read and write to one or more channels. (Number of available processors) * 2 Any positive integer clientBootstrapBossGroupSize Number of boss threads to accept incoming connections. Number of available processors Any positive integer clientBootstrapWorkerGroupSize Number of worker threads to accept the connections from boss threads and perform non-blocking read and write from one or more channels. (Number of available processors) * 2 Any positive integer trustStoreLocation The default truststore file path. ${carbon.home}/resources/security/client-truststore.jks Path to client truststore .jks file trustStorePassword The default truststore password. wso2carbon Truststore password as string Examples EXAMPLE 1 @sink(type = 'http', publisher.url = 'http://stocks.com/stocks', @map(type = 'json')) define stream StockStream (symbol string, price float, volume long); Events arriving on the StockStream will be published to the HTTP endpoint http://stocks.com/stocks using POST method with Content-Type application/json by converting those events to the default JSON format as following: { \"event\": { \"symbol\": \"FB\", \"price\": 24.5, \"volume\": 5000 } } EXAMPLE 2 @sink(type='http', publisher.url = 'http://localhost:8009/foo', client.bootstrap.configurations = \"'client.bootstrap.socket.timeout:20'\", max.pool.active.connections = '1', headers = \"{{headers}}\", @map(type='xml', @payload(\"\"\" stock {{payloadBody}} /stock \"\"\"))) define stream FooStream (payloadBody String, headers string); Events arriving on FooStream will be published to the HTTP endpoint http://localhost:8009/foo using POST method with Content-Type application/xml and setting payloadBody and header attribute values. If the payloadBody contains symbol WSO2 /symbol price 55.6 /price volume 100 /volume and header contains 'topic:foobar' values, then the system will generate an output with the body: stock symbol WSO2 /symbol price 55.6 /price volume 100 /volume /stock and HTTP headers: Content-Length:xxx , Content-Location:'xxx' , Content-Type:'application/xml' , HTTP_METHOD:'POST' http-call (Sink) The http-call sink publishes messages to endpoints via HTTP or HTTPS protocols using methods such as POST, GET, PUT, and DELETE on formats text , XML or JSON and consume responses through its corresponding http-call-response source. It also supports calling endpoints protected with basic authentication or OAuth 2.0. Origin: siddhi-io-http:2.1.2 Syntax @sink(type=\"http-call\", publisher.url=\" STRING \", sink.id=\" STRING \", basic.auth.username=\" STRING \", basic.auth.password=\" STRING \", https.truststore.file=\" STRING \", https.truststore.password=\" STRING \", oauth.username=\" STRING \", oauth.password=\" STRING \", consumer.key=\" STRING \", consumer.secret=\" STRING \", token.url=\" STRING \", refresh.token=\" STRING \", headers=\" STRING \", method=\" STRING \", downloading.enabled=\" BOOL \", download.path=\" STRING \", blocking.io=\" BOOL \", socket.idle.timeout=\" INT \", chunk.disabled=\" BOOL \", ssl.protocol=\" STRING \", ssl.verification.disabled=\" BOOL \", ssl.configurations=\" STRING \", proxy.host=\" STRING \", proxy.port=\" STRING \", proxy.username=\" STRING \", proxy.password=\" STRING \", client.bootstrap.configurations=\" STRING \", max.pool.active.connections=\" INT \", min.pool.idle.connections=\" INT \", max.pool.idle.connections=\" INT \", min.evictable.idle.time=\" STRING \", time.between.eviction.runs=\" STRING \", max.wait.time=\" STRING \", test.on.borrow=\" BOOL \", test.while.idle=\" BOOL \", exhausted.action=\" INT \", hostname.verification.enabled=\" BOOL \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic publisher.url The URL which should be called. Examples: http://localhost:8080/endpoint , https://localhost:8080/endpoint STRING No No sink.id Identifier to correlate the http-call sink to its corresponding http-call-response sources to retrieved the responses. STRING No No basic.auth.username The username to be included in the authentication header when calling endpoints protected by basic authentication. basic.auth.password property should be also set when using this property. - STRING Yes No basic.auth.password The password to be included in the authentication header when calling endpoints protected by basic authentication. basic.auth.username property should be also set when using this property. - STRING Yes No https.truststore.file The file path of the client truststore when sending messages through https protocol. ${carbon.home}/resources/security/client-truststore.jks STRING Yes No https.truststore.password The password for the client-truststore. wso2carbon STRING Yes No oauth.username The username to be included in the authentication header when calling endpoints protected by OAuth 2.0. oauth.password property should be also set when using this property. - STRING Yes No oauth.password The password to be included in the authentication header when calling endpoints protected by OAuth 2.0. oauth.username property should be also set when using this property. - STRING Yes No consumer.key Consumer key used for calling endpoints protected by OAuth 2.0 - STRING Yes No consumer.secret Consumer secret used for calling endpoints protected by OAuth 2.0 - STRING Yes No token.url Token URL to generate a new access tokens when calling endpoints protected by OAuth 2.0 - STRING Yes No refresh.token Refresh token used for generating new access tokens when calling endpoints protected by OAuth 2.0 - STRING Yes No headers HTTP request headers in format \"' key : value ',' key : value '\" . When the Content-Type header is not provided the system decides the Content-Type based on the provided sink mapper as following: - @map(type='xml') : application/xml - @map(type='json') : application/json - @map(type='text') : plain/text - @map(type='keyvalue') : application/x-www-form-urlencoded - For all other cases system defaults to plain/text Also the Content-Length header need not to be provided, as the system automatically defines it by calculating the size of the payload. Content-Type and Content-Length headers STRING Yes No method The HTTP method used for calling the endpoint. POST STRING Yes No downloading.enabled Enable response received by the http-call-response source to be written to a file. When this is enabled the download.path property should be also set. false BOOL Yes No download.path The absolute file path along with the file name where the downloads should be saved. - STRING Yes Yes blocking.io Blocks the request thread until a response it received from HTTP call-response source before sending any other request. false BOOL Yes No socket.idle.timeout Socket timeout in millis. 6000 INT Yes No chunk.disabled Disable chunked transfer encoding. false BOOL Yes No ssl.protocol SSL/TLS protocol. TLS STRING Yes No ssl.verification.disabled Disable SSL verification. false BOOL Yes No ssl.configurations SSL/TSL configurations. Expected format \"' key : value ',' key : value '\" . Some supported parameters: - SSL/TLS protocols: 'sslEnabledProtocols:TLSv1.1,TLSv1.2' - List of ciphers: 'ciphers:TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256' - Enable session creation: 'client.enable.session.creation:true' - Supported server names: 'server.suported.server.names:server' - Add HTTP SNIMatcher: 'server.supported.snimatchers:SNIMatcher' - STRING Yes No proxy.host Proxy server host - STRING Yes No proxy.port Proxy server port - STRING Yes No proxy.username Proxy server username - STRING Yes No proxy.password Proxy server password - STRING Yes No client.bootstrap.configurations Client bootstrap configurations in format \"' key : value ',' key : value '\" . Some supported configurations : - Client connect timeout in millis: 'client.bootstrap.connect.timeout:15000' - Client socket timeout in seconds: 'client.bootstrap.socket.timeout:15' - Client socket reuse: 'client.bootstrap.socket.reuse:true' - Enable TCP no delay: 'client.bootstrap.nodelay:true' - Enable client keep alive: 'client.bootstrap.keepalive:true' - Send buffer size: 'client.bootstrap.sendbuffersize:1048576' - Receive buffer size: 'client.bootstrap.recievebuffersize:1048576' - STRING Yes No max.pool.active.connections Maximum possible number of active connection per client pool. -1 INT Yes No min.pool.idle.connections Minimum number of idle connections that can exist per client pool. 0 INT Yes No max.pool.idle.connections Maximum number of idle connections that can exist per client pool. 100 INT Yes No min.evictable.idle.time Minimum time (in millis) a connection may sit idle in the client pool before it become eligible for eviction. 300000 STRING Yes No time.between.eviction.runs Time between two eviction operations (in millis) on the client pool. 30000 STRING Yes No max.wait.time The maximum time (in millis) the pool will wait (when there are no available connections) for a connection to be returned to the pool. 60000 STRING Yes No test.on.borrow Enable connections to be validated before being borrowed from the client pool. true BOOL Yes No test.while.idle Enable connections to be validated during the eviction operation (if any). true BOOL Yes No exhausted.action Action that should be taken when the maximum number of active connections are being used. This action should be indicated as an int and possible action values are following. 0 - Fail the request. 1 - Block the request, until a connection returns to the pool. 2 - Grow the connection pool size. 1 (Block when exhausted) INT Yes No hostname.verification.enabled Enable hostname verification true BOOL Yes No System Parameters Name Description Default Value Possible Parameters clientBootstrapClientGroupSize Number of client threads to perform non-blocking read and write to one or more channels. (Number of available processors) * 2 Any positive integer clientBootstrapBossGroupSize Number of boss threads to accept incoming connections. Number of available processors Any positive integer clientBootstrapWorkerGroupSize Number of worker threads to accept the connections from boss threads and perform non-blocking read and write from one or more channels. (Number of available processors) * 2 Any positive integer trustStoreLocation The default truststore file path. ${carbon.home}/resources/security/client-truststore.jks Path to client truststore .jks file trustStorePassword The default truststore password. wso2carbon Truststore password as string Examples EXAMPLE 1 @sink(type='http-call', sink.id='foo', publisher.url='http://localhost:8009/foo', @map(type='xml', @payload('{{payloadBody}}'))) define stream FooStream (payloadBody string); @source(type='http-call-response', sink.id='foo', @map(type='text', regex.A='((.|\\n)*)', @attributes(headers='trp:headers', message='A[1]'))) define stream ResponseStream(message string, headers string); When events arrive in FooStream , http-call sink makes calls to endpoint on url http://localhost:8009/foo with POST method and Content-Type application/xml . If the event payloadBody attribute contains following XML: item name apple /name price 55 /price quantity 5 /quantity /item the http-call sink maps that and sends it to the endpoint. When endpoint sends a response it will be consumed by the corresponding http-call-response source correlated via the same sink.id foo and that will map the response message and send it via ResponseStream steam by assigning the message body as message attribute and response headers as headers attribute of the event. EXAMPLE 2 @sink(type='http-call', publisher.url='http://localhost:8005/files/{{name}}' downloading.enabled='true', download.path='{{downloadPath}}{{name}}', method='GET', sink.id='download', @map(type='json')) define stream DownloadRequestStream(name String, id int, downloadPath string); @source(type='http-call-response', sink.id='download', http.status.code='2\\\\d+', @map(type='text', regex.A='((.|\\n)*)', @attributes(name='trp:name', id='trp:id', file='A[1]'))) define stream ResponseStream2xx(name string, id string, file string); @source(type='http-call-response', sink.id='download', http.status.code='4\\\\d+', @map(type='text', regex.A='((.|\\n)*)', @attributes(errorMsg='A[1]'))) define stream ResponseStream4xx(errorMsg string); When events arrive in DownloadRequestStream with name : foo.txt , id : 75 and downloadPath : /user/download/ the http-call sink sends a GET request to the url http://localhost:8005/files/foo.txt to download the file to the given path /user/download/foo.txt and capture the response via its corresponding http-call-response source based on the response status code. If the response status code is in the range of 200 the message will be received by the http-call-response source associated with the ResponseStream2xx stream which expects http.status.code with regex 2\\d+ while downloading the file to the local file system on the path /user/download/foo.txt and mapping the response message having the absolute file path to event's file attribute. If the response status code is in the range of 400 then the message will be received by the http-call-response source associated with the ResponseStream4xx stream which expects http.status.code with regex 4\\d+ while mapping the error response to the errorMsg attribute of the event. http-request (Sink) Deprecated (Use http-call sink instead). The http-request sink publishes messages to endpoints via HTTP or HTTPS protocols using methods such as POST, GET, PUT, and DELETE on formats text , XML or JSON and consume responses through its corresponding http-response source. It also supports calling endpoints protected with basic authentication or OAuth 2.0. Origin: siddhi-io-http:2.1.2 Syntax @sink(type=\"http-request\", publisher.url=\" STRING \", sink.id=\" STRING \", basic.auth.username=\" STRING \", basic.auth.password=\" STRING \", https.truststore.file=\" STRING \", https.truststore.password=\" STRING \", oauth.username=\" STRING \", oauth.password=\" STRING \", consumer.key=\" STRING \", consumer.secret=\" STRING \", token.url=\" STRING \", refresh.token=\" STRING \", headers=\" STRING \", method=\" STRING \", downloading.enabled=\" BOOL \", download.path=\" STRING \", blocking.io=\" BOOL \", socket.idle.timeout=\" INT \", chunk.disabled=\" BOOL \", ssl.protocol=\" STRING \", ssl.verification.disabled=\" BOOL \", ssl.configurations=\" STRING \", proxy.host=\" STRING \", proxy.port=\" STRING \", proxy.username=\" STRING \", proxy.password=\" STRING \", client.bootstrap.configurations=\" STRING \", max.pool.active.connections=\" INT \", min.pool.idle.connections=\" INT \", max.pool.idle.connections=\" INT \", min.evictable.idle.time=\" STRING \", time.between.eviction.runs=\" STRING \", max.wait.time=\" STRING \", test.on.borrow=\" BOOL \", test.while.idle=\" BOOL \", exhausted.action=\" INT \", hostname.verification.enabled=\" BOOL \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic publisher.url The URL which should be called. Examples: http://localhost:8080/endpoint , https://localhost:8080/endpoint STRING No No sink.id Identifier to correlate the http-request sink to its corresponding http-response sources to retrieved the responses. STRING No No basic.auth.username The username to be included in the authentication header when calling endpoints protected by basic authentication. basic.auth.password property should be also set when using this property. - STRING Yes No basic.auth.password The password to be included in the authentication header when calling endpoints protected by basic authentication. basic.auth.username property should be also set when using this property. - STRING Yes No https.truststore.file The file path of the client truststore when sending messages through https protocol. ${carbon.home}/resources/security/client-truststore.jks STRING Yes No https.truststore.password The password for the client-truststore. wso2carbon STRING Yes No oauth.username The username to be included in the authentication header when calling endpoints protected by OAuth 2.0. oauth.password property should be also set when using this property. - STRING Yes No oauth.password The password to be included in the authentication header when calling endpoints protected by OAuth 2.0. oauth.username property should be also set when using this property. - STRING Yes No consumer.key Consumer key used for calling endpoints protected by OAuth 2.0 - STRING Yes No consumer.secret Consumer secret used for calling endpoints protected by OAuth 2.0 - STRING Yes No token.url Token URL to generate a new access tokens when calling endpoints protected by OAuth 2.0 - STRING Yes No refresh.token Refresh token used for generating new access tokens when calling endpoints protected by OAuth 2.0 - STRING Yes No headers HTTP request headers in format \"' key : value ',' key : value '\" . When the Content-Type header is not provided the system decides the Content-Type based on the provided sink mapper as following: - @map(type='xml') : application/xml - @map(type='json') : application/json - @map(type='text') : plain/text - @map(type='keyvalue') : application/x-www-form-urlencoded - For all other cases system defaults to plain/text Also the Content-Length header need not to be provided, as the system automatically defines it by calculating the size of the payload. Content-Type and Content-Length headers STRING Yes No method The HTTP method used for calling the endpoint. POST STRING Yes No downloading.enabled Enable response received by the http-response source to be written to a file. When this is enabled the download.path property should be also set. false BOOL Yes No download.path The absolute file path along with the file name where the downloads should be saved. - STRING Yes Yes blocking.io Blocks the request thread until a response it received from HTTP call-response source before sending any other request. false BOOL Yes No socket.idle.timeout Socket timeout in millis. 6000 INT Yes No chunk.disabled Disable chunked transfer encoding. false BOOL Yes No ssl.protocol SSL/TLS protocol. TLS STRING Yes No ssl.verification.disabled Disable SSL verification. false BOOL Yes No ssl.configurations SSL/TSL configurations in format \"' key : value ',' key : value '\" . Some supported parameters: - SSL/TLS protocols: 'sslEnabledProtocols:TLSv1.1,TLSv1.2' - List of ciphers: 'ciphers:TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256' - Enable session creation: 'client.enable.session.creation:true' - Supported server names: 'server.suported.server.names:server' - Add HTTP SNIMatcher: 'server.supported.snimatchers:SNIMatcher' - STRING Yes No proxy.host Proxy server host - STRING Yes No proxy.port Proxy server port - STRING Yes No proxy.username Proxy server username - STRING Yes No proxy.password Proxy server password - STRING Yes No client.bootstrap.configurations Client bootstrap configurations in format \"' key : value ',' key : value '\" . Some supported configurations : - Client connect timeout in millis: 'client.bootstrap.connect.timeout:15000' - Client socket timeout in seconds: 'client.bootstrap.socket.timeout:15' - Client socket reuse: 'client.bootstrap.socket.reuse:true' - Enable TCP no delay: 'client.bootstrap.nodelay:true' - Enable client keep alive: 'client.bootstrap.keepalive:true' - Send buffer size: 'client.bootstrap.sendbuffersize:1048576' - Receive buffer size: 'client.bootstrap.recievebuffersize:1048576' - STRING Yes No max.pool.active.connections Maximum possible number of active connection per client pool. -1 INT Yes No min.pool.idle.connections Minimum number of idle connections that can exist per client pool. 0 INT Yes No max.pool.idle.connections Maximum number of idle connections that can exist per client pool. 100 INT Yes No min.evictable.idle.time Minimum time (in millis) a connection may sit idle in the client pool before it become eligible for eviction. 300000 STRING Yes No time.between.eviction.runs Time between two eviction operations (in millis) on the client pool. 30000 STRING Yes No max.wait.time The maximum time (in millis) the pool will wait (when there are no available connections) for a connection to be returned to the pool. 60000 STRING Yes No test.on.borrow Enable connections to be validated before being borrowed from the client pool. true BOOL Yes No test.while.idle Enable connections to be validated during the eviction operation (if any). true BOOL Yes No exhausted.action Action that should be taken when the maximum number of active connections are being used. This action should be indicated as an int and possible action values are following. 0 - Fail the request. 1 - Block the request, until a connection returns to the pool. 2 - Grow the connection pool size. 1 (Block when exhausted) INT Yes No hostname.verification.enabled Enable hostname verification true BOOL Yes No System Parameters Name Description Default Value Possible Parameters clientBootstrapClientGroupSize Number of client threads to perform non-blocking read and write to one or more channels. (Number of available processors) * 2 Any positive integer clientBootstrapBossGroupSize Number of boss threads to accept incoming connections. Number of available processors Any positive integer clientBootstrapWorkerGroupSize Number of worker threads to accept the connections from boss threads and perform non-blocking read and write from one or more channels. (Number of available processors) * 2 Any positive integer trustStoreLocation The default truststore file path. ${carbon.home}/resources/security/client-truststore.jks Path to client truststore .jks file trustStorePassword The default truststore password. wso2carbon Truststore password as string Examples EXAMPLE 1 @sink(type='http-request', sink.id='foo', publisher.url='http://localhost:8009/foo', @map(type='xml', @payload('{{payloadBody}}'))) define stream FooStream (payloadBody string); @source(type='http-response', sink.id='foo', @map(type='text', regex.A='((.|\\n)*)', @attributes(headers='trp:headers', message='A[1]'))) define stream ResponseStream(message string, headers string); When events arrive in FooStream , http-request sink makes calls to endpoint on url http://localhost:8009/foo with POST method and Content-Type application/xml . If the event payloadBody attribute contains following XML: item name apple /name price 55 /price quantity 5 /quantity /item the http-request sink maps that and sends it to the endpoint. When endpoint sends a response it will be consumed by the corresponding http-response source correlated via the same sink.id foo and that will map the response message and send it via ResponseStream steam by assigning the message body as message attribute and response headers as headers attribute of the event. EXAMPLE 2 @sink(type='http-request', publisher.url='http://localhost:8005/files/{{name}}' downloading.enabled='true', download.path='{{downloadPath}}{{name}}', method='GET', sink.id='download', @map(type='json')) define stream DownloadRequestStream(name String, id int, downloadPath string); @source(type='http-response', sink.id='download', http.status.code='2\\\\d+', @map(type='text', regex.A='((.|\\n)*)', @attributes(name='trp:name', id='trp:id', file='A[1]'))) define stream ResponseStream2xx(name string, id string, file string); @source(type='http-response', sink.id='download', http.status.code='4\\\\d+', @map(type='text', regex.A='((.|\\n)*)', @attributes(errorMsg='A[1]'))) define stream ResponseStream4xx(errorMsg string); When events arrive in DownloadRequestStream with name : foo.txt , id : 75 and downloadPath : /user/download/ the http-request sink sends a GET request to the url http://localhost:8005/files/foo.txt to download the file to the given path /user/download/foo.txt and capture the response via its corresponding http-response source based on the response status code. If the response status code is in the range of 200 the message will be received by the http-response source associated with the ResponseStream2xx stream which expects http.status.code with regex 2\\d+ while downloading the file to the local file system on the path /user/download/foo.txt and mapping the response message having the absolute file path to event's file attribute. If the response status code is in the range of 400 then the message will be received by the http-response source associated with the ResponseStream4xx stream which expects http.status.code with regex 4\\d+ while mapping the error response to the errorMsg attribute of the event. http-response (Sink) Deprecated (Use http-service-response sink instead). The http-response sink send responses of the requests consumed by its corresponding http-request source, by mapping the response messages to formats such as text , XML and JSON . Origin: siddhi-io-http:2.1.2 Syntax @sink(type=\"http-response\", source.id=\" STRING \", message.id=\" STRING \", headers=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic source.id Identifier to correlate the http-response sink to its corresponding http-request source which consumed the request. STRING No No message.id Identifier to correlate the response with the request received by http-request source. STRING No Yes headers HTTP request headers in format \"' key : value ',' key : value '\" . When the Content-Type header is not provided the system decides the Content-Type based on the provided sink mapper as following: - @map(type='xml') : application/xml - @map(type='json') : application/json - @map(type='text') : plain/text - @map(type='keyvalue') : application/x-www-form-urlencoded - For all other cases system defaults to plain/text Also the Content-Length header need not to be provided, as the system automatically defines it by calculating the size of the payload. Content-Type and Content-Length headers STRING Yes No Examples EXAMPLE 1 @source(type='http-request', receiver.url='http://localhost:5005/add', source.id='adder', @map(type='json, @attributes(messageId='trp:messageId', value1='$.event.value1', value2='$.event.value2'))) define stream AddStream (messageId string, value1 long, value2 long); @sink(type='http-response', source.id='adder', message.id='{{messageId}}', @map(type = 'json')) define stream ResultStream (messageId string, results long); @info(name = 'query1') from AddStream select messageId, value1 + value2 as results insert into ResultStream; The http-request source on stream AddStream listens on url http://localhost:5005/stocks for JSON messages with format: { \"event\": { \"value1\": 3, \"value2\": 4 } } and when events arrive it maps to AddStream events and pass them to query query1 for processing. The query results produced on ResultStream are sent as a response via http-response sink with format: { \"event\": { \"results\": 7 } } Here the request and response are correlated by passing the messageId produced by the http-request to the respective http-response sink. http-service-response (Sink) The http-service-response sink send responses of the requests consumed by its corresponding http-service source, by mapping the response messages to formats such as text , XML and JSON . Origin: siddhi-io-http:2.1.2 Syntax @sink(type=\"http-service-response\", source.id=\" STRING \", message.id=\" STRING \", headers=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic source.id Identifier to correlate the http-service-response sink to its corresponding http-service source which consumed the request. STRING No No message.id Identifier to correlate the response with the request received by http-service source. STRING No Yes headers HTTP request headers in format \"' key : value ',' key : value '\" . When the Content-Type header is not provided the system decides the Content-Type based on the provided sink mapper as following: - @map(type='xml') : application/xml - @map(type='json') : application/json - @map(type='text') : plain/text - @map(type='keyvalue') : application/x-www-form-urlencoded - For all other cases system defaults to plain/text Also the Content-Length header need not to be provided, as the system automatically defines it by calculating the size of the payload. Content-Type and Content-Length headers STRING Yes No Examples EXAMPLE 1 @source(type='http-service', receiver.url='http://localhost:5005/add', source.id='adder', @map(type='json, @attributes(messageId='trp:messageId', value1='$.event.value1', value2='$.event.value2'))) define stream AddStream (messageId string, value1 long, value2 long); @sink(type='http-service-response', source.id='adder', message.id='{{messageId}}', @map(type = 'json')) define stream ResultStream (messageId string, results long); @info(name = 'query1') from AddStream select messageId, value1 + value2 as results insert into ResultStream; The http-service source on stream AddStream listens on url http://localhost:5005/stocks for JSON messages with format: { \"event\": { \"value1\": 3, \"value2\": 4 } } and when events arrive it maps to AddStream events and pass them to query query1 for processing. The query results produced on ResultStream are sent as a response via http-service-response sink with format: { \"event\": { \"results\": 7 } } Here the request and response are correlated by passing the messageId produced by the http-service to the respective http-service-response sink. inMemory (Sink) In-memory sink publishes events to In-memory sources that are subscribe to the same topic to which the sink publishes. This provides a way to connect multiple Siddhi Apps deployed under the same Siddhi Manager (JVM). Here both the publisher and subscriber should have the same event schema (stream definition) for successful data transfer. Origin: siddhi-core:5.1.7 Syntax @sink(type=\"inMemory\", topic=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic topic Event are delivered to allthe subscribers subscribed on this topic. STRING No No Examples EXAMPLE 1 @sink(type='inMemory', topic='Stocks', @map(type='passThrough')) define stream StocksStream (symbol string, price float, volume long); Here the StocksStream uses inMemory sink to emit the Siddhi events to all the inMemory sources deployed in the same JVM and subscribed to the topic Stocks . jms (Sink) JMS Sink allows users to subscribe to a JMS broker and publish JMS messages. Origin: siddhi-io-jms:2.0.2 Syntax @sink(type=\"jms\", destination=\" STRING \", connection.factory.jndi.name=\" STRING \", factory.initial=\" STRING \", provider.url=\" STRING \", connection.factory.type=\" STRING \", connection.username=\" STRING \", connection.password=\" STRING \", connection.factory.nature=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic destination Queue/Topic name which JMS Source should subscribe to STRING No Yes connection.factory.jndi.name JMS Connection Factory JNDI name. This value will be used for the JNDI lookup to find the JMS Connection Factory. QueueConnectionFactory STRING Yes No factory.initial Naming factory initial value STRING No No provider.url Java naming provider URL. Property for specifying configuration information for the service provider to use. The value of the property should contain a URL string (e.g. \"ldap://somehost:389\") STRING No No connection.factory.type Type of the connection connection factory. This can be either queue or topic. queue STRING Yes No connection.username username for the broker. None STRING Yes No connection.password Password for the broker None STRING Yes No connection.factory.nature Connection factory nature for the broker(cached/pooled). default STRING Yes No Examples EXAMPLE 1 @sink(type='jms', @map(type='xml'), factory.initial='org.apache.activemq.jndi.ActiveMQInitialContextFactory', provider.url='vm://localhost',destination='DAS_JMS_OUTPUT_TEST', connection.factory.type='topic',connection.factory.jndi.name='TopicConnectionFactory') define stream inputStream (name string, age int, country string); This example shows how to publish to an ActiveMQ topic. EXAMPLE 2 @sink(type='jms', @map(type='xml'), factory.initial='org.apache.activemq.jndi.ActiveMQInitialContextFactory', provider.url='vm://localhost',destination='DAS_JMS_OUTPUT_TEST') define stream inputStream (name string, age int, country string); This example shows how to publish to an ActiveMQ queue. Note that we are not providing properties like connection factory type kafka (Sink) A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to use the Kafka transport, the type parameter should have kafka as its value. Origin: siddhi-io-kafka:5.0.4 Syntax @sink(type=\"kafka\", bootstrap.servers=\" STRING \", topic=\" STRING \", partition.no=\" INT \", sequence.id=\" STRING \", key=\" STRING \", is.binary.message=\" BOOL \", optional.configuration=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No is.binary.message In order to send the binary events via kafka sink, this parameter is set to 'True'. null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='topic_with_partitions', partition.no='0', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka sink configuration publishes to 0 th partition of the topic named topic_with_partitions . EXAMPLE 2 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='{{symbol}}', partition.no='{{volume}}', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes dynamic topic and partitions that are taken from the Siddhi event. The value for partition.no is taken from the volume attribute, and the topic value is taken from the symbol attribute. kafkaMultiDC (Sink) A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to publish events via the Kafka transport, and using two Kafka brokers to publish events to the same topic, the type parameter must have kafkaMultiDC as its value. Origin: siddhi-io-kafka:5.0.4 Syntax @sink(type=\"kafkaMultiDC\", bootstrap.servers=\" STRING \", topic=\" STRING \", sequence.id=\" STRING \", key=\" STRING \", partition.no=\" INT \", is.binary.message=\" BOOL \", optional.configuration=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma -separated values. There must be at least two servers in this list. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No is.binary.message In order to send the binary events via kafkaMultiDCSink, it is required to set this parameter to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink(type='kafkaMultiDC', topic='myTopic', partition.no='0',bootstrap.servers='host1:9092, host2:9092', @map(type='xml'))Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes to the default (i.e., 0 th ) partition of the brokers in two data centers log (Sink) This is a sink that can be used as a logger. This will log the output events in the output stream with user specified priority and a prefix Origin: siddhi-core:5.1.7 Syntax @sink(type=\"log\", priority=\" STRING \", prefix=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic priority This will set the logger priority i.e log level. Accepted values are INFO, DEBUG, WARN, FATAL, ERROR, OFF, TRACE INFO STRING Yes No prefix This will be the prefix to the output message. If the output stream has event [2,4] and the prefix is given as \"Hello\" then the log will show \"Hello : [2,4]\" default prefix will be : STRING Yes No Examples EXAMPLE 1 @sink(type='log', prefix='My Log', priority='DEBUG') define stream BarStream (symbol string, price float, volume long) In this example BarStream uses log sink and the prefix is given as My Log. Also the priority is set to DEBUG. EXAMPLE 2 @sink(type='log', priority='DEBUG') define stream BarStream (symbol string, price float, volume long) In this example BarStream uses log sink and the priority is set to DEBUG. User has not specified prefix so the default prefix will be in the form Siddhi App Name : Stream Name EXAMPLE 3 @sink(type='log', prefix='My Log') define stream BarStream (symbol string, price float, volume long) In this example BarStream uses log sink and the prefix is given as My Log. User has not given a priority so it will be set to default INFO. EXAMPLE 4 @sink(type='log') define stream BarStream (symbol string, price float, volume long) In this example BarStream uses log sink. The user has not given prefix or priority so they will be set to their default values. nats (Sink) NATS Sink allows users to subscribe to a NATS broker and publish messages. Origin: siddhi-io-nats:2.0.6 Syntax @sink(type=\"nats\", destination=\" STRING \", bootstrap.servers=\" STRING \", client.id=\" STRING \", cluster.id=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic destination Subject name which NATS sink should publish to. STRING No Yes bootstrap.servers The NATS based url of the NATS server. nats://localhost:4222 STRING Yes No client.id The identifier of the client publishing/connecting to the NATS broker. Should be unique for each client connecting to the server/cluster. None STRING Yes No cluster.id The identifier of the NATS server/cluster. test-cluster STRING Yes No Examples EXAMPLE 1 @sink(type='nats', @map(type='xml'), destination='SP_NATS_OUTPUT_TEST', bootstrap.servers='nats://localhost:4222',client.id='nats_client',server.id='test-cluster') define stream outputStream (name string, age int, country string); This example shows how to publish to a NATS subject with all supporting configurations. With the following configuration the sink identified as 'nats-client' will publish to a subject named as 'SP_NATS_OUTPUT_TEST' which resides in a nats instance with a cluster id of 'test-cluster', running in localhost and listening to the port 4222 for client connection. EXAMPLE 2 @sink(type='nats', @map(type='xml'), destination='SP_NATS_OUTPUT_TEST') define stream outputStream (name string, age int, country string); This example shows how to publish to a NATS subject with mandatory configurations. With the following configuration the sink identified with an auto generated client id will publish to a subject named as 'SP_NATS_OUTPUT_TEST' which resides in a nats instance with a cluster id of 'test-cluster', running in localhost and listening to the port 4222 for client connection. prometheus (Sink) This sink publishes events processed by Siddhi into Prometheus metrics and exposes them to the Prometheus server at the specified URL. The created metrics can be published to Prometheus via 'server' or 'pushGateway', depending on your preference. The metric types that are supported by the Prometheus sink are 'counter', 'gauge', 'histogram', and 'summary'. The values and labels of the Prometheus metrics can be updated through the events. Origin: siddhi-io-prometheus:2.1.0 Syntax @sink(type=\"prometheus\", job=\" STRING \", publish.mode=\" STRING \", push.url=\" STRING \", server.url=\" STRING \", metric.type=\" STRING \", metric.help=\" STRING \", metric.name=\" STRING \", buckets=\" STRING \", quantiles=\" STRING \", quantile.error=\" DOUBLE \", value.attribute=\" STRING \", push.operation=\" STRING \", grouping.key=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic job This parameter specifies the job name of the metric. This must be the same job name that is defined in the Prometheus configuration file. siddhiJob STRING Yes No publish.mode The mode in which the metrics need to be exposed to the Prometheus server.The possible publishing modes are 'server' and 'pushgateway'.The server mode exposes the metrics through an HTTP server at the specified URL, and the 'pushGateway' mode pushes the metrics to the pushGateway that needs to be running at the specified URL. server STRING Yes No push.url This parameter specifies the target URL of the Prometheus pushGateway. This is the URL at which the pushGateway must be listening. This URL needs to be defined in the Prometheus configuration file as a target before it can be used here. http://localhost:9091 STRING Yes No server.url This parameter specifies the URL where the HTTP server is initiated to expose metrics in the 'server' publish mode. This URL needs to be defined in the Prometheus configuration file as a target before it can be used here. http://localhost:9080 STRING Yes No metric.type The type of Prometheus metric that needs to be created at the sink. The supported metric types are 'counter', 'gauge',c'histogram' and 'summary'. STRING No No metric.help A brief description of the metric and its purpose. STRING Yes No metric.name This parameter allows you to assign a preferred name for the metric. The metric name must match the regex format, i.e., [a-zA-Z_:][a-zA-Z0-9_:]*. STRING Yes No buckets The bucket values preferred by the user for histogram metrics. The bucket values must be in the 'string' format with each bucket value separated by a comma as shown in the example below. \"2,4,6,8\" null STRING Yes No quantiles This parameter allows you to specify quantile values for summary metrics as preferred. The quantile values must be in the 'string' format with each quantile value separated by a comma as shown in the example below. \"0.5,0.75,0.95\" null STRING Yes No quantile.error The error tolerance value for calculating quantiles in summary metrics. This must be a positive value, but less than 1. 0.001 DOUBLE Yes No value.attribute The name of the attribute in the stream definition that specifies the metric value. The defined 'value' attribute must be included in the stream definition. The system increases the metric value for the counter and gauge metric types by the value of the 'value attribute. The system observes the value of the 'value' attribute for the calculations of 'summary' and 'histogram' metric types. value STRING Yes No push.operation This parameter defines the mode for pushing metrics to the pushGateway. The available push operations are 'push' and 'pushadd'. The operations differ according to the existing metrics in pushGateway where 'push' operation replaces the existing metrics, and 'pushadd' operation only updates the newly created metrics. pushadd STRING Yes No grouping.key This parameter specifies the grouping key of created metrics in key-value pairs. The grouping key is used only in pushGateway mode in order to distinguish the metrics from already existing metrics. The expected format of the grouping key is as follows: \"'key1:value1','key2:value2'\" STRING Yes No System Parameters Name Description Default Value Possible Parameters jobName This property specifies the default job name for the metric. This job name must be the same as the job name defined in the Prometheus configuration file. siddhiJob Any string publishMode The default publish mode for the Prometheus sink for exposing metrics to the Prometheus server. The mode can be either 'server' or 'pushgateway'. server server or pushgateway serverURL This property configures the URL where the HTTP server is initiated to expose metrics. This URL needs to be defined in the Prometheus configuration file as a target to be identified by Prometheus before it can be used here. By default, the HTTP server is initiated at 'http://localhost:9080'. http://localhost:9080 Any valid URL pushURL This property configures the target URL of the Prometheus pushGateway (where the pushGateway needs to listen). This URL needs to be defined in the Prometheus configuration file as a target to be identified by Prometheus before it can be used here. http://localhost:9091 Any valid URL groupingKey This property configures the grouping key of created metrics in key-value pairs. Grouping key is used only in pushGateway mode in order to distinguish these metrics from already existing metrics under the same job. The expected format of the grouping key is as follows: \"'key1:value1','key2:value2'\" . null Any key value pairs in the supported format Examples EXAMPLE 1 @sink(type='prometheus',job='fooOrderCount', server.url ='http://localhost:9080', publish.mode='server', metric.type='counter', metric.help= 'Number of foo orders', @map(type='keyvalue')) define stream FooCountStream (Name String, quantity int, value int); In the above example, the Prometheus-sink creates a counter metric with the stream name and defined attributes as labels. The metric is exposed through an HTTP server at the target URL. EXAMPLE 2 @sink(type='prometheus',job='inventoryLevel', push.url='http://localhost:9080', publish.mode='pushGateway', metric.type='gauge', metric.help= 'Current level of inventory', @map(type='keyvalue')) define stream InventoryLevelStream (Name String, value int); In the above example, the Prometheus-sink creates a gauge metric with the stream name and defined attributes as labels.The metric is pushed to the Prometheus pushGateway at the target URL. rabbitmq (Sink) The rabbitmq sink pushes the events into a rabbitmq broker using the AMQP protocol Origin: siddhi-io-rabbitmq:3.0.2 Syntax @sink(type=\"rabbitmq\", uri=\" STRING \", heartbeat=\" INT \", exchange.name=\" STRING \", exchange.type=\" STRING \", exchange.durable.enabled=\" BOOL \", exchange.autodelete.enabled=\" BOOL \", delivery.mode=\" INT \", content.type=\" STRING \", content.encoding=\" STRING \", priority=\" INT \", correlation.id=\" STRING \", reply.to=\" STRING \", expiration=\" STRING \", message.id=\" STRING \", timestamp=\" STRING \", type=\" STRING \", user.id=\" STRING \", app.id=\" STRING \", routing.key=\" STRING \", headers=\" STRING \", tls.enabled=\" BOOL \", tls.truststore.path=\" STRING \", tls.truststore.password=\" STRING \", tls.truststore.type=\" STRING \", tls.version=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic uri The URI that used to connect to an AMQP server. If no URI is specified, an error is logged in the CLI.e.g., amqp://guest:guest , amqp://guest:guest@localhost:5672 STRING No No heartbeat The period of time (in seconds) after which the peer TCP connection should be considered unreachable (down) by RabbitMQ and client libraries. 60 INT Yes No exchange.name The name of the exchange that decides what to do with a message it sends.If the exchange.name already exists in the RabbitMQ server, then the system uses that exchange.name instead of redeclaring. STRING No Yes exchange.type The type of the exchange.name. The exchange types available are direct , fanout , topic and headers . For a detailed description of each type, see RabbitMQ - AMQP Concepts direct STRING Yes Yes exchange.durable.enabled If this is set to true , the exchange remains declared even if the broker restarts. false BOOL Yes Yes exchange.autodelete.enabled If this is set to true , the exchange is automatically deleted when it is not used anymore. false BOOL Yes Yes delivery.mode This determines whether the connection should be persistent or not. The value must be either 1 or 2 .If the delivery.mode = 1, then the connection is not persistent. If the delivery.mode = 2, then the connection is persistent. 1 INT Yes No content.type The message content type. This should be the MIME content type. null STRING Yes No content.encoding The message content encoding. The value should be MIME content encoding. null STRING Yes No priority Specify a value within the range 0 to 9 in this parameter to indicate the message priority. 0 INT Yes Yes correlation.id The message correlated to the current message. e.g., The request to which this message is a reply. When a request arrives, a message describing the task is pushed to the queue by the front end server. After that the frontend server blocks to wait for a response message with the same correlation ID. A pool of worker machines listen on queue, and one of them picks up the task, performs it, and returns the result as message. Once a message with right correlation ID arrives, thefront end server continues to return the response to the caller. null STRING Yes Yes reply.to This is an anonymous exclusive callback queue. When the RabbitMQ receives a message with the reply.to property, it sends the response to the mentioned queue. This is commonly used to name a reply queue (or any other identifier that helps a consumer application to direct its response). null STRING Yes No expiration The expiration time after which the message is deleted. The value of the expiration field describes the TTL (Time To Live) period in milliseconds. null STRING Yes No message.id The message identifier. If applications need to identify messages, it is recommended that they use this attribute instead of putting it into the message payload. null STRING Yes Yes timestamp Timestamp of the moment when the message was sent. If you do not specify a value for this parameter, the system automatically generates the current date and time as the timestamp value. The format of the timestamp value is dd/mm/yyyy . current timestamp STRING Yes No type The type of the message. e.g., The type of the event or the command represented by the message. null STRING Yes No user.id The user ID specified here is verified by RabbitMQ against theuser name of the actual connection. This is an optional parameter. null STRING Yes No app.id The identifier of the application that produced the message. null STRING Yes No routing.key The key based on which the excahnge determines how to route the message to the queue. The routing key is similar to an address for the message. empty STRING Yes Yes headers The headers of the message. The attributes used for routing are taken from the this paremeter. A message is considered matching if the value of the header equals the value specified upon binding. null STRING Yes Yes tls.enabled This parameter specifies whether an encrypted communication channel should be established or not. When this parameter is set to true , the tls.truststore.path and tls.truststore.password parameters are initialized. false BOOL Yes No tls.truststore.path The file path to the location of the truststore of the client that sends the RabbitMQ events via the AMQP protocol. A custom client-truststore can be specified if required. If a custom truststore is not specified, then the system uses the default client-trustore in the {carbon.home}/resources/security /code directory. /p /td td style=\"vertical-align: top\" {carbon.home}/resources/security</code> directory.</p></td> <td style=\"vertical-align: top\"> /resources/security/client-truststore.jks STRING Yes No tls.truststore.password The password for the client-truststore. A custom password can be specified if required. If no custom password is specified, then the system uses wso2carbon as the default password. wso2carbon STRING Yes No tls.truststore.type The type of the truststore. JKS STRING Yes No tls.version The version of the tls/ssl. SSL STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink(type ='rabbitmq', uri = 'amqp://guest:guest@localhost:5672', exchange.name = 'direct', routing.key= 'direct', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes events to the direct exchange with the direct exchange type and the directTest routing key. tcp (Sink) A Siddhi application can be configured to publish events via the TCP transport by adding the @Sink(type = 'tcp') annotation at the top of an event stream definition. Origin: siddhi-io-tcp:3.0.4 Syntax @sink(type=\"tcp\", url=\" STRING \", sync=\" STRING \", tcp.no.delay=\" BOOL \", keep.alive=\" BOOL \", worker.threads=\" INT|LONG \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic url The URL to which outgoing events should be published via TCP. The URL should adhere to tcp:// host : port / context format. STRING No No sync This parameter defines whether the events should be published in a synchronized manner or not. If sync = 'true', then the worker will wait for the ack after sending the message. Else it will not wait for an ack. false STRING Yes Yes tcp.no.delay This is to specify whether to disable Nagle algorithm during message passing. If tcp.no.delay = 'true', the execution of Nagle algorithm will be disabled in the underlying TCP logic. Hence there will be no delay between two successive writes to the TCP connection. Else there can be a constant ack delay. true BOOL Yes No keep.alive This property defines whether the server should be kept alive when there are no connections available. true BOOL Yes No worker.threads Number of threads to publish events. 10 INT LONG Yes No Examples EXAMPLE 1 @Sink(type = 'tcp', url='tcp://localhost:8080/abc', sync='true' @map(type='binary')) define stream Foo (attribute1 string, attribute2 int); A sink of type 'tcp' has been defined. All events arriving at Foo stream via TCP transport will be sent to the url tcp://localhost:8080/abc in a synchronous manner. Sinkmapper avro (Sink Mapper) This extension is a Siddhi Event to Avro Message output mapper.Transports that publish messages to Avro sink can utilize this extension to convert Siddhi events to Avro messages. You can either specify the Avro schema or provide the schema registry URL and the schema reference ID as parameters in the stream definition. If no Avro schema is specified, a flat Avro schema of the 'record' type is generated with the stream attributes as schema fields. Origin: siddhi-map-avro:2.0.5 Syntax @sink(..., @map(type=\"avro\", schema.def=\" STRING \", schema.registry=\" STRING \", schema.id=\" STRING \") QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic schema.def This specifies the required Avro schema to be used to convert Siddhi events to Avro messages. The schema needs to be specified as a quoted JSON string. STRING No No schema.registry This specifies the URL of the schema registry. STRING No No schema.id This specifies the ID of the avro schema. This ID is the global ID that is returned from the schema registry when posting the schema to the registry. The specified ID is used to retrieve the schema from the schema registry. STRING No No Examples EXAMPLE 1 @sink(type='inMemory', topic='stock', @map(type='avro',schema.def = \"\"\"{\"type\":\"record\",\"name\":\"stock\",\"namespace\":\"stock.example\",\"fields\":[{\"name\":\"symbol\",\"type\":\"string\"},{\"name\":\"price\",\"type\":\"float\"},{\"name\":\"volume\",\"type\":\"long\"}]}\"\"\")) define stream StockStream (symbol string, price float, volume long); The above configuration performs a default Avro mapping that generates an Avro message as an output ByteBuffer. EXAMPLE 2 @sink(type='inMemory', topic='stock', @map(type='avro',schema.registry = 'http://localhost:8081', schema.id ='22',@payload(\"\"\"{\"Symbol\":{{symbol}},\"Price\":{{price}},\"Volume\":{{volume}}}\"\"\" ))) define stream StockStream (symbol string, price float, volume long); The above configuration performs a custom Avro mapping that generates an Avro message as an output ByteBuffer. The Avro schema is retrieved from the given schema registry (localhost:8081) using the schema ID provided. binary (Sink Mapper) This section explains how to map events processed via Siddhi in order to publish them in the binary format. Origin: siddhi-map-binary:2.0.4 Syntax @sink(..., @map(type=\"binary\") Examples EXAMPLE 1 @sink(type='inMemory', topic='WSO2', @map(type='binary')) define stream FooStream (symbol string, price float, volume long); This will publish Siddhi event in binary format. csv (Sink Mapper) This output mapper extension allows you to convert Siddhi events processed by the WSO2 SP to CSV message before publishing them. You can either use custom placeholder to map a custom CSV message or use pre-defined CSV format where event conversion takes place without extra configurations. Origin: siddhi-map-csv:2.0.3 Syntax @sink(..., @map(type=\"csv\", delimiter=\" STRING \", header=\" BOOL \", event.grouping.enabled=\" BOOL \") QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic delimiter This parameter used to separate the output CSV data, when converting a Siddhi event to CSV format, , STRING Yes No header This parameter specifies whether the CSV messages will be generated with header or not. If this parameter is set to true, message will be generated with header false BOOL Yes No event.grouping.enabled If this parameter is set to true , events are grouped via a line.separator when multiple events are received. It is required to specify a value for the System.lineSeparator() when the value for this parameter is true . false BOOL Yes No Examples EXAMPLE 1 @sink(type='inMemory', topic='{{symbol}}', @map(type='csv')) define stream BarStream (symbol string, price float, volume long); Above configuration will perform a default CSV output mapping, which will generate output as follows: WSO2,55.6,100 OS supported line separator If header is true and delimiter is \"-\", then the output will be as follows: symbol-price-volume OS supported line separator WSO2-55.6-100 OS supported line separator EXAMPLE 2 @sink(type='inMemory', topic='{{symbol}}', @map(type='csv',header='true',delimiter='-',@payload(symbol='0',price='2',volume='1')))define stream BarStream (symbol string, price float,volume long); Above configuration will perform a custom CSV mapping. Here, user can add custom place order in the @payload. The place order indicates that where the attribute name's value will be appear in the output message, The output will be produced output as follows: WSO2,100,55.6 If header is true and delimiter is \"-\", then the output will be as follows: symbol-price-volume WSO2-55.6-100 OS supported line separator If event grouping is enabled, then the output is as follows: WSO2-55.6-100 OS supported line separator WSO2-55.6-100 OS supported line separator WSO2-55.6-100 OS supported line separator json (Sink Mapper) This extension is an Event to JSON output mapper. Transports that publish messages can utilize this extension to convert Siddhi events to JSON messages. You can either send a pre-defined JSON format or a custom JSON message. Origin: siddhi-map-json:5.0.4 Syntax @sink(..., @map(type=\"json\", validate.json=\" BOOL \", enclosing.element=\" STRING \") QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic validate.json If this property is set to true , it enables JSON validation for the JSON messages generated. When validation is carried out, messages that do not adhere to proper JSON standards are dropped. This property is set to 'false' by default. false BOOL Yes No enclosing.element This specifies the enclosing element to be used if multiple events are sent in the same JSON message. Siddhi treats the child elements of the given enclosing element as events and executes JSON expressions on them. If an enclosing.element is not provided, the multiple event scenario is disregarded and JSON path is evaluated based on the root element. $ STRING Yes No Examples EXAMPLE 1 @sink(type='inMemory', topic='stock', @map(type='json')) define stream FooStream (symbol string, price float, volume long); Above configuration does a default JSON input mapping that generates the output given below. { \"event\":{ \"symbol\":WSO2, \"price\":55.6, \"volume\":100 } } EXAMPLE 2 @sink(type='inMemory', topic='{{symbol}}', @map(type='json', enclosing.element='$.portfolio', validate.json='true', @payload( \"\"\"{\"StockData\":{\"Symbol\":\"{{symbol}}\",\"Price\":{{price}}}\"\"\"))) define stream BarStream (symbol string, price float, volume long); The above configuration performs a custom JSON mapping that generates the following JSON message as the output. {\"portfolio\":{ \"StockData\":{ \"Symbol\":WSO2, \"Price\":55.6 } } } keyvalue (Sink Mapper) The Event to Key-Value Map output mapper extension allows you to convert Siddhi events processed by WSO2 SP to key-value map events before publishing them. You can either use pre-defined keys where conversion takes place without extra configurations, or use custom keys with which the messages can be published. Origin: siddhi-map-keyvalue:2.0.4 Syntax @sink(..., @map(type=\"keyvalue\") Examples EXAMPLE 1 @sink(type='inMemory', topic='stock', @map(type='keyvalue')) define stream FooStream (symbol string, price float, volume long); This query performs a default Key-Value output mapping. The expected output is something similar to the following: symbol:'WSO2' price : 55.6f volume: 100L EXAMPLE 2 @sink(type='inMemory', topic='stock', @map(type='keyvalue', @payload(a='symbol',b='price',c='volume'))) define stream FooStream (symbol string, price float, volume long); This query performs a custom Key-Value output mapping where values are passed as objects. Values for symbol , price , and volume attributes are published with the keys a , b and c respectively. The expected output is a map similar to the following: a:'WSO2' b : 55.6f c: 100L EXAMPLE 3 @sink(type='inMemory', topic='stock', @map(type='keyvalue', @payload(a='{{symbol}} is here',b='`price`',c='volume'))) define stream FooStream (symbol string, price float, volume long); This query performs a custom Key-Value output mapping where the values of the a and b attributes are strings and c is object. The expected output should be a Map similar to the following: a:'WSO2 is here' b : 'price' c: 100L passThrough (Sink Mapper) Pass-through mapper passed events (Event[]) through without any mapping or modifications. Origin: siddhi-core:5.1.7 Syntax @sink(..., @map(type=\"passThrough\") Examples EXAMPLE 1 @sink(type='inMemory', @map(type='passThrough')) define stream BarStream (symbol string, price float, volume long); In the following example BarStream uses passThrough outputmapper which emit Siddhi event directly without any transformation into sink. protobuf (Sink Mapper) This output mapper allows you to convert Events to protobuf messages before publishing them. To work with this mapper you have to add auto-generated protobuf classes to the project classpath. When you use this output mapper, you can either define stream attributes as the same names as the protobuf message attributes or you can use custom mapping to map stream definition attributes with the protobuf attributes..Please find the sample proto definition here Origin: siddhi-map-protobuf:1.0.1 Syntax @sink(..., @map(type=\"protobuf\", class=\" STRING \") QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic class This specifies the class name of the protobuf message class, If sink type is grpc then it's not necessary to provide this parameter. - STRING Yes No Examples EXAMPLE 1 @sink(type='grpc', url = 'grpc://localhost:2000/org.wso2.grpc.test.MyService/process @map(type='protobuf')) define stream BarStream (stringValue string, intValue int,longValue long,booleanValue bool,floatValue float,doubleValue double) Above definition will map BarStream values into the protobuf message type of the 'process' method in 'MyService' service EXAMPLE 2 @sink(type='grpc', url = 'grpc://localhost:2000/org.wso2.grpc.test.MyService/process @map(type='protobuf'), @payload(stringValue='a',longValue='b',intValue='c',booleanValue='d',floatValue = 'e', doubleValue = 'f'))) define stream BarStream (a string, b long, c int,d bool,e float,f double); The above definition will map BarStream values to request message type of the 'process' method in 'MyService' service. and stream values will map like this, - value of 'a' will be assign 'stringValue' variable in the message class - value of 'b' will be assign 'longValue' variable in the message class - value of 'c' will be assign 'intValue' variable in the message class - value of 'd' will be assign 'booleanValue' variable in the message class - value of 'e' will be assign 'floatValue' variable in the message class - value of 'f' will be assign 'doubleValue' variable in the message class EXAMPLE 3 @sink(type='grpc', url = 'grpc://localhost:2000/org.wso2.grpc.test.MyService/testMap' @map(type='protobuf')) define stream BarStream (stringValue string,intValue int,map object); The above definition will map BarStream values to request message type of the 'testMap' method in 'MyService' service and since there is an object data type is inthe stream(map object) , mapper will assume that 'map' is an instance of 'java.util.Map' class, otherwise it will throws and error. EXAMPLE 4 @sink(type='inMemory', topic='test01', @map(type='protobuf', class='org.wso2.grpc.test.Request')) define stream BarStream (stringValue string, intValue int,longValue long,booleanValue bool,floatValue float,doubleValue double); The above definition will map BarStream values to 'org.wso2.grpc.test.Request'protobuf class type. If sink type is not a grpc, sink is expecting to get the mapping protobuf class from the 'class' parameter in the @map extension text (Sink Mapper) This extension is a Event to Text output mapper. Transports that publish text messages can utilize this extension to convert the Siddhi events to text messages. Users can use a pre-defined text format where event conversion is carried out without any additional configurations, or use custom placeholder(using {{ and }} ) to map custom text messages. Again, you can also enable mustache based custom mapping. In mustache based custom mapping you can use custom placeholder (using {{ and }} or {{{ and }}} ) to map custom text. In mustache based custom mapping, all variables are HTML escaped by default. For example: is replaced with amp; \" is replaced with quot; = is replaced with #61; If you want to return unescaped HTML, use the triple mustache {{{ instead of double {{ . Origin: siddhi-map-text:2.0.4 Syntax @sink(..., @map(type=\"text\", event.grouping.enabled=\" BOOL \", delimiter=\" STRING \", new.line.character=\" STRING \", mustache.enabled=\" BOOL \") QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic event.grouping.enabled If this parameter is set to true , events are grouped via a delimiter when multiple events are received. It is required to specify a value for the delimiter parameter when the value for this parameter is true . false BOOL Yes No delimiter This parameter specifies how events are separated when a grouped event is received. This must be a whole line and not a single character. ~ ~ ~ ~ STRING Yes No new.line.character This attribute indicates the new line character of the event that is expected to be received. This is used mostly when communication between 2 types of operating systems is expected. For example, Linux uses \\n whereas Windows uses \\r\\n as the end of line character. \\n STRING Yes No mustache.enabled If this parameter is set to true , then mustache mapping gets enabled forcustom text mapping. false BOOL Yes No Examples EXAMPLE 1 @sink(type='inMemory', topic='stock', @map(type='text')) define stream FooStream (symbol string, price float, volume long); This query performs a default text input mapping. The expected output is as follows: symbol:\"WSO2\", price:55.6, volume:100 EXAMPLE 2 @sink(type='inMemory', topic='stock', @map(type='text', event.grouping.enabled='true')) define stream FooStream (symbol string, price float, volume long); This query performs a default text input mapping with event grouping. The expected output is as follows: symbol:\"WSO2\", price:55.6, volume:100 ~ ~ ~ ~ symbol:\"WSO2\", price:55.6, volume:100 EXAMPLE 3 @sink(type='inMemory', topic='stock', @map(type='text', @payload(\"SensorID : {{symbol}}/{{volume}}, SensorPrice : Rs{{price}}/=, Value : {{volume}}ml\"))) define stream FooStream (symbol string, price float, volume long); This query performs a custom text mapping. The expected output is as follows: SensorID : wso2/100, SensorPrice : Rs1000/=, Value : 100ml for the following siddhi event. {wso2,1000,100} EXAMPLE 4 @sink(type='inMemory', topic='stock', @map(type='text', event.grouping.enabled='true', @payload(\"Stock price of {{symbol}} is {{price}}\"))) define stream FooStream (symbol string, price float, volume long); This query performs a custom text mapping with event grouping. The expected output is as follows: Stock price of WSO2 is 55.6 ~ ~ ~ ~ Stock price of WSO2 is 55.6 ~ ~ ~ ~ Stock price of WSO2 is 55.6 for the following siddhi event. {WSO2,55.6,10} EXAMPLE 5 @sink(type='inMemory', topic='stock', @map(type='text', mustache.enabled='true', @payload(\"SensorID : {{{symbol}}}/{{{volume}}}, SensorPrice : Rs{{{price}}}/=, Value : {{{volume}}}ml\"))) define stream FooStream (symbol string, price float, volume long); This query performs a custom text mapping to return unescaped HTML. The expected output is as follows: SensorID : a b/100, SensorPrice : Rs1000/=, Value : 100ml for the following siddhi event. {a b,1000,100} xml (Sink Mapper) This mapper converts Siddhi output events to XML before they are published via transports that publish in XML format. Users can either send a pre-defined XML format or a custom XML message containing event data. Origin: siddhi-map-xml:5.0.3 Syntax @sink(..., @map(type=\"xml\", validate.xml=\" BOOL \", enclosing.element=\" STRING \") QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic validate.xml This parameter specifies whether the XML messages generated should be validated or not. If this parameter is set to true, messages that do not adhere to proper XML standards are dropped. false BOOL Yes No enclosing.element When an enclosing element is specified, the child elements (e.g., the immediate child elements) of that element are considered as events. This is useful when you need to send multiple events in a single XML message. When an enclosing element is not specified, one XML message per every event will be emitted without enclosing. None in custom mapping and events in default mapping STRING Yes No Examples EXAMPLE 1 @sink(type='inMemory', topic='stock', @map(type='xml')) define stream FooStream (symbol string, price float, volume long); Above configuration will do a default XML input mapping which will generate below output events event symbol WSO2 /symbol price 55.6 /price volume 100 /volume /event /events EXAMPLE 2 @sink(type='inMemory', topic='{{symbol}}', @map(type='xml', enclosing.element=' portfolio ', validate.xml='true', @payload( \" StockData Symbol {{symbol}} /Symbol Price {{price}} /Price /StockData \"))) define stream BarStream (symbol string, price float, volume long); Above configuration will perform a custom XML mapping. Inside @payload you can specify the custom template that you want to send the messages out and addd placeholders to places where you need to add event attributes.Above config will produce below output XML message portfolio StockData Symbol WSO2 /Symbol Price 55.6 /Price /StockData /portfolio Source cdc (Source) The CDC source receives events when change events (i.e., INSERT, UPDATE, DELETE) are triggered for a database table. Events are received in the 'key-value' format. There are two modes you could perform CDC: Listening mode and Polling mode. In polling mode, the datasource is periodically polled for capturing the changes. The polling period can be configured. In polling mode, you can only capture INSERT and UPDATE changes. On listening mode, the Source will keep listening to the Change Log of the database and notify in case a change has taken place. Here, you are immediately notified about the change, compared to polling mode. The key values of the map of a CDC change event are as follows. For 'listening' mode: For insert: Keys are specified as columns of the table. For delete: Keys are followed by the specified table columns. This is achieved via 'before_'. e.g., specifying 'before_X' results in the key being added before the column named 'X'. For update: Keys are followed followed by the specified table columns. This is achieved via 'before_'. e.g., specifying 'before_X' results in the key being added before the column named 'X'. For 'polling' mode: Keys are specified as the columns of the table. See parameter: mode for supported databases and change events. Origin: siddhi-io-cdc:2.0.3 Syntax @source(type=\"cdc\", url=\" STRING \", mode=\" STRING \", jdbc.driver.name=\" STRING \", username=\" STRING \", password=\" STRING \", pool.properties=\" STRING \", datasource.name=\" STRING \", table.name=\" STRING \", polling.column=\" STRING \", polling.interval=\" INT \", operation=\" STRING \", connector.properties=\" STRING \", database.server.id=\" STRING \", database.server.name=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic url The connection URL to the database. F=The format used is: 'jdbc:mysql:// host : port / database_name ' STRING No No mode Mode to capture the change data. The type of events that can be received, and the required parameters differ based on the mode. The mode can be one of the following: 'polling': This mode uses a column named 'polling.column' to monitor the given table. It captures change events of the 'RDBMS', 'INSERT, and 'UPDATE' types. 'listening': This mode uses logs to monitor the given table. It currently supports change events only of the 'MySQL', 'INSERT', 'UPDATE', and 'DELETE' types. listening STRING Yes No jdbc.driver.name The driver class name for connecting the database. It is required to specify a value for this parameter when the mode is 'polling'. STRING Yes No username The username to be used for accessing the database. This user needs to have the 'SELECT', 'RELOAD', 'SHOW DATABASES', 'REPLICATION SLAVE', and 'REPLICATION CLIENT'privileges for the change data capturing table (specified via the 'table.name' parameter). To operate in the polling mode, the user needs 'SELECT' privileges. STRING No No password The password of the username you specified for accessing the database. STRING No No pool.properties The pool parameters for the database connection can be specified as key-value pairs. STRING Yes No datasource.name Name of the wso2 datasource to connect to the database. When datasource name is provided, the URL, username and password are not needed. A datasource based connection is given more priority over the URL based connection. This parameter is applicable only when the mode is set to 'polling', and it can be applied only when you use this extension with WSO2 Stream Processor. STRING Yes No table.name The name of the table that needs to be monitored for data changes. STRING No No polling.column The column name that is polled to capture the change data. It is recommended to have a TIMESTAMP field as the 'polling.column' in order to capture the inserts and updates. Numeric auto-incremental fields and char fields can also be used as 'polling.column'. However, note that fields of these types only support insert change capturing, and the possibility of using a char field also depends on how the data is input. It is required to enter a value for this parameter when the mode is 'polling'. STRING Yes No polling.interval The time interval (specified in seconds) to poll the given table for changes. This parameter is applicable only when the mode is set to 'polling'. 1 INT Yes No operation The change event operation you want to carry out. Possible values are 'insert', 'update' or 'delete'. It is required to specify a value when the mode is 'listening'. This parameter is not case sensitive. STRING No No connector.properties Here, you can specify Debezium connector properties as a comma-separated string. The properties specified here are given more priority over the parameters. This parameter is applicable only for the 'listening' mode. Empty_String STRING Yes No database.server.id An ID to be used when joining MySQL database cluster to read the bin log. This should be a unique integer between 1 to 2^32. This parameter is applicable only when the mode is 'listening'. Random integer between 5400 and 6400 STRING Yes No database.server.name A logical name that identifies and provides a namespace for the database server. This parameter is applicable only when the mode is 'listening'. {host}_{port} STRING Yes No Examples EXAMPLE 1 @source(type = 'cdc' , url = 'jdbc:mysql://localhost:3306/SimpleDB', username = 'cdcuser', password = 'pswd4cdc', table.name = 'students', operation = 'insert', @map(type='keyvalue', @attributes(id = 'id', name = 'name'))) define stream inputStream (id string, name string); In this example, the CDC source listens to the row insertions that are made in the 'students' table with the column name, and the ID. This table belongs to the 'SimpleDB' MySQL database that can be accessed via the given URL. EXAMPLE 2 @source(type = 'cdc' , url = 'jdbc:mysql://localhost:3306/SimpleDB', username = 'cdcuser', password = 'pswd4cdc', table.name = 'students', operation = 'update', @map(type='keyvalue', @attributes(id = 'id', name = 'name', before_id = 'before_id', before_name = 'before_name'))) define stream inputStream (before_id string, id string, before_name string , name string); In this example, the CDC source listens to the row updates that are made in the 'students' table. This table belongs to the 'SimpleDB' MySQL database that can be accessed via the given URL. EXAMPLE 3 @source(type = 'cdc' , url = 'jdbc:mysql://localhost:3306/SimpleDB', username = 'cdcuser', password = 'pswd4cdc', table.name = 'students', operation = 'delete', @map(type='keyvalue', @attributes(before_id = 'before_id', before_name = 'before_name'))) define stream inputStream (before_id string, before_name string); In this example, the CDC source listens to the row deletions made in the 'students' table. This table belongs to the 'SimpleDB' database that can be accessed via the given URL. EXAMPLE 4 @source(type = 'cdc', mode='polling', polling.column = 'id', jdbc.driver.name = 'com.mysql.jdbc.Driver', url = 'jdbc:mysql://localhost:3306/SimpleDB', username = 'cdcuser', password = 'pswd4cdc', table.name = 'students', @map(type='keyvalue'), @attributes(id = 'id', name = 'name')) define stream inputStream (id int, name string); In this example, the CDC source polls the 'students' table for inserts. 'id' that is specified as the polling colum' is an auto incremental field. The connection to the database is made via the URL, username, password, and the JDBC driver name. EXAMPLE 5 @source(type = 'cdc', mode='polling', polling.column = 'id', datasource.name = 'SimpleDB', table.name = 'students', @map(type='keyvalue'), @attributes(id = 'id', name = 'name')) define stream inputStream (id int, name string); In this example, the CDC source polls the 'students' table for inserts. The given polling column is a char column with the 'S001, S002, ... .' pattern. The connection to the database is made via a data source named 'SimpleDB'. Note that the 'datasource.name' parameter works only with the Stream Processor. EXAMPLE 6 @source(type = 'cdc', mode='polling', polling.column = 'last_updated', datasource.name = 'SimpleDB', table.name = 'students', @map(type='keyvalue')) define stream inputStream (name string); In this example, the CDC source polls the 'students' table for inserts and updates. The polling column is a timestamp field. email (Source) The 'Email' source allows you to receive events via emails. An 'Email' source can be configured using the 'imap' or 'pop3' server to receive events. This allows you to filter the messages that satisfy the criteria specified under the 'search term' option. The email source parameters can be defined in either the ' SP_HOME /conf/ PROFILE /deployment yaml' file or the stream definition. If the parameter configurations are not available in either place, the default values are considered (i.e., if default values are available). If you need to configure server system parameters that are not provided as options in the stream definition, they need to be defined in the 'deployment yaml' file under 'email source properties'. For more information about 'imap' and 'pop3' server system parameters, see the following. JavaMail Reference Implementation - IMAP Store JavaMail Reference Implementation - POP3 Store Store Origin: siddhi-io-email:2.0.4 Syntax @source(type=\"email\", username=\" STRING \", password=\" STRING \", store=\" STRING \", host=\" STRING \", port=\" INT \", folder=\" STRING \", search.term=\" STRING \", polling.interval=\" LONG \", action.after.processed=\" STRING \", folder.to.move=\" STRING \", content.type=\" STRING \", ssl.enable=\" BOOL \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic username The user name of the email account. e.g., 'wso2mail' is the username of the 'wso2mail@gmail.com' mail account. STRING No No password The password of the email account STRING No No store The store type that used to receive emails. Possible values are 'imap' and 'pop3'. imap STRING Yes No host The host name of the server (e.g., 'imap.gmail.com' is the host name for a gmail account with an IMAP store.). The default value 'imap.gmail.com' is only valid if the email account is a gmail account with IMAP enabled. If store type is 'imap', then the default value is 'imap.gmail.com'. If the store type is 'pop3', then thedefault value is 'pop3.gmail.com'. STRING Yes No port The port that is used to create the connection. '993', the default value is valid only if the store is 'imap' and ssl-enabled. INT Yes No folder The name of the folder to which the emails should be fetched. INBOX STRING Yes No search.term The option that includes conditions such as key-value pairs to search for emails. In a string search term, the key and the value should be separated by a semicolon (';'). Each key-value pair must be within inverted commas (' '). The string search term can define multiple comma-separated key-value pairs. This string search term currently supports only the 'subject', 'from', 'to', 'bcc', and 'cc' keys. e.g., if you enter 'subject:DAS, from:carbon, bcc:wso2', the search term creates a search term instance that filters emails that contain 'DAS' in the subject, 'carbon' in the 'from' address, and 'wso2' in one of the 'bcc' addresses. The string search term carries out sub string matching that is case-sensitive. If '@' in included in the value for any key other than the 'subject' key, it checks for an address that is equal to the value given. e.g., If you search for 'abc@', the string search terms looks for an address that contains 'abc' before the '@' symbol. None STRING Yes No polling.interval This defines the time interval in seconds at which th email source should poll the account to check for new mail arrivals.in seconds. 600 LONG Yes No action.after.processed The action to be performed by the email source for the processed mail. Possible values are as follows: 'FLAGGED': Sets the flag as 'flagged'. 'SEEN': Sets the flag as 'read'. 'ANSWERED': Sets the flag as 'answered'. 'DELETE': Deletes tha mail after the polling cycle. 'MOVE': Moves the mail to the folder specified in the 'folder.to.move' parameter. If the folder specified is 'pop3', then the only option available is 'DELETE'. NONE STRING Yes No folder.to.move The name of the folder to which the mail must be moved once it is processed. If the action after processing is 'MOVE', it is required to specify a value for this parameter. STRING No No content.type The content type of the email. It can be either 'text/plain' or 'text/html.' text/plain STRING Yes No ssl.enable If this is set to 'true', a secure port is used to establish the connection. The possible values are 'true' and 'false'. true BOOL Yes No System Parameters Name Description Default Value Possible Parameters mail.imap.partialfetch This determines whether the IMAP partial-fetch capability should be used. true true or false mail.imap.fetchsize The partial fetch size in bytes. 16K value in bytes mail.imap.peek If this is set to 'true', the IMAP PEEK option should be used when fetching body parts to avoid setting the 'SEEN' flag on messages. The default value is 'false'. This can be overridden on a per-message basis by the 'setPeek method' in 'IMAPMessage'. false true or false mail.imap.connectiontimeout The socket connection timeout value in milliseconds. This timeout is implemented by 'java.net.Socket'. infinity timeout Any Integer value mail.imap.timeout The socket read timeout value in milliseconds. This timeout is implemented by 'java.net.Socket'. infinity timeout Any Integer value mail.imap.writetimeout The socket write timeout value in milliseconds. This timeout is implemented by using a 'java.util.concurrent.ScheduledExecutorService' per connection that schedules a thread to close the socket if the timeout period elapses. Therefore, the overhead of using this timeout is one thread per connection. infinity timeout Any Integer value mail.imap.statuscachetimeout The timeout value in milliseconds for the cache of 'STATUS' command response. 1000ms Time out in miliseconds mail.imap.appendbuffersize The maximum size of a message to buffer in memory when appending to an IMAP folder. None Any Integer value mail.imap.connectionpoolsize The maximum number of available connections in the connection pool. 1 Any Integer value mail.imap.connectionpooltimeout The timeout value in milliseconds for connection pool connections. 45000ms Any Integer mail.imap.separatestoreconnection If this parameter is set to 'true', it indicates that a dedicated store connection needs to be used for store commands. true true or false mail.imap.auth.login.disable If this is set to 'true', it is not possible to use the non-standard 'AUTHENTICATE LOGIN' command instead of the plain 'LOGIN' command. false true or false mail.imap.auth.plain.disable If this is set to 'true', the 'AUTHENTICATE PLAIN' command cannot be used. false true or false mail.imap.auth.ntlm.disable If true, prevents use of the AUTHENTICATE NTLM command. false true or false mail.imap.proxyauth.user If the server supports the PROXYAUTH extension, this property specifies the name of the user to act as. Authentication to log in to the server is carried out using the administrator's credentials. After authentication, the IMAP provider issues the 'PROXYAUTH' command with the user name specified in this property. None Valid string value mail.imap.localaddress The local address (host name) to bind to when creating the IMAP socket. Defaults to the address picked by the Socket class. Valid string value mail.imap.localport The local port number to bind to when creating the IMAP socket. Defaults to the port number picked by the Socket class. Valid String value mail.imap.sasl.enable If this parameter is set to 'true', the system attempts to use the 'javax.security.sasl' package to choose an authentication mechanism for the login. false true or false mail.imap.sasl.mechanisms A list of SASL mechanism names that the system should to try to use. The names can be separated by spaces or commas. None Valid string value mail.imap.sasl.authorizationid The authorization ID to use in the SASL authentication. If this parameter is not set, the authentication ID (username) is used. Valid string value mail.imap.sasl.realm The realm to use with SASL authentication mechanisms that require a realm, such as 'DIGEST-MD5'. None Valid string value mail.imap.auth.ntlm.domain The NTLM authentication domain. None Valid string value The NTLM authentication domain. NTLM protocol-specific flags. None Valid integer value mail.imap.socketFactory If this parameter is set to a class that implements the 'javax.net.SocketFactory' interface, this class is used to create IMAP sockets. None Valid SocketFactory mail.imap.socketFactory.class If this parameter is set, it specifies the name of a class that implements the 'javax.net.SocketFactory' interface. This class is used to create IMAP sockets. None Valid string mail.imap.socketFactory.fallback If this parameter is set to 'true', failure to create a socket using the specified socket factory class results in the socket being created using the 'java.net.Socket' class. true true or false mail.imap.socketFactory.port This specifies the port to connect to when using the specified socket factory. If this parameter is not set, the default port is used. 143 Valid Integer mail.imap.ssl.checkserveridentity If this parameter is set to 'true', the system checks the server identity as specified by RFC 2595. false true or false mail.imap.ssl.trust If this parameter is set and a socket factory has not been specified, it enables the use of a 'MailSSLSocketFactory'. If this parameter is set to ' ', all the hosts are trusted. If this parameter specifies list of hosts separated by white spaces, only those hosts are trusted. If the parameter is not set to any of the values mentioned above, trust depends on the certificate presented by the server. Valid String mail.imap.ssl.socketFactory If this parameter is set to a class that extends the 'javax.net.ssl.SSLSocketFactory' class this class is used to create IMAP SSL sockets. None SSL Socket Factory mail.imap.ssl.socketFactory.class If this parameter is set, it specifies the name of a class that extends the 'javax.net.ssl.SSLSocketFactory' class. This class is used to create IMAP SSL sockets. None Valid String mail.imap.ssl.socketFactory.port This specifies the port to connect to when using the specified socket factory. the default port 993 is used. valid port number mail.imap.ssl.protocols This specifies the SSL protocols that are enabled for SSL connections. The property value is a whitespace-separated list of tokens acceptable to the 'javax.net.ssl.SSLSocket.setEnabledProtocols' method. None Valid string mail.imap.starttls.enable If this parameter is set to 'true', it is possible to use the 'STARTTLS' command (if supported by the server) to switch the connection to a TLS-protected connection before issuing any login commands. false true or false mail.imap.socks.host This specifies the host name of a 'SOCKS5' proxy server that is used to connect to the mail server. None Valid String mail.imap.socks.port This specifies the port number for the 'SOCKS5' proxy server. This is needed if the proxy server is not using the standard port number 1080. 1080 Valid String mail.imap.minidletime This property sets the delay in milliseconds. 10 milliseconds time in seconds (Integer) mail.imap.enableimapevents If this property is set to 'true', it enables special IMAP-specific events to be delivered to the 'ConnectionListener' of the store. The unsolicited responses received during the idle method of the store are sent as connection events with 'IMAPStore.RESPONSE' as the type. The event's message is the raw IMAP response string. false true or false mail.imap.folder.class The class name of a subclass of 'com.sun.mail.imap.IMAPFolder'. The subclass can be used to provide support for additional IMAP commands. The subclass must have public constructors of the form 'public MyIMAPFolder'(String fullName, char separator, IMAPStore store, Boolean isNamespace) and public 'MyIMAPFolder'(ListInfo li, IMAPStore store) None Valid String mail.pop3.connectiontimeout The socket connection timeout value in milliseconds. Infinite timeout Integer value mail.pop3.timeout The socket I/O timeout value in milliseconds. Infinite timeout Integer value mail.pop3.message.class The class name of a subclass of 'com.sun.mail.pop3.POP3Message'. None Valid String mail.pop3.localaddress The local address (host name) to bind to when creating the POP3 socket. Defaults to the address picked by the Socket class. Valid String mail.pop3.localport The local port number to bind to when creating the POP3 socket. Defaults to the port number picked by the Socket class. Valid port number mail.pop3.apop.enable If this parameter is set to 'true', use 'APOP' instead of 'USER/PASS' to log in to the 'POP3' server (if the 'POP3' server supports 'APOP'). APOP sends a digest of the password instead of clearing the text password. false true or false mail.pop3.socketFactory If this parameter is set to a class that implements the 'javax.net.SocketFactory' interface, this class is used to create 'POP3' sockets. None Socket Factory mail.pop3.socketFactory.class If this parameter is set, it specifies the name of a class that implements the 'javax.net.SocketFactory' interface. This class is used to create 'POP3' sockets. None Valid String mail.pop3.socketFactory.fallback If this parameter is set to 'true', failure to create a socket using the specified socket factory class results in the socket being created using the 'java.net.Socket' class. false true or false mail.pop3.socketFactory.port This specifies the port to connect to when using the specified socket factory. Default port Valid port number mail.pop3.ssl.checkserveridentity If this parameter is set to 'true', check the server identity as specified by RFC 2595. false true or false mail.pop3.ssl.trust If this parameter is set and a socket factory has not been specified, it is possible to use a 'MailSSLSocketFactory'. If this parameter is set to ' ', all the hosts are trusted. If the parameter is set to a whitespace-separated list of hosts, only those hosts are trusted. If the parameter is not set to any of the values mentioned above, trust depends on the certificate presented by the server. Valid String mail.pop3.ssl.socketFactory If this parameter is set to a class that extends the 'javax.net.ssl.SSLSocketFactory' class, this class is used to create 'POP3' SSL sockets. None SSL Socket Factory mail.pop3.ssl.checkserveridentity If this parameter is set to 'true', the system checks the server identity as specified by 'RFC 2595'. false true or false mail.pop3.ssl.trust If this parameter is set and a socket factory has not been specified, it is possible to use a 'MailSSLSocketFactory'. If this parameter is set to '*', all the hosts are trusted. If the parameter is set to a whitespace-separated list of hosts, only those hosts are trusted. Trust depends on the certificate presented by the server. Valid String mail.pop3.ssl.socketFactory If this parameter is set to a class that extends the 'javax.net.ssl.SSLSocketFactory' class, this class is used to create 'POP3 SSL' sockets. None SSL Socket Factory mail.pop3.ssl.socketFactory.class If this parameter is set, it specifies the name of a class that extends the 'javax.net.ssl.SSLSocketFactory' class. This class is used to create 'POP3 SSL' sockets. None Valid String mail.pop3.ssl.socketFactory.p This parameter pecifies the port to connect to when using the specified socket factory. 995 Valid Integer mail.pop3.ssl.protocols This parameter specifies the SSL protocols that are enabled for SSL connections. The property value is a whitespace-separated list of tokens acceptable to the 'javax.net.ssl.SSLSocket.setEnabledProtocols' method. None Valid String mail.pop3.starttls.enable If this parameter is set to 'true', it is possible to use the 'STLS' command (if supported by the server) to switch the connection to a TLS-protected connection before issuing any login commands. false true or false mail.pop3.starttls.required If this parameter is set to 'true', it is required to use the 'STLS' command. The connect method fails if the server does not support the 'STLS' command or if the command fails. false true or false mail.pop3.socks.host This parameter specifies the host name of a 'SOCKS5' proxy server that can be used to connect to the mail server. None Valid String mail.pop3.socks.port This parameter specifies the port number for the 'SOCKS5' proxy server. None Valid String mail.pop3.disabletop If this parameter is set to 'true', the 'POP3 TOP' command is not used to fetch message headers. false true or false mail.pop3.forgettopheaders If this parameter is set to 'true', the headers that might have been retrieved using the 'POP3 TOP' command is forgotten and replaced by the headers retrieved when the 'POP3 RETR' command is executed. false true or false mail.pop3.filecache.enable If this parameter is set to 'true', the 'POP3' provider caches message data in a temporary file instead of caching them in memory. Messages are only added to the cache when accessing the message content. Message headers are always cached in memory (on demand). The file cache is removed when the folder is closed or the JVM terminates. false true or false mail.pop3.filecache.dir If the file cache is enabled, this property is used to override the default directory used by the JDK for temporary files. None Valid String mail.pop3.cachewriteto This parameter controls the behavior of the 'writeTo' method on a 'POP3' message object. If the parameter is set to 'true', the message content has not been cached yet, and the 'ignoreList' is null, the message is cached before being written. If not, the message is streamed directly to the output stream without being cached. false true or false mail.pop3.keepmessagecontent If this property is set to 'true', a hard reference to the cached content is retained, preventing the memory from being reused until the folder is closed, or until the cached content is explicitly invalidated (using the 'invalidate' method). false true or false Examples EXAMPLE 1 @source(type='email', @map(type='xml'), username='receiver.account', password='account.password',)define stream inputStream (name string, age int, country string); This example illustrates how to receive events in 'xml' format via the email source. In this example, only the required parameters are defined in the stream definition. The default values are taken for the other parameters. The search term is not defined, and therefore, all the new messages in the inbox folder are polled and taken. EXAMPLE 2 @source(type='email', @map(type='xml'), username='receiver.account', password='account.password',store = 'imap',host = 'imap.gmail.com',port = '993',searchTerm = 'subject:Stream Processor, from: from.account@ , cc: cc.account',polling.interval='500',action.after.processed='DELETE',content.type='text/html,)define stream inputStream (name string, age int, country string); This example illustrates how to receive events in 'xml' format via the email source. The email source polls the mail account every 500 seconds to check whether any new mails have arrived. It processes new mails only if they satisfy the conditions specified for the email search term (the value for 'from' of the email message should be 'from.account@. host name ', and the message should contain 'cc.account' in the cc receipient list and the word 'Stream Processor' in the mail subject). in this example, the action after processing is 'DELETE'. Therefore,after processing the event, corresponding mail is deleted from the mail folder. file (Source) File Source provides the functionality for user to feed data to siddhi from files. Both text and binary files are supported by file source. Origin: siddhi-io-file:2.0.3 Syntax @source(type=\"file\", dir.uri=\" STRING \", file.uri=\" STRING \", mode=\" STRING \", tailing=\" BOOL \", action.after.process=\" STRING \", action.after.failure=\" STRING \", move.after.process=\" STRING \", move.after.failure=\" STRING \", begin.regex=\" STRING \", end.regex=\" STRING \", file.polling.interval=\" STRING \", dir.polling.interval=\" STRING \", timeout=\" STRING \", file.read.wait.timeout=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic dir.uri Used to specify a directory to be processed. All the files inside this directory will be processed. Only one of 'dir.uri' and 'file.uri' should be provided. This uri MUST have the respective protocol specified. STRING No No file.uri Used to specify a file to be processed. Only one of 'dir.uri' and 'file.uri' should be provided. This uri MUST have the respective protocol specified. STRING No No mode This parameter is used to specify how files in given directory should.Possible values for this parameter are, 1. TEXT.FULL : to read a text file completely at once. 2. BINARY.FULL : to read a binary file completely at once. 3. LINE : to read a text file line by line. 4. REGEX : to read a text file and extract data using a regex. line STRING Yes No tailing This can either have value true or false. By default it will be true. This attribute allows user to specify whether the file should be tailed or not. If tailing is enabled, the first file of the directory will be tailed. Also tailing should not be enabled in 'binary.full' or 'text.full' modes. true BOOL Yes No action.after.process This parameter is used to specify the action which should be carried out after processing a file in the given directory. It can be either DELETE or MOVE and default value will be 'DELETE'. If the action.after.process is MOVE, user must specify the location to move consumed files using 'move.after.process' parameter. delete STRING Yes No action.after.failure This parameter is used to specify the action which should be carried out if a failure occurred during the process. It can be either DELETE or MOVE and default value will be 'DELETE'. If the action.after.failure is MOVE, user must specify the location to move consumed files using 'move.after.failure' parameter. delete STRING Yes No move.after.process If action.after.process is MOVE, user must specify the location to move consumed files using 'move.after.process' parameter. This should be the absolute path of the file that going to be created after moving is done. This uri MUST have the respective protocol specified. STRING No No move.after.failure If action.after.failure is MOVE, user must specify the location to move consumed files using 'move.after.failure' parameter. This should be the absolute path of the file that going to be created after moving is done. This uri MUST have the respective protocol specified. STRING No No begin.regex This will define the regex to be matched at the beginning of the retrieved content. None STRING Yes No end.regex This will define the regex to be matched at the end of the retrieved content. None STRING Yes No file.polling.interval This parameter is used to specify the time period (in milliseconds) of a polling cycle for a file. 1000 STRING Yes No dir.polling.interval This parameter is used to specify the time period (in milliseconds) of a polling cycle for a directory. 1000 STRING Yes No timeout This parameter is used to specify the maximum time period (in milliseconds) for waiting until a file is processed. 5000 STRING Yes No file.read.wait.timeout This parameter is used to specify the maximum time period (in milliseconds) till it waits before retrying to read the full file content. 1000 STRING Yes No Examples EXAMPLE 1 @source(type='file', mode='text.full', tailing='false' dir.uri='file://abc/xyz', action.after.process='delete', @map(type='json')) define stream FooStream (symbol string, price float, volume long); Under above configuration, all the files in directory will be picked and read one by one. In this case, it's assumed that all the files contains json valid json strings with keys 'symbol','price' and 'volume'. Once a file is read, its content will be converted to an event using siddhi-map-json extension and then, that event will be received to the FooStream. Finally, after reading is finished, the file will be deleted. EXAMPLE 2 @source(type='file', mode='files.repo.line', tailing='true', dir.uri='file://abc/xyz', @map(type='json')) define stream FooStream (symbol string, price float, volume long); Under above configuration, the first file in directory '/abc/xyz' will be picked and read line by line. In this case, it is assumed that the file contains lines json strings. For each line, line content will be converted to an event using siddhi-map-json extension and then, that event will be received to the FooStream. Once file content is completely read, it will keep checking whether a new entry is added to the file or not. If such entry is added, it will be immediately picked up and processed. grpc (Source) This extension starts a grpc server during initialization time. The server listens to requests from grpc stubs. This source has a default mode of operation and custom user defined grpc service mode. By default this uses EventService. Please find the proto definition here . In the default mode this source will use EventService consume method. If we want to use our custom gRPC services, we have to pack auto-generated gRPC service classes and protobuf classes into a jar file and add it into the project classpath (or to the jars folder in the siddhi-tooling folder if we use it with siddhi-tooling ). Please find the custom protobuf definition that uses in examples here . This method will receive requests and injects them into stream through a mapper. Origin: siddhi-io-grpc:1.0.2 Syntax @source(type=\"grpc\", receiver.url=\" STRING \", max.inbound.message.size=\" INT \", max.inbound.metadata.size=\" INT \", server.shutdown.waiting.time=\" LONG \", truststore.file=\" STRING \", truststore.password=\" STRING \", truststore.algorithm=\" STRING \", tls.store.type=\" STRING \", keystore.file=\" STRING \", keystore.password=\" STRING \", keystore.algorithm=\" STRING \", enable.ssl=\" BOOL \", threadpool.size=\" INT \", threadpool.buffer.size=\" INT \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic receiver.url The url which can be used by a client to access the grpc server in this extension. This url should consist the host hostPort, port, fully qualified service name, method name in the following format. grpc://0.0.0.0:9763/ serviceName / methodName For example: grpc://0.0.0.0:9763/org.wso2.grpc.EventService/consume STRING No No max.inbound.message.size Sets the maximum message size in bytes allowed to be received on the server. 4194304 INT Yes No max.inbound.metadata.size Sets the maximum size of metadata in bytes allowed to be received. 8192 INT Yes No server.shutdown.waiting.time The time in seconds to wait for the server to shutdown, giving up if the timeout is reached. 5 LONG Yes No truststore.file the file path of truststore. If this is provided then server authentication is enabled - STRING Yes No truststore.password the password of truststore. If this is provided then the integrity of the keystore is checked - STRING Yes No truststore.algorithm the encryption algorithm to be used for server authentication - STRING Yes No tls.store.type TLS store type - STRING Yes No keystore.file the file path of keystore. If this is provided then client authentication is enabled - STRING Yes No keystore.password the password of keystore - STRING Yes No keystore.algorithm the encryption algorithm to be used for client authentication - STRING Yes No enable.ssl to enable ssl. If set to true and truststore.file is not given then it will be set to default carbon jks by default FALSE BOOL Yes No threadpool.size Sets the maximum size of threadpool dedicated to serve requests at the gRPC server 100 INT Yes No threadpool.buffer.size Sets the maximum size of threadpool buffer server 100 INT Yes No Examples EXAMPLE 1 @source(type='grpc', receiver.url='grpc://localhost:8888/org.wso2.grpc.EventService/consume', @map(type='json')) define stream BarStream (message String); Here the port is given as 8888. So a grpc server will be started on port 8888 and the server will expose EventService. This is the default service packed with the source. In EventService the consume method is EXAMPLE 2 @source(type='grpc', receiver.url='grpc://localhost:8888/org.wso2.grpc.EventService/consume', @map(type='json', @attributes(name='trp:name', age='trp:age', message='message'))) define stream BarStream (message String, name String, age int); Here we are getting headers sent with the request as transport properties and injecting them into the stream. With each request a header will be sent in MetaData in the following format: 'Name:John', 'Age:23' EXAMPLE 3 @source(type='grpc', receiver.url='grpc://localhost:8888/org.wso2.grpc.MyService/send', @map(type='protobuf')) define stream BarStream (stringValue string, intValue int,longValue long,booleanValue bool,floatValue float,doubleValue double); Here the port is given as 8888. So a grpc server will be started on port 8888 and sever will keep listening to the 'send' RPC method in the 'MyService' service. EXAMPLE 4 @source(type='grpc', receiver.url='grpc://localhost:8888/org.wso2.grpc.MyService/send', @map(type='protobuf', @attributes(a = 'stringValue', b = 'intValue', c = 'longValue',d = 'booleanValue', e ='floatValue', f ='doubleValue'))) define stream BarStream (a string ,c long,b int, d bool,e float,f double); Here the port is given as 8888. So a grpc server will be started on port 8888 and sever will keep listening to the 'send' method in the 'MyService' service. Since we provide mapping in the stream we can use any names for stream attributes, but we have to map those names with correct protobuf message attributes' names. If we want to send metadata, we should map the attributes. EXAMPLE 5 @source(type='grpc', receiver.url='grpc://localhost:8888/org.wso2.grpc.StreamService/clientStream', @map(type='protobuf')) define stream BarStream (stringValue string, intValue int,longValue long,booleanValue bool,floatValue float,doubleValue double); Here we receive a stream of requests to the grpc source. Whenever we want to use streaming with grpc source, we have to define the RPC method as client streaming method (look at the sample proto file provided in the resource folder here ), when we define a stream method siddhi will identify it as a stream RPC method and ready to accept stream of request from the client. grpc-call-response (Source) This grpc source receives responses received from gRPC server for requests sent from a grpc-call sink. The source will receive responses for sink with the same sink.id. For example if you have a gRPC sink with sink.id 15 then we need to set the sink.id as 15 in the source to receives responses. Sinks and sources have 1:1 mapping Origin: siddhi-io-grpc:1.0.2 Syntax @source(type=\"grpc-call-response\", sink.id=\" INT \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic sink.id a unique ID that should be set for each grpc-call source. There is a 1:1 mapping between grpc-call sinks and grpc-call-response sources. Each sink has one particular source listening to the responses to requests published from that sink. So the same sink.id should be given when writing the sink also. INT No No Examples EXAMPLE 1 @source(type='grpc-call-response', sink.id= '1') define stream BarStream (message String);@sink(type='grpc-call', publisher.url = 'grpc://194.23.98.100:8080/EventService/process', sink.id= '1', @map(type='json')) define stream FooStream (message String); Here we are listening to responses for requests sent from the sink with sink.id 1 will be received here. The results will be injected into BarStream grpc-service (Source) This extension implements a grpc server for receiving and responding to requests. During initialization time a grpc server is started on the user specified port exposing the required service as given in the url. This source also has a default mode and a user defined grpc service mode. By default this uses EventService. Please find the proto definition here In the default mode this will use the EventService process method. If we want to use our custom gRPC services, we have to pack auto-generated gRPC service classes and protobuf classes into a jar file and add it into the project classpath (or to the jars folder in the siddhi-tooling folder if we use it with siddhi-tooling ). Please find the custom protobuf definition that uses in examples here . This accepts grpc message class Event as defined in the EventService proto. This uses GrpcServiceResponse sink to send reponses back in the same Event message format. Origin: siddhi-io-grpc:1.0.2 Syntax @source(type=\"grpc-service\", receiver.url=\" STRING \", max.inbound.message.size=\" INT \", max.inbound.metadata.size=\" INT \", service.timeout=\" INT \", server.shutdown.waiting.time=\" LONG \", truststore.file=\" STRING \", truststore.password=\" STRING \", truststore.algorithm=\" STRING \", tls.store.type=\" STRING \", keystore.file=\" STRING \", keystore.password=\" STRING \", keystore.algorithm=\" STRING \", enable.ssl=\" BOOL \", threadpool.size=\" INT \", threadpool.buffer.size=\" INT \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic receiver.url The url which can be used by a client to access the grpc server in this extension. This url should consist the host hostPort, port, fully qualified service name, method name in the following format. grpc://0.0.0.0:9763/ serviceName / methodName For example: grpc://0.0.0.0:9763/org.wso2.grpc.EventService/consume STRING No No max.inbound.message.size Sets the maximum message size in bytes allowed to be received on the server. 4194304 INT Yes No max.inbound.metadata.size Sets the maximum size of metadata in bytes allowed to be received. 8192 INT Yes No service.timeout The period of time in milliseconds to wait for siddhi to respond to a request received. After this time period of receiving a request it will be closed with an error message. 10000 INT Yes No server.shutdown.waiting.time The time in seconds to wait for the server to shutdown, giving up if the timeout is reached. 5 LONG Yes No truststore.file the file path of truststore. If this is provided then server authentication is enabled - STRING Yes No truststore.password the password of truststore. If this is provided then the integrity of the keystore is checked - STRING Yes No truststore.algorithm the encryption algorithm to be used for server authentication - STRING Yes No tls.store.type TLS store type - STRING Yes No keystore.file the file path of keystore. If this is provided then client authentication is enabled - STRING Yes No keystore.password the password of keystore - STRING Yes No keystore.algorithm the encryption algorithm to be used for client authentication - STRING Yes No enable.ssl to enable ssl. If set to true and truststore.file is not given then it will be set to default carbon jks by default FALSE BOOL Yes No threadpool.size Sets the maximum size of threadpool dedicated to serve requests at the gRPC server 100 INT Yes No threadpool.buffer.size Sets the maximum size of threadpool buffer server 100 INT Yes No Examples EXAMPLE 1 @source(type='grpc-service', receiver.url='grpc://localhost:8888/org.wso2.grpc.EventService/process', source.id='1', @map(type='json', @attributes(messageId='trp:messageId', message='message'))) define stream FooStream (messageId String, message String); Here a grpc server will be started at port 8888. The process method of EventService will be exposed for clients. source.id is set as 1. So a grpc-service-response sink with source.id = 1 will send responses back for requests received to this source. Note that it is required to specify the transport property messageId since we need to correlate the request message with the response. EXAMPLE 2 @sink(type='grpc-service-response', source.id='1', @map(type='json')) define stream BarStream (messageId String, message String); @source(type='grpc-service', receiver.url='grpc://134.23.43.35:8080/org.wso2.grpc.EventService/process', source.id='1', @map(type='json', @attributes(messageId='trp:messageId', message='message'))) define stream FooStream (messageId String, message String); from FooStream select * insert into BarStream; The grpc requests are received through the grpc-service sink. Each received event is sent back through grpc-service-source. This is just a passthrough through Siddhi as we are selecting everything from FooStream and inserting into BarStream. EXAMPLE 3 @source(type='grpc-service', source.id='1' receiver.url='grpc://locanhost:8888/org.wso2.grpc.EventService/consume', @map(type='json', @attributes(name='trp:name', age='trp:age', message='message'))) define stream BarStream (message String, name String, age int); Here we are getting headers sent with the request as transport properties and injecting them into the stream. With each request a header will be sent in MetaData in the following format: 'Name:John', 'Age:23' EXAMPLE 4 @sink(type='grpc-service-response', source.id='1', message.id='{{messageId}}', @map(type='protobuf', @payload(stringValue='a',intValue='b',longValue='c',booleanValue='d',floatValue = 'e', doubleValue ='f'))) define stream BarStream (a string,messageId string, b int,c long,d bool,e float,f double); @source(type='grpc-service', receiver.url='grpc://134.23.43.35:8888/org.wso2.grpc.test.MyService/process', source.id='1', @map(type='protobuf', @attributes(messageId='trp:message.id', a = 'stringValue', b = 'intValue', c = 'longValue',d = 'booleanValue', e = 'floatValue', f ='doubleValue'))) define stream FooStream (a string,messageId string, b int,c long,d bool,e float,f double); from FooStream select * insert into BarStream; Here a grpc server will be started at port 8888. The process method of the MyService will be exposed to the clients. 'source.id' is set as 1. So a grpc-service-response sink with source.id = 1 will send responses back for requests received to this source. Note that it is required to specify the transport property messageId since we need to correlate the request message with the response and also we should map stream attributes with correct protobuf message attributes even they define using the same name as protobuf message attributes. http (Source) HTTP source receives POST requests via HTTP and HTTPS protocols in format such as text , XML and JSON . It also supports basic authentication to ensure events are received from authorized users/systems. The request headers and properties can be accessed via transport properties in the format trp: header . Origin: siddhi-io-http:2.1.2 Syntax @source(type=\"http\", receiver.url=\" STRING \", basic.auth.enabled=\" STRING \", worker.count=\" INT \", socket.idle.timeout=\" INT \", ssl.verify.client=\" STRING \", ssl.protocol=\" STRING \", tls.store.type=\" STRING \", ssl.configurations=\" STRING \", request.size.validation.configurations=\" STRING \", header.validation.configurations=\" STRING \", server.bootstrap.configurations=\" STRING \", trace.log.enabled=\" BOOL \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic receiver.url The URL on which events should be received. To enable SSL use https protocol in the url. http://0.0.0.0:9763/ appNAme / streamName STRING Yes No basic.auth.enabled This only works in VM, Docker and Kubernetes. Where when enabled it authenticates each request using the Authorization:'Basic encodeBase64(username:Password)' header. false STRING Yes No worker.count The number of active worker threads to serve the incoming events. By default the value is set to 1 to ensure events are processed in the same order they arrived. By increasing this value, higher performance can be achieved in the expense of loosing event ordering. 1 INT Yes No socket.idle.timeout Idle timeout for HTTP connection in millis. 120000 INT Yes No ssl.verify.client The type of client certificate verification. Supported values are require , optional . - STRING Yes No ssl.protocol SSL/TLS protocol. TLS STRING Yes No tls.store.type TLS store type. JKS STRING Yes No ssl.configurations SSL/TSL configurations in format \"' key : value ',' key : value '\" . Some supported parameters: - SSL/TLS protocols: 'sslEnabledProtocols:TLSv1.1,TLSv1.2' - List of ciphers: 'ciphers:TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256' - Enable session creation: 'client.enable.session.creation:true' - Supported server names: 'server.suported.server.names:server' - Add HTTP SNIMatcher: 'server.supported.snimatchers:SNIMatcher' - STRING Yes No request.size.validation.configurations Configurations to validate the HTTP request size. Expected format \"' key : value ',' key : value '\" . Some supported configurations : - Enable request size validation: 'request.size.validation:true' If request size is validated - Maximum request size: 'request.size.validation.maximum.value:2048' - Response status code when request size validation fails: 'request.size.validation.reject.status.code:401' - Response message when request size validation fails: 'request.size.validation.reject.message:Message is bigger than the valid size' - Response Content-Type when request size validation fails: 'request.size.validation.reject.message.content.type:plain/text' - STRING Yes No header.validation.configurations Configurations to validate HTTP headers. Expected format \"' key : value ',' key : value '\" . Some supported configurations : - Enable header size validation: 'header.size.validation:true' If header size is validated - Maximum length of initial line: 'header.validation.maximum.request.line:4096' - Maximum length of all headers: 'header.validation.maximum.size:8192' - Maximum length of the content or each chunk: 'header.validation.maximum.chunk.size:8192' - Response status code when header validation fails: 'header.validation.reject.status.code:401' - Response message when header validation fails: 'header.validation.reject.message:Message header is bigger than the valid size' - Response Content-Type when header validation fails: 'header.validation.reject.message.content.type:plain/text' - STRING Yes No server.bootstrap.configurations Server bootstrap configurations in format \"' key : value ',' key : value '\" . Some supported configurations : - Server connect timeout in millis: 'server.bootstrap.connect.timeout:15000' - Server socket timeout in seconds: 'server.bootstrap.socket.timeout:15' - Enable TCP no delay: 'server.bootstrap.nodelay:true' - Enable server keep alive: 'server.bootstrap.keepalive:true' - Send buffer size: 'server.bootstrap.sendbuffersize:1048576' - Receive buffer size: 'server.bootstrap.recievebuffersize:1048576' - Number of connections queued: 'server.bootstrap.socket.backlog:100' - STRING Yes No trace.log.enabled Enable trace log for traffic monitoring. false BOOL Yes No System Parameters Name Description Default Value Possible Parameters serverBootstrapBossGroupSize Number of boss threads to accept incoming connections. Number of available processors Any positive integer serverBootstrapWorkerGroupSize Number of worker threads to accept the connections from boss threads and perform non-blocking read and write from one or more channels. (Number of available processors) * 2 Any positive integer serverBootstrapClientGroupSize Number of client threads to perform non-blocking read and write to one or more channels. (Number of available processors) * 2 Any positive integer defaultHost The default host of the transport. 0.0.0.0 Any valid host defaultScheme The default protocol. http http https defaultHttpPort The default HTTP port when default scheme is http . 8280 Any valid port defaultHttpsPort The default HTTPS port when default scheme is https . 8243 Any valid port keyStoreLocation The default keystore file path. ${carbon.home}/resources/security/wso2carbon.jks Path to .jks file keyStorePassword The default keystore password. wso2carbon Keystore password as string Examples EXAMPLE 1 @app.name('StockProcessor') @source(type='http', @map(type = 'json')) define stream StockStream (symbol string, price float, volume long); Above HTTP source listeners on url http://0.0.0.0:9763/StockProcessor/StockStream for JSON messages on the format: { \"event\": { \"symbol\": \"FB\", \"price\": 24.5, \"volume\": 5000 } } It maps the incoming messages and sends them to StockStream for processing. EXAMPLE 2 @source(type='http', receiver.url='http://localhost:5005/stocks', @map(type = 'xml')) define stream StockStream (symbol string, price float, volume long); Above HTTP source listeners on url http://localhost:5005/stocks for JSON messages on the format: events event symbol Fb /symbol price 55.6 /price volume 100 /volume /event /events It maps the incoming messages and sends them to StockStream for processing. http-call-response (Source) The http-call-response source receives the responses for the calls made by its corresponding http-call sink, and maps them from formats such as text , XML and JSON . To handle messages with different http status codes having different formats, multiple http-call-response sources are allowed to associate with a single http-call sink. It allows accessing the attributes of the event that initiated the call, and the response headers and properties via transport properties in the format trp: attribute name and trp: header/property respectively. Origin: siddhi-io-http:2.1.2 Syntax @source(type=\"http-call-response\", sink.id=\" STRING \", http.status.code=\" STRING \", allow.streaming.responses=\" BOOL \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic sink.id Identifier to correlate the http-call-response source with its corresponding http-call sink that published the messages. STRING No No http.status.code The matching http responses status code regex, that is used to filter the the messages which will be processed by the source.Eg: http.status.code = '200' , http.status.code = '4\\d+' 200 STRING Yes No allow.streaming.responses Enable consuming responses on a streaming manner. false BOOL Yes No Examples EXAMPLE 1 @sink(type='http-call', method='POST', publisher.url='http://localhost:8005/registry/employee', sink.id='employee-info', @map(type='json')) define stream EmployeeRequestStream (name string, id int); @source(type='http-call-response', sink.id='employee-info', http.status.code='2\\\\d+', @map(type='json', @attributes(name='trp:name', id='trp:id', location='$.town', age='$.age'))) define stream EmployeeResponseStream(name string, id int, location string, age int); @source(type='http-call-response', sink.id='employee-info', http.status.code='4\\\\d+', @map(type='text', regex.A='((.|\\n)*)', @attributes(error='A[1]'))) define stream EmployeeErrorStream(error string); When events arrive in EmployeeRequestStream , http-call sink makes calls to endpoint on url http://localhost:8005/registry/employee with POST method and Content-Type application/json . If the arriving event has attributes name : John and id : 1423 it will send a message with default JSON mapping as follows: { \"event\": { \"name\": \"John\", \"id\": 1423 } } When the endpoint responds with status code in the range of 200 the message will be received by the http-call-response source associated with the EmployeeResponseStream stream, because it is correlated with the sink by the same sink.id employee-info and as that expects messages with http.status.code in regex format 2\\d+ . If the response message is in the format { \"town\": \"NY\", \"age\": 24 } the source maps the location and age attributes by executing JSON path on the message and maps the name and id attributes by extracting them from the request event via as transport properties. If the response status code is in the range of 400 then the message will be received by the http-call-response source associated with the EmployeeErrorStream stream, because it is correlated with the sink by the same sink.id employee-info and it expects messages with http.status.code in regex format 4\\d+ , and maps the error response to the error attribute of the event. http-request (Source) Deprecated (Use http-service source instead). The http-request source receives POST requests via HTTP and HTTPS protocols in format such as text , XML and JSON and sends responses via its corresponding http-response sink correlated through a unique source.id . For request and response correlation, it generates a messageId upon each incoming request and expose it via transport properties in the format trp:messageId to correlate them with the responses at the http-response sink. The request headers and properties can be accessed via transport properties in the format trp: header . It also supports basic authentication to ensure events are received from authorized users/systems. Origin: siddhi-io-http:2.1.2 Syntax @source(type=\"http-request\", receiver.url=\" STRING \", source.id=\" STRING \", connection.timeout=\" INT \", basic.auth.enabled=\" STRING \", worker.count=\" INT \", socket.idle.timeout=\" INT \", ssl.verify.client=\" STRING \", ssl.protocol=\" STRING \", tls.store.type=\" STRING \", ssl.configurations=\" STRING \", request.size.validation.configurations=\" STRING \", header.validation.configurations=\" STRING \", server.bootstrap.configurations=\" STRING \", trace.log.enabled=\" BOOL \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic receiver.url The URL on which events should be received. To enable SSL use https protocol in the url. http://0.0.0.0:9763/ appNAme / streamName STRING Yes No source.id Identifier to correlate the http-request source to its corresponding http-response sinks to send responses. STRING No No connection.timeout Connection timeout in millis. The system will send a timeout, if a corresponding response is not sent by an associated http-response sink within the given time. 120000 INT Yes No basic.auth.enabled This only works in VM, Docker and Kubernetes. Where when enabled it authenticates each request using the Authorization:'Basic encodeBase64(username:Password)' header. false STRING Yes No worker.count The number of active worker threads to serve the incoming events. By default the value is set to 1 to ensure events are processed in the same order they arrived. By increasing this value, higher performance can be achieved in the expense of loosing event ordering. 1 INT Yes No socket.idle.timeout Idle timeout for HTTP connection in millis. 120000 INT Yes No ssl.verify.client The type of client certificate verification. Supported values are require , optional . - STRING Yes No ssl.protocol SSL/TLS protocol. TLS STRING Yes No tls.store.type TLS store type. JKS STRING Yes No ssl.configurations SSL/TSL configurations in format \"' key : value ',' key : value '\" . Some supported parameters: - SSL/TLS protocols: 'sslEnabledProtocols:TLSv1.1,TLSv1.2' - List of ciphers: 'ciphers:TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256' - Enable session creation: 'client.enable.session.creation:true' - Supported server names: 'server.suported.server.names:server' - Add HTTP SNIMatcher: 'server.supported.snimatchers:SNIMatcher' - STRING Yes No request.size.validation.configurations Configurations to validate the HTTP request size. Expected format \"' key : value ',' key : value '\" . Some supported configurations : - Enable request size validation: 'request.size.validation:true' If request size is validated - Maximum request size: 'request.size.validation.maximum.value:2048' - Response status code when request size validation fails: 'request.size.validation.reject.status.code:401' - Response message when request size validation fails: 'request.size.validation.reject.message:Message is bigger than the valid size' - Response Content-Type when request size validation fails: 'request.size.validation.reject.message.content.type:plain/text' - STRING Yes No header.validation.configurations Configurations to validate HTTP headers. Expected format \"' key : value ',' key : value '\" . Some supported configurations : - Enable header size validation: 'header.size.validation:true' If header size is validated - Maximum length of initial line: 'header.validation.maximum.request.line:4096' - Maximum length of all headers: 'header.validation.maximum.size:8192' - Maximum length of the content or each chunk: 'header.validation.maximum.chunk.size:8192' - Response status code when header validation fails: 'header.validation.reject.status.code:401' - Response message when header validation fails: 'header.validation.reject.message:Message header is bigger than the valid size' - Response Content-Type when header validation fails: 'header.validation.reject.message.content.type:plain/text' - STRING Yes No server.bootstrap.configurations Server bootstrap configurations in format \"' key : value ',' key : value '\" . Some supported configurations : - Server connect timeout in millis: 'server.bootstrap.connect.timeout:15000' - Server socket timeout in seconds: 'server.bootstrap.socket.timeout:15' - Enable TCP no delay: 'server.bootstrap.nodelay:true' - Enable server keep alive: 'server.bootstrap.keepalive:true' - Send buffer size: 'server.bootstrap.sendbuffersize:1048576' - Receive buffer size: 'server.bootstrap.recievebuffersize:1048576' - Number of connections queued: 'server.bootstrap.socket.backlog:100' - STRING Yes No trace.log.enabled Enable trace log for traffic monitoring. false BOOL Yes No System Parameters Name Description Default Value Possible Parameters serverBootstrapBossGroupSize Number of boss threads to accept incoming connections. Number of available processors Any positive integer serverBootstrapWorkerGroupSize Number of worker threads to accept the connections from boss threads and perform non-blocking read and write from one or more channels. (Number of available processors) * 2 Any positive integer serverBootstrapClientGroupSize Number of client threads to perform non-blocking read and write to one or more channels. (Number of available processors) * 2 Any positive integer defaultHost The default host of the transport. 0.0.0.0 Any valid host defaultScheme The default protocol. http http https defaultHttpPort The default HTTP port when default scheme is http . 8280 Any valid port defaultHttpsPort The default HTTPS port when default scheme is https . 8243 Any valid port keyStoreLocation The default keystore file path. ${carbon.home}/resources/security/wso2carbon.jks Path to .jks file keyStorePassword The default keystore password. wso2carbon Keystore password as string Examples EXAMPLE 1 @source(type='http-request', receiver.url='http://localhost:5005/add', source.id='adder', @map(type='json, @attributes(messageId='trp:messageId', value1='$.event.value1', value2='$.event.value2'))) define stream AddStream (messageId string, value1 long, value2 long); @sink(type='http-response', source.id='adder', message.id='{{messageId}}', @map(type = 'json')) define stream ResultStream (messageId string, results long); @info(name = 'query1') from AddStream select messageId, value1 + value2 as results insert into ResultStream; Above sample listens events on http://localhost:5005/stocks url for JSON messages on the format: { \"event\": { \"value1\": 3, \"value2\": 4 } } Map the vents into AddStream, process the events through query query1 , and sends the results produced on ResultStream via http-response sink on the message format: { \"event\": { \"results\": 7 } } http-response (Source) Deprecated (Use http-call-response source instead). The http-response source receives the responses for the calls made by its corresponding http-request sink, and maps them from formats such as text , XML and JSON . To handle messages with different http status codes having different formats, multiple http-response sources are allowed to associate with a single http-request sink. It allows accessing the attributes of the event that initiated the call, and the response headers and properties via transport properties in the format trp: attribute name and trp: header/property respectively. Origin: siddhi-io-http:2.1.2 Syntax @source(type=\"http-response\", sink.id=\" STRING \", http.status.code=\" STRING \", allow.streaming.responses=\" BOOL \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic sink.id Identifier to correlate the http-response source with its corresponding http-request sink that published the messages. STRING No No http.status.code The matching http responses status code regex, that is used to filter the the messages which will be processed by the source.Eg: http.status.code = '200' , http.status.code = '4\\d+' 200 STRING Yes No allow.streaming.responses Enable consuming responses on a streaming manner. false BOOL Yes No Examples EXAMPLE 1 @sink(type='http-request', method='POST', publisher.url='http://localhost:8005/registry/employee', sink.id='employee-info', @map(type='json')) define stream EmployeeRequestStream (name string, id int); @source(type='http-response', sink.id='employee-info', http.status.code='2\\\\d+', @map(type='json', @attributes(name='trp:name', id='trp:id', location='$.town', age='$.age'))) define stream EmployeeResponseStream(name string, id int, location string, age int); @source(type='http-response', sink.id='employee-info', http.status.code='4\\\\d+', @map(type='text', regex.A='((.|\\n)*)', @attributes(error='A[1]'))) define stream EmployeeErrorStream(error string); When events arrive in EmployeeRequestStream , http-request sink makes calls to endpoint on url http://localhost:8005/registry/employee with POST method and Content-Type application/json . If the arriving event has attributes name : John and id : 1423 it will send a message with default JSON mapping as follows: { \"event\": { \"name\": \"John\", \"id\": 1423 } } When the endpoint responds with status code in the range of 200 the message will be received by the http-response source associated with the EmployeeResponseStream stream, because it is correlated with the sink by the same sink.id employee-info and as that expects messages with http.status.code in regex format 2\\d+ . If the response message is in the format { \"town\": \"NY\", \"age\": 24 } the source maps the location and age attributes by executing JSON path on the message and maps the name and id attributes by extracting them from the request event via as transport properties. If the response status code is in the range of 400 then the message will be received by the http-response source associated with the EmployeeErrorStream stream, because it is correlated with the sink by the same sink.id employee-info and it expects messages with http.status.code in regex format 4\\d+ , and maps the error response to the error attribute of the event. http-service (Source) The http-service source receives POST requests via HTTP and HTTPS protocols in format such as text , XML and JSON and sends responses via its corresponding http-service-response sink correlated through a unique source.id . For request and response correlation, it generates a messageId upon each incoming request and expose it via transport properties in the format trp:messageId to correlate them with the responses at the http-service-response sink. The request headers and properties can be accessed via transport properties in the format trp: header . It also supports basic authentication to ensure events are received from authorized users/systems. Origin: siddhi-io-http:2.1.2 Syntax @source(type=\"http-service\", receiver.url=\" STRING \", source.id=\" STRING \", connection.timeout=\" INT \", basic.auth.enabled=\" STRING \", worker.count=\" INT \", socket.idle.timeout=\" INT \", ssl.verify.client=\" STRING \", ssl.protocol=\" STRING \", tls.store.type=\" STRING \", ssl.configurations=\" STRING \", request.size.validation.configurations=\" STRING \", header.validation.configurations=\" STRING \", server.bootstrap.configurations=\" STRING \", trace.log.enabled=\" BOOL \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic receiver.url The URL on which events should be received. To enable SSL use https protocol in the url. http://0.0.0.0:9763/ appNAme / streamName STRING Yes No source.id Identifier to correlate the http-service source to its corresponding http-service-response sinks to send responses. STRING No No connection.timeout Connection timeout in millis. The system will send a timeout, if a corresponding response is not sent by an associated http-service-response sink within the given time. 120000 INT Yes No basic.auth.enabled This only works in VM, Docker and Kubernetes. Where when enabled it authenticates each request using the Authorization:'Basic encodeBase64(username:Password)' header. false STRING Yes No worker.count The number of active worker threads to serve the incoming events. By default the value is set to 1 to ensure events are processed in the same order they arrived. By increasing this value, higher performance can be achieved in the expense of loosing event ordering. 1 INT Yes No socket.idle.timeout Idle timeout for HTTP connection in millis. 120000 INT Yes No ssl.verify.client The type of client certificate verification. Supported values are require , optional . - STRING Yes No ssl.protocol SSL/TLS protocol. TLS STRING Yes No tls.store.type TLS store type. JKS STRING Yes No ssl.configurations SSL/TSL configurations in format \"' key : value ',' key : value '\" . Some supported parameters: - SSL/TLS protocols: 'sslEnabledProtocols:TLSv1.1,TLSv1.2' - List of ciphers: 'ciphers:TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256' - Enable session creation: 'client.enable.session.creation:true' - Supported server names: 'server.suported.server.names:server' - Add HTTP SNIMatcher: 'server.supported.snimatchers:SNIMatcher' - STRING Yes No request.size.validation.configurations Configurations to validate the HTTP request size. Expected format \"' key : value ',' key : value '\" . Some supported configurations : - Enable request size validation: 'request.size.validation:true' If request size is validated - Maximum request size: 'request.size.validation.maximum.value:2048' - Response status code when request size validation fails: 'request.size.validation.reject.status.code:401' - Response message when request size validation fails: 'request.size.validation.reject.message:Message is bigger than the valid size' - Response Content-Type when request size validation fails: 'request.size.validation.reject.message.content.type:plain/text' - STRING Yes No header.validation.configurations Configurations to validate HTTP headers. Expected format \"' key : value ',' key : value '\" . Some supported configurations : - Enable header size validation: 'header.size.validation:true' If header size is validated - Maximum length of initial line: 'header.validation.maximum.request.line:4096' - Maximum length of all headers: 'header.validation.maximum.size:8192' - Maximum length of the content or each chunk: 'header.validation.maximum.chunk.size:8192' - Response status code when header validation fails: 'header.validation.reject.status.code:401' - Response message when header validation fails: 'header.validation.reject.message:Message header is bigger than the valid size' - Response Content-Type when header validation fails: 'header.validation.reject.message.content.type:plain/text' - STRING Yes No server.bootstrap.configurations Server bootstrap configurations in format \"' key : value ',' key : value '\" . Some supported configurations : - Server connect timeout in millis: 'server.bootstrap.connect.timeout:15000' - Server socket timeout in seconds: 'server.bootstrap.socket.timeout:15' - Enable TCP no delay: 'server.bootstrap.nodelay:true' - Enable server keep alive: 'server.bootstrap.keepalive:true' - Send buffer size: 'server.bootstrap.sendbuffersize:1048576' - Receive buffer size: 'server.bootstrap.recievebuffersize:1048576' - Number of connections queued: 'server.bootstrap.socket.backlog:100' - STRING Yes No trace.log.enabled Enable trace log for traffic monitoring. false BOOL Yes No System Parameters Name Description Default Value Possible Parameters serverBootstrapBossGroupSize Number of boss threads to accept incoming connections. Number of available processors Any positive integer serverBootstrapWorkerGroupSize Number of worker threads to accept the connections from boss threads and perform non-blocking read and write from one or more channels. (Number of available processors) * 2 Any positive integer serverBootstrapClientGroupSize Number of client threads to perform non-blocking read and write to one or more channels. (Number of available processors) * 2 Any positive integer defaultHost The default host of the transport. 0.0.0.0 Any valid host defaultScheme The default protocol. http http https defaultHttpPort The default HTTP port when default scheme is http . 8280 Any valid port defaultHttpsPort The default HTTPS port when default scheme is https . 8243 Any valid port keyStoreLocation The default keystore file path. ${carbon.home}/resources/security/wso2carbon.jks Path to .jks file keyStorePassword The default keystore password. wso2carbon Keystore password as string Examples EXAMPLE 1 @source(type='http-service', receiver.url='http://localhost:5005/add', source.id='adder', @map(type='json, @attributes(messageId='trp:messageId', value1='$.event.value1', value2='$.event.value2'))) define stream AddStream (messageId string, value1 long, value2 long); @sink(type='http-service-response', source.id='adder', message.id='{{messageId}}', @map(type = 'json')) define stream ResultStream (messageId string, results long); @info(name = 'query1') from AddStream select messageId, value1 + value2 as results insert into ResultStream; Above sample listens events on http://localhost:5005/stocks url for JSON messages on the format: { \"event\": { \"value1\": 3, \"value2\": 4 } } Map the vents into AddStream, process the events through query query1 , and sends the results produced on ResultStream via http-service-response sink on the message format: { \"event\": { \"results\": 7 } } inMemory (Source) In-memory source subscribes to a topic to consume events which are published on the same topic by In-memory sinks. This provides a way to connect multiple Siddhi Apps deployed under the same Siddhi Manager (JVM). Here both the publisher and subscriber should have the same event schema (stream definition) for successful data transfer. Origin: siddhi-core:5.1.7 Syntax @source(type=\"inMemory\", topic=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic topic Subscribes to the events sent on the given topic. STRING No No Examples EXAMPLE 1 @source(type='inMemory', topic='Stocks', @map(type='passThrough')) define stream StocksStream (symbol string, price float, volume long); Here the StocksStream uses inMemory source to consume events published on the topic Stocks by the inMemory sinks deployed in the same JVM. jms (Source) JMS Source allows users to subscribe to a JMS broker and receive JMS messages. It has the ability to receive Map messages and Text messages. Origin: siddhi-io-jms:2.0.2 Syntax @source(type=\"jms\", destination=\" STRING \", connection.factory.jndi.name=\" STRING \", factory.initial=\" STRING \", provider.url=\" STRING \", connection.factory.type=\" STRING \", worker.count=\" INT \", connection.username=\" STRING \", connection.password=\" STRING \", retry.interval=\" INT \", retry.count=\" INT \", use.receiver=\" BOOL \", subscription.durable=\" BOOL \", connection.factory.nature=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic destination Queue/Topic name which JMS Source should subscribe to STRING No No connection.factory.jndi.name JMS Connection Factory JNDI name. This value will be used for the JNDI lookup to find the JMS Connection Factory. QueueConnectionFactory STRING Yes No factory.initial Naming factory initial value STRING No No provider.url Java naming provider URL. Property for specifying configuration information for the service provider to use. The value of the property should contain a URL string (e.g. \"ldap://somehost:389\") STRING No No connection.factory.type Type of the connection connection factory. This can be either queue or topic. queue STRING Yes No worker.count Number of worker threads listening on the given queue/topic. 1 INT Yes No connection.username username for the broker. None STRING Yes No connection.password Password for the broker None STRING Yes No retry.interval Interval between each retry attempt in case of connection failure in milliseconds. 10000 INT Yes No retry.count Number of maximum reties that will be attempted in case of connection failure with broker. 5 INT Yes No use.receiver Implementation to be used when consuming JMS messages. By default transport will use MessageListener and tweaking this property will make make use of MessageReceiver false BOOL Yes No subscription.durable Property to enable durable subscription. false BOOL Yes No connection.factory.nature Connection factory nature for the broker. default STRING Yes No Examples EXAMPLE 1 @source(type='jms', @map(type='json'), factory.initial='org.apache.activemq.jndi.ActiveMQInitialContextFactory', provider.url='tcp://localhost:61616',destination='DAS_JMS_TEST', connection.factory.type='topic',connection.factory.jndi.name='TopicConnectionFactory') define stream inputStream (name string, age int, country string); This example shows how to connect to an ActiveMQ topic and receive messages. EXAMPLE 2 @source(type='jms', @map(type='json'), factory.initial='org.apache.activemq.jndi.ActiveMQInitialContextFactory', provider.url='tcp://localhost:61616',destination='DAS_JMS_TEST' ) define stream inputStream (name string, age int, country string); This example shows how to connect to an ActiveMQ queue and receive messages. Note that we are not providing properties like connection factory type kafka (Source) A Kafka source receives events to be processed by WSO2 SP from a topic with a partition for a Kafka cluster. The events received can be in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. Origin: siddhi-io-kafka:5.0.4 Syntax @source(type=\"kafka\", bootstrap.servers=\" STRING \", topic.list=\" STRING \", group.id=\" STRING \", threading.option=\" STRING \", partition.no.list=\" STRING \", seq.enabled=\" BOOL \", is.binary.message=\" BOOL \", topic.offsets.map=\" STRING \", optional.configuration=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This specifies the list of Kafka servers to which the Kafka source must listen. This list can be provided as a set of comma-separated values. e.g., localhost:9092,localhost:9093 STRING No No topic.list This specifies the list of topics to which the source must listen. This list can be provided as a set of comma-separated values. e.g., topic_one,topic_two STRING No No group.id This is an ID to identify the Kafka source group. The group ID ensures that sources with the same topic and partition that are in the same group do not receive the same event. STRING No No threading.option This specifies whether the Kafka source is to be run on a single thread, or in multiple threads based on a condition. Possible values are as follows: single.thread : To run the Kafka source on a single thread. topic.wise : To use a separate thread per topic. partition.wise : To use a separate thread per partition. STRING No No partition.no.list The partition number list for the given topic. This is provided as a list of comma-separated values. e.g., 0,1,2, . 0 STRING Yes No seq.enabled If this parameter is set to true , the sequence of the events received via the source is taken into account. Therefore, each event should contain a sequence number as an attribute value to indicate the sequence. false BOOL Yes No is.binary.message In order to receive binary events via the Kafka source,it is required to setthis parameter to 'True'. false BOOL Yes No topic.offsets.map This parameter specifies reading offsets for each topic and partition. The value for this parameter is specified in the following format: topic = offset , topic = offset , When an offset is defined for a topic, the Kafka source skips reading the message with the number specified as the offset as well as all the messages sent previous to that message. If the offset is not defined for a specific topic it reads messages from the beginning. e.g., stocks=100,trades=50 reads from the 101th message of the stocks topic, and from the 51 st message of the trades topic. null STRING Yes No optional.configuration This parameter contains all the other possible configurations that the consumer is created with. e.g., ssl.keystore.type:JKS,batch.size:200 . null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic,kafka_topic2', group.id='test', threading.option='partition.wise', bootstrap.servers='localhost:9092', partition.no.list='0,1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This kafka source configuration listens to the kafka_topic and kafka_topic2 topics with 0 and 1 partitions. A thread is created for each topic and partition combination. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . EXAMPLE 2 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic', group.id='test', threading.option='single.thread', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka source configuration listens to the kafka_topic topic for the default partition because no partition.no.list is defined. Only one thread is created for the topic. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . kafkaMultiDC (Source) The Kafka Multi-Datacenter(DC) source receives records from the same topic in brokers deployed in two different kafka clusters. It filters out all the duplicate messages and ensuresthat the events are received in the correct order using sequential numbering. It receives events in formats such as TEXT , XML JSON and Binary`.The Kafka Source creates the default partition '0' for a given topic, if the topic has not yet been created in the Kafka cluster. Origin: siddhi-io-kafka:5.0.4 Syntax @source(type=\"kafkaMultiDC\", bootstrap.servers=\" STRING \", topic=\" STRING \", partition.no=\" INT \", is.binary.message=\" BOOL \", optional.configuration=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This contains the kafka server list which the kafka source listens to. This is given using comma-separated values. eg: 'localhost:9092,localhost:9093' STRING No No topic This is the topic that the source listens to. eg: 'topic_one' STRING No No partition.no This is the partition number of the given topic. 0 INT Yes No is.binary.message In order to receive the binary events via the Kafka Multi-DC source, the value of this parameter needs to be set to 'True'. false BOOL Yes No optional.configuration This contains all the other possible configurations with which the consumer can be created.eg: producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source(type='kafkaMultiDC', topic='kafka_topic', bootstrap.servers='host1:9092,host1:9093', partition.no='1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; The following query listens to 'kafka_topic' topic, deployed in the broker host1:9092 and host1:9093, with partition 1. A thread is created for each broker. The receiving xml events are mapped to a siddhi event and sent to the FooStream. nats (Source) NATS Source allows users to subscribe to a NATS broker and receive messages. It has the ability to receive all the message types supported by NATS. Origin: siddhi-io-nats:2.0.6 Syntax @source(type=\"nats\", destination=\" STRING \", bootstrap.servers=\" STRING \", client.id=\" STRING \", cluster.id=\" STRING \", queue.group.name=\" STRING \", durable.name=\" STRING \", subscription.sequence=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic destination Subject name which NATS Source should subscribe to. STRING No No bootstrap.servers The NATS based url of the NATS server. nats://localhost:4222 STRING Yes No client.id The identifier of the client subscribing/connecting to the NATS broker. None STRING Yes No cluster.id The identifier of the NATS server/cluster. test-cluster STRING Yes No queue.group.name This can be used when there is a requirement to share the load of a NATS subject. Clients belongs to the same queue group share the subscription load. None STRING Yes No durable.name This can be used to subscribe to a subject from the last acknowledged message when a client or connection failure happens. The client can be uniquely identified using the tuple (client.id, durable.name). None STRING Yes No subscription.sequence This can be used to subscribe to a subject from a given number of message sequence. All the messages from the given point of sequence number will be passed to the client. If not provided then the either the persisted value or 0 will be used. None STRING Yes No Examples EXAMPLE 1 @source(type='nats', @map(type='text'), destination='SP_NATS_INPUT_TEST', bootstrap.servers='nats://localhost:4222',client.id='nats_client',server.id='test-cluster',queue.group.name = 'group_nats',durable.name = 'nats-durable',subscription.sequence = '100') define stream inputStream (name string, age int, country string); This example shows how to subscribe to a NATS subject with all supporting configurations.With the following configuration the source identified as 'nats-client' will subscribes to a subject named as 'SP_NATS_INPUT_TEST' which resides in a nats instance with a cluster id of 'test-cluster', running in localhost and listening to the port 4222 for client connection. This subscription will receive all the messages from 100 th in the subject. EXAMPLE 2 @source(type='nats', @map(type='text'), destination='SP_NATS_INPUT_TEST', ) define stream inputStream (name string, age int, country string); This example shows how to subscribe to a NATS subject with mandatory configurations.With the following configuration the source identified with an auto generated client id will subscribes to a subject named as 'SP_NATS_INTPUT_TEST' which resides in a nats instance with a cluster id of 'test-cluster', running in localhost and listening to the port 4222 for client connection. This will receive all available messages in the subject prometheus (Source) This source consumes Prometheus metrics that are exported from a specified URL as Siddhi events by sending HTTP requests to the URL. Based on the source configuration, it analyzes metrics from the text response and sends them as Siddhi events through key-value mapping.The user can retrieve metrics of the 'including', 'counter', 'gauge', 'histogram', and 'summary' types. The source retrieves the metrics from a text response of the target. Therefore, it is you need to use 'string' as the attribute type for the attributes that correspond with the Prometheus metric labels. Further, the Prometheus metric value is passed through the event as 'value'. This requires you to include an attribute named 'value' in the stream definition. The supported types for the 'value' attribute are 'INT', 'LONG', 'FLOAT', and 'DOUBLE'. Origin: siddhi-io-prometheus:2.1.0 Syntax @source(type=\"prometheus\", target.url=\" STRING \", scrape.interval=\" INT \", scrape.timeout=\" INT \", scheme=\" STRING \", metric.name=\" STRING \", metric.type=\" STRING \", username=\" STRING \", password=\" STRING \", client.truststore.file=\" STRING \", client.truststore.password=\" STRING \", headers=\" STRING \", job=\" STRING \", instance=\" STRING \", grouping.key=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic target.url This property specifies the target URL to which the Prometheus metrics are exported in the 'TEXT' format. STRING No No scrape.interval This property specifies the time interval in seconds within which the source should send an HTTP request to the specified target URL. 60 INT Yes No scrape.timeout This property is the time duration in seconds for a scrape request to get timed-out if the server at the URL does not respond. 10 INT Yes No scheme This property specifies the scheme of the target URL. The supported schemes are 'HTTP' and 'HTTPS'. HTTP STRING Yes No metric.name This property specifies the name of the metrics that are to be fetched. The metric name must match the regex format, i.e., '[a-zA-Z_:][a-zA-Z0-9_:]* '. Stream name STRING Yes No metric.type This property specifies the type of the Prometheus metric that is required to be fetched. The supported metric types are 'counter', 'gauge',\" 'histogram', and 'summary'. STRING No No username This property specifies the username that needs to be added in the authorization header of the HTTP request if basic authentication is enabled at the target. It is required to specify both the username and the password to enable basic authentication. If you do not provide a value for one or both of these parameters, an error is logged in the console. STRING Yes No password This property specifies the password that needs to be added in the authorization header of the HTTP request if basic authentication is enabled at the target. It is required to specify both the username and the password to enable basic authentication. If you do not provide a value for one or both of these parameters, an error is logged in the console. STRING Yes No client.truststore.file The file path to the location of the truststore to which the client needs to send HTTPS requests via the 'HTTPS' protocol. STRING Yes No client.truststore.password The password for the client-truststore. This is required to send HTTPS requests. A custom password can be specified if required. STRING Yes No headers Headers that need to be included as HTTP request headers in the request. The format of the supported input is as follows, \"'header1:value1','header2:value2'\" STRING Yes No job This property defines the job name of the exported Prometheus metrics that needs to be fetched. STRING Yes No instance This property defines the instance of the exported Prometheus metrics that needs to be fetched. STRING Yes No grouping.key This parameter specifies the grouping key of the required metrics in key-value pairs. The grouping key is used if the metrics are exported by Prometheus 'pushGateway' in order to distinguish those metrics from already existing metrics. The expected format of the grouping key is as follows: \"'key1:value1','key2:value2'\" STRING Yes No System Parameters Name Description Default Value Possible Parameters scrapeInterval The default time interval in seconds for the Prometheus source to send HTTP requests to the target URL. 60 Any integer value scrapeTimeout The default time duration (in seconds) for an HTTP request to time-out if the server at the URL does not respond. 10 Any integer value scheme The scheme of the target for the Prometheus source to send HTTP requests. The supported schemes are 'HTTP' and 'HTTPS'. HTTP HTTP or HTTPS username The username that needs to be added in the authorization header of the HTTP request if basic authentication is enabled at the target. It is required to specify both the username and password to enable basic authentication. If you do not specify a value for one or both of these parameters, an error is logged in the console. Any string password The password that needs to be added in the authorization header of the HTTP request if basic authentication is enabled at the target. It is required to specify both the username and password to enable basic authentication. If you do not specify a value for one or both of these parameters, an error is logged in the console. Any string trustStoreFile The default file path to the location of truststore that the client needs to access in order to send HTTPS requests through 'HTTPS' protocol. ${carbon.home}/resources/security/client-truststore.jks Any valid path for the truststore file trustStorePassword The default password for the client-truststore that the client needs to access in order to send HTTPS requests through 'HTTPS' protocol. wso2carbon Any string headers The headers that need to be included as HTTP request headers in the scrape request. The format of the supported input is as follows, \"'header1:value1','header2:value2'\" Any valid http headers job The default job name of the exported Prometheus metrics that needs to be fetched. Any valid job name instance The default instance of the exported Prometheus metrics that needs to be fetched. Any valid instance name groupingKey The default grouping key of the required Prometheus metrics in key-value pairs. The grouping key is used if the metrics are exported by the Prometheus pushGateway in order to distinguish these metrics from already existing metrics. The expected format of the grouping key is as follows: \"'key1:value1','key2:value2'\" Any valid grouping key pairs Examples EXAMPLE 1 @source(type= 'prometheus', target.url= 'http://localhost:9080/metrics', metric.type= 'counter', metric.name= 'sweet_production_counter', @map(type= 'keyvalue')) define stream FooStream1(metric_name string, metric_type string, help string, subtype string, name string, quantity string, value double); In this example, the Prometheus source sends an HTTP request to the 'target.url' and analyzes the response. From the analyzed response, the source retrieves the Prometheus counter metrics with the 'sweet_production_counter' nameand converts the filtered metrics into Siddhi events using the key-value mapper. The generated maps have keys and values as follows: metric_name - sweet_production_counter metric_type - counter help - help_string_of_metric subtype - null name - value_of_label_name quantity - value_of_label_quantity value - value_of_metric EXAMPLE 2 @source(type= 'prometheus', target.url= 'http://localhost:9080/metrics', metric.type= 'summary', metric.name= 'sweet_production_summary', @map(type= 'keyvalue')) define stream FooStream2(metric_name string, metric_type string, help string, subtype string, name string, quantity string, quantile string, value double); In this example, the Prometheus source sends an HTTP request to the 'target.url' and analyzes the response. From the analysed response, the source retrieves the Prometheus summary metrics with the 'sweet_production_summary' nameand converts the filtered metrics into Siddhi events using the key-value mapper. The generated maps have keys and values as follows: metric_name - sweet_production_summary metric_type - summary help - help_string_of_metric subtype - 'sum'/'count'/'null' name - value_of_label_name quantity - value_of_label_quantity quantile - value of the quantile value - value_of_metric EXAMPLE 3 @source(type= 'prometheus', target.url= 'http://localhost:9080/metrics', metric.type= 'histogram', metric.name= 'sweet_production_histogram', @map(type= 'keyvalue')) define stream FooStream3(metric_name string, metric_type string, help string, subtype string, name string, quantity string, le string, value double); In this example, the prometheus source sends an HTTP request to the 'target.url' and analyzes the response. From the analyzed response, the source retrieves the Prometheus histogram metrics with the 'sweet_production_histogram' name and converts the filtered metrics into Siddhi events using the key-value mapper. The generated maps have keys and values as follows, metric_name - sweet_production_histogram metric_type - histogram help - help_string_of_metric subtype - 'sum'/'count'/'bucket' name - value_of_label_name quantity - value_of_label_quantity le - value of the bucket value - value_of_metric rabbitmq (Source) The rabbitmq source receives the events from the rabbitmq broker via the AMQP protocol. Origin: siddhi-io-rabbitmq:3.0.2 Syntax @source(type=\"rabbitmq\", uri=\" STRING \", heartbeat=\" INT \", exchange.name=\" STRING \", exchange.type=\" STRING \", exchange.durable.enabled=\" BOOL \", exchange.autodelete.enabled=\" BOOL \", routing.key=\" STRING \", headers=\" STRING \", queue.name=\" STRING \", queue.durable.enabled=\" BOOL \", queue.exclusive.enabled=\" BOOL \", queue.autodelete.enabled=\" BOOL \", tls.enabled=\" BOOL \", tls.truststore.path=\" STRING \", tls.truststore.password=\" STRING \", tls.truststore.type=\" STRING \", tls.version=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic uri The URI that is used to connect to an AMQP server. If no URI is specified,an error is logged in the CLI.e.g., amqp://guest:guest , amqp://guest:guest@localhost:5672 STRING No No heartbeat The period of time (in seconds) after which the peer TCP connection should be considered unreachable (down) by RabbitMQ and client libraries. 60 INT Yes No exchange.name The name of the exchange that decides what to do with a message it receives.If the exchange.name already exists in the RabbitMQ server, then the system uses that exchange.name instead of redeclaring. STRING No No exchange.type The type of the exchange name. The exchange types available are direct , fanout , topic and headers . For a detailed description of each type, see RabbitMQ - AMQP Concepts . direct STRING Yes No exchange.durable.enabled If this is set to true , the exchange remains declared even if the broker restarts. false BOOL Yes No exchange.autodelete.enabled If this is set to true , the exchange is automatically deleted when it is not used anymore. false BOOL Yes No routing.key The key based on which the exchange determines how to route the message to queues. The routing key is like an address for the message. The routing.key must be initialized when the value for the exchange.type parameter is direct or topic . empty STRING Yes No headers The headers of the message. The attributes used for routing are taken from the this paremeter. A message is considered matching if the value of the header equals the value specified upon binding. null STRING Yes No queue.name A queue is a buffer that stores messages. If the queue name already exists in the RabbitMQ server, then the system usees that queue name instead of redeclaring it. If no value is specified for this parameter, the system uses the unique queue name that is automatically generated by the RabbitMQ server. system generated queue name STRING Yes No queue.durable.enabled If this parameter is set to true , the queue remains declared even if the broker restarts false BOOL Yes No queue.exclusive.enabled If this parameter is set to true , the queue is exclusive for the current connection. If it is set to false , it is also consumable by other connections. false BOOL Yes No queue.autodelete.enabled If this parameter is set to true , the queue is automatically deleted when it is not used anymore. false BOOL Yes No tls.enabled This parameter specifies whether an encrypted communication channel should be established or not. When this parameter is set to true , the tls.truststore.path and tls.truststore.password parameters are initialized. false BOOL Yes No tls.truststore.path The file path to the location of the truststore of the client that receives the RabbitMQ events via the AMQP protocol. A custom client-truststore can be specified if required. If a custom truststore is not specified, then the system uses the default client-trustore in the {carbon.home}/resources/security /code directory. /p /td td style=\"vertical-align: top\" {carbon.home}/resources/security</code> directory.</p></td> <td style=\"vertical-align: top\"> /resources/security/client-truststore.jks STRING Yes No tls.truststore.password The password for the client-truststore. A custom password can be specified if required. If no custom password is specified, then the system uses wso2carbon as the default password. wso2carbon STRING Yes No tls.truststore.type The type of the truststore. JKS STRING Yes No tls.version The version of the tls/ssl. SSL STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @source(type ='rabbitmq', uri = 'amqp://guest:guest@localhost:5672', exchange.name = 'direct', routing.key= 'direct', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query receives events from the direct exchange with the direct exchange type, and the directTest routing key. tcp (Source) A Siddhi application can be configured to receive events via the TCP transport by adding the @Source(type = 'tcp') annotation at the top of an event stream definition. When this is defined the associated stream will receive events from the TCP transport on the host and port defined in the system. Origin: siddhi-io-tcp:3.0.4 Syntax @source(type=\"tcp\", context=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic context The URL 'context' that should be used to receive the events. / STRING Yes No System Parameters Name Description Default Value Possible Parameters host Tcp server host. 0.0.0.0 Any valid host or IP port Tcp server port. 9892 Any integer representing valid port receiver.threads Number of threads to receive connections. 10 Any positive integer worker.threads Number of threads to serve events. 10 Any positive integer tcp.no.delay This is to specify whether to disable Nagle algorithm during message passing. If tcp.no.delay = 'true', the execution of Nagle algorithm will be disabled in the underlying TCP logic. Hence there will be no delay between two successive writes to the TCP connection. Else there can be a constant ack delay. true true false keep.alive This property defines whether the server should be kept alive when there are no connections available. true true false Examples EXAMPLE 1 @Source(type = 'tcp', context='abc', @map(type='binary')) define stream Foo (attribute1 string, attribute2 int ); Under this configuration, events are received via the TCP transport on default host,port, abc context, and they are passed to Foo stream for processing. Sourcemapper avro (Source Mapper) This extension is an Avro to Event input mapper. Transports that accept Avro messages can utilize this extension to convert the incoming Avro messages to Siddhi events. The Avro schema to be used for creating Avro messages can be specified as a parameter in the stream definition. If no Avro schema is specified, a flat avro schema of the 'record' type is generated with the stream attributes as schema fields. The generated/specified Avro schema is used to convert Avro messages to Siddhi events. Origin: siddhi-map-avro:2.0.5 Syntax @source(..., @map(type=\"avro\", schema.def=\" STRING \", schema.registry=\" STRING \", schema.id=\" STRING \", fail.on.missing.attribute=\" BOOL \") QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic schema.def This specifies the schema of the Avro message. The full schema used to create the Avro message needs to be specified as a quoted JSON string. STRING No No schema.registry This specifies the URL of the schema registry. STRING No No schema.id This specifies the ID of the Avro schema. This ID is the global ID that is returned from the schema registry when posting the schema to the registry. The schema is retrieved from the schema registry via the specified ID. STRING No No fail.on.missing.attribute If this parameter is set to 'true', a JSON execution failing or returning a null value results in that message being dropped by the system. If this parameter is set to 'false', a JSON execution failing or returning a null value results in the system being prompted to send the event with a null value to Siddhi so that the user can handle it as required (i.e., by assigning a default value. true BOOL Yes No Examples EXAMPLE 1 @source(type='inMemory', topic='user', @map(type='avro', schema .def = \"\"\"{\"type\":\"record\",\"name\":\"userInfo\",\"namespace\":\"user.example\",\"fields\":[{\"name\":\"name\",\"type\":\"string\"}, {\"name\":\"age\",\"type\":\"int\"}]}\"\"\")) define stream UserStream (name string, age int ); The above Siddhi query performs a default Avro input mapping. The input Avro message that contains user information is converted to a Siddhi event. The expected input is a byte array or ByteBuffer. EXAMPLE 2 @source(type='inMemory', topic='user', @map(type='avro', schema .def = \"\"\"{\"type\":\"record\",\"name\":\"userInfo\",\"namespace\":\"avro.userInfo\",\"fields\":[{\"name\":\"username\",\"type\":\"string\"}, {\"name\":\"age\",\"type\":\"int\"}]}\"\"\",@attributes(name=\"username\",age=\"age\"))) define stream userStream (name string, age int ); The above Siddhi query performs a custom Avro input mapping. The input Avro message that contains user information is converted to a Siddhi event. The expected input is a byte array or ByteBuffer. EXAMPLE 3 @source(type='inMemory', topic='user', @map(type='avro',schema.registry='http://192.168.2.5:9090', schema.id='1',@attributes(name=\"username\",age=\"age\"))) define stream UserStream (name string, age int ); The above Siddhi query performs a custom Avro input mapping. The input Avro message that contains user information is converted to a Siddhi event via the schema retrieved from the given schema registry(localhost:8081). The expected input is a byte array or ByteBuffer. binary (Source Mapper) This extension is a binary input mapper that converts events received in binary format to Siddhi events before they are processed. Origin: siddhi-map-binary:2.0.4 Syntax @source(..., @map(type=\"binary\") Examples EXAMPLE 1 @source(type='inMemory', topic='WSO2', @map(type='binary'))define stream FooStream (symbol string, price float, volume long); This query performs a mapping to convert an event of the binary format to a Siddhi event. csv (Source Mapper) This extension is used to convert CSV message to Siddhi event input mapper. You can either receive pre-defined CSV message where event conversion takes place without extra configurations,or receive custom CSV message where a custom place order to map from custom CSV message. Origin: siddhi-map-csv:2.0.3 Syntax @source(..., @map(type=\"csv\", delimiter=\" STRING \", header.present=\" BOOL \", fail.on.unknown.attribute=\" BOOL \", event.grouping.enabled=\" BOOL \") QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic delimiter When converting a CSV format message to Siddhi event, this parameter indicatesinput CSV message's data should be split by this parameter , STRING Yes No header.present When converting a CSV format message to Siddhi event, this parameter indicates whether CSV message has header or not. This can either have value true or false.If it's set to false then it indicates that CSV message has't header. false BOOL Yes No fail.on.unknown.attribute This parameter specifies how unknown attributes should be handled. If it's set to true and one or more attributes don't havevalues, then SP will drop that message. If this parameter is set to false , the Stream Processor adds the required attribute's values to such events with a null value and the event is converted to a Siddhi event. true BOOL Yes No event.grouping.enabled This parameter specifies whether event grouping is enabled or not. To receive a group of events together and generate multiple events, this parameter must be set to true . false BOOL Yes No Examples EXAMPLE 1 @source(type='inMemory', topic='stock', @map(type='csv')) define stream FooStream (symbol string, price float, volume int); Above configuration will do a default CSV input mapping. Expected input will look like below: WSO2 ,55.6 , 100OR \"WSO2,No10,Palam Groove Rd,Col-03\" ,55.6 , 100If header.present is true and delimiter is \"-\", then the input is as follows: symbol-price-volumeWSO2-55.6-100 EXAMPLE 2 @source(type='inMemory', topic='stock', @map(type='csv',header='true', @attributes(symbol = \"2\", price = \"0\", volume = \"1\"))) define stream FooStream (symbol string, price float, volume long); Above configuration will perform a custom CSV mapping. Here, user can add place order of each attribute in the @attribute. The place order indicates where the attribute name's value has appeared in the input.Expected input will look like below: 55.6,100,WSO2 OR55.6,100,\"WSO2,No10,Palm Groove Rd,Col-03\" If header is true and delimiter is \"-\", then the output is as follows: price-volume-symbol 55.6-100-WSO2 If group events is enabled then input should be as follows: price-volume-symbol 55.6-100-WSO2System.lineSeparator() 55.6-100-IBMSystem.lineSeparator() 55.6-100-IFSSystem.lineSeparator() json (Source Mapper) This extension is a JSON-to-Event input mapper. Transports that accept JSON messages can utilize this extension to convert an incoming JSON message into a Siddhi event. Users can either send a pre-defined JSON format, where event conversion happens without any configurations, or use the JSON path to map from a custom JSON message. In default mapping, the JSON string of the event can be enclosed by the element \"event\", though optional. Origin: siddhi-map-json:5.0.4 Syntax @source(..., @map(type=\"json\", enclosing.element=\" STRING \", fail.on.missing.attribute=\" BOOL \") QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic enclosing.element This is used to specify the enclosing element when sending multiple events in the same JSON message. Mapper treats the child elements of a given enclosing element as events and executes the JSON path expressions on these child elements. If the enclosing.element is not provided then the multiple-event scenario is disregarded and the JSON path is evaluated based on the root element. $ STRING Yes No fail.on.missing.attribute This parameter allows users to handle unknown attributes.The value of this can either be true or false. By default it is true. If a JSON execution fails or returns null, mapper drops that message. However, setting this property to false prompts mapper to send an event with a null value to Siddhi, where users can handle it as required, ie., assign a default value.) true BOOL Yes No Examples EXAMPLE 1 @source(type='inMemory', topic='stock', @map(type='json')) define stream FooStream (symbol string, price float, volume long); This configuration performs a default JSON input mapping. For a single event, the input is required to be in one of the following formats: { \"event\":{ \"symbol\":\"WSO2\", \"price\":55.6, \"volume\":100 } } or { \"symbol\":\"WSO2\", \"price\":55.6, \"volume\":100 } EXAMPLE 2 @source(type='inMemory', topic='stock', @map(type='json')) define stream FooStream (symbol string, price float, volume long); This configuration performs a default JSON input mapping. For multiple events, the input is required to be in one of the following formats: [ {\"event\":{\"symbol\":\"WSO2\",\"price\":55.6,\"volume\":100}}, {\"event\":{\"symbol\":\"WSO2\",\"price\":56.6,\"volume\":99}}, {\"event\":{\"symbol\":\"WSO2\",\"price\":57.6,\"volume\":80}} ] or [ {\"symbol\":\"WSO2\",\"price\":55.6,\"volume\":100}, {\"symbol\":\"WSO2\",\"price\":56.6,\"volume\":99}, {\"symbol\":\"WSO2\",\"price\":57.6,\"volume\":80} ] EXAMPLE 3 @source(type='inMemory', topic='stock', @map(type='json', enclosing.element=\"$.portfolio\", @attributes(symbol = \"company.symbol\", price = \"price\", volume = \"volume\"))) This configuration performs a custom JSON mapping. For a single event, the expected input is similar to the one shown below: { \"portfolio\":{ \"stock\":{ \"volume\":100, \"company\":{ \"symbol\":\"WSO2\" }, \"price\":55.6 } } } EXAMPLE 4 @source(type='inMemory', topic='stock', @map(type='json', enclosing.element=\"$.portfolio\", @attributes(symbol = \"stock.company.symbol\", price = \"stock.price\", volume = \"stock.volume\"))) define stream FooStream (symbol string, price float, volume long); The configuration performs a custom JSON mapping. For multiple events, expected input looks as follows. .{\"portfolio\": [ {\"stock\":{\"volume\":100,\"company\":{\"symbol\":\"wso2\"},\"price\":56.6}}, {\"stock\":{\"volume\":200,\"company\":{\"symbol\":\"wso2\"},\"price\":57.6}} ] } keyvalue (Source Mapper) Key-Value Map to Event input mapper extension allows transports that accept events as key value maps to convert those events to Siddhi events. You can either receive pre-defined keys where conversion takes place without extra configurations, or use custom keys to map from the message. Origin: siddhi-map-keyvalue:2.0.4 Syntax @source(..., @map(type=\"keyvalue\", fail.on.missing.attribute=\" BOOL \") QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic fail.on.missing.attribute If this parameter is set to true , if an event arrives without a matching key for a specific attribute in the connected stream, it is dropped and not processed by the Stream Processor. If this parameter is set to false the Stream Processor adds the required key to such events with a null value, and the event is converted to a Siddhi event so that you could handle them as required before they are further processed. true BOOL Yes No Examples EXAMPLE 1 @source(type='inMemory', topic='stock', @map(type='keyvalue')) define stream FooStream (symbol string, price float, volume long); This query performs a default key value input mapping. The expected input is a map similar to the following: symbol: 'WSO2' price: 55.6f volume: 100 EXAMPLE 2 @source(type='inMemory', topic='stock', @map(type='keyvalue', fail.on.missing.attribute='true', @attributes(symbol = 's', price = 'p', volume = 'v')))define stream FooStream (symbol string, price float, volume long); This query performs a custom key value input mapping. The matching keys for the symbol , price and volume attributes are be s , p , and v respectively. The expected input is a map similar to the following: s: 'WSO2' p: 55.6 v: 100 passThrough (Source Mapper) Pass-through mapper passed events (Event[]) through without any mapping or modifications. Origin: siddhi-core:5.1.7 Syntax @source(..., @map(type=\"passThrough\") Examples EXAMPLE 1 @source(type='tcp', @map(type='passThrough')) define stream BarStream (symbol string, price float, volume long); In this example BarStream uses passThrough inputmapper which passes the received Siddhi event directly without any transformation into source. protobuf (Source Mapper) This input mapper allows you to convert protobuf messages into Events. To work with this input mapper you have to add auto-generated protobuf classes to the project classpath. When you use this input mapper, you can either define stream attributes as the same names as the protobuf message attributes or you can use custom mapping to map stream definition attributes with the protobuf attributes..Please find the sample proto definition here Origin: siddhi-map-protobuf:1.0.1 Syntax @source(..., @map(type=\"protobuf\", class=\" STRING \") QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic class This specifies the class name of the protobuf message class, If sink type is grpc then it's not necessary to provide this field. - STRING Yes No Examples EXAMPLE 1 source(type='grpc', receiver.url = 'grpc://localhost: 2000/org.wso2.grpc.test.MyService/process', @map(type='protobuf')) define stream FooStream (stringValue string, intValue int,longValue long,booleanValue bool,floatValue float,doubleValue double); Above definition will convert the protobuf messages that are received to this source into siddhi events. EXAMPLE 2 source(type='grpc', receiver.url = 'grpc://localhost: 2000/org.wso2.grpc.test.MyService/process', @map(type='protobuf', @attributes(a = 'stringValue', b = 'intValue', c = 'longValue',d = 'booleanValue',' e = floatValue', f ='doubleValue'))) define stream FooStream (a string ,c long,b int, d bool,e float,f double); Above definition will convert the protobuf messages that are received to this source into siddhi events. since there's a mapping available for the stream, protobuf message object will be map like this, -'stringValue' of the protobuf message will be assign to the 'a' attribute of the stream - 'intValue' of the protobuf message will be assign to the 'b' attribute of the stream - 'longValue' of the protobuf message will be assign to the 'c' attribute of the stream - 'booleanValue' of the protobuf message will be assign to the 'd' attribute of the stream - 'floatValue' of the protobuf message will be assign to the 'e' attribute of the stream - 'doubleValue' of the protobuf message will be assign to the 'f' attribute of the stream EXAMPLE 3 source(type='grpc', receiver.url = 'grpc://localhost: 2000/org.wso2.grpc.test.MyService/testMap', @map(type='protobuf')) define stream FooStream (stringValue string ,intValue int,map object); Above definition will convert the protobuf messages that are received to this source into siddhi events. since there's an object type attribute available in the stream (map object), mapper will assume that object is an instance of 'java.util.Map' class. otherwise mapper will throws an exception EXAMPLE 4 @source(type='inMemory', topic='test01', @map(type='protobuf', class='org.wso2.grpc.test.Request')) define stream FooStream (stringValue string, intValue int,longValue long,booleanValue bool,floatValue float,doubleValue double); The above definition will convert the 'org.wso2.grpc.test.Request' type protobuf messages into siddhi events. If we did not provide the 'receiver.url' in the stream definition we have to provide the protobuf class name in the 'class' parameter inside @map. text (Source Mapper) This extension is a text to Siddhi event input mapper. Transports that accept text messages can utilize this extension to convert the incoming text message to Siddhi event. Users can either use a pre-defined text format where event conversion happens without any additional configurations, or specify a regex to map a text message using custom configurations. Origin: siddhi-map-text:2.0.4 Syntax @source(..., @map(type=\"text\", regex.groupid=\" STRING \", fail.on.missing.attribute=\" BOOL \", event.grouping.enabled=\" BOOL \", delimiter=\" STRING \", new.line.character=\" STRING \") QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic regex.groupid This parameter specifies a regular expression group. The groupid can be any capital letter (e.g., regex.A,regex.B .. etc). You can specify any number of regular expression groups. In the attribute annotation, you need to map all attributes to the regular expression group with the matching group index. If you need to to enable custom mapping, it is required to specifythe matching group for each and every attribute. STRING No No fail.on.missing.attribute This parameter specifies how unknown attributes should be handled. If it is set to true a message is dropped if its execution fails, or if one or more attributes do not have values. If this parameter is set to false , null values are assigned to attributes with missing values, and messages with such attributes are not dropped. true BOOL Yes No event.grouping.enabled This parameter specifies whether event grouping is enabled or not. To receive a group of events together and generate multiple events, this parameter must be set to true . false BOOL Yes No delimiter This parameter specifies how events must be separated when multiple events are received. This must be whole line and not a single character. ~ ~ ~ ~ STRING Yes No new.line.character This attribute indicates the new line character of the event that is expected to be received. This is used mostly when communication between 2 types of operating systems is expected. For example, Linux uses \\n as the end of line character whereas windows uses \\r\\n . \\n STRING Yes No Examples EXAMPLE 1 @source(type='inMemory', topic='stock', @map(type='text')) define stream FooStream (symbol string, price float, volume long); This query performs a default text input mapping. The expected input is as follows: symbol:\"WSO2\", price:55.6, volume:100 OR symbol:'WSO2', price:55.6, volume:100 If group events is enabled then input should be as follows: symbol:\"WSO2\", price:55.6, volume:100 ~ ~ ~ ~ symbol:\"WSO2\", price:55.6, volume:100 EXAMPLE 2 @source(type='inMemory', topic='stock', @map(type='text', fail.on.missing.attribute = 'true', regex.A='(\\w+)\\s([-0-9]+)',regex.B='volume\\s([-0-9]+)', @attributes(symbol = 'A[1]',price = 'A[2]',volume = 'B'))) define stream FooStream (symbol string, price float, volume long); This query performs a custom text mapping. The expected input is as follows: wos2 550 volume 100 If group events is enabled then input should be as follows: wos2 550 volume 100 ~ ~ ~ ~ wos2 550 volume 100 ~ ~ ~ ~ wos2 550 volume 100 xml (Source Mapper) This mapper converts XML input to Siddhi event. Transports which accepts XML messages can utilize this extension to convert the incoming XML message to Siddhi event. Users can either send a pre-defined XML format where event conversion will happen without any configs or can use xpath to map from a custom XML message. Origin: siddhi-map-xml:5.0.3 Syntax @source(..., @map(type=\"xml\", namespaces=\" STRING \", enclosing.element=\" STRING \", fail.on.missing.attribute=\" BOOL \") QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic namespaces Used to provide namespaces used in the incoming XML message beforehand to configure xpath expressions. User can provide a comma separated list. If these are not provided xpath evaluations will fail None STRING Yes No enclosing.element Used to specify the enclosing element in case of sending multiple events in same XML message. WSO2 DAS will treat the child element of given enclosing element as events and execute xpath expressions on child elements. If enclosing.element is not provided multiple event scenario is disregarded and xpaths will be evaluated with respect to root element. Root element STRING Yes No fail.on.missing.attribute This can either have value true or false. By default it will be true. This attribute allows user to handle unknown attributes. By default if an xpath execution fails or returns null DAS will drop that message. However setting this property to false will prompt DAS to send and event with null value to Siddhi where user can handle it accordingly(ie. Assign a default value) True BOOL Yes No Examples EXAMPLE 1 @source(type='inMemory', topic='stock', @map(type='xml')) define stream FooStream (symbol string, price float, volume long); Above configuration will do a default XML input mapping. Expected input will look like below. events event symbol WSO2 /symbol price 55.6 /price volume 100 /volume /event /events EXAMPLE 2 @source(type='inMemory', topic='stock', @map(type='xml', namespaces = \"dt=urn:schemas-microsoft-com:datatypes\", enclosing.element=\"//portfolio\", @attributes(symbol = \"company/symbol\", price = \"price\", volume = \"volume\"))) define stream FooStream (symbol string, price float, volume long); Above configuration will perform a custom XML mapping. In the custom mapping user can add xpath expressions representing each event attribute using @attribute annotation. Expected input will look like below. portfolio xmlns:dt=\"urn:schemas-microsoft-com:datatypes\" stock exchange=\"nasdaq\" volume 100 /volume company symbol WSO2 /symbol /company price dt:type=\"number\" 55.6 /price /stock /portfolio Store mongodb (Store) Using this extension a MongoDB Event Table can be configured to persist events in a MongoDB of user's choice. Origin: siddhi-store-mongodb:2.0.3 Syntax @Store(type=\"mongodb\", mongodb.uri=\" STRING \", collection.name=\" STRING \", secure.connection=\" STRING \", trust.store=\" STRING \", trust.store.password=\" STRING \", key.store=\" STRING \", key.store.password=\" STRING \") @PrimaryKey(\"PRIMARY_KEY\") @Index(\"INDEX\") QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic mongodb.uri The MongoDB URI for the MongoDB data store. The uri must be of the format mongodb://[username:password@]host1[:port1][,hostN[:portN]][/[database][?options]] The options specified in the uri will override any connection options specified in the deployment yaml file. Note: The user should have read permissions to the admindb as well as read/write permissions to the database accessed. STRING No No collection.name The name of the collection in the store this Event Table should be persisted as. Name of the siddhi event table. STRING Yes No secure.connection Describes enabling the SSL for the mongodb connection false STRING Yes No trust.store File path to the trust store. {carbon.home}/resources/security/client-truststore.jks /td td style=\"vertical-align: top\" STRING /td td style=\"vertical-align: top\" Yes /td td style=\"vertical-align: top\" No /td /tr tr td style=\"vertical-align: top\" trust.store.password /td td style=\"vertical-align: top; word-wrap: break-word\" p style=\"word-wrap: break-word;margin: 0;\" Password to access the trust store /p /td td style=\"vertical-align: top\" wso2carbon /td td style=\"vertical-align: top\" STRING /td td style=\"vertical-align: top\" Yes /td td style=\"vertical-align: top\" No /td /tr tr td style=\"vertical-align: top\" key.store /td td style=\"vertical-align: top; word-wrap: break-word\" p style=\"word-wrap: break-word;margin: 0;\" File path to the keystore. /p /td td style=\"vertical-align: top\" {carbon.home}/resources/security/client-truststore.jks</td> <td style=\"vertical-align: top\">STRING</td> <td style=\"vertical-align: top\">Yes</td> <td style=\"vertical-align: top\">No</td> </tr> <tr> <td style=\"vertical-align: top\">trust.store.password</td> <td style=\"vertical-align: top; word-wrap: break-word\"><p style=\"word-wrap: break-word;margin: 0;\">Password to access the trust store</p></td> <td style=\"vertical-align: top\">wso2carbon</td> <td style=\"vertical-align: top\">STRING</td> <td style=\"vertical-align: top\">Yes</td> <td style=\"vertical-align: top\">No</td> </tr> <tr> <td style=\"vertical-align: top\">key.store</td> <td style=\"vertical-align: top; word-wrap: break-word\"><p style=\"word-wrap: break-word;margin: 0;\">File path to the keystore.</p></td> <td style=\"vertical-align: top\"> /resources/security/client-truststore.jks STRING Yes No key.store.password Password to access the keystore wso2carbon STRING Yes No System Parameters Name Description Default Value Possible Parameters applicationName Sets the logical name of the application using this MongoClient. The application name may be used by the client to identify the application to the server, for use in server logs, slow query logs, and profile collection. null the logical name of the application using this MongoClient. The UTF-8 encoding may not exceed 128 bytes. cursorFinalizerEnabled Sets whether cursor finalizers are enabled. true true false requiredReplicaSetName The name of the replica set null the logical name of the replica set sslEnabled Sets whether to initiate connection with TSL/SSL enabled. true: Initiate the connection with TLS/SSL. false: Initiate the connection without TLS/SSL. false true false trustStore File path to the trust store. {carbon.home}/resources/security/client-truststore.jks /td td style=\"vertical-align: top\" Any valid file path. /td /tr tr td style=\"vertical-align: top\" trustStorePassword /td td style=\"vertical-align: top;\" p style=\"word-wrap: break-word;margin: 0;\" Password to access the trust store /p /td td style=\"vertical-align: top\" wso2carbon /td td style=\"vertical-align: top\" Any valid password. /td /tr tr td style=\"vertical-align: top\" keyStore /td td style=\"vertical-align: top;\" p style=\"word-wrap: break-word;margin: 0;\" File path to the keystore. /p /td td style=\"vertical-align: top\" {carbon.home}/resources/security/client-truststore.jks</td> <td style=\"vertical-align: top\">Any valid file path.</td> </tr> <tr> <td style=\"vertical-align: top\">trustStorePassword</td> <td style=\"vertical-align: top;\"><p style=\"word-wrap: break-word;margin: 0;\">Password to access the trust store</p></td> <td style=\"vertical-align: top\">wso2carbon</td> <td style=\"vertical-align: top\">Any valid password.</td> </tr> <tr> <td style=\"vertical-align: top\">keyStore</td> <td style=\"vertical-align: top;\"><p style=\"word-wrap: break-word;margin: 0;\">File path to the keystore.</p></td> <td style=\"vertical-align: top\"> /resources/security/client-truststore.jks Any valid file path. keyStorePassword Password to access the keystore wso2carbon Any valid password. connectTimeout The time in milliseconds to attempt a connection before timing out. 10000 Any positive integer connectionsPerHost The maximum number of connections in the connection pool. 100 Any positive integer minConnectionsPerHost The minimum number of connections in the connection pool. 0 Any natural number maxConnectionIdleTime The maximum number of milliseconds that a connection can remain idle in the pool before being removed and closed. A zero value indicates no limit to the idle time. A pooled connection that has exceeded its idle time will be closed and replaced when necessary by a new connection. 0 Any positive integer maxWaitTime The maximum wait time in milliseconds that a thread may wait for a connection to become available. A value of 0 means that it will not wait. A negative value means to wait indefinitely 120000 Any integer threadsAllowedToBlockForConnectionMultiplier The maximum number of connections allowed per host for this MongoClient instance. Those connections will be kept in a pool when idle. Once the pool is exhausted, any operation requiring a connection will block waiting for an available connection. 100 Any natural number maxConnectionLifeTime The maximum life time of a pooled connection. A zero value indicates no limit to the life time. A pooled connection that has exceeded its life time will be closed and replaced when necessary by a new connection. 0 Any positive integer socketKeepAlive Sets whether to keep a connection alive through firewalls false true false socketTimeout The time in milliseconds to attempt a send or receive on a socket before the attempt times out. Default 0 means never to timeout. 0 Any natural integer writeConcern The write concern to use. acknowledged acknowledged w1 w2 w3 unacknowledged fsynced journaled replica_acknowledged normal safe majority fsync_safe journal_safe replicas_safe readConcern The level of isolation for the reads from replica sets. default local majority linearizable readPreference Specifies the replica set read preference for the connection. primary primary secondary secondarypreferred primarypreferred nearest localThreshold The size (in milliseconds) of the latency window for selecting among multiple suitable MongoDB instances. 15 Any natural number serverSelectionTimeout Specifies how long (in milliseconds) to block for server selection before throwing an exception. A value of 0 means that it will timeout immediately if no server is available. A negative value means to wait indefinitely. 30000 Any integer heartbeatSocketTimeout The socket timeout for connections used for the cluster heartbeat. A value of 0 means that it will timeout immediately if no cluster member is available. A negative value means to wait indefinitely. 20000 Any integer heartbeatConnectTimeout The connect timeout for connections used for the cluster heartbeat. A value of 0 means that it will timeout immediately if no cluster member is available. A negative value means to wait indefinitely. 20000 Any integer heartbeatFrequency Specify the interval (in milliseconds) between checks, counted from the end of the previous check until the beginning of the next one. 10000 Any positive integer minHeartbeatFrequency Sets the minimum heartbeat frequency. In the event that the driver has to frequently re-check a server's availability, it will wait at least this long since the previous check to avoid wasted effort. 500 Any positive integer Examples EXAMPLE 1 @Store(type=\"mongodb\",mongodb.uri=\"mongodb://admin:admin@localhost/Foo\") @PrimaryKey(\"symbol\") @Index(\"volume:1\", {background:true,unique:true}\") define table FooTable (symbol string, price float, volume long); This will create a collection called FooTable for the events to be saved with symbol as Primary Key(unique index at mongoDB level) and index for the field volume will be created in ascending order with the index option to create the index in the background. Note: @PrimaryKey: This specifies a list of comma-separated values to be treated as unique fields in the table. Each record in the table must have a unique combination of values for the fields specified here. @Index: This specifies the fields that must be indexed at the database level. You can specify multiple values as a come-separated list. A single value to be in the format, FieldName : SortOrder . The last element is optional through which a valid index options can be passed. SortOrder : 1 for Ascending -1 for Descending. Optional, with default value as 1. IndexOptions : Index Options must be defined inside curly brackets. Options must follow the standard mongodb index options format. https://docs.mongodb.com/manual/reference/method/db.collection.createIndex/ Example 1: @Index( 'symbol:1' , '{\"unique\":true}' ) Example 2: @Index( 'symbol' , '{\"unique\":true}' ) Example 3: @Index( 'symbol:1' , 'volume:-1' , '{\"unique\":true}' ) rdbms (Store) This extension assigns data sources and connection instructions to event tables. It also implements read-write operations on connected data sources. Origin: siddhi-store-rdbms:7.0.1 Syntax @Store(type=\"rdbms\", jdbc.url=\" STRING \", username=\" STRING \", password=\" STRING \", jdbc.driver.name=\" STRING \", pool.properties=\" STRING \", jndi.resource=\" STRING \", datasource=\" STRING \", table.name=\" STRING \", field.length=\" STRING \", table.check.query=\" STRING \") @PrimaryKey(\"PRIMARY_KEY\") @Index(\"INDEX\") QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic jdbc.url The JDBC URL via which the RDBMS data store is accessed. STRING No No username The username to be used to access the RDBMS data store. STRING No No password The password to be used to access the RDBMS data store. STRING No No jdbc.driver.name The driver class name for connecting the RDBMS data store. STRING No No pool.properties Any pool parameters for the database connection must be specified as key-value pairs. null STRING Yes No jndi.resource The name of the JNDI resource through which the connection is attempted. If this is found, the pool properties described above are not taken into account and the connection is attempted via JNDI lookup instead. null STRING Yes No datasource The name of the Carbon datasource that should be used for creating the connection with the database. If this is found, neither the pool properties nor the JNDI resource name described above are taken into account and the connection is attempted via Carbon datasources instead. Only works in Siddhi Distribution null STRING Yes No table.name The name with which the event table should be persisted in the store. If no name is specified via this parameter, the event table is persisted with the same name as the Siddhi table. The table name defined in the Siddhi App query. STRING Yes No field.length The number of characters that the values for fields of the 'STRING' type in the table definition must contain. Each required field must be provided as a comma-separated list of key-value pairs in the ' field.name : length ' format. If this is not specified, the default number of characters specific to the database type is considered. null STRING Yes No table.check.query This query will be used to check whether the table is exist in the given database. But the provided query should return an SQLException if the table does not exist in the database. Furthermore if the provided table is a database view, and it is not exists in the database a table from given name will be created in the database The tableCheckQuery which define in store rdbms configs STRING Yes No System Parameters Name Description Default Value Possible Parameters {{RDBMS-Name}}.maxVersion The latest version supported for {{RDBMS-Name}}. 0 N/A {{RDBMS-Name}}.minVersion The earliest version supported for {{RDBMS-Name}}. 0 N/A {{RDBMS-Name}}.tableCheckQuery The template query for the 'check table' operation in {{RDBMS-Name}}. H2 : CREATE TABLE {{TABLE_NAME}} ({{COLUMNS, PRIMARY_KEYS}}) MySQL : CREATE TABLE {{TABLE_NAME}} ({{COLUMNS, PRIMARY_KEYS}}) Oracle : CREATE TABLE {{TABLE_NAME}} ({{COLUMNS, PRIMARY_KEYS}}) Microsoft SQL Server : CREATE TABLE {{TABLE_NAME}} ({{COLUMNS, PRIMARY_KEYS}}) PostgreSQL : CREATE TABLE {{TABLE_NAME}} ({{COLUMNS, PRIMARY_KEYS}}) DB2. : CREATE TABLE {{TABLE_NAME}} ({{COLUMNS, PRIMARY_KEYS}}) N/A {{RDBMS-Name}}.tableCreateQuery The template query for the 'create table' operation in {{RDBMS-Name}}. H2 : SELECT 1 FROM {{TABLE_NAME}} LIMIT 1 MySQL : SELECT 1 FROM {{TABLE_NAME}} LIMIT 1 Oracle : SELECT 1 FROM {{TABLE_NAME}} WHERE rownum=1 Microsoft SQL Server : SELECT TOP 1 1 from {{TABLE_NAME}} PostgreSQL : SELECT 1 FROM {{TABLE_NAME}} LIMIT 1 DB2. : SELECT 1 FROM {{TABLE_NAME}} FETCH FIRST 1 ROWS ONLY N/A {{RDBMS-Name}}.indexCreateQuery The template query for the 'create index' operation in {{RDBMS-Name}}. H2 : CREATE INDEX {{TABLE_NAME}}_INDEX ON {{TABLE_NAME}} ({{INDEX_COLUMNS}}) MySQL : CREATE INDEX {{TABLE_NAME}}_INDEX ON {{TABLE_NAME}} ({{INDEX_COLUMNS}}) Oracle : CREATE INDEX {{TABLE_NAME}}_INDEX ON {{TABLE_NAME}} ({{INDEX_COLUMNS}}) Microsoft SQL Server : CREATE INDEX {{TABLE_NAME}}_INDEX ON {{TABLE_NAME}} ({{INDEX_COLUMNS}}) {{TABLE_NAME}} ({{INDEX_COLUMNS}}) PostgreSQL : CREATE INDEX {{TABLE_NAME}}_INDEX ON {{TABLE_NAME}} ({{INDEX_COLUMNS}}) DB2. : CREATE INDEX {{TABLE_NAME}}_INDEX ON {{TABLE_NAME}} ({{INDEX_COLUMNS}}) N/A {{RDBMS-Name}}.recordInsertQuery The template query for the 'insert record' operation in {{RDBMS-Name}}. H2 : INSERT INTO {{TABLE_NAME}} ({{COLUMNS}}) VALUES ({{Q}}) MySQL : INSERT INTO {{TABLE_NAME}} ({{COLUMNS}}) VALUES ({{Q}}) Oracle : INSERT INTO {{TABLE_NAME}} ({{COLUMNS}}) VALUES ({{Q}}) Microsoft SQL Server : INSERT INTO {{TABLE_NAME}} ({{COLUMNS}}) VALUES ({{Q}}) PostgreSQL : INSERT INTO {{TABLE_NAME}} ({{COLUMNS}}) VALUES ({{Q}}) DB2. : INSERT INTO {{TABLE_NAME}} ({{COLUMNS}}) VALUES ({{Q}}) N/A {{RDBMS-Name}}.recordUpdateQuery The template query for the 'update record' operation in {{RDBMS-Name}}. H2 : UPDATE {{TABLE_NAME}} SET {{COLUMNS_AND_VALUES}} {{CONDITION}} MySQL : UPDATE {{TABLE_NAME}} SET {{COLUMNS_AND_VALUES}} {{CONDITION}} Oracle : UPDATE {{TABLE_NAME}} SET {{COLUMNS_AND_VALUES}} {{CONDITION}} Microsoft SQL Server : UPDATE {{TABLE_NAME}} SET {{COLUMNS_AND_VALUES}} {{CONDITION}} PostgreSQL : UPDATE {{TABLE_NAME}} SET {{COLUMNS_AND_VALUES}} {{CONDITION}} DB2. : UPDATE {{TABLE_NAME}} SET {{COLUMNS_AND_VALUES}} {{CONDITION}} N/A {{RDBMS-Name}}.recordSelectQuery The template query for the 'select record' operation in {{RDBMS-Name}}. H2 : SELECT * FROM {{TABLE_NAME}} {{CONDITION}} MySQL : SELECT * FROM {{TABLE_NAME}} {{CONDITION}} Oracle : SELECT * FROM {{TABLE_NAME}} {{CONDITION}} Microsoft SQL Server : SELECT * FROM {{TABLE_NAME}} {{CONDITION}} PostgreSQL : SELECT * FROM {{TABLE_NAME}} {{CONDITION}} DB2. : SELECT * FROM {{TABLE_NAME}} {{CONDITION}} N/A {{RDBMS-Name}}.recordExistsQuery The template query for the 'check record existence' operation in {{RDBMS-Name}}. H2 : SELECT TOP 1 1 FROM {{TABLE_NAME}} {{CONDITION}} MySQL : SELECT 1 FROM {{TABLE_NAME}} {{CONDITION}} Oracle : SELECT COUNT(1) INTO existence FROM {{TABLE_NAME}} {{CONDITION}} Microsoft SQL Server : SELECT TOP 1 FROM {{TABLE_NAME}} {{CONDITION}} PostgreSQL : SELECT 1 FROM {{TABLE_NAME}} {{CONDITION}} LIMIT 1 DB2. : SELECT 1 FROM {{TABLE_NAME}} {{CONDITION}} FETCH FIRST 1 ROWS ONLY N/A {{RDBMS-Name}}.recordDeleteQuery The query for the 'delete record' operation in {{RDBMS-Name}}. H2 : DELETE FROM {{TABLE_NAME}} {{CONDITION}} MySQL : DELETE FROM {{TABLE_NAME}} {{CONDITION}} Oracle : DELETE FROM {{TABLE_NAME}} {{CONDITION}} Microsoft SQL Server : DELETE FROM {{TABLE_NAME}} {{CONDITION}} PostgreSQL : DELETE FROM {{TABLE_NAME}} {{CONDITION}} DB2. : DELETE FROM {{TABLE_NAME}} {{CONDITION}} N/A {{RDBMS-Name}}.stringSize This defines the length for the string fields in {{RDBMS-Name}}. H2 : 254 MySQL : 254 Oracle : 254 Microsoft SQL Server : 254 PostgreSQL : 254 DB2. : 254 N/A {{RDBMS-Name}}.fieldSizeLimit This defines the field size limit for select/switch to big string type from the default string type if the 'bigStringType' is available in field type list. H2 : N/A MySQL : N/A Oracle : 2000 Microsoft SQL Server : N/A PostgreSQL : N/A DB2. : N/A 0 = n = INT_MAX {{RDBMS-Name}}.batchSize This defines the batch size when operations are performed for batches of events. H2 : 1000 MySQL : 1000 Oracle : 1000 Microsoft SQL Server : 1000 PostgreSQL : 1000 DB2. : 1000 N/A {{RDBMS-Name}}.batchEnable This specifies whether 'Update' and 'Insert' operations can be performed for batches of events or not. H2 : true MySQL : true Oracle (versions 12.0 and less) : false Oracle (versions 12.1 and above) : true Microsoft SQL Server : true PostgreSQL : true DB2. : true N/A {{RDBMS-Name}}.transactionSupported This is used to specify whether the JDBC connection that is used supports JDBC transactions or not. H2 : true MySQL : true Oracle : true Microsoft SQL Server : true PostgreSQL : true DB2. : true N/A {{RDBMS-Name}}.typeMapping.binaryType This is used to specify the binary data type. An attribute defines as 'object' type in Siddhi stream will be stored into RDBMS with this type. H2 : BLOB MySQL : BLOB Oracle : BLOB Microsoft SQL Server : VARBINARY(max) PostgreSQL : BYTEA DB2. : BLOB(64000) N/A {{RDBMS-Name}}.typeMapping.booleanType This is used to specify the boolean data type. An attribute defines as 'bool' type in Siddhi stream will be stored into RDBMS with this type. H2 : TINYINT(1) MySQL : TINYINT(1) Oracle : NUMBER(1) Microsoft SQL Server : BIT PostgreSQL : BOOLEAN DB2. : SMALLINT N/A {{RDBMS-Name}}.typeMapping.doubleType This is used to specify the double data type. An attribute defines as 'double' type in Siddhi stream will be stored into RDBMS with this type. H2 : DOUBLE MySQL : DOUBLE Oracle : NUMBER(19,4) Microsoft SQL Server : FLOAT(32) PostgreSQL : DOUBLE PRECISION DB2. : DOUBLE N/A {{RDBMS-Name}}.typeMapping.floatType This is used to specify the float data type. An attribute defines as 'float' type in Siddhi stream will be stored into RDBMS with this type. H2 : FLOAT MySQL : FLOAT Oracle : NUMBER(19,4) Microsoft SQL Server : REAL PostgreSQL : REAL DB2. : REAL N/A {{RDBMS-Name}}.typeMapping.integerType This is used to specify the integer data type. An attribute defines as 'int' type in Siddhi stream will be stored into RDBMS with this type. H2 : INTEGER MySQL : INTEGER Oracle : NUMBER(10) Microsoft SQL Server : INTEGER PostgreSQL : INTEGER DB2. : INTEGER N/A {{RDBMS-Name}}.typeMapping.longType This is used to specify the long data type. An attribute defines as 'long' type in Siddhi stream will be stored into RDBMS with this type. H2 : BIGINT MySQL : BIGINT Oracle : NUMBER(19) Microsoft SQL Server : BIGINT PostgreSQL : BIGINT DB2. : BIGINT N/A {{RDBMS-Name}}.typeMapping.stringType This is used to specify the string data type. An attribute defines as 'string' type in Siddhi stream will be stored into RDBMS with this type. H2 : VARCHAR(stringSize) MySQL : VARCHAR(stringSize) Oracle : VARCHAR(stringSize) Microsoft SQL Server : VARCHAR(stringSize) PostgreSQL : VARCHAR(stringSize) DB2. : VARCHAR(stringSize) N/A {{RDBMS-Name}}.typeMapping.bigStringType This is used to specify the big string data type. An attribute defines as 'string' type in Siddhi stream and field.length define in the annotation is greater than the fieldSizeLimit, will be stored into RDBMS with this type. H2 : N/A MySQL : N/A Oracle : CLOB Microsoft SQL Server : N/A PostgreSQL : N/A DB2.* : N/A N/A Examples EXAMPLE 1 @Store(type=\"rdbms\", jdbc.url=\"jdbc:mysql://localhost:3306/stocks\", username=\"root\", password=\"root\", jdbc.driver.name=\"com.mysql.jdbc.Driver\",field.length=\"symbol:100\") @PrimaryKey(\"id\", \"symbol\") @Index(\"volume\") define table StockTable (id string, symbol string, price float, volume long); The above example creates an event table named 'StockTable' in the database if it does not already exist (with four attributes named id , symbol , price , and volume of the types 'string', 'string', 'float', and 'long' respectively). The connection is made as specified by the parameters configured for the '@Store' annotation. The @PrimaryKey() and @Index() annotations can be used to define primary keys or indexes for the table and they follow Siddhi query syntax. RDBMS store supports having more than one attributes in the @PrimaryKey or @Index annotations. In this example a composite Primary key of both attributes id and symbol will be created. EXAMPLE 2 @Store(type=\"rdbms\", jdbc.url=\"jdbc:mysql://localhost:3306/das\", username=\"root\", password=\"root\" , jdbc.driver.name=\"org.h2.Driver\",field.length=\"symbol:100\") @PrimaryKey(\"symbol\") @Index(\"symbol\") define table StockTable (symbol string, price float, volume long); define stream InputStream (symbol string, volume long); from InputStream as a join StockTable as b on str:contains(b.symbol, a.symbol) select a.symbol as symbol, b.volume as volume insert into FooStream; The above example creates an event table named 'StockTable' in the database if it does not already exist (with three attributes named 'symbol', 'price', and 'volume' of the types 'string', 'float' and 'long' respectively). Then the table is joined with a stream named 'InputStream' based on a condition. The following operations are included in the condition: [ AND, OR, Comparisons( = = == !=), IS NULL, NOT, str:contains(Table Column , Stream Attribute or Search.String)] EXAMPLE 3 @Store(type=\"rdbms\", jdbc.url=\"jdbc:mysql://localhost:3306/das\", table.name=\"StockTable\", username=\"root\", password=\"root\" , jdbc.driver.name=\"org.h2.Driver\", field.length=\"symbol:100\", table.check.query=\"SELECT 1 FROM StockTable LIMIT 1\") @PrimaryKey(\"symbol\") @Index(\"symbol\") define table StockTable (symbol string, price float, volume long); define stream InputStream (symbol string, volume long); from InputStream as a join StockTable as b on str:contains(b.symbol, a.symbol) select a.symbol as symbol, b.volume as volume insert into FooStream; The above example creates an event table named 'StockTable' in the database if it does not already exist (with three attributes named 'symbol', 'price', and 'volume' of the types 'string', 'float' and 'long' respectively). Then the table is joined with a stream named 'InputStream' based on a condition. The following operations are included in the condition: [ AND, OR, Comparisons( = = == !=), IS NULL, NOT, str:contains(Table Column , Stream Attribute or Search.String)] redis (Store) This extension assigns data source and connection instructions to event tables. It also implements read write operations on connected datasource. This extension only can be used to read the data which persisted using the same extension since unique implementation has been used to map the relational data in to redis's key and value representation Origin: siddhi-store-redis:3.1.1 Syntax @Store(type=\"redis\", table.name=\" STRING \", cluster.mode=\" BOOL \", nodes=\" STRING \", ttl.seconds=\" LONG \", ttl.on.update=\" BOOL \", ttl.on.read=\" BOOL \") @PrimaryKey(\"PRIMARY_KEY\") @Index(\"INDEX\") QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic table.name The name with which the event table should be persisted in the store. If noname is specified via this parameter, the event table is persisted with the same name as the Siddhi table. The tale name defined in the siddhi app STRING Yes No cluster.mode This will decide the redis mode. if this is false, client will connect to a single redis node. false BOOL No No nodes host, port and the password of the node(s).In single node mode node details can be provided as follows- \"node='hosts:port@password'\" In clustered mode host and port of all the master nodes should be provided separated by a comma(,). As an example \"nodes = 'localhost:30001,localhost:30002'\". localhost:6379@root STRING Yes No ttl.seconds Time to live in seconds for each record -1 LONG Yes No ttl.on.update Set ttl on row update false BOOL Yes No ttl.on.read Set ttl on read rows false BOOL Yes No Examples EXAMPLE 1 @store(type='redis',nodes='localhost:6379@root',table.name='fooTable',cluster.mode=false)define table fooTable(time long, date String) Above example will create a redis table with the name fooTable and work on asingle redis node. EXAMPLE 2 @Store(type='redis', table.name='SweetProductionTable', nodes='localhost:30001,localhost:30002,localhost:30003', cluster.mode='true') @primaryKey('symbol') @index('price') define table SweetProductionTable (symbol string, price float, volume long); Above example demonstrate how to use the redis extension to connect in to redis cluster. Please note that, as nodes all the master node's host and port should be provided in order to work correctly. In clustered node password will not besupported EXAMPLE 3 @store(type='redis',nodes='localhost:6379@root',table.name='fooTable', ttl.seconds='30', ttl.onUpdate='true', ttl.onRead='true')define table fooTable(time long, date String) Above example will create a redis table with the name fooTable and work on asingle redis node. All rows inserted, updated or read will have its ttl set to 30 seconds Str groupConcat (Aggregate Function) This function aggregates the received events by concatenating the keys in those events using a separator, e.g.,a comma (,) or a hyphen (-), and returns the concatenated key string. Origin: siddhi-execution-string:5.0.7 Syntax STRING str:groupConcat( STRING key) STRING str:groupConcat( STRING key, STRING ...) STRING str:groupConcat( STRING key, STRING separator, BOOL distinct) STRING str:groupConcat( STRING key, STRING separator, BOOL distinct, STRING order) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic key The string that needs to be aggregated. STRING No Yes separator The separator that separates each string key after concatenating the keys. , STRING Yes Yes distinct This is used to only have distinct values in the concatenated string that is returned. false BOOL Yes Yes order This parameter accepts 'ASC' or 'DESC' strings to sort the string keys in either ascending or descending order respectively. No order STRING Yes Yes Examples EXAMPLE 1 from InputStream#window.time(5 min) select str:groupConcat(\"key\") as groupedKeys input OutputStream; When we input events having values for the key as 'A' , 'B' , 'S' , 'C' , 'A' , it returns \"A,B,S,C,A\" to the 'OutputStream'. EXAMPLE 2 from InputStream#window.time(5 min) select groupConcat(\"key\",\"-\",true,\"ASC\") as groupedKeys input OutputStream; When we input events having values for the key as 'A' , 'B' , 'S' , 'C' , 'A' , specify the seperator as hyphen and choose the order to be ascending, the function returns \"A-B-C-S\" to the 'OutputStream'. charAt (Function) This function returns the 'char' value that is present at the given index position. of the input string. Origin: siddhi-execution-string:5.0.7 Syntax STRING str:charAt( STRING input.value, INT index) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.value The input string of which the char value at the given position needs to be returned. STRING No Yes index The variable that specifies the index of the char value that needs to be returned. INT No Yes Examples EXAMPLE 1 charAt(\"WSO2\", 1) In this case, the functiion returns the character that exists at index 1. Hence, it returns 'S'. coalesce (Function) This returns the first input parameter value of the given argument, that is not null. Origin: siddhi-execution-string:5.0.7 Syntax INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT str:coalesce( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg, INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT ...) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg It can have one or more input parameters in any data type. However, all the specified parameters are required to be of the same type. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No Yes Examples EXAMPLE 1 coalesce(null, \"BBB\", \"CCC\") This returns the first input parameter that is not null. In this example, it returns \"BBB\". concat (Function) This function returns a string value that is obtained as a result of concatenating two or more input string values. Origin: siddhi-execution-string:5.0.7 Syntax STRING str:concat( STRING arg, STRING ...) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg This can have two or more string type input parameters. STRING No Yes Examples EXAMPLE 1 concat(\"D533\", \"8JU^\", \"XYZ\") This returns a string value by concatenating two or more given arguments. In the example shown above, it returns \"D5338JU^XYZ\". contains (Function) This function returns true if the input.string contains the specified sequence of char values in the search.string . Origin: siddhi-execution-string:5.0.7 Syntax BOOL str:contains( STRING input.string, STRING search.string) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string Input string value. STRING No Yes search.string The string value to be searched for in the input.string . STRING No Yes Examples EXAMPLE 1 contains(\"21 products are produced by WSO2 currently\", \"WSO2\") This returns a boolean value as the output. In this case, it returns true . equalsIgnoreCase (Function) This returns a boolean value by comparing two strings lexicographically without considering the letter case. Origin: siddhi-execution-string:5.0.7 Syntax BOOL str:equalsIgnoreCase( STRING arg1, STRING arg2) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg1 The first input string argument. STRING No Yes arg2 The second input string argument. This is compared with the first argument. STRING No Yes Examples EXAMPLE 1 equalsIgnoreCase(\"WSO2\", \"wso2\") This returns a boolean value as the output. In this scenario, it returns \"true\". fillTemplate (Function) fillTemplate(string, map) will replace all the keys in the string using values in the map. fillTemplate(string, r1, r2 ..) replace all the entries {{1}}, {{2}}, {{3}} with r1 , r2, r3. Origin: siddhi-execution-string:5.0.7 Syntax STRING str:fillTemplate( STRING template, STRING|INT|LONG|DOUBLE|FLOAT|BOOL replacement.type, STRING|INT|LONG|DOUBLE|FLOAT|BOOL ...) STRING str:fillTemplate( STRING template, OBJECT map) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic template The string with templated fields that needs to be filled with the given strings. The format of the templated fields should be as follows: {{KEY}} where 'KEY' is a STRING if you are using fillTemplate(string, map) {{KEY}} where 'KEY' is an INT if you are using fillTemplate(string, r1, r2 ..) This KEY is used to map the values STRING No Yes replacement.type A set of arguments with any type string|int|long|double|float|bool. - STRING INT LONG DOUBLE FLOAT BOOL Yes Yes map A map with key-value pairs to be replaced. - OBJECT Yes Yes Examples EXAMPLE 1 str:fillTemplate(\"{{prize}} 100 {{salary}} 10000\", map:create('prize', 300, 'salary', 10000)) In this example, the template is '{{prize}} 100 {{salary}} 10000'.Here, the templated string {{prize}} is replaced with the value corresponding to the 'prize' key in the given map. Likewise salary replace with the salary value of the map EXAMPLE 2 str:fillTemplate(\"{{1}} 100 {{2}} 10000\", 200, 300) In this example, the template is '{{1}} 100 {{2}} 10000'.Here, the templated string {{1}} is replaced with the corresponding 1 st value 200. Likewise {{2}} replace with the 300 hex (Function) This function returns a hexadecimal string by converting each byte of each character in the input string to two hexadecimal digits. Origin: siddhi-execution-string:5.0.7 Syntax STRING str:hex( STRING input.string) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string The input string to derive the hexadecimal value. STRING No Yes Examples EXAMPLE 1 hex(\"MySQL\") This returns the hexadecimal value of the input.string. In this scenario, the output is \"4d7953514c\". length (Function) Returns the length of the input string. Origin: siddhi-execution-string:5.0.7 Syntax INT str:length( STRING input.string) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string The input string to derive the length. STRING No Yes Examples EXAMPLE 1 length(\"Hello World\") This outputs the length of the provided string. In this scenario, the, output is 11 . lower (Function) Converts the capital letters in the input string to the equivalent simple letters. Origin: siddhi-execution-string:5.0.7 Syntax STRING str:lower( STRING input.string) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string The input string to convert to the lower case (i.e., equivalent simple letters). STRING No Yes Examples EXAMPLE 1 lower(\"WSO2 cep \") This converts the capital letters in the input.string to the equivalent simple letters. In this scenario, the output is \"wso2 cep \". regexp (Function) Returns a boolean value based on the matchability of the input string and the given regular expression. Origin: siddhi-execution-string:5.0.7 Syntax BOOL str:regexp( STRING input.string, STRING regex) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string The input string to match with the given regular expression. STRING No Yes regex The regular expression to be matched with the input string. STRING No Yes Examples EXAMPLE 1 regexp(\"WSO2 abcdh\", \"WSO(.*h)\") This returns a boolean value after matching regular expression with the given string. In this scenario, it returns \"true\" as the output. repeat (Function) Repeats the input string for a specified number of times. Origin: siddhi-execution-string:5.0.7 Syntax STRING str:repeat( STRING input.string, INT times) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string The input string that is repeated the number of times as defined by the user. STRING No Yes times The number of times the input.string needs to be repeated . INT No Yes Examples EXAMPLE 1 repeat(\"StRing 1\", 3) This returns a string value by repeating the string for a specified number of times. In this scenario, the output is \"StRing 1StRing 1StRing 1\". replaceAll (Function) Finds all the substrings of the input string that matches with the given expression, and replaces them with the given replacement string. Origin: siddhi-execution-string:5.0.7 Syntax STRING str:replaceAll( STRING input.string, STRING regex, STRING replacement.string) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string The input string to be replaced. STRING No Yes regex The regular expression to be matched with the input string. STRING No Yes replacement.string The string with which each substring that matches the given expression should be replaced. STRING No Yes Examples EXAMPLE 1 replaceAll(\"hello hi hello\", 'hello', 'test') This returns a string after replacing the substrings of the input string with the replacement string. In this scenario, the output is \"test hi test\" . replaceFirst (Function) Finds the first substring of the input string that matches with the given regular expression, and replaces itwith the given replacement string. Origin: siddhi-execution-string:5.0.7 Syntax STRING str:replaceFirst( STRING input.string, STRING regex, STRING replacement.string) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string The input string that should be replaced. STRING No Yes regex The regular expression with which the input string should be matched. STRING No Yes replacement.string The string with which the first substring of input string that matches the regular expression should be replaced. STRING No Yes Examples EXAMPLE 1 replaceFirst(\"hello WSO2 A hello\", 'WSO2(.*)A', 'XXXX') This returns a string after replacing the first substring with the given replacement string. In this scenario, the output is \"hello XXXX hello\". reverse (Function) Returns the input string in the reverse order character-wise and string-wise. Origin: siddhi-execution-string:5.0.7 Syntax STRING str:reverse( STRING input.string) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string The input string to be reversed. STRING No Yes Examples EXAMPLE 1 reverse(\"Hello World\") This outputs a string value by reversing the incoming input.string . In this scenario, the output is \"dlroW olleH\". split (Function) Splits the input.string into substrings using the value parsed in the split.string and returns the substring at the position specified in the group.number . Origin: siddhi-execution-string:5.0.7 Syntax STRING str:split( STRING input.string, STRING split.string, INT group.number) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string The input string to be replaced. STRING No Yes split.string The string value to be used to split the input.string . STRING No Yes group.number The index of the split group INT No Yes Examples EXAMPLE 1 split(\"WSO2,ABM,NSFT\", \",\", 0) This splits the given input.string by given split.string and returns the string in the index given by group.number. In this scenario, the output will is \"WSO2\". strcmp (Function) Compares two strings lexicographically and returns an integer value. If both strings are equal, 0 is returned. If the first string is lexicographically greater than the second string, a positive value is returned. If the first string is lexicographically greater than the second string, a negative value is returned. Origin: siddhi-execution-string:5.0.7 Syntax INT str:strcmp( STRING arg1, STRING arg2) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg1 The first input string argument. STRING No Yes arg2 The second input string argument that should be compared with the first argument lexicographically. STRING No Yes Examples EXAMPLE 1 strcmp(\"AbCDefghiJ KLMN\", 'Hello') This compares two strings lexicographically and outputs an integer value. substr (Function) Returns a substring of the input string by considering a subset or all of the following factors: starting index, length, regular expression, and regex group number. Origin: siddhi-execution-string:5.0.7 Syntax STRING str:substr( STRING input.string, INT begin.index) STRING str:substr( STRING input.string, INT begin.index, INT length) STRING str:substr( STRING input.string, STRING regex) STRING str:substr( STRING input.string, STRING regex, INT group.number) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string The input string to be processed. STRING No Yes begin.index Starting index to consider for the substring. - INT Yes Yes length The length of the substring. input.string .length - begin.index INT Yes Yes regex The regular expression that should be matched with the input string. - STRING Yes Yes group.number The regex group number 0 INT Yes Yes Examples EXAMPLE 1 substr(\"AbCDefghiJ KLMN\", 4) This outputs the substring based on the given begin.index . In this scenario, the output is \"efghiJ KLMN\". EXAMPLE 2 substr(\"AbCDefghiJ KLMN\", 2, 4) This outputs the substring based on the given begin.index and length. In this scenario, the output is \"CDef\". EXAMPLE 3 substr(\"WSO2D efghiJ KLMN\", '^WSO2(.*)') This outputs the substring by applying the regex. In this scenario, the output is \"WSO2D efghiJ KLMN\". EXAMPLE 4 substr(\"WSO2 cep WSO2 XX E hi hA WSO2 heAllo\", 'WSO2(.*)A(.*)', 2) This outputs the substring by applying the regex and considering the group.number . In this scenario, the output is \" ello\". trim (Function) Returns a copy of the input string without the leading and trailing whitespace (if any). Origin: siddhi-execution-string:5.0.7 Syntax STRING str:trim( STRING input.string) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string The input string that needs to be trimmed. STRING No Yes Examples EXAMPLE 1 trim(\" AbCDefghiJ KLMN \") This returns a copy of the input.string with the leading and/or trailing white-spaces omitted. In this scenario, the output is \"AbCDefghiJ KLMN\". unhex (Function) Returns a string by converting the hexadecimal characters in the input string. Origin: siddhi-execution-string:5.0.7 Syntax STRING str:unhex( STRING input.string) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string The hexadecimal input string that needs to be converted to string. STRING No Yes Examples EXAMPLE 1 unhex(\"4d7953514c\") This converts the hexadecimal value to string. upper (Function) Converts the simple letters in the input string to the equivalent capital/block letters. Origin: siddhi-execution-string:5.0.7 Syntax STRING str:upper( STRING input.string) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string The input string that should be converted to the upper case (equivalent capital/block letters). STRING No Yes Examples EXAMPLE 1 upper(\"Hello World\") This converts the simple letters in the input.string to theequivalent capital letters. In this scenario, the output is \"HELLO WORLD\". tokenize (Stream Processor) This function splits the input string into tokens using a given regular expression and returns the split tokens. Origin: siddhi-execution-string:5.0.7 Syntax str:tokenize( STRING input.string, STRING regex) str:tokenize( STRING input.string, STRING regex, BOOL distinct) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string The input string which needs to be split. STRING No Yes regex The string value which is used to tokenize the 'input.string'. STRING No Yes distinct This flag is used to return only distinct values. false BOOL Yes Yes Extra Return Attributes Name Description Possible Types token The attribute which contains a single token. STRING Examples EXAMPLE 1 define stream inputStream (str string); @info(name = 'query1') from inputStream#str:tokenize(str , ',') select token insert into outputStream; This query performs tokenization on the given string. If the str is \"Android,Windows8,iOS\", then the string is split into 3 events containing the token attribute values, i.e., Android , Windows8 and iOS . Time currentDate (Function) Function returns the system time in yyyy-MM-dd format. Origin: siddhi-execution-time:5.0.4 Syntax STRING time:currentDate() Examples EXAMPLE 1 time:currentDate() Returns the current date in the yyyy-MM-dd format, such as 2019-06-21 . currentTime (Function) Function returns system time in the HH ss format. Origin: siddhi-execution-time:5.0.4 Syntax STRING time:currentTime() Examples EXAMPLE 1 time:currentTime() Returns the current date in the HH ss format, such as 15:23:24 . currentTimestamp (Function) When no argument is provided, function returns the system current timestamp in yyyy-MM-dd HH ss format, and when a timezone is provided as an argument, it converts and return the current system time to the given timezone format. Origin: siddhi-execution-time:5.0.4 Syntax STRING time:currentTimestamp() STRING time:currentTimestamp( STRING timezone) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic timezone The timezone to which the current time need to be converted. For example, Asia/Kolkata , PST . Get the supported timezone IDs from here System timezone STRING Yes No Examples EXAMPLE 1 time:currentTimestamp() Returns current system time in yyyy-MM-dd HH ss format, such as 2019-03-31 14:07:00 . EXAMPLE 2 time:currentTimestamp('Asia/Kolkata') Returns current system time converted to 'Asia/Kolkata' timezone yyyy-MM-dd HH ss format, such as 2019-03-31 19:07:00 . Get the supported timezone IDs from here EXAMPLE 3 time:currentTimestamp('CST') Returns current system time converted to 'CST' timezone yyyy-MM-dd HH ss format, such as 2019-03-31 02:07:00 . Get the supported timezone IDs from here date (Function) Extracts the date part of a date or date-time and return it in yyyy-MM-dd format. Origin: siddhi-execution-time:5.0.4 Syntax STRING time:date( STRING date.value, STRING date.format) STRING time:date( STRING date.value) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic date.value The value of the date. For example, 2014-11-11 13:23:44.657 , 2014-11-11 , 13:23:44.657 . STRING No Yes date.format The format of the date value provided. For example, yyyy/MM/dd HH ss.SSS . yyyy-MM-dd HH:mm:ss.SSS STRING Yes Yes Examples EXAMPLE 1 time:date('2014/11/11 13:23:44', 'yyyy/MM/dd HH:mm:ss') Extracts the date and returns 2014-11-11 . EXAMPLE 2 time:date('2014-11-23 13:23:44.345') Extracts the date and returns 2014-11-13 . EXAMPLE 3 time:date('13:23:44', 'HH:mm:ss') Extracts the date and returns 1970-01-01 . dateAdd (Function) Adds the specified time interval to a date. Origin: siddhi-execution-time:5.0.4 Syntax STRING time:dateAdd( STRING date.value, INT expr, STRING unit) STRING time:dateAdd( LONG timestamp.in.milliseconds, INT expr, STRING unit) STRING time:dateAdd( STRING date.value, INT expr, STRING unit, STRING date.format) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic date.value The value of the date. For example, 2014-11-11 13:23:44.657 , 2014-11-11 , 13:23:44.657 . - STRING Yes Yes expr The amount by which the selected part of the date should be incremented. For example 2 , 5 , 10 , etc. INT No Yes unit This is the part of the date that needs to be modified. For example, MINUTE , HOUR , MONTH , YEAR , QUARTER , WEEK , DAY , SECOND . STRING No No date.format The format of the date value provided. For example, yyyy-MM-dd HH ss.SSS . yyyy-MM-dd HH:mm:ss.SSS STRING Yes Yes timestamp.in.milliseconds The date value in milliseconds. For example, 1415712224000L . - LONG Yes Yes Examples EXAMPLE 1 time:dateAdd('2014-11-11 13:23:44.657', 5, 'YEAR', 'yyyy-MM-dd HH:mm:ss.SSS') Adds five years to the given date value and returns 2019-11-11 13:23:44.657 . EXAMPLE 2 time:dateAdd('2014-11-11 13:23:44.657', 5, 'YEAR') Adds five years to the given date value and returns 2019-11-11 13:23:44.657 using the default date.format yyyy-MM-dd HH ss.SSS . EXAMPLE 3 time:dateAdd( 1415712224000L, 1, 'HOUR') Adds one hour and 1415715824000 as a string . dateDiff (Function) Returns difference between two dates in days. Origin: siddhi-execution-time:5.0.4 Syntax INT time:dateDiff( STRING date.value1, STRING date.value2, STRING date.format1, STRING date.format2) INT time:dateDiff( STRING date.value1, STRING date.value2) INT time:dateDiff( LONG timestamp.in.milliseconds1, LONG timestamp.in.milliseconds2) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic date.value1 The value of the first date parameter. For example, 2014-11-11 13:23:44.657 , 2014-11-11 , 13:23:44.657 . - STRING Yes Yes date.value2 The value of the second date parameter. For example, 2014-11-11 13:23:44.657 , 2014-11-11 , 13:23:44.657 . - STRING Yes Yes date.format1 The format of the first date value provided. For example, yyyy-MM-dd HH ss.SSS . yyyy-MM-dd HH:mm:ss.SSS STRING Yes Yes date.format2 The format of the second date value provided. For example, yyyy-MM-dd HH ss.SSS . yyyy-MM-dd HH:mm:ss.SSS STRING Yes Yes timestamp.in.milliseconds1 The first date value in milliseconds from the epoch. For example, 1415712224000L . - LONG Yes Yes timestamp.in.milliseconds2 The second date value in milliseconds from the epoch. For example, 1415712224000L . - LONG Yes Yes Examples EXAMPLE 1 time:dateDiff('2014-11-11 13:23:44', '2014-11-9 13:23:44', 'yyyy-MM-dd HH:mm:ss', 'yyyy-MM-dd HH:mm:ss') Returns the date difference between the two given dates as 2 . EXAMPLE 2 time:dateDiff('2014-11-13 13:23:44', '2014-11-9 13:23:44') Returns the date difference between the two given dates as 4 . EXAMPLE 3 time:dateDiff(1415692424000L, 1412841224000L) Returns the date difference between the two given dates as 33 . dateFormat (Function) Formats the data in string or milliseconds format to the given date format. Origin: siddhi-execution-time:5.0.4 Syntax STRING time:dateFormat( STRING date.value, STRING date.target.format, STRING date.source.format) STRING time:dateFormat( STRING date.value, STRING date.target.format) STRING time:dateFormat( LONG timestamp.in.milliseconds, STRING date.target.format) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic date.value The value of the date. For example, 2014-11-11 13:23:44.657 , 2014-11-11 , 13:23:44.657 . - STRING Yes Yes date.target.format The format of the date into which the date value needs to be converted. For example, yyyy/MM/dd HH ss . STRING No Yes date.source.format The format input date.value.For example, yyyy-MM-dd HH ss.SSS . yyyy-MM-dd HH:mm:ss.SSS STRING Yes Yes timestamp.in.milliseconds The date value in milliseconds from the epoch. For example, 1415712224000L . - LONG Yes Yes Examples EXAMPLE 1 time:dateFormat('2014/11/11 13:23:44', 'mm:ss', 'yyyy/MM/dd HH:mm:ss') Converts date based on the target date format mm:ss and returns 23:44 . EXAMPLE 2 time:dateFormat('2014-11-11 13:23:44', 'HH:mm:ss') Converts date based on the target date format HH ss and returns 13:23:44 . EXAMPLE 3 time:dateFormat(1415692424000L, 'yyyy-MM-dd') Converts date in millisecond based on the target date format yyyy-MM-dd and returns 2014-11-11 . dateSub (Function) Subtracts the specified time interval from the given date. Origin: siddhi-execution-time:5.0.4 Syntax STRING time:dateSub( STRING date.value, INT expr, STRING unit) STRING time:dateSub( STRING date.value, INT expr, STRING unit, STRING date.format) STRING time:dateSub( LONG timestamp.in.milliseconds, INT expr, STRING unit) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic date.value The value of the date. For example, 2014-11-11 13:23:44.657 , 2014-11-11 , 13:23:44.657 . - STRING Yes Yes expr The amount by which the selected part of the date should be decremented. For example 2 , 5 , 10 , etc. INT No Yes unit This is the part of the date that needs to be modified. For example, MINUTE , HOUR , MONTH , YEAR , QUARTER , WEEK , DAY , SECOND . STRING No No date.format The format of the date value provided. For example, yyyy-MM-dd HH ss.SSS . yyyy-MM-dd HH:mm:ss.SSS STRING Yes Yes timestamp.in.milliseconds The date value in milliseconds. For example, 1415712224000L . - LONG Yes Yes Examples EXAMPLE 1 time:dateSub('2019-11-11 13:23:44.657', 5, 'YEAR', 'yyyy-MM-dd HH:mm:ss.SSS') Subtracts five years to the given date value and returns 2014-11-11 13:23:44.657 . EXAMPLE 2 time:dateSub('2019-11-11 13:23:44.657', 5, 'YEAR') Subtracts five years to the given date value and returns 2014-11-11 13:23:44.657 using the default date.format yyyy-MM-dd HH ss.SSS . EXAMPLE 3 time:dateSub( 1415715824000L, 1, 'HOUR') Subtracts one hour and 1415712224000 as a string . dayOfWeek (Function) Extracts the day on which a given date falls. Origin: siddhi-execution-time:5.0.4 Syntax STRING time:dayOfWeek( STRING date.value, STRING date.format) STRING time:dayOfWeek( STRING date.value) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic date.value The value of the date. For example, 2014-11-11 13:23:44.657 , 2014-11-11 , 13:23:44.657 . STRING No Yes date.format The format of the date value provided. For example, yyyy/MM/dd HH ss.SSS . yyyy-MM-dd HH:mm:ss.SSS STRING Yes Yes Examples EXAMPLE 1 time:date('2014/12/11 13:23:44', 'yyyy/MM/dd HH:mm:ss') Extracts the date and returns Thursday . EXAMPLE 2 time:date('2014-11-11 13:23:44.345') Extracts the date and returns Tuesday . extract (Function) Function extracts a date unit from the date. Origin: siddhi-execution-time:5.0.4 Syntax INT time:extract( STRING unit, STRING date.value) INT time:extract( STRING unit, STRING date.value, STRING date.format) INT time:extract( STRING unit, STRING date.value, STRING date.format, STRING locale) INT time:extract( LONG timestamp.in.milliseconds, STRING unit) INT time:extract( LONG timestamp.in.milliseconds, STRING unit, STRING locale) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic unit This is the part of the date that needs to be modified. For example, MINUTE , HOUR , MONTH , YEAR , QUARTER , WEEK , DAY , SECOND . STRING No No date.value The value of the date. For example, 2014-11-11 13:23:44.657 , 2014-11-11 , 13:23:44.657 . - STRING Yes Yes date.format The format of the date value provided. For example, yyyy-MM-dd HH ss.SSS . yyyy-MM-dd HH:mm:ss.SSS STRING Yes Yes timestamp.in.milliseconds The date value in milliseconds. For example, 1415712224000L . - LONG Yes Yes locale Represents a specific geographical, political or cultural region. For example en_US and fr_FR Current default locale set in the Java Virtual Machine. STRING Yes No Examples EXAMPLE 1 time:extract('YEAR', '2019/11/11 13:23:44.657', 'yyyy/MM/dd HH:mm:ss.SSS') Extracts the year amount and returns 2019 . EXAMPLE 2 time:extract('DAY', '2019-11-12 13:23:44.657') Extracts the day amount and returns 12 . EXAMPLE 3 time:extract(1394556804000L, 'HOUR') Extracts the hour amount and returns 22 . timestampInMilliseconds (Function) Returns the system time or the given time in milliseconds. Origin: siddhi-execution-time:5.0.4 Syntax LONG time:timestampInMilliseconds() LONG time:timestampInMilliseconds( STRING date.value, STRING date.format) LONG time:timestampInMilliseconds( STRING date.value) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic date.value The value of the date. For example, 2014-11-11 13:23:44.657 , 2014-11-11 , 13:23:44.657 . Current system time STRING Yes Yes date.format The format of the date value provided. For example, yyyy/MM/dd HH ss.SSS . yyyy-MM-dd HH:mm:ss.SSS STRING Yes Yes Examples EXAMPLE 1 time:timestampInMilliseconds() Returns the system current time in milliseconds. EXAMPLE 2 time:timestampInMilliseconds('2007-11-30 10:30:19', 'yyyy-MM-DD HH:MM:SS') Converts 2007-11-30 10:30:19 in yyyy-MM-DD HH:MM:SS format to milliseconds as 1170131400019 . EXAMPLE 3 time:timestampInMilliseconds('2007-11-30 10:30:19.000') Converts 2007-11-30 10:30:19 in yyyy-MM-DD HH:MM:ss.SSS format to milliseconds as 1196398819000 . utcTimestamp (Function) Function returns the system current time in UTC timezone with yyyy-MM-dd HH ss format. Origin: siddhi-execution-time:5.0.4 Syntax STRING time:utcTimestamp() Examples EXAMPLE 1 time:utcTimestamp() Returns the system current time in UTC timezone with yyyy-MM-dd HH ss format, and a sample output will be like 2019-07-03 09:58:34 . Unique deduplicate (Stream Processor) Removes duplicate events based on the unique.key parameter that arrive within the time.interval gap from one another. Origin: siddhi-execution-unique:5.0.4 Syntax unique:deduplicate( INT|LONG|FLOAT|BOOL|DOUBLE|STRING unique.key, INT|LONG time.interval) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic unique.key Parameter to uniquely identify events. INT LONG FLOAT BOOL DOUBLE STRING No Yes time.interval The sliding time period within which the duplicate events are dropped. INT LONG No No Examples EXAMPLE 1 define stream TemperatureStream (sensorId string, temperature double) from TemperatureStream#unique:deduplicate(sensorId, 30 sec) select * insert into UniqueTemperatureStream; Query that removes duplicate events of TemperatureStream stream based on sensorId attribute when they arrive within 30 seconds. ever (Window) Window that retains the latest events based on a given unique keys. When a new event arrives with the same key it replaces the one that exist in the window. b This function is not recommended to be used when the maximum number of unique attributes are undefined, as there is a risk of system going out to memory /b . Origin: siddhi-execution-unique:5.0.4 Syntax unique:ever( INT|LONG|FLOAT|BOOL|DOUBLE|STRING unique.key) unique:ever( INT|LONG|FLOAT|BOOL|DOUBLE|STRING unique.key, INT|LONG|FLOAT|BOOL|DOUBLE|STRING ...) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic unique.key The attribute used to checked for uniqueness. INT LONG FLOAT BOOL DOUBLE STRING No Yes Examples EXAMPLE 1 define stream LoginEvents (timestamp long, ip string); from LoginEvents#window.unique:ever(ip) select count(ip) as ipCount insert events into UniqueIps; Query collects all unique events based on the ip attribute by retaining the latest unique events from the LoginEvents stream. Then the query counts the unique ip s arrived so far and outputs the ipCount via the UniqueIps stream. EXAMPLE 2 define stream DriverChangeStream (trainID string, driver string); from DriverChangeStream#window.unique:ever(trainID) select trainID, driver insert expired events into PreviousDriverChangeStream; Query collects all unique events based on the trainID attribute by retaining the latest unique events from the DriverChangeStream stream. The query outputs the previous unique event stored in the window as the expired events are emitted via PreviousDriverChangeStream stream. EXAMPLE 3 define stream StockStream (symbol string, price float); define stream PriceRequestStream(symbol string); from StockStream#window.unique:ever(symbol) as s join PriceRequestStream as p on s.symbol == p.symbol select s.symbol as symbol, s.price as price insert events into PriceResponseStream; Query stores the last unique event for each symbol attribute of StockStream stream, and joins them with events arriving on the PriceRequestStream for equal symbol attributes to fetch the latest price for each requested symbol and output via PriceResponseStream stream. externalTimeBatch (Window) This is a batch (tumbling) time window that is determined based on an external time, i.e., time stamps that are specified via an attribute in the events. It holds the latest unique events that arrived during the last window time period. The unique events are determined based on the value for a specified unique key parameter. When a new event arrives within the time window with a value for the unique key parameter that is the same as that of an existing event in the window, the existing event expires and it is replaced by the new event. Origin: siddhi-execution-unique:5.0.4 Syntax unique:externalTimeBatch( INT|LONG|FLOAT|BOOL|DOUBLE|STRING unique.key, LONG time.stamp, INT|LONG window.time) unique:externalTimeBatch( INT|LONG|FLOAT|BOOL|DOUBLE|STRING unique.key, LONG time.stamp, INT|LONG window.time, INT start.time) unique:externalTimeBatch( INT|LONG|FLOAT|BOOL|DOUBLE|STRING unique.key, LONG time.stamp, INT|LONG window.time, INT start.time, INT|LONG time.out) unique:externalTimeBatch( INT|LONG|FLOAT|BOOL|DOUBLE|STRING unique.key, LONG time.stamp, INT|LONG window.time, INT start.time, INT|LONG time.out, INT|LONG replace.time.stamp.with.batch.end.time) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic unique.key The attribute that should be checked for uniqueness. INT LONG FLOAT BOOL DOUBLE STRING No Yes time.stamp The time which the window determines as the current time and acts upon. The value of this parameter should be monotonically increasing. LONG No No window.time The sliding time period for which the window should hold events. INT LONG No No start.time This specifies an offset in milliseconds in order to start the window at a time different to the standard time. Timestamp of first event INT Yes No time.out Time to wait for arrival of a new event, before flushing and returning the output for events belonging to a specific batch. The system waits till an event from the next batch arrives to flush the current batch INT LONG Yes No replace.time.stamp.with.batch.end.time Replaces the 'timestamp' value with the corresponding batch end time stamp. false INT LONG Yes No Examples EXAMPLE 1 define stream LoginEvents (timestamp long, ip string); from LoginEvents#window.unique:externalTimeBatch(ip, timestamp, 1 sec, 0, 2 sec) select timestamp, ip, count() as total insert into UniqueIps ; In this query, the window holds the latest unique events that arrive from the 'LoginEvent' stream during each second. The latest events are determined based on the external time stamp. At a given time, all the events held in the window have unique values for the 'ip' and monotonically increasing values for 'timestamp' attributes. The events in the window are inserted into the 'UniqueIps' output stream. The system waits for 2 seconds for the arrival of a new event before flushing the current batch. first (Window) This is a window that holds only the first set of unique events according to the unique key parameter. When a new event arrives with a key that is already in the window, that event is not processed by the window. Origin: siddhi-execution-unique:5.0.4 Syntax unique:first( INT|LONG|FLOAT|BOOL|DOUBLE|STRING unique.key) unique:first( INT|LONG|FLOAT|BOOL|DOUBLE|STRING unique.key, INT|LONG|FLOAT|BOOL|DOUBLE|STRING ...) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic unique.key The attribute that should be checked for uniqueness. If there is more than one parameter to check for uniqueness, it can be specified as an array separated by commas. INT LONG FLOAT BOOL DOUBLE STRING No Yes Examples EXAMPLE 1 define stream LoginEvents (timeStamp long, ip string); from LoginEvents#window.unique:first(ip) insert into UniqueIps ; This returns the first set of unique items that arrive from the 'LoginEvents' stream, and returns them to the 'UniqueIps' stream. The unique events are only those with a unique value for the 'ip' attribute. firstLengthBatch (Window) This is a batch (tumbling) window that holds a specific number of unique events (depending on which events arrive first). The unique events are selected based on a specific parameter that is considered as the unique key. When a new event arrives with a value for the unique key parameter that matches the same of an existing event in the window, that event is not processed by the window. Origin: siddhi-execution-unique:5.0.4 Syntax unique:firstLengthBatch( INT|LONG|FLOAT|BOOL|DOUBLE|STRING unique.key, INT window.length) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic unique.key The attribute that should be checked for uniqueness. INT LONG FLOAT BOOL DOUBLE STRING No Yes window.length The number of events the window should tumble. INT No Yes Examples EXAMPLE 1 define window CseEventWindow (symbol string, price float, volume int) from CseEventStream#window.unique:firstLengthBatch(symbol, 10) select symbol, price, volume insert all events into OutputStream ; The window in this configuration holds the first unique events from the 'CseEventStream' stream every second, and outputs them all into the the 'OutputStream' stream. All the events in a window during a given second should have a unique value for the 'symbol' attribute. firstTimeBatch (Window) A batch-time or tumbling window that holds the unique events according to the unique key parameters that have arrived within the time period of that window and gets updated for each such time window. When a new event arrives with a key which is already in the window, that event is not processed by the window. Origin: siddhi-execution-unique:5.0.4 Syntax unique:firstTimeBatch( INT|LONG|FLOAT|BOOL|DOUBLE|STRING unique.key, INT|LONG window.time) unique:firstTimeBatch( INT|LONG|FLOAT|BOOL|DOUBLE|STRING unique.key, INT|LONG window.time, INT|LONG start.time) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic unique.key The attribute that should be checked for uniqueness. INT LONG FLOAT BOOL DOUBLE STRING No Yes window.time The sliding time period for which the window should hold events. INT LONG No Yes start.time This specifies an offset in milliseconds in order to start the window at a time different to the standard time. Timestamp of the first event. INT LONG Yes Yes Examples EXAMPLE 1 define stream CseEventStream (symbol string, price float, volume int) from CseEventStream#window.unique:firstTimeBatch(symbol,1 sec) select symbol, price, volume insert all events into OutputStream ; This holds the first unique events that arrive from the 'cseEventStream' input stream during each second, based on the symbol,as a batch, and returns all the events to the 'OutputStream'. length (Window) This is a sliding length window that holds the events of the latest window length with the unique key and gets updated for the expiry and arrival of each event. When a new event arrives with the key that is already there in the window, then the previous event expires and new event is kept within the window. Origin: siddhi-execution-unique:5.0.4 Syntax unique:length( INT|LONG|FLOAT|BOOL|DOUBLE|STRING unique.key, INT window.length) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic unique.key The attribute that should be checked for uniqueness. INT LONG FLOAT BOOL DOUBLE STRING No Yes window.length The number of events that should be included in a sliding length window. INT No Yes Examples EXAMPLE 1 define stream CseEventStream (symbol string, price float, volume int) from CseEventStream#window.unique:length(symbol,10) select symbol, price, volume insert all events into OutputStream; In this configuration, the window holds the latest 10 unique events. The latest events are selected based on the symbol attribute. If the 'CseEventStream' receives an event for which the value for the symbol attribute is the same as that of an existing event in the window, the existing event is replaced by the new event. All the events are returned to the 'OutputStream' event stream once an event expires or is added to the window. lengthBatch (Window) This is a batch (tumbling) window that holds a specified number of latest unique events. The unique events are determined based on the value for a specified unique key parameter. The window is updated for every window length, i.e., for the last set of events of the specified number in a tumbling manner. When a new event arrives within the window length having the same value for the unique key parameter as an existing event in the window, the previous event is replaced by the new event. Origin: siddhi-execution-unique:5.0.4 Syntax unique:lengthBatch( INT|LONG|FLOAT|BOOL|DOUBLE|STRING unique.key, INT window.length) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic unique.key The attribute that should be checked for uniqueness. INT LONG FLOAT BOOL DOUBLE STRING No Yes window.length The number of events the window should tumble. INT No Yes Examples EXAMPLE 1 define window CseEventWindow (symbol string, price float, volume int) from CseEventStream#window.unique:lengthBatch(symbol, 10) select symbol, price, volume insert expired events into OutputStream ; In this query, the window at any give time holds the last 10 unique events from the 'CseEventStream' stream. Each of the 10 events within the window at a given time has a unique value for the symbol attribute. If a new event has the same value for the symbol attribute as an existing event within the window length, the existing event expires and it is replaced by the new event. The query returns expired individual events as well as expired batches of events to the 'OutputStream' stream. time (Window) This is a sliding time window that holds the latest unique events that arrived during the previous time window. The unique events are determined based on the value for a specified unique key parameter. The window is updated with the arrival and expiry of each event. When a new event that arrives within a window time period has the same value for the unique key parameter as an existing event in the window, the previous event is replaced by the new event. Origin: siddhi-execution-unique:5.0.4 Syntax unique:time( INT|LONG|FLOAT|BOOL|DOUBLE|STRING unique.key, INT|LONG window.time) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic unique.key The attribute that should be checked for uniqueness. INT LONG FLOAT BOOL DOUBLE STRING No No window.time The sliding time period for which the window should hold events. INT LONG No No Examples EXAMPLE 1 define stream CseEventStream (symbol string, price float, volume int) from CseEventStream#window.unique:time(symbol, 1 sec) select symbol, price, volume insert expired events into OutputStream ; In this query, the window holds the latest unique events that arrived within the last second from the 'CseEventStream', and returns the expired events to the 'OutputStream' stream. During any given second, each event in the window should have a unique value for the 'symbol' attribute. If a new event that arrives within the same second has the same value for the symbol attribute as an existing event in the window, the existing event expires. timeBatch (Window) This is a batch (tumbling) time window that is updated with the latest events based on a unique key parameter. If a new event that arrives within the time period of a windowhas a value for the key parameter which matches that of an existing event, the existing event expires and it is replaced by the latest event. Origin: siddhi-execution-unique:5.0.4 Syntax unique:timeBatch( INT|LONG|FLOAT|BOOL|DOUBLE|STRING unique.key, INT|LONG window.time) unique:timeBatch( INT|LONG|FLOAT|BOOL|DOUBLE|STRING unique.key, INT|LONG window.time, INT|LONG start.time) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic unique.key The attribute that should be checked for uniqueness. INT LONG FLOAT BOOL DOUBLE STRING No Yes window.time The tumbling time period for which the window should hold events. INT LONG No Yes start.time This specifies an offset in milliseconds in order to start the window at a time different to the standard time. Timestamp of first event INT LONG Yes Yes Examples EXAMPLE 1 define stream CseEventStream (symbol string, price float, volume int) from CseEventStream#window.unique:timeBatch(symbol, 1 sec) select symbol, price, volume insert all events into OutputStream ; This window holds the latest unique events that arrive from the 'CseEventStream' at a given time, and returns all the events to the 'OutputStream' stream. It is updated every second based on the latest values for the 'symbol' attribute. timeLengthBatch (Window) This is a batch or tumbling time length window that is updated with the latest events based on a unique key parameter. The window tumbles upon the elapse of the time window, or when a number of unique events have arrived. If a new event that arrives within the period of the window has a value for the key parameter which matches the value of an existing event, the existing event expires and it is replaced by the new event. Origin: siddhi-execution-unique:5.0.4 Syntax unique:timeLengthBatch( INT|LONG|FLOAT|BOOL|DOUBLE|STRING unique.key, INT|LONG window.time, INT window.length) unique:timeLengthBatch( INT|LONG|FLOAT|BOOL|DOUBLE|STRING unique.key, INT|LONG window.time, INT|LONG start.time, INT window.length) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic unique.key The attribute that should be checked for uniqueness. INT LONG FLOAT BOOL DOUBLE STRING No Yes window.time The sliding time period for which the window should hold the events. INT LONG No Yes start.time This specifies an offset in milliseconds in order to start the window at a time different to the standard time. Timestamp of first event INT LONG Yes Yes window.length The number of events the window should tumble. INT No Yes Examples EXAMPLE 1 define stream CseEventStream (symbol string, price float, volume int) from CseEventStream#window.unique:timeLengthBatch(symbol, 1 sec, 20) select symbol, price, volume insert all events into OutputStream; This window holds the latest unique events that arrive from the 'CseEventStream' at a given time, and returns all the events to the 'OutputStream' stream. It is updated every second based on the latest values for the 'symbol' attribute. Unitconversion MmTokm (Function) This converts the input given in megameters into kilometers. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:MmTokm( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from megameters into kilometers. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:MmTokm(1) The megameter value '1' is converted into kilometers as '1000.0' . cmToft (Function) This converts the input given in centimeters into feet. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:cmToft( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from centimeters into feet. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:cmToft(100) The centimeters value '100' is converted into feet as '3.280' . cmToin (Function) This converts the input given in centimeters into inches. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:cmToin( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from centimeters into inches. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:cmToin(100) Input centimeters value '100' is converted into inches as '39.37'. cmTokm (Function) This converts the input value given in centimeters into kilometers. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:cmTokm( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from centimeters into kilometers. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:cmTokm(100) The centimeters value '100' is converted into kilometers as '0.001'. cmTom (Function) This converts the input given in centimeters into meters. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:cmTom( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from centimeters into meters. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:cmTom(100) The centimeters value '100' is converted into meters as '1.0' . cmTomi (Function) This converts the input given in centimeters into miles. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:cmTomi( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from centimeters into miles. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:cmTomi(10000) The centimeters value '10000' is converted into miles as '0.062' . cmTomm (Function) This converts the input given in centimeters into millimeters. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:cmTomm( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from centimeters into millimeters. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:cmTomm(1) The centimeter value '1' is converted into millimeters as '10.0' . cmTonm (Function) This converts the input given in centimeters into nanometers. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:cmTonm( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from centimeters into nanometers. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:cmTonm(1) The centimeter value '1' is converted into nanometers as '10000000' . cmToum (Function) This converts the input in centimeters into micrometers. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:cmToum( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from centimeters into micrometers. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:cmToum(100) The centimeters value '100' is converted into micrometers as '1000000.0' . cmToyd (Function) This converts the input given in centimeters into yards. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:cmToyd( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from centimeters into yards. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:cmToyd(1) The centimeter value '1' is converted into yards as '0.01' . dToh (Function) This converts the input given in days into hours. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:dToh( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from days into hours. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:dToh(1) The day value '1' is converted into hours as '24.0'. gTokg (Function) This converts the input given in grams into kilograms. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:gTokg( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from grams into kilograms. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:gTokg(1000) The grams value '1000' is converted into kilogram as '1.0' . gTomg (Function) This converts the input given in grams into milligrams. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:gTomg( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from grams into milligrams. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:gTomg(1) The gram value '1' is converted into milligrams as '1000.0' . gToug (Function) This converts the input given in grams into micrograms. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:gToug( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from grams into micrograms. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:gToug(1) The gram value '1' is converted into micrograms as '1000000.0' . hTom (Function) This converts the input given in hours into minutes. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:hTom( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from hours into minutes. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:hTom(1) The hour value '1' is converted into minutes as '60.0' . hTos (Function) This converts the input given in hours into seconds. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:hTos( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from hours into seconds. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:hTos(1) The hour value '1' is converted into seconds as '3600.0'. kgToLT (Function) This converts the input given in kilograms into imperial tons. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:kgToLT( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from kilograms into imperial tons. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:kgToLT(1000) The kilograms value '1000' is converted into imperial tons as '0.9842' . kgToST (Function) This converts the input given in kilograms into US tons. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:kgToST( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from kilograms into US tons. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:kgToST(1000) The kilograms value '1000 is converted into US tons as '1.10' . kgTog (Function) This converts the input given in kilograms into grams. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:kgTog( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from kilograms into grams. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:kgTog(1) The kilogram value '1' is converted into grams as '1000'. kgTolb (Function) This converts the input given in kilograms into pounds. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:kgTolb( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from kilograms into pounds. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:kgTolb(1) The kilogram value '1' is converted into pounds as '2.2' . kgTooz (Function) This converts the input given in kilograms into ounces. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:kgTooz( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from kilograms into ounces. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:kgTooz(1) The kilogram value '1' is converted into ounces as ' 35.274' . kgTost (Function) This converts the input given in kilograms into imperial stones. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:kgTost( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from kilograms into imperial stones. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:kgTost(1) The kilogram value '1' is converted into imperial stones as '0.157' . kgTot (Function) This converts the input given in kilograms into tonnes. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:kgTot( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from kilograms into tonnes. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:kgTot(1) The kilogram value '1' is converted into tonnes as '0.001' . kmTocm (Function) This converts the input given in kilometers into centimeters. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:kmTocm( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from kilometers into centimeters. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:kmTocm(1) The kilometer value '1' is converted into centimeters as '100000.0' . kmToft (Function) This converts the input given in kilometers into feet. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:kmToft( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from kilometers into feet. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:kmToft(1) The kilometer value '1' is converted into feet as '3280.8' . kmToin (Function) This converts the input given in kilometers into inches. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:kmToin( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from kilometers into inches. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:kmToin(1) The kilometer value '1' is converted into inches as '39370.08' . kmTom (Function) This converts the input given in kilometers into meters. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:kmTom( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from kilometers into meters. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:kmTom(1) The kilometer value '1' is converted into meters as '1000.0' . kmTomi (Function) This converts the input given in kilometers into miles. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:kmTomi( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from kilometers into miles. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:kmTomi(1) The kilometer value '1' is converted into miles as '0.621' . kmTomm (Function) This converts the input given in kilometers into millimeters. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:kmTomm( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from kilometers into millimeters. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:kmTomm(1) The kilometer value '1' is converted into millimeters as '1000000.0' . kmTonm (Function) This converts the input given in kilometers into nanometers. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:kmTonm( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from kilometers into nanometers. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:kmTonm(1) The kilometer value '1' is converted into nanometers as '1000000000000.0' . kmToum (Function) This converts the input given in kilometers into micrometers. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:kmToum( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from kilometers into micrometers. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:kmToum(1) The kilometer value '1' is converted into micrometers as '1000000000.0' . kmToyd (Function) This converts the input given in kilometers into yards. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:kmToyd( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from kilometers into yards. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:kmToyd(1) The kilometer value '1' is converted into yards as '1093.6' . lTom3 (Function) This converts the input given in liters into cubic meters. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:lTom3( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from liters into cubic meters. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:lTom3(1000) The liters value '1000' is converted into cubic meters as '1' . lToml (Function) This converts the input given in liters into milliliters. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:lToml( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from liters into milliliters. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:lToml(1) The liter value '1' is converted into milliliters as '1000.0' . m3Tol (Function) This converts the input given in cubic meters into liters. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:m3Tol( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from meters into liters. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:m3Tol(1) The cubic meter value '1' is converted into liters as '1000.0' . mTocm (Function) This converts the input given in meters into centimeters. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:mTocm( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from meters into centimeters. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:mTocm(1) The meter value '1' is converted to centimeters as '100.0' . mToft (Function) This converts the input given in meters into feet. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:mToft( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from meters into feet. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:mToft(1) The meter value '1' is converted into feet as '3.280' . mTomm (Function) This converts the input given in meters into millimeters. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:mTomm( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from meters into millimeters. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:mTomm(1) The meter value '1' is converted into millimeters as '1000.0' . mTos (Function) This converts the input given in minutes into seconds. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:mTos( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from minutes into seconds. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:mTos(1) The minute value '1' is converted into seconds as '60.0' . mToyd (Function) This converts the input given in meters into yards. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:mToyd( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from meters into yards. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:mToyd(1) The meter value '1' is converted into yards as '1.093' . miTokm (Function) This converts the input given in miles into kilometers. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:miTokm( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from miles into kilometers. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:miTokm(1) The mile value '1' is converted into kilometers as '1.6' . mlTol (Function) This converts the input given in milliliters into liters. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:mlTol( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from milliliters into liters. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:mlTol(1000) The milliliters value '1000' is converted into liters as '1'. sToms (Function) This converts the input given in seconds into milliseconds. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:sToms( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from seconds into milliseconds. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:sToms(1) The second value '1' is converted into milliseconds as '1000.0' . sTons (Function) This converts the input given in seconds into nanoseconds. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:sTons( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from seconds into nanoseconds. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:sTons(1) The second value '1' is converted into nanoseconds as '1000000000.0' . sTous (Function) This converts the input given in seconds into microseconds. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:sTous( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from seconds into microseconds. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:sTous(1) The second value '1' is converted into microseconds as '1000000.0' . tTog (Function) This converts the input given in tonnes into grams. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:tTog( INT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from Tonnes into grams. INT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:tTog(1) The tonne value '1' is converted into grams as '1000000.0' . tTokg (Function) This converts the input given in tonnes into kilograms. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:tTokg( INT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from tonnes into kilograms. INT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:tTokg(inValue) The tonne value is converted into kilograms as '1000.0' . yTod (Function) This converts the given input in years into days. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:yTod( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from years into days. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:yTod(1) The year value '1' is converted into days as '365.2525' .","title":"5.1.0"},{"location":"docs/api/5.1.0/#api-docs-v510","text":"","title":"API Docs - v5.1.0"},{"location":"docs/api/5.1.0/#core","text":"","title":"Core"},{"location":"docs/api/5.1.0/#and-aggregate-function","text":"Returns the results of AND operation for all the events. Origin: siddhi-core:5.1.7 Syntax BOOL and( BOOL arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be AND operation. BOOL No Yes Examples EXAMPLE 1 from cscStream#window.lengthBatch(10) select and(isFraud) as isFraudTransaction insert into alertStream; This will returns the result for AND operation of isFraud values as a boolean value for event chunk expiry by window length batch.","title":"and (Aggregate Function)"},{"location":"docs/api/5.1.0/#avg-aggregate-function","text":"Calculates the average for all the events. Origin: siddhi-core:5.1.7 Syntax DOUBLE avg( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that need to be averaged. INT LONG DOUBLE FLOAT No Yes Examples EXAMPLE 1 from fooStream#window.timeBatch select avg(temp) as avgTemp insert into barStream; avg(temp) returns the average temp value for all the events based on their arrival and expiry.","title":"avg (Aggregate Function)"},{"location":"docs/api/5.1.0/#count-aggregate-function","text":"Returns the count of all the events. Origin: siddhi-core:5.1.7 Syntax LONG count() LONG count( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg This function accepts one parameter. It can belong to any one of the available types. INT LONG DOUBLE FLOAT STRING BOOL OBJECT Yes Yes Examples EXAMPLE 1 from fooStream#window.timeBatch(10 sec) select count() as count insert into barStream; This will return the count of all the events for time batch in 10 seconds.","title":"count (Aggregate Function)"},{"location":"docs/api/5.1.0/#distinctcount-aggregate-function","text":"This returns the count of distinct occurrences for a given arg. Origin: siddhi-core:5.1.7 Syntax LONG distinctCount( INT|LONG|DOUBLE|FLOAT|STRING arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The object for which the number of distinct occurences needs to be counted. INT LONG DOUBLE FLOAT STRING No Yes Examples EXAMPLE 1 from fooStream select distinctcount(pageID) as count insert into barStream; distinctcount(pageID) for the following output returns '3' when the available values are as follows. \"WEB_PAGE_1\" \"WEB_PAGE_1\" \"WEB_PAGE_2\" \"WEB_PAGE_3\" \"WEB_PAGE_1\" \"WEB_PAGE_2\" The three distinct occurences identified are 'WEB_PAGE_1', 'WEB_PAGE_2', and 'WEB_PAGE_3'.","title":"distinctCount (Aggregate Function)"},{"location":"docs/api/5.1.0/#max-aggregate-function","text":"Returns the maximum value for all the events. Origin: siddhi-core:5.1.7 Syntax INT|LONG|DOUBLE|FLOAT max( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be compared to find the maximum value. INT LONG DOUBLE FLOAT No Yes Examples EXAMPLE 1 from fooStream#window.timeBatch(10 sec) select max(temp) as maxTemp insert into barStream; max(temp) returns the maximum temp value recorded for all the events based on their arrival and expiry.","title":"max (Aggregate Function)"},{"location":"docs/api/5.1.0/#maxforever-aggregate-function","text":"This is the attribute aggregator to store the maximum value for a given attribute throughout the lifetime of the query regardless of any windows in-front. Origin: siddhi-core:5.1.7 Syntax INT|LONG|DOUBLE|FLOAT maxForever( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be compared to find the maximum value. INT LONG DOUBLE FLOAT No Yes Examples EXAMPLE 1 from inputStream select maxForever(temp) as max insert into outputStream; maxForever(temp) returns the maximum temp value recorded for all the events throughout the lifetime of the query.","title":"maxForever (Aggregate Function)"},{"location":"docs/api/5.1.0/#min-aggregate-function","text":"Returns the minimum value for all the events. Origin: siddhi-core:5.1.7 Syntax INT|LONG|DOUBLE|FLOAT min( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be compared to find the minimum value. INT LONG DOUBLE FLOAT No Yes Examples EXAMPLE 1 from inputStream select min(temp) as minTemp insert into outputStream; min(temp) returns the minimum temp value recorded for all the events based on their arrival and expiry.","title":"min (Aggregate Function)"},{"location":"docs/api/5.1.0/#minforever-aggregate-function","text":"This is the attribute aggregator to store the minimum value for a given attribute throughout the lifetime of the query regardless of any windows in-front. Origin: siddhi-core:5.1.7 Syntax INT|LONG|DOUBLE|FLOAT minForever( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be compared to find the minimum value. INT LONG DOUBLE FLOAT No Yes Examples EXAMPLE 1 from inputStream select minForever(temp) as max insert into outputStream; minForever(temp) returns the minimum temp value recorded for all the events throughoutthe lifetime of the query.","title":"minForever (Aggregate Function)"},{"location":"docs/api/5.1.0/#or-aggregate-function","text":"Returns the results of OR operation for all the events. Origin: siddhi-core:5.1.7 Syntax BOOL or( BOOL arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be OR operation. BOOL No Yes Examples EXAMPLE 1 from cscStream#window.lengthBatch(10) select or(isFraud) as isFraudTransaction insert into alertStream; This will returns the result for OR operation of isFraud values as a boolean value for event chunk expiry by window length batch.","title":"or (Aggregate Function)"},{"location":"docs/api/5.1.0/#stddev-aggregate-function","text":"Returns the calculated standard deviation for all the events. Origin: siddhi-core:5.1.7 Syntax DOUBLE stdDev( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that should be used to calculate the standard deviation. INT LONG DOUBLE FLOAT No Yes Examples EXAMPLE 1 from inputStream select stddev(temp) as stdTemp insert into outputStream; stddev(temp) returns the calculated standard deviation of temp for all the events based on their arrival and expiry.","title":"stdDev (Aggregate Function)"},{"location":"docs/api/5.1.0/#sum-aggregate-function","text":"Returns the sum for all the events. Origin: siddhi-core:5.1.7 Syntax LONG|DOUBLE sum( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be summed. INT LONG DOUBLE FLOAT No Yes Examples EXAMPLE 1 from inputStream select sum(volume) as sumOfVolume insert into outputStream; This will returns the sum of volume values as a long value for each event arrival and expiry.","title":"sum (Aggregate Function)"},{"location":"docs/api/5.1.0/#unionset-aggregate-function","text":"Union multiple sets. This attribute aggregator maintains a union of sets. The given input set is put into the union set and the union set is returned. Origin: siddhi-core:5.1.7 Syntax OBJECT unionSet( OBJECT set) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic set The java.util.Set object that needs to be added into the union set. OBJECT No Yes Examples EXAMPLE 1 from stockStream select createSet(symbol) as initialSet insert into initStream from initStream#window.timeBatch(10 sec) select unionSet(initialSet) as distinctSymbols insert into distinctStockStream; distinctStockStream will return the set object which contains the distinct set of stock symbols received during a sliding window of 10 seconds.","title":"unionSet (Aggregate Function)"},{"location":"docs/api/5.1.0/#uuid-function","text":"Generates a UUID (Universally Unique Identifier). Origin: siddhi-core:5.1.7 Syntax STRING UUID() Examples EXAMPLE 1 from TempStream select convert(roomNo, 'string') as roomNo, temp, UUID() as messageID insert into RoomTempStream; This will converts a room number to string, introducing a message ID to each event asUUID() returns a34eec40-32c2-44fe-8075-7f4fde2e2dd8 from TempStream select convert(roomNo, 'string') as roomNo, temp, UUID() as messageID insert into RoomTempStream;","title":"UUID (Function)"},{"location":"docs/api/5.1.0/#cast-function","text":"Converts the first parameter according to the cast.to parameter. Incompatible arguments cause Class Cast exceptions if further processed. This function is used with map extension that returns attributes of the object type. You can use this function to cast the object to an accurate and concrete type. Origin: siddhi-core:5.1.7 Syntax INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT cast( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT to.be.caster, STRING cast.to) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic to.be.caster This specifies the attribute to be casted. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No Yes cast.to A string constant parameter expressing the cast to type using one of the following strings values: int, long, float, double, string, bool. STRING No Yes Examples EXAMPLE 1 from fooStream select symbol as name, cast(temp, 'double') as temp insert into barStream; This will cast the fooStream temp field value into 'double' format.","title":"cast (Function)"},{"location":"docs/api/5.1.0/#coalesce-function","text":"Returns the value of the first input parameter that is not null, and all input parameters have to be on the same type. Origin: siddhi-core:5.1.7 Syntax INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT coalesce( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg, INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT ...) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg This function accepts one or more parameters. They can belong to any one of the available types. All the specified parameters should be of the same type. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No Yes Examples EXAMPLE 1 from fooStream select coalesce('123', null, '789') as value insert into barStream; This will returns first null value 123. EXAMPLE 2 from fooStream select coalesce(null, 76, 567) as value insert into barStream; This will returns first null value 76. EXAMPLE 3 from fooStream select coalesce(null, null, null) as value insert into barStream; This will returns null as there are no notnull values.","title":"coalesce (Function)"},{"location":"docs/api/5.1.0/#convert-function","text":"Converts the first input parameter according to the convertedTo parameter. Origin: siddhi-core:5.1.7 Syntax INT|LONG|DOUBLE|FLOAT|STRING|BOOL convert( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT to.be.converted, STRING converted.to) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic to.be.converted This specifies the value to be converted. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No Yes converted.to A string constant parameter to which type the attribute need to be converted using one of the following strings values: 'int', 'long', 'float', 'double', 'string', 'bool'. STRING No Yes Examples EXAMPLE 1 from fooStream select convert(temp, 'double') as temp insert into barStream; This will convert fooStream temp value into 'double'. EXAMPLE 2 from fooStream select convert(temp, 'int') as temp insert into barStream; This will convert fooStream temp value into 'int' (value = \"convert(45.9, 'int') returns 46\").","title":"convert (Function)"},{"location":"docs/api/5.1.0/#createset-function","text":"Includes the given input parameter in a java.util.HashSet and returns the set. Origin: siddhi-core:5.1.7 Syntax OBJECT createSet( INT|LONG|DOUBLE|FLOAT|STRING|BOOL input) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input The input that needs to be added into the set. INT LONG DOUBLE FLOAT STRING BOOL No Yes Examples EXAMPLE 1 from stockStream select createSet(symbol) as initialSet insert into initStream; For every incoming stockStream event, the initStream stream will produce a set object having only one element: the symbol in the incoming stockStream.","title":"createSet (Function)"},{"location":"docs/api/5.1.0/#currenttimemillis-function","text":"Returns the current timestamp of siddhi application in milliseconds. Origin: siddhi-core:5.1.7 Syntax LONG currentTimeMillis() Examples EXAMPLE 1 from fooStream select symbol as name, currentTimeMillis() as eventTimestamp insert into barStream; This will extract current siddhi application timestamp.","title":"currentTimeMillis (Function)"},{"location":"docs/api/5.1.0/#default-function","text":"Checks if the 'attribute' parameter is null and if so returns the value of the 'default' parameter Origin: siddhi-core:5.1.7 Syntax INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT default( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT attribute, INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT default) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic attribute The attribute that could be null. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No Yes default The default value that will be used when 'attribute' parameter is null INT LONG DOUBLE FLOAT STRING BOOL OBJECT No Yes Examples EXAMPLE 1 from TempStream select default(temp, 0.0) as temp, roomNum insert into StandardTempStream; This will replace TempStream's temp attribute with default value if the temp is null.","title":"default (Function)"},{"location":"docs/api/5.1.0/#eventtimestamp-function","text":"Returns the timestamp of the processed event. Origin: siddhi-core:5.1.7 Syntax LONG eventTimestamp() Examples EXAMPLE 1 from fooStream select symbol as name, eventTimestamp() as eventTimestamp insert into barStream; This will extract current events timestamp.","title":"eventTimestamp (Function)"},{"location":"docs/api/5.1.0/#ifthenelse-function","text":"Evaluates the 'condition' parameter and returns value of the 'if.expression' parameter if the condition is true, or returns value of the 'else.expression' parameter if the condition is false. Here both 'if.expression' and 'else.expression' should be of the same type. Origin: siddhi-core:5.1.7 Syntax INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT ifThenElse( BOOL condition, INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT if.expression, INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT else.expression) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic condition This specifies the if then else condition value. BOOL No Yes if.expression This specifies the value to be returned if the value of the condition parameter is true. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No Yes else.expression This specifies the value to be returned if the value of the condition parameter is false. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No Yes Examples EXAMPLE 1 @info(name = 'query1') from sensorEventStream select sensorValue, ifThenElse(sensorValue 35,'High','Low') as status insert into outputStream; This will returns High if sensorValue = 50. EXAMPLE 2 @info(name = 'query1') from sensorEventStream select sensorValue, ifThenElse(voltage 5, 0, 1) as status insert into outputStream; This will returns 1 if voltage= 12. EXAMPLE 3 @info(name = 'query1') from userEventStream select userName, ifThenElse(password == 'admin', true, false) as passwordState insert into outputStream; This will returns passwordState as true if password = admin.","title":"ifThenElse (Function)"},{"location":"docs/api/5.1.0/#instanceofboolean-function","text":"Checks whether the parameter is an instance of Boolean or not. Origin: siddhi-core:5.1.7 Syntax BOOL instanceOfBoolean( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No Yes Examples EXAMPLE 1 from fooStream select instanceOfBoolean(switchState) as state insert into barStream; This will return true if the value of switchState is true. EXAMPLE 2 from fooStream select instanceOfBoolean(value) as state insert into barStream; if the value = 32 then this will returns false as the value is not an instance of the boolean.","title":"instanceOfBoolean (Function)"},{"location":"docs/api/5.1.0/#instanceofdouble-function","text":"Checks whether the parameter is an instance of Double or not. Origin: siddhi-core:5.1.7 Syntax BOOL instanceOfDouble( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No Yes Examples EXAMPLE 1 from fooStream select instanceOfDouble(value) as state insert into barStream; This will return true if the value field format is double ex : 56.45. EXAMPLE 2 from fooStream select instanceOfDouble(switchState) as state insert into barStream; if the switchState = true then this will returns false as the value is not an instance of the double.","title":"instanceOfDouble (Function)"},{"location":"docs/api/5.1.0/#instanceoffloat-function","text":"Checks whether the parameter is an instance of Float or not. Origin: siddhi-core:5.1.7 Syntax BOOL instanceOfFloat( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No Yes Examples EXAMPLE 1 from fooStream select instanceOfFloat(value) as state insert into barStream; This will return true if the value field format is float ex : 56.45f. EXAMPLE 2 from fooStream select instanceOfFloat(switchState) as state insert into barStream; if the switchState = true then this will returns false as the value is an instance of the boolean not a float.","title":"instanceOfFloat (Function)"},{"location":"docs/api/5.1.0/#instanceofinteger-function","text":"Checks whether the parameter is an instance of Integer or not. Origin: siddhi-core:5.1.7 Syntax BOOL instanceOfInteger( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No Yes Examples EXAMPLE 1 from fooStream select instanceOfInteger(value) as state insert into barStream; This will return true if the value field format is integer. EXAMPLE 2 from fooStream select instanceOfInteger(switchState) as state insert into barStream; if the switchState = true then this will returns false as the value is an instance of the boolean not a long.","title":"instanceOfInteger (Function)"},{"location":"docs/api/5.1.0/#instanceoflong-function","text":"Checks whether the parameter is an instance of Long or not. Origin: siddhi-core:5.1.7 Syntax BOOL instanceOfLong( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No Yes Examples EXAMPLE 1 from fooStream select instanceOfLong(value) as state insert into barStream; This will return true if the value field format is long ex : 56456l. EXAMPLE 2 from fooStream select instanceOfLong(switchState) as state insert into barStream; if the switchState = true then this will returns false as the value is an instance of the boolean not a long.","title":"instanceOfLong (Function)"},{"location":"docs/api/5.1.0/#instanceofstring-function","text":"Checks whether the parameter is an instance of String or not. Origin: siddhi-core:5.1.7 Syntax BOOL instanceOfString( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No Yes Examples EXAMPLE 1 from fooStream select instanceOfString(value) as state insert into barStream; This will return true if the value field format is string ex : 'test'. EXAMPLE 2 from fooStream select instanceOfString(switchState) as state insert into barStream; if the switchState = true then this will returns false as the value is an instance of the boolean not a string.","title":"instanceOfString (Function)"},{"location":"docs/api/5.1.0/#maximum-function","text":"Returns the maximum value of the input parameters. Origin: siddhi-core:5.1.7 Syntax INT|LONG|DOUBLE|FLOAT maximum( INT|LONG|DOUBLE|FLOAT arg, INT|LONG|DOUBLE|FLOAT ...) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg This function accepts one or more parameters. They can belong to any one of the available types. All the specified parameters should be of the same type. INT LONG DOUBLE FLOAT No Yes Examples EXAMPLE 1 @info(name = 'query1') from inputStream select maximum(price1, price2, price3) as max insert into outputStream; This will returns the maximum value of the input parameters price1, price2, price3.","title":"maximum (Function)"},{"location":"docs/api/5.1.0/#minimum-function","text":"Returns the minimum value of the input parameters. Origin: siddhi-core:5.1.7 Syntax INT|LONG|DOUBLE|FLOAT minimum( INT|LONG|DOUBLE|FLOAT arg, INT|LONG|DOUBLE|FLOAT ...) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg This function accepts one or more parameters. They can belong to any one of the available types. All the specified parameters should be of the same type. INT LONG DOUBLE FLOAT No Yes Examples EXAMPLE 1 @info(name = 'query1') from inputStream select maximum(price1, price2, price3) as max insert into outputStream; This will returns the minimum value of the input parameters price1, price2, price3.","title":"minimum (Function)"},{"location":"docs/api/5.1.0/#sizeofset-function","text":"Returns the size of an object of type java.util.Set. Origin: siddhi-core:5.1.7 Syntax INT sizeOfSet( OBJECT set) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic set The set object. This parameter should be of type java.util.Set. A set object may be created by the 'set' attribute aggregator in Siddhi. OBJECT No Yes Examples EXAMPLE 1 from stockStream select initSet(symbol) as initialSet insert into initStream; ;from initStream#window.timeBatch(10 sec) select union(initialSet) as distinctSymbols insert into distinctStockStream; from distinctStockStream select sizeOfSet(distinctSymbols) sizeOfSymbolSet insert into sizeStream; The sizeStream stream will output the number of distinct stock symbols received during a sliding window of 10 seconds.","title":"sizeOfSet (Function)"},{"location":"docs/api/5.1.0/#pol2cart-stream-function","text":"The pol2Cart function calculating the cartesian coordinates x y for the given theta, rho coordinates and adding them as new attributes to the existing events. Origin: siddhi-core:5.1.7 Syntax pol2Cart( DOUBLE theta, DOUBLE rho) pol2Cart( DOUBLE theta, DOUBLE rho, DOUBLE z) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic theta The theta value of the coordinates. DOUBLE No Yes rho The rho value of the coordinates. DOUBLE No Yes z z value of the cartesian coordinates. If z value is not given, drop the third parameter of the output. DOUBLE Yes Yes Examples EXAMPLE 1 from PolarStream#pol2Cart(theta, rho) select x, y insert into outputStream ; This will return cartesian coordinates (4.99953024681082, 0.06853693328228748) for theta: 0.7854 and rho: 5. EXAMPLE 2 from PolarStream#pol2Cart(theta, rho, 3.4) select x, y, z insert into outputStream ; This will return cartesian coordinates (4.99953024681082, 0.06853693328228748, 3.4)for theta: 0.7854 and rho: 5 and z: 3.4.","title":"pol2Cart (Stream Function)"},{"location":"docs/api/5.1.0/#log-stream-processor","text":"Logs the message on the given priority with or without the processed event. Origin: siddhi-core:5.1.7 Syntax log() log( STRING log.message) log( BOOL is.event.logged) log( STRING log.message, BOOL is.event.logged) log( STRING priority, STRING log.message) log( STRING priority, STRING log.message, BOOL is.event.logged) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic priority The priority/type of this log message (INFO, DEBUG, WARN, FATAL, ERROR, OFF, TRACE). INFO STRING Yes No log.message This message will be logged. : STRING Yes Yes is.event.logged To log the processed event. true BOOL Yes No Examples EXAMPLE 1 from FooStream#log() select * insert into BarStream; Logs events with SiddhiApp name message prefix on default log level INFO. EXAMPLE 2 from FooStream#log(\"Sample Event :\") select * insert into BarStream; Logs events with the message prefix \"Sample Event :\" on default log level INFO. EXAMPLE 3 from FooStream#log(\"DEBUG\", \"Sample Event :\", true) select * insert into BarStream; Logs events with the message prefix \"Sample Event :\" on log level DEBUG. EXAMPLE 4 from FooStream#log(\"Event Arrived\", false) select * insert into BarStream; For each event logs a message \"Event Arrived\" on default log level INFO. EXAMPLE 5 from FooStream#log(\"Sample Event :\", true) select * insert into BarStream; Logs events with the message prefix \"Sample Event :\" on default log level INFO. EXAMPLE 6 from FooStream#log(true) select * insert into BarStream; Logs events with on default log level INFO.","title":"log (Stream Processor)"},{"location":"docs/api/5.1.0/#batch-window","text":"A window that holds an incoming events batch. When a new set of events arrives, the previously arrived old events will be expired. Batch window can be used to aggregate events that comes in batches. If it has the parameter length specified, then batch window process the batch as several chunks. Origin: siddhi-core:5.1.7 Syntax batch() batch( INT window.length) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.length The length of a chunk If length value was not given it assign 0 as length and process the whole batch as once INT Yes No Examples EXAMPLE 1 define stream consumerItemStream (itemId string, price float) from consumerItemStream#window.batch() select price, str:groupConcat(itemId) as itemIds group by price insert into outputStream; This will output comma separated items IDs that have the same price for each incoming batch of events.","title":"batch (Window)"},{"location":"docs/api/5.1.0/#cron-window","text":"This window outputs the arriving events as and when they arrive, and resets (expires) the window periodically based on the given cron expression. Origin: siddhi-core:5.1.7 Syntax cron( STRING cron.expression) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic cron.expression The cron expression that resets the window. STRING No No Examples EXAMPLE 1 define stream InputEventStream (symbol string, price float, volume int); @info(name = 'query1') from InputEventStream#cron('*/5 * * * * ?') select symbol, sum(price) as totalPrice insert into OutputStream; This let the totalPrice to gradually increase and resets to zero as a batch every 5 seconds. EXAMPLE 2 define stream StockEventStream (symbol string, price float, volume int) define window StockEventWindow (symbol string, price float, volume int) cron('*/5 * * * * ?'); @info(name = 'query0') from StockEventStream insert into StockEventWindow; @info(name = 'query1') from StockEventWindow select symbol, sum(price) as totalPrice insert into OutputStream ; The defined window will let the totalPrice to gradually increase and resets to zero as a batch every 5 seconds.","title":"cron (Window)"},{"location":"docs/api/5.1.0/#delay-window","text":"A delay window holds events for a specific time period that is regarded as a delay period before processing them. Origin: siddhi-core:5.1.7 Syntax delay( INT|LONG|TIME window.delay) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.delay The time period (specified in sec, min, ms) for which the window should delay the events. INT LONG TIME No No Examples EXAMPLE 1 define window delayWindow(symbol string, volume int) delay(1 hour); define stream PurchaseStream(symbol string, volume int); define stream DeliveryStream(symbol string); define stream OutputStream(symbol string); @info(name='query1') from PurchaseStream select symbol, volume insert into delayWindow; @info(name='query2') from delayWindow join DeliveryStream on delayWindow.symbol == DeliveryStream.symbol select delayWindow.symbol insert into OutputStream; In this example, purchase events that arrive in the 'PurchaseStream' stream are directed to a delay window. At any given time, this delay window holds purchase events that have arrived within the last hour. These purchase events in the window are matched by the 'symbol' attribute, with delivery events that arrive in the 'DeliveryStream' stream. This monitors whether the delivery of products is done with a minimum delay of one hour after the purchase.","title":"delay (Window)"},{"location":"docs/api/5.1.0/#externaltime-window","text":"A sliding time window based on external time. It holds events that arrived during the last windowTime period from the external timestamp, and gets updated on every monotonically increasing timestamp. Origin: siddhi-core:5.1.7 Syntax externalTime( LONG timestamp, INT|LONG|TIME window.time) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic timestamp The time which the window determines as current time and will act upon. The value of this parameter should be monotonically increasing. LONG No Yes window.time The sliding time period for which the window should hold events. INT LONG TIME No No Examples EXAMPLE 1 define window cseEventWindow (symbol string, price float, volume int) externalTime(eventTime, 20 sec) output expired events; @info(name = 'query0') from cseEventStream insert into cseEventWindow; @info(name = 'query1') from cseEventWindow select symbol, sum(price) as price insert expired events into outputStream ; processing events arrived within the last 20 seconds from the eventTime and output expired events.","title":"externalTime (Window)"},{"location":"docs/api/5.1.0/#externaltimebatch-window","text":"A batch (tumbling) time window based on external time, that holds events arrived during windowTime periods, and gets updated for every windowTime. Origin: siddhi-core:5.1.7 Syntax externalTimeBatch( LONG timestamp, INT|LONG|TIME window.time) externalTimeBatch( LONG timestamp, INT|LONG|TIME window.time, INT|LONG|TIME start.time) externalTimeBatch( LONG timestamp, INT|LONG|TIME window.time, INT|LONG|TIME start.time, INT|LONG|TIME timeout) externalTimeBatch( LONG timestamp, INT|LONG|TIME window.time, INT|LONG|TIME start.time, INT|LONG|TIME timeout, BOOL replace.with.batchtime) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic timestamp The time which the window determines as current time and will act upon. The value of this parameter should be monotonically increasing. LONG No Yes window.time The batch time period for which the window should hold events. INT LONG TIME No No start.time User defined start time. This could either be a constant (of type int, long or time) or an attribute of the corresponding stream (of type long). If an attribute is provided, initial value of attribute would be considered as startTime. Timestamp of first event INT LONG TIME Yes Yes timeout Time to wait for arrival of new event, before flushing and giving output for events belonging to a specific batch. System waits till an event from next batch arrives to flush current batch INT LONG TIME Yes No replace.with.batchtime This indicates to replace the expired event timeStamp as the batch end timeStamp System waits till an event from next batch arrives to flush current batch BOOL Yes No Examples EXAMPLE 1 define window cseEventWindow (symbol string, price float, volume int) externalTimeBatch(eventTime, 1 sec) output expired events; @info(name = 'query0') from cseEventStream insert into cseEventWindow; @info(name = 'query1') from cseEventWindow select symbol, sum(price) as price insert expired events into outputStream ; This will processing events that arrive every 1 seconds from the eventTime. EXAMPLE 2 define window cseEventWindow (symbol string, price float, volume int) externalTimeBatch(eventTime, 20 sec, 0) output expired events; This will processing events that arrive every 1 seconds from the eventTime. Starts on 0 th millisecond of an hour. EXAMPLE 3 define window cseEventWindow (symbol string, price float, volume int) externalTimeBatch(eventTime, 2 sec, eventTimestamp, 100) output expired events; This will processing events that arrive every 2 seconds from the eventTim. Considers the first event's eventTimestamp value as startTime. Waits 100 milliseconds for the arrival of a new event before flushing current batch.","title":"externalTimeBatch (Window)"},{"location":"docs/api/5.1.0/#frequent-window","text":"Deprecated This window returns the latest events with the most frequently occurred value for a given attribute(s). Frequency calculation for this window processor is based on Misra-Gries counting algorithm. Origin: siddhi-core:5.1.7 Syntax frequent( INT event.count) frequent( INT event.count, STRING attribute) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic event.count The number of most frequent events to be emitted to the stream. INT No No attribute The attributes to group the events. If no attributes are given, the concatenation of all the attributes of the event is considered. The concatenation of all the attributes of the event is considered. STRING Yes Yes Examples EXAMPLE 1 @info(name = 'query1') from purchase[price = 30]#window.frequent(2) select cardNo, price insert all events into PotentialFraud; This will returns the 2 most frequent events. EXAMPLE 2 @info(name = 'query1') from purchase[price = 30]#window.frequent(2, cardNo) select cardNo, price insert all events into PotentialFraud; This will returns the 2 latest events with the most frequently appeared card numbers.","title":"frequent (Window)"},{"location":"docs/api/5.1.0/#length-window","text":"A sliding length window that holds the last 'window.length' events at a given time, and gets updated for each arrival and expiry. Origin: siddhi-core:5.1.7 Syntax length( INT window.length) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.length The number of events that should be included in a sliding length window. INT No No Examples EXAMPLE 1 define window StockEventWindow (symbol string, price float, volume int) length(10) output all events; @info(name = 'query0') from StockEventStream insert into StockEventWindow; @info(name = 'query1') from StockEventWindow select symbol, sum(price) as price insert all events into outputStream ; This will process last 10 events in a sliding manner.","title":"length (Window)"},{"location":"docs/api/5.1.0/#lengthbatch-window","text":"A batch (tumbling) length window that holds and process a number of events as specified in the window.length. Origin: siddhi-core:5.1.7 Syntax lengthBatch( INT window.length) lengthBatch( INT window.length, BOOL stream.current.event) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.length The number of events the window should tumble. INT No No stream.current.event Let the window stream the current events out as and when they arrive to the window while expiring them in batches. false BOOL Yes No Examples EXAMPLE 1 define stream InputEventStream (symbol string, price float, volume int); @info(name = 'query1') from InputEventStream#lengthBatch(10) select symbol, sum(price) as price insert into OutputStream; This collect and process 10 events as a batch and output them. EXAMPLE 2 define stream InputEventStream (symbol string, price float, volume int); @info(name = 'query1') from InputEventStream#lengthBatch(10, true) select symbol, sum(price) as sumPrice insert into OutputStream; This window sends the arriving events directly to the output letting the sumPrice to increase gradually, after every 10 events it clears the window as a batch and resets the sumPrice to zero. EXAMPLE 3 define stream InputEventStream (symbol string, price float, volume int); define window StockEventWindow (symbol string, price float, volume int) lengthBatch(10) output all events; @info(name = 'query0') from InputEventStream insert into StockEventWindow; @info(name = 'query1') from StockEventWindow select symbol, sum(price) as price insert all events into OutputStream ; This uses an defined window to process 10 events as a batch and output all events.","title":"lengthBatch (Window)"},{"location":"docs/api/5.1.0/#lossyfrequent-window","text":"Deprecated This window identifies and returns all the events of which the current frequency exceeds the value specified for the supportThreshold parameter. Origin: siddhi-core:5.1.7 Syntax lossyFrequent( DOUBLE support.threshold) lossyFrequent( DOUBLE support.threshold, DOUBLE error.bound) lossyFrequent( DOUBLE support.threshold, DOUBLE error.bound, STRING attribute) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic support.threshold The support threshold value. DOUBLE No No error.bound The error bound value. support.threshold /10 DOUBLE Yes No attribute The attributes to group the events. If no attributes are given, the concatenation of all the attributes of the event is considered. The concatenation of all the attributes of the event is considered. STRING Yes Yes Examples EXAMPLE 1 define stream purchase (cardNo string, price float); define window purchaseWindow (cardNo string, price float) lossyFrequent(0.1, 0.01); @info(name = 'query0') from purchase[price = 30] insert into purchaseWindow; @info(name = 'query1') from purchaseWindow select cardNo, price insert all events into PotentialFraud; lossyFrequent(0.1, 0.01) returns all the events of which the current frequency exceeds 0.1, with an error bound of 0.01. EXAMPLE 2 define stream purchase (cardNo string, price float); define window purchaseWindow (cardNo string, price float) lossyFrequent(0.3, 0.05, cardNo); @info(name = 'query0') from purchase[price = 30] insert into purchaseWindow; @info(name = 'query1') from purchaseWindow select cardNo, price insert all events into PotentialFraud; lossyFrequent(0.3, 0.05, cardNo) returns all the events of which the cardNo attributes frequency exceeds 0.3, with an error bound of 0.05.","title":"lossyFrequent (Window)"},{"location":"docs/api/5.1.0/#session-window","text":"This is a session window that holds events that belong to a specific session. The events that belong to a specific session are identified by a grouping attribute (i.e., a session key). A session gap period is specified to determine the time period after which the session is considered to be expired. A new event that arrives with a specific value for the session key is matched with the session window with the same session key. There can be out of order and late arrival of events, these events can arrive after the session is expired, to include those events to the matching session key specify a latency time period that is less than the session gap period.To have aggregate functions with session windows, the events need to be grouped by the session key via a 'group by' clause. Origin: siddhi-core:5.1.7 Syntax session( INT|LONG|TIME window.session) session( INT|LONG|TIME window.session, STRING window.key) session( INT|LONG|TIME window.session, STRING window.key, INT|LONG|TIME window.allowed.latency) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.session The time period for which the session considered is valid. This is specified in seconds, minutes, or milliseconds (i.e., 'min', 'sec', or 'ms'. INT LONG TIME No No window.key The grouping attribute for events. default-key STRING Yes Yes window.allowed.latency This specifies the time period for which the session window is valid after the expiration of the session. The time period specified here should be less than the session time gap (which is specified via the 'window.session' parameter). 0 INT LONG TIME Yes No Examples EXAMPLE 1 define stream PurchaseEventStream (user string, item_number int, price float, quantity int); @info(name='query0) from PurchaseEventStream#window.session(5 sec, user, 2 sec) select * insert all events into OutputStream; This query processes events that arrive at the PurchaseEvent input stream. The 'user' attribute is the session key, and the session gap is 5 seconds. '2 sec' is specified as the allowed latency. Therefore, events with the matching user name that arrive 2 seconds after the expiration of the session are also considered when performing aggregations for the session identified by the given user name.","title":"session (Window)"},{"location":"docs/api/5.1.0/#sort-window","text":"This window holds a batch of events that equal the number specified as the windowLength and sorts them in the given order. Origin: siddhi-core:5.1.7 Syntax sort( INT window.length, STRING|DOUBLE|INT|LONG|FLOAT|LONG attribute) sort( INT window.length, STRING|DOUBLE|INT|LONG|FLOAT|LONG attribute, STRING order, STRING ...) sort( INT window.length, STRING|DOUBLE|INT|LONG|FLOAT|LONG attribute, STRING order, STRING|DOUBLE|INT|LONG|FLOAT|LONG attribute, STRING|DOUBLE|INT|LONG|FLOAT|LONG ...) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.length The size of the window length. INT No No attribute The attribute that should be checked for the order. The concatenation of all the attributes of the event is considered. STRING DOUBLE INT LONG FLOAT LONG No Yes order The order define as \"asc\" or \"desc\". asc STRING Yes No Examples EXAMPLE 1 define stream cseEventStream (symbol string, price float, volume long); define window cseEventWindow (symbol string, price float, volume long) sort(2,volume, 'asc'); @info(name = 'query0') from cseEventStream insert into cseEventWindow; @info(name = 'query1') from cseEventWindow select volume insert all events into outputStream ; sort(5, price, 'asc') keeps the events sorted by price in the ascending order. Therefore, at any given time, the window contains the 5 lowest prices.","title":"sort (Window)"},{"location":"docs/api/5.1.0/#time-window","text":"A sliding time window that holds events that arrived during the last windowTime period at a given time, and gets updated for each event arrival and expiry. Origin: siddhi-core:5.1.7 Syntax time( INT|LONG|TIME window.time) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.time The sliding time period for which the window should hold events. INT LONG TIME No No Examples EXAMPLE 1 define window cseEventWindow (symbol string, price float, volume int) time(20) output all events; @info(name = 'query0') from cseEventStream insert into cseEventWindow; @info(name = 'query1') from cseEventWindow select symbol, sum(price) as price insert all events into outputStream ; This will processing events that arrived within the last 20 milliseconds.","title":"time (Window)"},{"location":"docs/api/5.1.0/#timebatch-window","text":"A batch (tumbling) time window that holds and process events that arrive during 'window.time' period as a batch. Origin: siddhi-core:5.1.7 Syntax timeBatch( INT|LONG|TIME window.time) timeBatch( INT|LONG|TIME window.time, INT|LONG start.time) timeBatch( INT|LONG|TIME window.time, BOOL stream.current.event) timeBatch( INT|LONG|TIME window.time, INT|LONG start.time, BOOL stream.current.event) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.time The batch time period in which the window process the events. INT LONG TIME No No start.time This specifies an offset in milliseconds in order to start the window at a time different to the standard time. Timestamp of first event INT LONG Yes No stream.current.event Let the window stream the current events out as and when they arrive to the window while expiring them in batches. false BOOL Yes No Examples EXAMPLE 1 define stream InputEventStream (symbol string, price float, volume int); @info(name = 'query1') from InputEventStream#timeBatch(20 sec) select symbol, sum(price) as price insert into OutputStream; This collect and process incoming events as a batch every 20 seconds and output them. EXAMPLE 2 define stream InputEventStream (symbol string, price float, volume int); @info(name = 'query1') from InputEventStream#timeBatch(20 sec, true) select symbol, sum(price) as sumPrice insert into OutputStream; This window sends the arriving events directly to the output letting the sumPrice to increase gradually and on every 20 second interval it clears the window as a batch resetting the sumPrice to zero. EXAMPLE 3 define stream InputEventStream (symbol string, price float, volume int); define window StockEventWindow (symbol string, price float, volume int) timeBatch(20 sec) output all events; @info(name = 'query0') from InputEventStream insert into StockEventWindow; @info(name = 'query1') from StockEventWindow select symbol, sum(price) as price insert all events into OutputStream ; This uses an defined window to process events arrived every 20 seconds as a batch and output all events.","title":"timeBatch (Window)"},{"location":"docs/api/5.1.0/#timelength-window","text":"A sliding time window that, at a given time holds the last window.length events that arrived during last window.time period, and gets updated for every event arrival and expiry. Origin: siddhi-core:5.1.7 Syntax timeLength( INT|LONG|TIME window.time, INT window.length) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.time The sliding time period for which the window should hold events. INT LONG TIME No No window.length The number of events that should be be included in a sliding length window.. INT No No Examples EXAMPLE 1 define stream cseEventStream (symbol string, price float, volume int); define window cseEventWindow (symbol string, price float, volume int) timeLength(2 sec, 10); @info(name = 'query0') from cseEventStream insert into cseEventWindow; @info(name = 'query1') from cseEventWindow select symbol, price, volume insert all events into outputStream; window.timeLength(2 sec, 10) holds the last 10 events that arrived during last 2 seconds and gets updated for every event arrival and expiry.","title":"timeLength (Window)"},{"location":"docs/api/5.1.0/#js","text":"","title":"Js"},{"location":"docs/api/5.1.0/#eval-function","text":"This extension evaluates a given string and return the output according to the user specified data type. Origin: siddhi-script-js:5.0.2 Syntax INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT js:eval( STRING expression, STRING return.type) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic expression Any single line js expression or function. STRING No Yes return.type The return type of the evaluated expression. Supported types are int|long|float|double|bool|string. STRING No No Examples EXAMPLE 1 js:eval(\"700 800\", 'bool') In this example, the expression 700 800 will be evaluated and return result as false because user specified return type as bool.","title":"eval (Function)"},{"location":"docs/api/5.1.0/#json","text":"","title":"Json"},{"location":"docs/api/5.1.0/#group-aggregate-function","text":"This function aggregates the JSON elements and returns a JSON object by adding enclosing.element if it is provided. If enclosing.element is not provided it aggregate the JSON elements returns a JSON array. Origin: siddhi-execution-json:2.0.4 Syntax OBJECT json:group( STRING|OBJECT json) OBJECT json:group( STRING|OBJECT json, BOOL distinct) OBJECT json:group( STRING|OBJECT json, STRING enclosing.element) OBJECT json:group( STRING|OBJECT json, STRING enclosing.element, BOOL distinct) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json The JSON element that needs to be aggregated. STRING OBJECT No Yes enclosing.element The JSON element used to enclose the aggregated JSON elements. EMPTY_STRING STRING Yes Yes distinct This is used to only have distinct JSON elements in the concatenated JSON object/array that is returned. false BOOL Yes Yes Examples EXAMPLE 1 from InputStream#window.length(5) select json:group(\"json\") as groupedJSONArray input OutputStream; When we input events having values for the json as {\"date\":\"2013-11-19\",\"time\":\"10:30\"} and {\"date\":\"2013-11-19\",\"time\":\"12:20\"} , it returns [{\"date\":\"2013-11-19\",\"time\":\"10:30\"}{\"date\":\"2013-11-19\",\"time\":\"12:20\"}] to the 'OutputStream'. EXAMPLE 2 from InputStream#window.length(5) select json:group(\"json\", true) as groupedJSONArray input OutputStream; When we input events having values for the json as {\"date\":\"2013-11-19\",\"time\":\"10:30\"} and {\"date\":\"2013-11-19\",\"time\":\"10:30\"} , it returns [{\"date\":\"2013-11-19\",\"time\":\"10:30\"}] to the 'OutputStream'. EXAMPLE 3 from InputStream#window.length(5) select json:group(\"json\", \"result\") as groupedJSONArray input OutputStream; When we input events having values for the json as {\"date\":\"2013-11-19\",\"time\":\"10:30\"} and {\"date\":\"2013-11-19\",\"time\":\"12:20\"} , it returns {\"result\":[{\"date\":\"2013-11-19\",\"time\":\"10:30\"},{\"date\":\"2013-11-19\",\"time\":\"12:20\"}} to the 'OutputStream'. EXAMPLE 4 from InputStream#window.length(5) select json:group(\"json\", \"result\", true) as groupedJSONArray input OutputStream; When we input events having values for the json as {\"date\":\"2013-11-19\",\"time\":\"10:30\"} and {\"date\":\"2013-11-19\",\"time\":\"10:30\"} , it returns {\"result\":[{\"date\":\"2013-11-19\",\"time\":\"10:30\"}]} to the 'OutputStream'.","title":"group (Aggregate Function)"},{"location":"docs/api/5.1.0/#groupasobject-aggregate-function","text":"This function aggregates the JSON elements and returns a JSON object by adding enclosing.element if it is provided. If enclosing.element is not provided it aggregate the JSON elements returns a JSON array. Origin: siddhi-execution-json:2.0.4 Syntax OBJECT json:groupAsObject( STRING|OBJECT json) OBJECT json:groupAsObject( STRING|OBJECT json, BOOL distinct) OBJECT json:groupAsObject( STRING|OBJECT json, STRING enclosing.element) OBJECT json:groupAsObject( STRING|OBJECT json, STRING enclosing.element, BOOL distinct) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json The JSON element that needs to be aggregated. STRING OBJECT No Yes enclosing.element The JSON element used to enclose the aggregated JSON elements. EMPTY_STRING STRING Yes Yes distinct This is used to only have distinct JSON elements in the concatenated JSON object/array that is returned. false BOOL Yes Yes Examples EXAMPLE 1 from InputStream#window.length(5) select json:groupAsObject(\"json\") as groupedJSONArray input OutputStream; When we input events having values for the json as {\"date\":\"2013-11-19\",\"time\":\"10:30\"} and {\"date\":\"2013-11-19\",\"time\":\"12:20\"} , it returns [{\"date\":\"2013-11-19\",\"time\":\"10:30\"}{\"date\":\"2013-11-19\",\"time\":\"12:20\"}] to the 'OutputStream'. EXAMPLE 2 from InputStream#window.length(5) select json:groupAsObject(\"json\", true) as groupedJSONArray input OutputStream; When we input events having values for the json as {\"date\":\"2013-11-19\",\"time\":\"10:30\"} and {\"date\":\"2013-11-19\",\"time\":\"10:30\"} , it returns [{\"date\":\"2013-11-19\",\"time\":\"10:30\"}] to the 'OutputStream'. EXAMPLE 3 from InputStream#window.length(5) select json:groupAsObject(\"json\", \"result\") as groupedJSONArray input OutputStream; When we input events having values for the json as {\"date\":\"2013-11-19\",\"time\":\"10:30\"} and {\"date\":\"2013-11-19\",\"time\":\"12:20\"} , it returns {\"result\":[{\"date\":\"2013-11-19\",\"time\":\"10:30\"},{\"date\":\"2013-11-19\",\"time\":\"12:20\"}} to the 'OutputStream'. EXAMPLE 4 from InputStream#window.length(5) select json:groupAsObject(\"json\", \"result\", true) as groupedJSONArray input OutputStream; When we input events having values for the json as {\"date\":\"2013-11-19\",\"time\":\"10:30\"} and {\"date\":\"2013-11-19\",\"time\":\"10:30\"} , it returns {\"result\":[{\"date\":\"2013-11-19\",\"time\":\"10:30\"}]} to the 'OutputStream'.","title":"groupAsObject (Aggregate Function)"},{"location":"docs/api/5.1.0/#getbool-function","text":"Function retrieves the 'boolean' value specified in the given path of the JSON element. Origin: siddhi-execution-json:2.0.4 Syntax BOOL json:getBool( STRING|OBJECT json, STRING path) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json The JSON input containing boolean value. STRING OBJECT No Yes path The JSON path to fetch the boolean value. STRING No Yes Examples EXAMPLE 1 json:getBool(json,'$.married') If the json is the format {'name' : 'John', 'married' : true} , the function returns true as there is a matching boolean at .married /code . /p p /p span id=\"example-2\" class=\"md-typeset\" style=\"display: block; color: rgba(0, 0, 0, 0.54); font-size: 12.8px; font-weight: bold;\" EXAMPLE 2 /span json:getBool(json,'$.name') p /p p style=\"word-wrap: break-word;margin: 0;\" If the code json /code is the format code {'name' : 'John', 'married' : true} /code , the function returns code null /code as there is no matching boolean at code .married</code>.</p> <p></p> <span id=\"example-2\" class=\"md-typeset\" style=\"display: block; color: rgba(0, 0, 0, 0.54); font-size: 12.8px; font-weight: bold;\">EXAMPLE 2</span> <pre class=\"codehilite\"><code>json:getBool(json,'$.name')</code></pre> <p></p> <p style=\"word-wrap: break-word;margin: 0;\">If the <code>json</code> is the format <code>{'name' : 'John', 'married' : true}</code>, the function returns <code>null</code> as there is no matching boolean at <code> .name . EXAMPLE 3 json:getBool(json,'$.foo') If the json is the format {'name' : 'John', 'married' : true} , the function returns null as there is no matching element at $.foo .","title":"getBool (Function)"},{"location":"docs/api/5.1.0/#getdouble-function","text":"Function retrieves the 'double' value specified in the given path of the JSON element. Origin: siddhi-execution-json:2.0.4 Syntax DOUBLE json:getDouble( STRING|OBJECT json, STRING path) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json The JSON input containing double value. STRING OBJECT No Yes path The JSON path to fetch the double value. STRING No Yes Examples EXAMPLE 1 json:getDouble(json,'$.salary') If the json is the format {'name' : 'John', 'salary' : 12000.0} , the function returns 12000.0 as there is a matching double at .salary /code . /p p /p span id=\"example-2\" class=\"md-typeset\" style=\"display: block; color: rgba(0, 0, 0, 0.54); font-size: 12.8px; font-weight: bold;\" EXAMPLE 2 /span json:getDouble(json,'$.salary') p /p p style=\"word-wrap: break-word;margin: 0;\" If the code json /code is the format code {'name' : 'John', 'age' : 23} /code , the function returns code null /code as there are no matching element at code .salary</code>.</p> <p></p> <span id=\"example-2\" class=\"md-typeset\" style=\"display: block; color: rgba(0, 0, 0, 0.54); font-size: 12.8px; font-weight: bold;\">EXAMPLE 2</span> <pre class=\"codehilite\"><code>json:getDouble(json,'$.salary')</code></pre> <p></p> <p style=\"word-wrap: break-word;margin: 0;\">If the <code>json</code> is the format <code>{'name' : 'John', 'age' : 23}</code>, the function returns <code>null</code> as there are no matching element at <code> .salary . EXAMPLE 3 json:getDouble(json,'$.name') If the json is the format {'name' : 'John', 'age' : 23} , the function returns null as there are no matching double at $.name .","title":"getDouble (Function)"},{"location":"docs/api/5.1.0/#getfloat-function","text":"Function retrieves the 'float' value specified in the given path of the JSON element. Origin: siddhi-execution-json:2.0.4 Syntax FLOAT json:getFloat( STRING|OBJECT json, STRING path) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json The JSON input containing float value. STRING OBJECT No Yes path The JSON path to fetch the float value. STRING No Yes Examples EXAMPLE 1 json:getFloat(json,'$.salary') If the json is the format {'name' : 'John', 'salary' : 12000.0} , the function returns 12000 as there is a matching float at .salary /code . /p p /p span id=\"example-2\" class=\"md-typeset\" style=\"display: block; color: rgba(0, 0, 0, 0.54); font-size: 12.8px; font-weight: bold;\" EXAMPLE 2 /span json:getFloat(json,'$.salary') p /p p style=\"word-wrap: break-word;margin: 0;\" If the code json /code is the format code {'name' : 'John', 'age' : 23} /code , the function returns code null /code as there are no matching element at code .salary</code>.</p> <p></p> <span id=\"example-2\" class=\"md-typeset\" style=\"display: block; color: rgba(0, 0, 0, 0.54); font-size: 12.8px; font-weight: bold;\">EXAMPLE 2</span> <pre class=\"codehilite\"><code>json:getFloat(json,'$.salary')</code></pre> <p></p> <p style=\"word-wrap: break-word;margin: 0;\">If the <code>json</code> is the format <code>{'name' : 'John', 'age' : 23}</code>, the function returns <code>null</code> as there are no matching element at <code> .salary . EXAMPLE 3 json:getFloat(json,'$.name') If the json is the format {'name' : 'John', 'age' : 23} , the function returns null as there are no matching float at $.name .","title":"getFloat (Function)"},{"location":"docs/api/5.1.0/#getint-function","text":"Function retrieves the 'int' value specified in the given path of the JSON element. Origin: siddhi-execution-json:2.0.4 Syntax INT json:getInt( STRING|OBJECT json, STRING path) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json The JSON input containing int value. STRING OBJECT No Yes path The JSON path to fetch the int value. STRING No Yes Examples EXAMPLE 1 json:getInt(json,'$.age') If the json is the format {'name' : 'John', 'age' : 23} , the function returns 23 as there is a matching int at .age /code . /p p /p span id=\"example-2\" class=\"md-typeset\" style=\"display: block; color: rgba(0, 0, 0, 0.54); font-size: 12.8px; font-weight: bold;\" EXAMPLE 2 /span json:getInt(json,'$.salary') p /p p style=\"word-wrap: break-word;margin: 0;\" If the code json /code is the format code {'name' : 'John', 'age' : 23} /code , the function returns code null /code as there are no matching element at code .age</code>.</p> <p></p> <span id=\"example-2\" class=\"md-typeset\" style=\"display: block; color: rgba(0, 0, 0, 0.54); font-size: 12.8px; font-weight: bold;\">EXAMPLE 2</span> <pre class=\"codehilite\"><code>json:getInt(json,'$.salary')</code></pre> <p></p> <p style=\"word-wrap: break-word;margin: 0;\">If the <code>json</code> is the format <code>{'name' : 'John', 'age' : 23}</code>, the function returns <code>null</code> as there are no matching element at <code> .salary . EXAMPLE 3 json:getInt(json,'$.name') If the json is the format {'name' : 'John', 'age' : 23} , the function returns null as there are no matching int at $.name .","title":"getInt (Function)"},{"location":"docs/api/5.1.0/#getlong-function","text":"Function retrieves the 'long' value specified in the given path of the JSON element. Origin: siddhi-execution-json:2.0.4 Syntax LONG json:getLong( STRING|OBJECT json, STRING path) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json The JSON input containing long value. STRING OBJECT No Yes path The JSON path to fetch the long value. STRING No Yes Examples EXAMPLE 1 json:getLong(json,'$.age') If the json is the format {'name' : 'John', 'age' : 23} , the function returns 23 as there is a matching long at .age /code . /p p /p span id=\"example-2\" class=\"md-typeset\" style=\"display: block; color: rgba(0, 0, 0, 0.54); font-size: 12.8px; font-weight: bold;\" EXAMPLE 2 /span json:getLong(json,'$.salary') p /p p style=\"word-wrap: break-word;margin: 0;\" If the code json /code is the format code {'name' : 'John', 'age' : 23} /code , the function returns code null /code as there are no matching element at code .age</code>.</p> <p></p> <span id=\"example-2\" class=\"md-typeset\" style=\"display: block; color: rgba(0, 0, 0, 0.54); font-size: 12.8px; font-weight: bold;\">EXAMPLE 2</span> <pre class=\"codehilite\"><code>json:getLong(json,'$.salary')</code></pre> <p></p> <p style=\"word-wrap: break-word;margin: 0;\">If the <code>json</code> is the format <code>{'name' : 'John', 'age' : 23}</code>, the function returns <code>null</code> as there are no matching element at <code> .salary . EXAMPLE 3 json:getLong(json,'$.name') If the json is the format {'name' : 'John', 'age' : 23} , the function returns null as there are no matching long at $.name .","title":"getLong (Function)"},{"location":"docs/api/5.1.0/#getobject-function","text":"Function retrieves the object specified in the given path of the JSON element. Origin: siddhi-execution-json:2.0.4 Syntax OBJECT json:getObject( STRING|OBJECT json, STRING path) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json The JSON input containing the object. STRING OBJECT No Yes path The JSON path to fetch the object. STRING No Yes Examples EXAMPLE 1 json:getObject(json,'$.address') If the json is the format {'name' : 'John', 'address' : {'city' : 'NY', 'country' : 'USA'}} , the function returns {'city' : 'NY', 'country' : 'USA'} as there is a matching object at .address /code . /p p /p span id=\"example-2\" class=\"md-typeset\" style=\"display: block; color: rgba(0, 0, 0, 0.54); font-size: 12.8px; font-weight: bold;\" EXAMPLE 2 /span json:getObject(json,'$.age') p /p p style=\"word-wrap: break-word;margin: 0;\" If the code json /code is the format code {'name' : 'John', 'age' : 23} /code , the function returns code 23 /code as there is a matching object at code .address</code>.</p> <p></p> <span id=\"example-2\" class=\"md-typeset\" style=\"display: block; color: rgba(0, 0, 0, 0.54); font-size: 12.8px; font-weight: bold;\">EXAMPLE 2</span> <pre class=\"codehilite\"><code>json:getObject(json,'$.age')</code></pre> <p></p> <p style=\"word-wrap: break-word;margin: 0;\">If the <code>json</code> is the format <code>{'name' : 'John', 'age' : 23}</code>, the function returns <code>23</code> as there is a matching object at <code> .age . EXAMPLE 3 json:getObject(json,'$.salary') If the json is the format {'name' : 'John', 'age' : 23} , the function returns null as there are no matching element at $.salary .","title":"getObject (Function)"},{"location":"docs/api/5.1.0/#getstring-function","text":"Function retrieves value specified in the given path of the JSON element as a string. Origin: siddhi-execution-json:2.0.4 Syntax STRING json:getString( STRING|OBJECT json, STRING path) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json The JSON input containing value. STRING OBJECT No Yes path The JSON path to fetch the value. STRING No Yes Examples EXAMPLE 1 json:getString(json,'$.name') If the json is the format {'name' : 'John', 'age' : 23} , the function returns John as there is a matching string at .name /code . /p p /p span id=\"example-2\" class=\"md-typeset\" style=\"display: block; color: rgba(0, 0, 0, 0.54); font-size: 12.8px; font-weight: bold;\" EXAMPLE 2 /span json:getString(json,'$.salary') p /p p style=\"word-wrap: break-word;margin: 0;\" If the code json /code is the format code {'name' : 'John', 'age' : 23} /code , the function returns code null /code as there are no matching element at code .name</code>.</p> <p></p> <span id=\"example-2\" class=\"md-typeset\" style=\"display: block; color: rgba(0, 0, 0, 0.54); font-size: 12.8px; font-weight: bold;\">EXAMPLE 2</span> <pre class=\"codehilite\"><code>json:getString(json,'$.salary')</code></pre> <p></p> <p style=\"word-wrap: break-word;margin: 0;\">If the <code>json</code> is the format <code>{'name' : 'John', 'age' : 23}</code>, the function returns <code>null</code> as there are no matching element at <code> .salary . EXAMPLE 3 json:getString(json,'$.age') If the json is the format {'name' : 'John', 'age' : 23} , the function returns 23 as a string as there is a matching element at .age /code . /p p /p span id=\"example-4\" class=\"md-typeset\" style=\"display: block; color: rgba(0, 0, 0, 0.54); font-size: 12.8px; font-weight: bold;\" EXAMPLE 4 /span json:getString(json,'$.address') p /p p style=\"word-wrap: break-word;margin: 0;\" If the code json /code is the format code {'name' : 'John', 'address' : {'city' : 'NY', 'country' : 'USA'}} /code , the function returns code {'city' : 'NY', 'country' : 'USA'} /code as a string as there is a matching element at code .age</code>.</p> <p></p> <span id=\"example-4\" class=\"md-typeset\" style=\"display: block; color: rgba(0, 0, 0, 0.54); font-size: 12.8px; font-weight: bold;\">EXAMPLE 4</span> <pre class=\"codehilite\"><code>json:getString(json,'$.address')</code></pre> <p></p> <p style=\"word-wrap: break-word;margin: 0;\">If the <code>json</code> is the format <code>{'name' : 'John', 'address' : {'city' : 'NY', 'country' : 'USA'}}</code>, the function returns <code>{'city' : 'NY', 'country' : 'USA'}</code> as a string as there is a matching element at <code> .address .","title":"getString (Function)"},{"location":"docs/api/5.1.0/#isexists-function","text":"Function checks whether there is a JSON element present in the given path or not. Origin: siddhi-execution-json:2.0.4 Syntax BOOL json:isExists( STRING|OBJECT json, STRING path) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json The JSON input that needs to be searched for an elements. STRING OBJECT No Yes path The JSON path to check for the element. STRING No Yes Examples EXAMPLE 1 json:isExists(json, '$.name') If the json is the format {'name' : 'John', 'age' : 23} , the function returns true as there is an element in the given path. EXAMPLE 2 json:isExists(json, '$.salary') If the json is the format {'name' : 'John', 'age' : 23} , the function returns false as there is no element in the given path.","title":"isExists (Function)"},{"location":"docs/api/5.1.0/#setelement-function","text":"Function sets JSON element into a given JSON at the specific path. Origin: siddhi-execution-json:2.0.4 Syntax OBJECT json:setElement( STRING|OBJECT json, STRING path, STRING|BOOL|DOUBLE|FLOAT|INT|LONG|OBJECT json.element) OBJECT json:setElement( STRING|OBJECT json, STRING path, STRING|BOOL|DOUBLE|FLOAT|INT|LONG|OBJECT json.element, STRING key) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json The JSON to which a JSON element needs to be added/replaced. STRING OBJECT No Yes path The JSON path where the JSON element should be added/replaced. STRING No Yes json.element The JSON element being added. STRING BOOL DOUBLE FLOAT INT LONG OBJECT No Yes key The key to be used to refer the newly added element in the input JSON. Assumes the element is added to a JSON array, or the element selected by the JSON path will be updated. STRING Yes Yes Examples EXAMPLE 1 json:setElement(json, '$', \"{'country' : 'USA'}\", 'address') If the json is the format {'name' : 'John', 'married' : true} ,the function updates the json as {'name' : 'John', 'married' : true, 'address' : {'country' : 'USA'}} by adding 'address' element and returns the updated JSON. EXAMPLE 2 json:setElement(json, '$', 40, 'age') If the json is the format {'name' : 'John', 'married' : true} ,the function updates the json as {'name' : 'John', 'married' : true, 'age' : 40} by adding 'age' element and returns the updated JSON. EXAMPLE 3 json:setElement(json, '$', 45, 'age') If the json is the format {'name' : 'John', 'married' : true, 'age' : 40} , the function updates the json as {'name' : 'John', 'married' : true, 'age' : 45} by replacing 'age' element and returns the updated JSON. EXAMPLE 4 json:setElement(json, '$.items', 'book') If the json is the format {'name' : 'Stationary', 'items' : ['pen', 'pencil']} , the function updates the json as {'name' : 'John', 'items' : ['pen', 'pencil', 'book']} by adding 'book' in the items array and returns the updated JSON. EXAMPLE 5 json:setElement(json, '$.item', 'book') If the json is the format {'name' : 'Stationary', 'item' : 'pen'} , the function updates the json as {'name' : 'John', 'item' : 'book'} by replacing 'item' element and returns the updated JSON. EXAMPLE 6 json:setElement(json, '$.address', 'city', 'SF') If the json is the format {'name' : 'John', 'married' : true} ,the function will not update, but returns the original JSON as there are no valid path for $.address .","title":"setElement (Function)"},{"location":"docs/api/5.1.0/#toobject-function","text":"Function generate JSON object from the given JSON string. Origin: siddhi-execution-json:2.0.4 Syntax OBJECT json:toObject( STRING json) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json A valid JSON string that needs to be converted to a JSON object. STRING No Yes Examples EXAMPLE 1 json:toJson(json) This returns the JSON object corresponding to the given JSON string.","title":"toObject (Function)"},{"location":"docs/api/5.1.0/#tostring-function","text":"Function generates a JSON string corresponding to a given JSON object. Origin: siddhi-execution-json:2.0.4 Syntax STRING json:toString( OBJECT json) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json A valid JSON object to generates a JSON string. OBJECT No Yes Examples EXAMPLE 1 json:toString(json) This returns the JSON string corresponding to a given JSON object.","title":"toString (Function)"},{"location":"docs/api/5.1.0/#tokenize-stream-processor","text":"Stream processor tokenizes the given JSON into to multiple JSON string elements and sends them as separate events. Origin: siddhi-execution-json:2.0.4 Syntax json:tokenize( STRING|OBJECT json, STRING path) json:tokenize( STRING|OBJECT json, STRING path, BOOL fail.on.missing.attribute) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json The input JSON that needs to be tokenized. STRING OBJECT No Yes path The path of the set of elements that will be tokenized. STRING No Yes fail.on.missing.attribute If there are no element on the given path, when set to true the system will drop the event, and when set to false the system will pass 'null' value to the jsonElement output attribute. true BOOL Yes No Extra Return Attributes Name Description Possible Types jsonElement The JSON element retrieved based on the given path will be returned as a JSON string. If the 'path' selects a JSON array then the system returns each element in the array as a JSON string via a separate events. STRING Examples EXAMPLE 1 define stream InputStream (json string, path string); @info(name = 'query1') from InputStream#json:tokenizeAsObject(json, path) select path, jsonElement insert into OutputStream; If the input 'json' is {name:'John', enrolledSubjects:['Mathematics', 'Physics']} , and the 'path' is passed as .enrolledSubjects /code then for both the elements in the selected JSON array, it generates it generates events as code (' .enrolledSubjects</code> then for both the elements in the selected JSON array, it generates it generates events as <code>(' .enrolledSubjects', 'Mathematics') , and (' .enrolledSubjects', 'Physics') /code . br For the same input JSON, if the 'path' is passed as code .enrolledSubjects', 'Physics')</code>.<br>For the same input JSON, if the 'path' is passed as <code> .name then it will only produce one event (' .name', 'John') /code as the 'path' provided a single JSON element. /p p /p span id=\"example-2\" class=\"md-typeset\" style=\"display: block; color: rgba(0, 0, 0, 0.54); font-size: 12.8px; font-weight: bold;\" EXAMPLE 2 /span define stream InputStream (json string, path string); @info(name = 'query1') from InputStream#json:tokenizeAsObject(json, path, true) select path, jsonElement insert into OutputStream; p /p p style=\"word-wrap: break-word;margin: 0;\" If the input 'json' is code {name:'John', age:25} /code ,and the 'path' is passed as code .name', 'John')</code> as the 'path' provided a single JSON element.</p> <p></p> <span id=\"example-2\" class=\"md-typeset\" style=\"display: block; color: rgba(0, 0, 0, 0.54); font-size: 12.8px; font-weight: bold;\">EXAMPLE 2</span> <pre class=\"codehilite\"><code>define stream InputStream (json string, path string); @info(name = 'query1') from InputStream#json:tokenizeAsObject(json, path, true) select path, jsonElement insert into OutputStream;</code></pre> <p></p> <p style=\"word-wrap: break-word;margin: 0;\">If the input 'json' is <code>{name:'John', age:25}</code>,and the 'path' is passed as <code> .salary then the system will produce (' .salary', null) /code , as the 'fail.on.missing.attribute' is code true /code and there are no matching element for code .salary', null)</code>, as the 'fail.on.missing.attribute' is <code>true</code> and there are no matching element for <code> .salary .","title":"tokenize (Stream Processor)"},{"location":"docs/api/5.1.0/#tokenizeasobject-stream-processor","text":"Stream processor tokenizes the given JSON into to multiple JSON object elements and sends them as separate events. Origin: siddhi-execution-json:2.0.4 Syntax json:tokenizeAsObject( STRING|OBJECT json, STRING path) json:tokenizeAsObject( STRING|OBJECT json, STRING path, BOOL fail.on.missing.attribute) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json The input JSON that needs to be tokenized. STRING OBJECT No Yes path The path of the set of elements that will be tokenized. STRING No Yes fail.on.missing.attribute If there are no element on the given path, when set to true the system will drop the event, and when set to false the system will pass 'null' value to the jsonElement output attribute. true BOOL Yes No Extra Return Attributes Name Description Possible Types jsonElement The JSON element retrieved based on the given path will be returned as a JSON object. If the 'path' selects a JSON array then the system returns each element in the array as a JSON object via a separate events. OBJECT Examples EXAMPLE 1 define stream InputStream (json string, path string); @info(name = 'query1') from InputStream#json:tokenizeAsObject(json, path) select path, jsonElement insert into OutputStream; If the input 'json' is {name:'John', enrolledSubjects:['Mathematics', 'Physics']} , and the 'path' is passed as .enrolledSubjects /code then for both the elements in the selected JSON array, it generates it generates events as code (' .enrolledSubjects</code> then for both the elements in the selected JSON array, it generates it generates events as <code>(' .enrolledSubjects', 'Mathematics') , and (' .enrolledSubjects', 'Physics') /code . br For the same input JSON, if the 'path' is passed as code .enrolledSubjects', 'Physics')</code>.<br>For the same input JSON, if the 'path' is passed as <code> .name then it will only produce one event (' .name', 'John') /code as the 'path' provided a single JSON element. /p p /p span id=\"example-2\" class=\"md-typeset\" style=\"display: block; color: rgba(0, 0, 0, 0.54); font-size: 12.8px; font-weight: bold;\" EXAMPLE 2 /span define stream InputStream (json string, path string); @info(name = 'query1') from InputStream#json:tokenizeAsObject(json, path, true) select path, jsonElement insert into OutputStream; p /p p style=\"word-wrap: break-word;margin: 0;\" If the input 'json' is code {name:'John', age:25} /code ,and the 'path' is passed as code .name', 'John')</code> as the 'path' provided a single JSON element.</p> <p></p> <span id=\"example-2\" class=\"md-typeset\" style=\"display: block; color: rgba(0, 0, 0, 0.54); font-size: 12.8px; font-weight: bold;\">EXAMPLE 2</span> <pre class=\"codehilite\"><code>define stream InputStream (json string, path string); @info(name = 'query1') from InputStream#json:tokenizeAsObject(json, path, true) select path, jsonElement insert into OutputStream;</code></pre> <p></p> <p style=\"word-wrap: break-word;margin: 0;\">If the input 'json' is <code>{name:'John', age:25}</code>,and the 'path' is passed as <code> .salary then the system will produce (' .salary', null) /code , as the 'fail.on.missing.attribute' is code true /code and there are no matching element for code .salary', null)</code>, as the 'fail.on.missing.attribute' is <code>true</code> and there are no matching element for <code> .salary .","title":"tokenizeAsObject (Stream Processor)"},{"location":"docs/api/5.1.0/#list","text":"","title":"List"},{"location":"docs/api/5.1.0/#collect-aggregate-function","text":"Collects multiple values to construct a list. Origin: siddhi-execution-list:1.0.0 Syntax OBJECT list:collect( OBJECT|INT|LONG|FLOAT|DOUBLE|BOOL|STRING value) OBJECT list:collect( OBJECT|INT|LONG|FLOAT|DOUBLE|BOOL|STRING value, BOOL is.distinct) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic value Value of the list element OBJECT INT LONG FLOAT DOUBLE BOOL STRING No Yes is.distinct If true only distinct elements are collected false BOOL Yes Yes Examples EXAMPLE 1 from StockStream#window.lengthBatch(10) select list:collect(symbol) as stockSymbols insert into OutputStream; For the window expiry of 10 events, the collect() function will collect attributes of symbol to a single list and return as stockSymbols.","title":"collect (Aggregate Function)"},{"location":"docs/api/5.1.0/#merge-aggregate-function","text":"Collects multiple lists to merge as a single list. Origin: siddhi-execution-list:1.0.0 Syntax OBJECT list:merge( OBJECT list) OBJECT list:merge( OBJECT list, BOOL is.distinct) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic list List to be merged OBJECT No Yes is.distinct Whether to return list with distinct values false BOOL Yes Yes Examples EXAMPLE 1 from StockStream#window.lengthBatch(2) select list:merge(list) as stockSymbols insert into OutputStream; For the window expiry of 2 events, the merge() function will collect attributes of list and merge them to a single list, returned as stockSymbols.","title":"merge (Aggregate Function)"},{"location":"docs/api/5.1.0/#add-function","text":"Function returns the updated list after adding the given value. Origin: siddhi-execution-list:1.0.0 Syntax OBJECT list:add( OBJECT list, OBJECT|INT|LONG|FLOAT|DOUBLE|BOOL|STRING value) OBJECT list:add( OBJECT list, OBJECT|INT|LONG|FLOAT|DOUBLE|BOOL|STRING value, INT index) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic list The list to which the value should be added. OBJECT No Yes value The value to be added. OBJECT INT LONG FLOAT DOUBLE BOOL STRING No Yes index The index in which the value should to be added. last INT Yes Yes Examples EXAMPLE 1 list:add(stockSymbols, 'IBM') Function returns the updated list after adding the value IBM in the last index. EXAMPLE 2 list:add(stockSymbols, 'IBM', 0) Function returns the updated list after adding the value IBM in the 0 th index`.","title":"add (Function)"},{"location":"docs/api/5.1.0/#addall-function","text":"Function returns the updated list after adding all the values from the given list. Origin: siddhi-execution-list:1.0.0 Syntax OBJECT list:addAll( OBJECT to.list, OBJECT from.list) OBJECT list:addAll( OBJECT to.list, OBJECT from.list, BOOL is.distinct) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic to.list The list into which the values need to copied. OBJECT No Yes from.list The list from which the values are copied. OBJECT No Yes is.distinct If true returns list with distinct values false BOOL Yes Yes Examples EXAMPLE 1 list:putAll(toList, fromList) If toList contains values ('IBM', 'WSO2), and if fromList contains values ('IBM', 'XYZ') then the function returns updated toList with values ('IBM', 'WSO2', 'IBM', 'XYZ'). EXAMPLE 2 list:putAll(toList, fromList, true) If toList contains values ('IBM', 'WSO2), and if fromList contains values ('IBM', 'XYZ') then the function returns updated toList with values ('IBM', 'WSO2', 'XYZ').","title":"addAll (Function)"},{"location":"docs/api/5.1.0/#clear-function","text":"Function returns the cleared list. Origin: siddhi-execution-list:1.0.0 Syntax OBJECT list:clear( OBJECT list) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic list The list which needs to be cleared OBJECT No Yes Examples EXAMPLE 1 list:clear(stockDetails) Returns an empty list.","title":"clear (Function)"},{"location":"docs/api/5.1.0/#clone-function","text":"Function returns the cloned list. Origin: siddhi-execution-list:1.0.0 Syntax OBJECT list:clone( OBJECT list) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic list The list to which needs to be cloned. OBJECT No Yes Examples EXAMPLE 1 list:clone(stockSymbols) Function returns cloned list of stockSymbols.","title":"clone (Function)"},{"location":"docs/api/5.1.0/#contains-function","text":"Function checks whether the list contains the specific value. Origin: siddhi-execution-list:1.0.0 Syntax BOOL list:contains( OBJECT list, OBJECT|INT|LONG|FLOAT|DOUBLE|BOOL|STRING value) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic list The list that needs to be checked on whether it contains the value or not. OBJECT No Yes value The value that needs to be checked. OBJECT INT LONG FLOAT DOUBLE BOOL STRING No Yes Examples EXAMPLE 1 list:contains(stockSymbols, 'IBM') Returns 'true' if the stockSymbols list contains value IBM else it returns false .","title":"contains (Function)"},{"location":"docs/api/5.1.0/#containsall-function","text":"Function checks whether the list contains all the values in the given list. Origin: siddhi-execution-list:1.0.0 Syntax BOOL list:containsAll( OBJECT list, OBJECT given.list) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic list The list that needs to be checked on whether it contains all the values or not. OBJECT No Yes given.list The list which contains all the values to be checked. OBJECT No Yes Examples EXAMPLE 1 list:containsAll(stockSymbols, latestStockSymbols) Returns 'true' if the stockSymbols list contains values in latestStockSymbols else it returns false .","title":"containsAll (Function)"},{"location":"docs/api/5.1.0/#create-function","text":"Function creates a list containing all values provided. Origin: siddhi-execution-list:1.0.0 Syntax OBJECT list:create() OBJECT list:create( OBJECT|INT|LONG|FLOAT|DOUBLE|BOOL|STRING value1) OBJECT list:create( OBJECT|INT|LONG|FLOAT|DOUBLE|BOOL|STRING value1, OBJECT|INT|LONG|FLOAT|DOUBLE|BOOL|STRING ...) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic value1 Value 1 OBJECT INT LONG FLOAT DOUBLE BOOL STRING Yes Yes Examples EXAMPLE 1 list:create(1, 2, 3, 4, 5, 6) This returns a list with values 1 , 2 , 3 , 4 , 5 and 6 . EXAMPLE 2 list:create() This returns an empty list.","title":"create (Function)"},{"location":"docs/api/5.1.0/#get-function","text":"Function returns the value at the specific index, null if index is out of range. Origin: siddhi-execution-list:1.0.0 Syntax OBJECT|INT|LONG|FLOAT|DOUBLE|BOOL|STRING list:get( OBJECT list, INT index) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic list Attribute containing the list OBJECT No Yes index Index of the element INT No Yes Examples EXAMPLE 1 list:get(stockSymbols, 1) This returns the element in the 1 st index in the stockSymbols list.","title":"get (Function)"},{"location":"docs/api/5.1.0/#indexof-function","text":"Function returns the last index of the given element. Origin: siddhi-execution-list:1.0.0 Syntax INT list:indexOf( OBJECT list, OBJECT|INT|LONG|FLOAT|DOUBLE|BOOL|STRING value) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic list The list to be checked to get index of an element. OBJECT No Yes value Value for which last index needs to be identified. OBJECT INT LONG FLOAT DOUBLE BOOL STRING No Yes Examples EXAMPLE 1 list:indexOf(stockSymbols. `IBM`) Returns the last index of the element IBM if present else it returns -1.","title":"indexOf (Function)"},{"location":"docs/api/5.1.0/#isempty-function","text":"Function checks if the list is empty. Origin: siddhi-execution-list:1.0.0 Syntax BOOL list:isEmpty( OBJECT list) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic list The list that needs to be checked whether it's empty or not. OBJECT No Yes Examples EXAMPLE 1 list:isEmpty(stockSymbols) Returns 'true' if the stockSymbols list is empty else it returns false .","title":"isEmpty (Function)"},{"location":"docs/api/5.1.0/#islist-function","text":"Function checks if the object is type of a list. Origin: siddhi-execution-list:1.0.0 Syntax BOOL list:isList( OBJECT|INT|LONG|FLOAT|DOUBLE|BOOL|STRING arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The argument the need to be determined whether it's a list or not. OBJECT INT LONG FLOAT DOUBLE BOOL STRING No Yes Examples EXAMPLE 1 list:isList(stockSymbols) Returns 'true' if the stockSymbols is and an instance of java.util.List else it returns false .","title":"isList (Function)"},{"location":"docs/api/5.1.0/#lastindexof-function","text":"Function returns the index of the given value. Origin: siddhi-execution-list:1.0.0 Syntax INT list:lastIndexOf( OBJECT list, OBJECT|INT|LONG|FLOAT|DOUBLE|BOOL|STRING value) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic list The list to be checked to get index of an element. OBJECT No Yes value Value for which last index needs to be identified. OBJECT INT LONG FLOAT DOUBLE BOOL STRING No Yes Examples EXAMPLE 1 list:lastIndexOf(stockSymbols. `IBM`) Returns the last index of the element IBM if present else it returns -1.","title":"lastIndexOf (Function)"},{"location":"docs/api/5.1.0/#remove-function","text":"Function returns the updated list after removing the element with the specified value. Origin: siddhi-execution-list:1.0.0 Syntax OBJECT list:remove( OBJECT list, OBJECT|INT|LONG|FLOAT|DOUBLE|BOOL|STRING value) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic list The list that needs to be updated. OBJECT No Yes value The value of the element that needs to removed. OBJECT INT LONG FLOAT DOUBLE BOOL STRING No Yes Examples EXAMPLE 1 list:remove(stockSymbols, 'IBM') This returns the updated list, stockSymbols after stockSymbols the value IBM .","title":"remove (Function)"},{"location":"docs/api/5.1.0/#removeall-function","text":"Function returns the updated list after removing all the element with the specified list. Origin: siddhi-execution-list:1.0.0 Syntax OBJECT list:removeAll( OBJECT list, OBJECT given.list) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic list The list that needs to be updated. OBJECT No Yes given.list The list with all the elements that needs to removed. OBJECT No Yes Examples EXAMPLE 1 list:removeAll(stockSymbols, latestStockSymbols) This returns the updated list, stockSymbols after removing all the values in latestStockSymbols.","title":"removeAll (Function)"},{"location":"docs/api/5.1.0/#removebyindex-function","text":"Function returns the updated list after removing the element with the specified index. Origin: siddhi-execution-list:1.0.0 Syntax OBJECT list:removeByIndex( OBJECT list, INT index) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic list The list that needs to be updated. OBJECT No Yes index The index of the element that needs to removed. INT No Yes Examples EXAMPLE 1 list:removeByIndex(stockSymbols, 0) This returns the updated list, stockSymbols after removing value at 0 th index.","title":"removeByIndex (Function)"},{"location":"docs/api/5.1.0/#retainall-function","text":"Function returns the updated list after retaining all the elements in the specified list. Origin: siddhi-execution-list:1.0.0 Syntax OBJECT list:retainAll( OBJECT list, OBJECT given.list) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic list The list that needs to be updated. OBJECT No Yes given.list The list with all the elements that needs to reatined. OBJECT No Yes Examples EXAMPLE 1 list:retainAll(stockSymbols, latestStockSymbols) This returns the updated list, stockSymbols after retaining all the values in latestStockSymbols.","title":"retainAll (Function)"},{"location":"docs/api/5.1.0/#setvalue-function","text":"Function returns the updated list after replacing the element in the given index by the given value. Origin: siddhi-execution-list:1.0.0 Syntax OBJECT list:setValue( OBJECT list, INT index, OBJECT|INT|LONG|FLOAT|DOUBLE|BOOL|STRING value) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic list The list to which the value should be updated. OBJECT No Yes index The index in which the value should to be updated. INT No Yes value The value to be updated with. OBJECT INT LONG FLOAT DOUBLE BOOL STRING No Yes Examples EXAMPLE 1 list:set(stockSymbols, 0, 'IBM') Function returns the updated list after replacing the value at 0 th index with the value IBM","title":"setValue (Function)"},{"location":"docs/api/5.1.0/#size-function","text":"Function to return the size of the list. Origin: siddhi-execution-list:1.0.0 Syntax INT list:size( OBJECT list) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic list The list for which size should be returned. OBJECT No Yes Examples EXAMPLE 1 list:size(stockSymbols) Returns size of the stockSymbols list.","title":"size (Function)"},{"location":"docs/api/5.1.0/#sort-function","text":"Function returns lists sorted in ascending or descending order. Origin: siddhi-execution-list:1.0.0 Syntax OBJECT list:sort( OBJECT list) OBJECT list:sort( OBJECT list, STRING order) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic list The list which should be sorted. OBJECT No Yes order Order in which the list needs to be sorted (ASC/DESC/REV). REV STRING Yes No Examples EXAMPLE 1 list:sort(stockSymbols) Function returns the sorted list in ascending order. EXAMPLE 2 list:sort(stockSymbols, 'DESC') Function returns the sorted list in descending order.","title":"sort (Function)"},{"location":"docs/api/5.1.0/#tokenize-stream-processor_1","text":"Tokenize the list and return each key, value as new attributes in events Origin: siddhi-execution-list:1.0.0 Syntax list:tokenize( OBJECT list) list:tokenize( OBJECT list, OBJECT ...) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic list Array list which needs to be tokenized OBJECT No Yes Extra Return Attributes Name Description Possible Types index Index of an entry consisted in the list INT value Value of an entry consisted in the list OBJECT Examples EXAMPLE 1 list:tokenize(customList) If custom list contains ('WSO2', 'IBM', 'XYZ') elements, then tokenize function will return 3 events with value attributes WSO2, IBM and XYZ respectively.","title":"tokenize (Stream Processor)"},{"location":"docs/api/5.1.0/#map","text":"","title":"Map"},{"location":"docs/api/5.1.0/#create-function_1","text":"Function creates a map pairing the keys and their corresponding values. Origin: siddhi-execution-map:5.0.4 Syntax OBJECT map:create() OBJECT map:create( OBJECT|INT|LONG|FLOAT|DOUBLE|FLOAT|BOOL|STRING key1, OBJECT|INT|LONG|FLOAT|DOUBLE|FLOAT|BOOL|STRING value1) OBJECT map:create( OBJECT|INT|LONG|FLOAT|DOUBLE|FLOAT|BOOL|STRING key1, OBJECT|INT|LONG|FLOAT|DOUBLE|FLOAT|BOOL|STRING value1, OBJECT|INT|LONG|FLOAT|DOUBLE|FLOAT|BOOL|STRING ...) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic key1 Key 1 - OBJECT INT LONG FLOAT DOUBLE FLOAT BOOL STRING Yes Yes value1 Value 1 - OBJECT INT LONG FLOAT DOUBLE FLOAT BOOL STRING Yes Yes Examples EXAMPLE 1 map:create(1, 'one', 2, 'two', 3, 'three') This returns a map with keys 1 , 2 , 3 mapped with their corresponding values, one , two , three . EXAMPLE 2 map:create() This returns an empty map.","title":"create (Function)"},{"location":"docs/api/5.1.0/#createfromjson-function","text":"Function returns the map created by pairing the keys with their corresponding values given in the JSON string. Origin: siddhi-execution-map:5.0.4 Syntax OBJECT map:createFromJSON( STRING json.string) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json.string JSON as a string, which is used to create the map. STRING No Yes Examples EXAMPLE 1 map:createFromJSON(\"{\u2018symbol' : 'IBM', 'price' : 200, 'volume' : 100}\") This returns a map with the keys symbol , price , and volume , and their values, IBM , 200 and 100 respectively.","title":"createFromJSON (Function)"},{"location":"docs/api/5.1.0/#createfromxml-function","text":"Function returns the map created by pairing the keys with their corresponding values,given as an XML string. Origin: siddhi-execution-map:5.0.4 Syntax OBJECT map:createFromXML( STRING xml.string) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic xml.string The XML string, which is used to create the map. STRING No Yes Examples EXAMPLE 1 map:createFromXML(\" stock symbol IBM /symbol price 200 /price volume 100 /volume /stock \") This returns a map with the keys symbol , price , volume , and with their values IBM , 200 and 100 respectively.","title":"createFromXML (Function)"},{"location":"docs/api/5.1.0/#get-function_1","text":"Function returns the value corresponding to the given key from the map. Origin: siddhi-execution-map:5.0.4 Syntax OBJECT map:get( OBJECT map, OBJECT|INT|LONG|FLOAT|DOUBLE|FLOAT|BOOL|STRING key) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic map The map from where the value should be obtained. OBJECT No Yes key The key to fetch the value. OBJECT INT LONG FLOAT DOUBLE FLOAT BOOL STRING No Yes Examples EXAMPLE 1 map:get(companyMap, 1) If the companyMap has key 1 and value ABC in it's set of key value pairs. The function returns ABC . EXAMPLE 2 map:get(companyMap, 2) If the companyMap does not have any value for key 2 then the function returns null .","title":"get (Function)"},{"location":"docs/api/5.1.0/#ismap-function","text":"Function checks if the object is type of a map. Origin: siddhi-execution-map:5.0.4 Syntax BOOL map:isMap( OBJECT|INT|LONG|FLOAT|DOUBLE|FLOAT|BOOL|STRING arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The argument the need to be determined whether it's a map or not. OBJECT INT LONG FLOAT DOUBLE FLOAT BOOL STRING No Yes Examples EXAMPLE 1 map:isMap(students) Returns 'true' if the students is and an instance of java.util.Map else it returns false .","title":"isMap (Function)"},{"location":"docs/api/5.1.0/#put-function","text":"Function returns the updated map after adding the given key-value pair. If the key already exist in the map the key is updated with the new value. Origin: siddhi-execution-map:5.0.4 Syntax OBJECT map:put( OBJECT map, OBJECT|INT|LONG|FLOAT|DOUBLE|FLOAT|BOOL|STRING key, OBJECT|INT|LONG|FLOAT|DOUBLE|FLOAT|BOOL|STRING value) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic map The map to which the value should be added. OBJECT No Yes key The key to be added. OBJECT INT LONG FLOAT DOUBLE FLOAT BOOL STRING No Yes value The value to be added. OBJECT INT LONG FLOAT DOUBLE FLOAT BOOL STRING No Yes Examples EXAMPLE 1 map:put(students , 1234 , 'sam') Function returns the updated map named students after adding the value sam with the key 1234 .","title":"put (Function)"},{"location":"docs/api/5.1.0/#putall-function","text":"Function returns the updated map after adding all the key-value pairs from another map. If there are duplicate keys, the key will be assigned new values from the map that's being copied. Origin: siddhi-execution-map:5.0.4 Syntax OBJECT map:putAll( OBJECT to.map, OBJECT from.map) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic to.map The map into which the key-values need to copied. OBJECT No Yes from.map The map from which the key-values are copied. OBJECT No Yes Examples EXAMPLE 1 map:putAll(toMap, fromMap) If toMap contains key-value pairs ('symbol': 'wso2'), ('volume' : 100), and if fromMap contains key-value pairs ('symbol': 'IBM'), ('price' : 12), then the function returns updated toMap with key-value pairs ('symbol': 'IBM'), ('price' : 12), ('volume' : 100).","title":"putAll (Function)"},{"location":"docs/api/5.1.0/#remove-function_1","text":"Function returns the updated map after removing the element with the specified key. Origin: siddhi-execution-map:5.0.4 Syntax OBJECT map:remove( OBJECT map, OBJECT|INT|LONG|FLOAT|DOUBLE|FLOAT|BOOL|STRING key) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic map The map that needs to be updated. OBJECT No Yes key The key of the element that needs to removed. OBJECT INT LONG FLOAT DOUBLE FLOAT BOOL STRING No Yes Examples EXAMPLE 1 map:remove(students, 1234) This returns the updated map, students after removing the key-value pair corresponding to the key 1234 .","title":"remove (Function)"},{"location":"docs/api/5.1.0/#tojson-function","text":"Function converts a map into a JSON object and returns the JSON as a string. Origin: siddhi-execution-map:5.0.4 Syntax STRING map:toJSON( OBJECT map) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic map The map that needs to be converted to JSON OBJECT No Yes Examples EXAMPLE 1 map:toJSON(company) If company is a map with key-value pairs, ('symbol': 'wso2'),('volume' : 100), and ('price', 200), it returns the JSON string {\"symbol\" : \"wso2\", \"volume\" : 100 , \"price\" : 200} .","title":"toJSON (Function)"},{"location":"docs/api/5.1.0/#toxml-function","text":"Function returns the map as an XML string. Origin: siddhi-execution-map:5.0.4 Syntax STRING map:toXML( OBJECT map) STRING map:toXML( OBJECT map, OBJECT|STRING root.element.name) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic map The map that needs to be converted to XML. OBJECT No Yes root.element.name The root element of the map. The XML root element will be ignored OBJECT STRING Yes Yes Examples EXAMPLE 1 toXML(company, 'abcCompany') If company is a map with key-value pairs, ('symbol' : 'wso2'), ('volume' : 100), and ('price' : 200), this function returns XML as a string, abcCompany symbol wso2 /symbol volume 100 /volume price 200 /price /abcCompany . EXAMPLE 2 toXML(company) If company is a map with key-value pairs, ('symbol' : 'wso2'), ('volume' : 100), and ('price' : 200), this function returns XML without root element as a string, symbol wso2 /symbol volume 100 /volume price 200 /price .","title":"toXML (Function)"},{"location":"docs/api/5.1.0/#math","text":"","title":"Math"},{"location":"docs/api/5.1.0/#percentile-aggregate-function","text":"This functions returns the pth percentile value of a given argument. Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:percentile( INT|LONG|FLOAT|DOUBLE arg, DOUBLE p) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value of the parameter whose percentile should be found. INT LONG FLOAT DOUBLE No Yes p Estimate of the percentile to be found (pth percentile) where p is any number greater than 0 or lesser than or equal to 100. DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (sensorId int, temperature double); from InValueStream select math:percentile(temperature, 97.0) as percentile insert into OutMediationStream; This function returns the percentile value based on the argument given. For example, math:percentile(temperature, 97.0) returns the 97 th percentile value of all the temperature events.","title":"percentile (Aggregate Function)"},{"location":"docs/api/5.1.0/#abs-function","text":"This function returns the absolute value of the given parameter. It wraps the java.lang.Math.abs() function. Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:abs( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The parameter whose absolute value is found. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:abs(inValue) as absValue insert into OutMediationStream; Irrespective of whether the 'invalue' in the input stream holds a value of abs(3) or abs(-3),the function returns 3 since the absolute value of both 3 and -3 is 3. The result directed to OutMediationStream stream.","title":"abs (Function)"},{"location":"docs/api/5.1.0/#acos-function","text":"If -1 = p1 = 1, this function returns the arc-cosine (inverse cosine) value of p1.If the domain is invalid, it returns NULL. The value returned is in radian scale. This function wraps the java.lang.Math.acos() function. Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:acos( FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose arc-cosine (inverse cosine) value is found. FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:acos(inValue) as acosValue insert into OutMediationStream; If the 'inValue' in the input stream is given, the function calculates the arc-cosine value of it and returns the arc-cosine value to the output stream, OutMediationStream. For example, acos(0.5) returns 1.0471975511965979.","title":"acos (Function)"},{"location":"docs/api/5.1.0/#asin-function","text":"If -1 = p1 = 1, this function returns the arc-sin (inverse sine) value of p1. If the domain is invalid, it returns NULL. The value returned is in radian scale. This function wraps the java.lang.Math.asin() function. Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:asin( FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose arc-sin (inverse sine) value is found. FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:asin(inValue) as asinValue insert into OutMediationStream; If the 'inValue' in the input stream is given, the function calculates the arc-sin value of it and returns the arc-sin value to the output stream, OutMediationStream. For example, asin(0.5) returns 0.5235987755982989.","title":"asin (Function)"},{"location":"docs/api/5.1.0/#atan-function","text":"1. If a single p1 is received, this function returns the arc-tangent (inverse tangent) value of p1 . 2. If p1 is received along with an optional p1 , it considers them as x and y coordinates and returns the arc-tangent (inverse tangent) value. The returned value is in radian scale. This function wraps the java.lang.Math.atan() function. Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:atan( INT|LONG|FLOAT|DOUBLE p1) DOUBLE math:atan( INT|LONG|FLOAT|DOUBLE p1, INT|LONG|FLOAT|DOUBLE p2) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose arc-tangent (inverse tangent) is found. If the optional second parameter is given this represents the x coordinate of the (x,y) coordinate pair. INT LONG FLOAT DOUBLE No Yes p2 This optional parameter represents the y coordinate of the (x,y) coordinate pair. 0D INT LONG FLOAT DOUBLE Yes Yes Examples EXAMPLE 1 define stream InValueStream (inValue1 double, inValue2 double); from InValueStream select math:atan(inValue1, inValue2) as convertedValue insert into OutMediationStream; If the 'inValue1' in the input stream is given, the function calculates the arc-tangent value of it and returns the arc-tangent value to the output stream, OutMediationStream. If both the 'inValue1' and 'inValue2' are given, then the function considers them to be x and y coordinates respectively and returns the calculated arc-tangent value to the output stream, OutMediationStream. For example, atan(12d, 5d) returns 1.1760052070951352.","title":"atan (Function)"},{"location":"docs/api/5.1.0/#bin-function","text":"This function returns a string representation of the p1 argument, that is of either 'integer' or 'long' data type, as an unsigned integer in base 2. It wraps the java.lang.Integer.toBinaryString and java.lang.Long.toBinaryString` methods. Origin: siddhi-execution-math:5.0.4 Syntax STRING math:bin( INT|LONG p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value in either 'integer' or 'long', that should be converted into an unsigned integer of base 2. INT LONG No Yes Examples EXAMPLE 1 define stream InValueStream (inValue long); from InValueStream select math:bin(inValue) as binValue insert into OutMediationStream; If the 'inValue' in the input stream is given, the function converts it into an unsigned integer in base 2 and directs the output to the output stream, OutMediationStream. For example, bin(9) returns '1001'.","title":"bin (Function)"},{"location":"docs/api/5.1.0/#cbrt-function","text":"This function returns the cube-root of 'p1' which is in radians. It wraps the java.lang.Math.cbrt() function. Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:cbrt( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose cube-root should be found. Input is required to be in radians. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:cbrt(inValue) as cbrtValue insert into OutMediationStream; If the 'inValue' is given, the function calculates the cube-root value for the same and directs the output to the output stream, OutMediationStream. For example, cbrt(17d) returns 2.5712815906582356.","title":"cbrt (Function)"},{"location":"docs/api/5.1.0/#ceil-function","text":"This function returns the smallest double value, i.e., the closest to the negative infinity, that is greater than or equal to the p1 argument, and is equal to a mathematical integer. It wraps the java.lang.Math.ceil() method. Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:ceil( FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose ceiling value is found. FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:ceil(inValue) as ceilingValue insert into OutMediationStream; This function calculates the ceiling value of the given 'inValue' and directs the result to 'OutMediationStream' output stream. For example, ceil(423.187d) returns 424.0.","title":"ceil (Function)"},{"location":"docs/api/5.1.0/#conv-function","text":"This function converts a from the fromBase base to the toBase base. Origin: siddhi-execution-math:5.0.4 Syntax STRING math:conv( STRING a, INT from.base, INT to.base) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic a The value whose base should be changed. Input should be given as a 'String'. STRING No Yes from.base The source base of the input parameter 'a'. INT No Yes to.base The target base that the input parameter 'a' should be converted into. INT No Yes Examples EXAMPLE 1 define stream InValueStream (inValue string,fromBase int,toBase int); from InValueStream select math:conv(inValue,fromBase,toBase) as convertedValue insert into OutMediationStream; If the 'inValue' in the input stream is given, and the base in which it currently resides in and the base to which it should be converted to is specified, the function converts it into a string in the target base and directs it to the output stream, OutMediationStream. For example, conv(\"7f\", 16, 10) returns \"127\".","title":"conv (Function)"},{"location":"docs/api/5.1.0/#copysign-function","text":"This function returns a value of an input with the received magnitude and sign of another input. It wraps the java.lang.Math.copySign() function. Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:copySign( INT|LONG|FLOAT|DOUBLE magnitude, INT|LONG|FLOAT|DOUBLE sign) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic magnitude The magnitude of this parameter is used in the output attribute. INT LONG FLOAT DOUBLE No Yes sign The sign of this parameter is used in the output attribute. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue1 double, inValue2 double); from InValueStream select math:copySign(inValue1,inValue2) as copysignValue insert into OutMediationStream; If two values are provided as 'inValue1' and 'inValue2', the function copies the magnitude and sign of the second argument into the first one and directs the result to the output stream, OutMediatonStream. For example, copySign(5.6d, -3.0d) returns -5.6.","title":"copySign (Function)"},{"location":"docs/api/5.1.0/#cos-function","text":"This function returns the cosine of p1 which is in radians. It wraps the java.lang.Math.cos() function. Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:cos( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose cosine value should be found.The input is required to be in radians. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:cos(inValue) as cosValue insert into OutMediationStream; If the 'inValue' is given, the function calculates the cosine value for the same and directs the output to the output stream, OutMediationStream. For example, cos(6d) returns 0.9601702866503661.","title":"cos (Function)"},{"location":"docs/api/5.1.0/#cosh-function","text":"This function returns the hyperbolic cosine of p1 which is in radians. It wraps the java.lang.Math.cosh() function. Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:cosh( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose hyperbolic cosine should be found. The input is required to be in radians. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:cosh(inValue) as cosValue insert into OutMediationStream; If the 'inValue' is given, the function calculates the hyperbolic cosine value for the same and directs the output to the output stream, OutMediationStream. For example, cosh (6d) returns 201.7156361224559.","title":"cosh (Function)"},{"location":"docs/api/5.1.0/#e-function","text":"This function returns the java.lang.Math.E constant, which is the closest double value to e, where e is the base of the natural logarithms. Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:e() Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:e() as eValue insert into OutMediationStream; This function returns the constant, 2.7182818284590452354 which is the closest double value to e and directs the output to 'OutMediationStream' output stream.","title":"e (Function)"},{"location":"docs/api/5.1.0/#exp-function","text":"This function returns the Euler's number e raised to the power of p1 . It wraps the java.lang.Math.exp() function. Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:exp( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The power that the Euler's number e is raised to. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:exp(inValue) as expValue insert into OutMediationStream; If the 'inValue' in the inputstream holds a value, this function calculates the corresponding Euler's number 'e' and directs it to the output stream, OutMediationStream. For example, exp(10.23) returns 27722.51006805505.","title":"exp (Function)"},{"location":"docs/api/5.1.0/#floor-function","text":"This function wraps the java.lang.Math.floor() function and returns the largest value, i.e., closest to the positive infinity, that is less than or equal to p1 , and is equal to a mathematical integer. Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:floor( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose floor value should be found. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:floor(inValue) as floorValue insert into OutMediationStream; This function calculates the floor value of the given 'inValue' input and directs the output to the 'OutMediationStream' output stream. For example, (10.23) returns 10.0.","title":"floor (Function)"},{"location":"docs/api/5.1.0/#getexponent-function","text":"This function returns the unbiased exponent that is used in the representation of p1 . This function wraps the java.lang.Math.getExponent() function. Origin: siddhi-execution-math:5.0.4 Syntax INT math:getExponent( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of whose unbiased exponent representation should be found. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:getExponent(inValue) as expValue insert into OutMediationStream; This function calculates the unbiased exponent of a given input, 'inValue' and directs the result to the 'OutMediationStream' output stream. For example, getExponent(60984.1) returns 15.","title":"getExponent (Function)"},{"location":"docs/api/5.1.0/#hex-function","text":"This function wraps the java.lang.Double.toHexString() function. It returns a hexadecimal string representation of the input, p1`. Origin: siddhi-execution-math:5.0.4 Syntax STRING math:hex( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose hexadecimal value should be found. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue int); from InValueStream select math:hex(inValue) as hexString insert into OutMediationStream; If the 'inValue' in the input stream is provided, the function converts this into its corresponding hexadecimal format and directs the output to the output stream, OutMediationStream. For example, hex(200) returns \"c8\".","title":"hex (Function)"},{"location":"docs/api/5.1.0/#isinfinite-function","text":"This function wraps the java.lang.Float.isInfinite() and java.lang.Double.isInfinite() and returns true if p1 is infinitely large in magnitude and false if otherwise. Origin: siddhi-execution-math:5.0.4 Syntax BOOL math:isInfinite( FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 This is the value of the parameter that the function determines to be either infinite or finite. FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue1 double,inValue2 int); from InValueStream select math:isInfinite(inValue1) as isInfinite insert into OutMediationStream; If the value given in the 'inValue' in the input stream is of infinitely large magnitude, the function returns the value, 'true' and directs the result to the output stream, OutMediationStream'. For example, isInfinite(java.lang.Double.POSITIVE_INFINITY) returns true.","title":"isInfinite (Function)"},{"location":"docs/api/5.1.0/#isnan-function","text":"This function wraps the java.lang.Float.isNaN() and java.lang.Double.isNaN() functions and returns true if p1 is NaN (Not-a-Number), and returns false if otherwise. Origin: siddhi-execution-math:5.0.4 Syntax BOOL math:isNan( FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter which the function determines to be either NaN or a number. FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue1 double,inValue2 int); from InValueStream select math:isNan(inValue1) as isNaN insert into OutMediationStream; If the 'inValue1' in the input stream has a value that is undefined, then the function considers it as an 'NaN' value and directs 'True' to the output stream, OutMediationStream. For example, isNan(java.lang.Math.log(-12d)) returns true.","title":"isNan (Function)"},{"location":"docs/api/5.1.0/#ln-function","text":"This function returns the natural logarithm (base e) of p1 . Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:ln( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose natural logarithm (base e) should be found. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:ln(inValue) as lnValue insert into OutMediationStream; If the 'inValue' in the input stream is given, the function calculates its natural logarithm (base e) and directs the results to the output stream, 'OutMeditionStream'. For example, ln(11.453) returns 2.438251704415579.","title":"ln (Function)"},{"location":"docs/api/5.1.0/#log-function","text":"This function returns the logarithm of the received number as per the given base . Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:log( INT|LONG|FLOAT|DOUBLE number, INT|LONG|FLOAT|DOUBLE base) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic number The value of the parameter whose base should be changed. INT LONG FLOAT DOUBLE No Yes base The base value of the ouput. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (number double, base double); from InValueStream select math:log(number, base) as logValue insert into OutMediationStream; If the number and the base to which it has to be converted into is given in the input stream, the function calculates the number to the base specified and directs the result to the output stream, OutMediationStream. For example, log(34, 2f) returns 5.08746284125034.","title":"log (Function)"},{"location":"docs/api/5.1.0/#log10-function","text":"This function returns the base 10 logarithm of p1 . Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:log10( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose base 10 logarithm should be found. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:log10(inValue) as lnValue insert into OutMediationStream; If the 'inValue' in the input stream is given, the function calculates the base 10 logarithm of the same and directs the result to the output stream, OutMediatioStream. For example, log10(19.234) returns 1.2840696117100832.","title":"log10 (Function)"},{"location":"docs/api/5.1.0/#log2-function","text":"This function returns the base 2 logarithm of p1 . Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:log2( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose base 2 logarithm should be found. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:log2(inValue) as lnValue insert into OutMediationStream; If the 'inValue' in the input stream is given, the function calculates the base 2 logarithm of the same and returns the value to the output stream, OutMediationStream. For example log2(91d) returns 6.507794640198696.","title":"log2 (Function)"},{"location":"docs/api/5.1.0/#max-function","text":"This function returns the greater value of p1 and p2 . Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:max( INT|LONG|FLOAT|DOUBLE p1, INT|LONG|FLOAT|DOUBLE p2) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 One of the input values to be compared in order to find the larger value of the two INT LONG FLOAT DOUBLE No Yes p2 The input value to be compared with 'p1' in order to find the larger value of the two. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue1 double,inValue2 int); from InValueStream select math:max(inValue1,inValue2) as maxValue insert into OutMediationStream; If two input values 'inValue1, and 'inValue2' are given, the function compares them and directs the larger value to the output stream, OutMediationStream. For example, max(123.67d, 91) returns 123.67.","title":"max (Function)"},{"location":"docs/api/5.1.0/#min-function","text":"This function returns the smaller value of p1 and p2 . Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:min( INT|LONG|FLOAT|DOUBLE p1, INT|LONG|FLOAT|DOUBLE p2) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 One of the input values that are to be compared in order to find the smaller value. INT LONG FLOAT DOUBLE No Yes p2 The input value that is to be compared with 'p1' in order to find the smaller value. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue1 double,inValue2 int); from InValueStream select math:min(inValue1,inValue2) as minValue insert into OutMediationStream; If two input values, 'inValue1' and 'inValue2' are given, the function compares them and directs the smaller value of the two to the output stream, OutMediationStream. For example, min(123.67d, 91) returns 91.","title":"min (Function)"},{"location":"docs/api/5.1.0/#oct-function","text":"This function converts the input parameter p1 to octal. Origin: siddhi-execution-math:5.0.4 Syntax STRING math:oct( INT|LONG p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose octal representation should be found. INT LONG No Yes Examples EXAMPLE 1 define stream InValueStream (inValue long); from InValueStream select math:oct(inValue) as octValue insert into OutMediationStream; If the 'inValue' in the input stream is given, this function calculates the octal value corresponding to the same and directs it to the output stream, OutMediationStream. For example, oct(99l) returns \"143\".","title":"oct (Function)"},{"location":"docs/api/5.1.0/#parsedouble-function","text":"This function returns the double value of the string received. Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:parseDouble( STRING p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that should be converted into a double value. STRING No Yes Examples EXAMPLE 1 define stream InValueStream (inValue string); from InValueStream select math:parseDouble(inValue) as output insert into OutMediationStream; If the 'inValue' in the input stream holds a value, this function converts it into the corresponding double value and directs it to the output stream, OutMediationStream. For example, parseDouble(\"123\") returns 123.0.","title":"parseDouble (Function)"},{"location":"docs/api/5.1.0/#parsefloat-function","text":"This function returns the float value of the received string. Origin: siddhi-execution-math:5.0.4 Syntax FLOAT math:parseFloat( STRING p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that should be converted into a float value. STRING No Yes Examples EXAMPLE 1 define stream InValueStream (inValue string); from InValueStream select math:parseFloat(inValue) as output insert into OutMediationStream; The function converts the input value given in 'inValue',into its corresponding float value and directs the result into the output stream, OutMediationStream. For example, parseFloat(\"123\") returns 123.0.","title":"parseFloat (Function)"},{"location":"docs/api/5.1.0/#parseint-function","text":"This function returns the integer value of the received string. Origin: siddhi-execution-math:5.0.4 Syntax INT math:parseInt( STRING p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that should be converted to an integer. STRING No Yes Examples EXAMPLE 1 define stream InValueStream (inValue string); from InValueStream select math:parseInt(inValue) as output insert into OutMediationStream; The function converts the 'inValue' into its corresponding integer value and directs the output to the output stream, OutMediationStream. For example, parseInt(\"123\") returns 123.","title":"parseInt (Function)"},{"location":"docs/api/5.1.0/#parselong-function","text":"This function returns the long value of the string received. Origin: siddhi-execution-math:5.0.4 Syntax LONG math:parseLong( STRING p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that should be converted to a long value. STRING No Yes Examples EXAMPLE 1 define stream InValueStream (inValue string); from InValueStream select math:parseLong(inValue) as output insert into OutMediationStream; The function converts the 'inValue' to its corresponding long value and directs the result to the output stream, OutMediationStream. For example, parseLong(\"123\") returns 123.","title":"parseLong (Function)"},{"location":"docs/api/5.1.0/#pi-function","text":"This function returns the java.lang.Math.PI constant, which is the closest value to pi, i.e., the ratio of the circumference of a circle to its diameter. Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:pi() Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:pi() as piValue insert into OutMediationStream; pi() always returns 3.141592653589793.","title":"pi (Function)"},{"location":"docs/api/5.1.0/#power-function","text":"This function raises the given value to a given power. Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:power( INT|LONG|FLOAT|DOUBLE value, INT|LONG|FLOAT|DOUBLE to.power) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic value The value that should be raised to the power of 'to.power' input parameter. INT LONG FLOAT DOUBLE No Yes to.power The power to which the 'value' input parameter should be raised. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue1 double, inValue2 double); from InValueStream select math:power(inValue1,inValue2) as powerValue insert into OutMediationStream; This function raises the 'inValue1' to the power of 'inValue2' and directs the output to the output stream, 'OutMediationStream. For example, (5.6d, 3.0d) returns 175.61599999999996.","title":"power (Function)"},{"location":"docs/api/5.1.0/#rand-function","text":"This returns a stream of pseudo-random numbers when a sequence of calls are sent to the rand() . Optionally, it is possible to define a seed, i.e., rand(seed) using which the pseudo-random numbers are generated. These functions internally use the java.util.Random class. Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:rand() DOUBLE math:rand( INT|LONG seed) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic seed An optional seed value that will be used to generate the random number sequence. defaultSeed INT LONG Yes Yes Examples EXAMPLE 1 define stream InValueStream (symbol string, price long, volume long); from InValueStream select symbol, math:rand() as randNumber select math:oct(inValue) as octValue insert into OutMediationStream; In the example given above, a random double value between 0 and 1 will be generated using math:rand().","title":"rand (Function)"},{"location":"docs/api/5.1.0/#round-function","text":"This function returns the value of the input argument rounded off to the closest integer/long value. Origin: siddhi-execution-math:5.0.4 Syntax INT|LONG math:round( FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that should be rounded off to the closest integer/long value. FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:round(inValue) as roundValue insert into OutMediationStream; The function rounds off 'inValue1' to the closest int/long value and directs the output to the output stream, 'OutMediationStream'. For example, round(3252.353) returns 3252.","title":"round (Function)"},{"location":"docs/api/5.1.0/#signum-function","text":"This returns +1, 0, or -1 for the given positive, zero and negative values respectively. This function wraps the java.lang.Math.signum() function. Origin: siddhi-execution-math:5.0.4 Syntax INT math:signum( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that should be checked to be positive, negative or zero. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:signum(inValue) as sign insert into OutMediationStream; The function evaluates the 'inValue' given to be positive, negative or zero and directs the result to the output stream, 'OutMediationStream'. For example, signum(-6.32d) returns -1.","title":"signum (Function)"},{"location":"docs/api/5.1.0/#sin-function","text":"This returns the sine of the value given in radians. This function wraps the java.lang.Math.sin() function. Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:sin( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose sine value should be found. Input is required to be in radians. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:sin(inValue) as sinValue insert into OutMediationStream; The function calculates the sine value of the given 'inValue' and directs the output to the output stream, 'OutMediationStream. For example, sin(6d) returns -0.27941549819892586.","title":"sin (Function)"},{"location":"docs/api/5.1.0/#sinh-function","text":"This returns the hyperbolic sine of the value given in radians. This function wraps the java.lang.Math.sinh() function. Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:sinh( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose hyperbolic sine value should be found. Input is required to be in radians. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:sinh(inValue) as sinhValue insert into OutMediationStream; This function calculates the hyperbolic sine value of 'inValue' and directs the output to the output stream, 'OutMediationStream'. For example, sinh(6d) returns 201.71315737027922.","title":"sinh (Function)"},{"location":"docs/api/5.1.0/#sqrt-function","text":"This function returns the square-root of the given value. It wraps the java.lang.Math.sqrt() s function. Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:sqrt( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose square-root value should be found. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:sqrt(inValue) as sqrtValue insert into OutMediationStream; The function calculates the square-root value of the 'inValue' and directs the output to the output stream, 'OutMediationStream'. For example, sqrt(4d) returns 2.","title":"sqrt (Function)"},{"location":"docs/api/5.1.0/#tan-function","text":"This function returns the tan of the given value in radians. It wraps the java.lang.Math.tan() function. Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:tan( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose tan value should be found. Input is required to be in radians. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:tan(inValue) as tanValue insert into OutMediationStream; This function calculates the tan value of the 'inValue' given and directs the output to the output stream, 'OutMediationStream'. For example, tan(6d) returns -0.29100619138474915.","title":"tan (Function)"},{"location":"docs/api/5.1.0/#tanh-function","text":"This function returns the hyperbolic tangent of the value given in radians. It wraps the java.lang.Math.tanh() function. Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:tanh( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose hyperbolic tangent value should be found. Input is required to be in radians. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:tanh(inValue) as tanhValue insert into OutMediationStream; If the 'inVaue' in the input stream is given, this function calculates the hyperbolic tangent value of the same and directs the output to 'OutMediationStream' stream. For example, tanh(6d) returns 0.9999877116507956.","title":"tanh (Function)"},{"location":"docs/api/5.1.0/#todegrees-function","text":"This function converts the value given in radians to degrees. It wraps the java.lang.Math.toDegrees() function. Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:toDegrees( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The input value in radians that should be converted to degrees. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:toDegrees(inValue) as degreesValue insert into OutMediationStream; The function converts the 'inValue' in the input stream from radians to degrees and directs the output to 'OutMediationStream' output stream. For example, toDegrees(6d) returns 343.77467707849394.","title":"toDegrees (Function)"},{"location":"docs/api/5.1.0/#toradians-function","text":"This function converts the value given in degrees to radians. It wraps the java.lang.Math.toRadians() function. Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:toRadians( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The input value in degrees that should be converted to radians. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:toRadians(inValue) as radiansValue insert into OutMediationStream; This function converts the input, from degrees to radians and directs the result to 'OutMediationStream' output stream. For example, toRadians(6d) returns 0.10471975511965977.","title":"toRadians (Function)"},{"location":"docs/api/5.1.0/#rdbms","text":"","title":"Rdbms"},{"location":"docs/api/5.1.0/#cud-stream-processor","text":"This function performs SQL CUD (INSERT, UPDATE, DELETE) queries on data sources. Note: This function to work data sources should be set at the Siddhi Manager level. Origin: siddhi-store-rdbms:7.0.1 Syntax rdbms:cud( STRING datasource.name, STRING query) rdbms:cud( STRING datasource.name, STRING query, STRING|BOOL|INT|DOUBLE|FLOAT|LONG parameter) rdbms:cud( STRING datasource.name, STRING query, STRING|BOOL|INT|DOUBLE|FLOAT|LONG parameter, STRING|BOOL|INT|DOUBLE|FLOAT|LONG ...) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic datasource.name The name of the datasource for which the query should be performed. If Siddhi is used as a Java/Python library the datasource should be explicitly set in the siddhi manager in order for the function to work. STRING No No query The update, delete, or insert query(formatted according to the relevant database type) that needs to be performed. STRING No Yes parameter If the second parameter is a parametrised SQL query, then siddhi attributes can be passed to set the values of the parameters STRING BOOL INT DOUBLE FLOAT LONG Yes Yes System Parameters Name Description Default Value Possible Parameters perform.CUD.operations If this parameter is set to 'true', the RDBMS CUD function is enabled to perform CUD operations. false true false Extra Return Attributes Name Description Possible Types numRecords The number of records manipulated by the query. INT Examples EXAMPLE 1 from TriggerStream#rdbms:cud(\"SAMPLE_DB\", \"UPDATE Customers_Table SET customerName='abc' where customerName='xyz'\") select numRecords insert into RecordStream; This query updates the events from the input stream named 'TriggerStream' with an additional attribute named 'numRecords', of which the value indicates the number of records manipulated. The updated events are inserted into an output stream named 'RecordStream'. EXAMPLE 2 from TriggerStream#rdbms:cud(\"SAMPLE_DB\", \"UPDATE Customers_Table SET customerName=? where customerName=?\", changedName, previousName) select numRecords insert into RecordStream; This query updates the events from the input stream named 'TriggerStream' with an additional attribute named 'numRecords', of which the value indicates the number of records manipulated. The updated events are inserted into an output stream named 'RecordStream'. Here the values of attributes changedName and previousName in the event will be set to the query.","title":"cud (Stream Processor)"},{"location":"docs/api/5.1.0/#query-stream-processor","text":"This function performs SQL retrieval queries on data sources. Note: This function to work data sources should be set at the Siddhi Manager level. Origin: siddhi-store-rdbms:7.0.1 Syntax rdbms:query( STRING datasource.name, STRING attribute.definition.list, STRING query) rdbms:query( STRING datasource.name, STRING attribute.definition.list, STRING query, STRING|BOOL|INT|DOUBLE|FLOAT|LONG parameter) rdbms:query( STRING datasource.name, STRING attribute.definition.list, STRING query, STRING|BOOL|INT|DOUBLE|FLOAT|LONG parameter, STRING|BOOL|INT|DOUBLE|FLOAT|LONG ...) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic datasource.name The name of the datasource for which the query should be performed. If Siddhi is used as a Java/Python library the datasource should be explicitly set in the siddhi manager in order for the function to work. STRING No No attribute.definition.list This is provided as a comma-separated list in the ' AttributeName AttributeType ' format. The SQL query is expected to return the attributes in the given order. e.g., If one attribute is defined here, the SQL query should return one column result set. If more than one column is returned, then the first column is processed. The Siddhi data types supported are 'STRING', 'INT', 'LONG', 'DOUBLE', 'FLOAT', and 'BOOL'. Mapping of the Siddhi data type to the database data type can be done as follows, Siddhi Datatype - Datasource Datatype STRING - CHAR , VARCHAR , LONGVARCHAR INT - INTEGER LONG - BIGINT DOUBLE - DOUBLE FLOAT - REAL BOOL - BIT STRING No No query The select query(formatted according to the relevant database type) that needs to be performed STRING No Yes parameter If the second parameter is a parametrised SQL query, then siddhi attributes can be passed to set the values of the parameters STRING BOOL INT DOUBLE FLOAT LONG Yes Yes Extra Return Attributes Name Description Possible Types attributeName The return attributes will be the ones defined in the parameter attribute.definition.list . STRING INT LONG DOUBLE FLOAT BOOL Examples EXAMPLE 1 from TriggerStream#rdbms:query('SAMPLE_DB', 'creditcardno string, country string, transaction string, amount int', 'select * from Transactions_Table') select creditcardno, country, transaction, amount insert into recordStream; Events inserted into recordStream includes all records matched for the query i.e an event will be generated for each record retrieved from the datasource. The event will include as additional attributes, the attributes defined in the attribute.definition.list (creditcardno, country, transaction, amount). EXAMPLE 2 from TriggerStream#rdbms:query('SAMPLE_DB', 'creditcardno string, country string,transaction string, amount int', 'select * from where country=?', countrySearchWord) select creditcardno, country, transaction, amount insert into recordStream; Events inserted into recordStream includes all records matched for the query i.e an event will be generated for each record retrieved from the datasource. The event will include as additional attributes, the attributes defined in the attribute.definition.list (creditcardno, country, transaction, amount). countrySearchWord value from the event will be set in the query when querying the datasource.","title":"query (Stream Processor)"},{"location":"docs/api/5.1.0/#regex","text":"","title":"Regex"},{"location":"docs/api/5.1.0/#find-function","text":"Finds the subsequence that matches the given regex pattern. Origin: siddhi-execution-regex:5.0.5 Syntax BOOL regex:find( STRING regex, STRING input.sequence) BOOL regex:find( STRING regex, STRING input.sequence, INT starting.index) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic regex A regular expression that is matched to a sequence in order to find the subsequence of the same. For example, \\d\\d(.*)WSO2 . STRING No Yes input.sequence The input sequence to be matched with the regular expression. For example, 21 products are produced by WSO2 . STRING No Yes starting.index The starting index of the input sequence from where the input sequence ismatched with the given regex pattern.For example, 10 . 0 INT Yes Yes Examples EXAMPLE 1 regex:find('\\d\\d(.*)WSO2', '21 products are produced by WSO2 currently') This method attempts to find the subsequence of the input.sequence that matches the regex pattern, \\d\\d(. )WSO2 . It returns true as a subsequence exists. EXAMPLE 2 regex:find('\\d\\d(.*)WSO2', '21 products are produced by WSO2.', 4) This method attempts to find the subsequence of the input.sequence that matches the regex pattern, \\d\\d(. )WSO2 starting from index 4 . It returns 'false' as subsequence does not exists.","title":"find (Function)"},{"location":"docs/api/5.1.0/#group-function","text":"Returns the subsequence captured by the given group during the regex match operation. Origin: siddhi-execution-regex:5.0.5 Syntax STRING regex:group( STRING regex, STRING input.sequence, INT group.id) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic regex A regular expression. For example, \\d\\d(.*)WSO2. STRING No Yes input.sequence The input sequence to be matched with the regular expression. For example, 2 1 products are produced by WSO2 . STRING No Yes group.id The given group id of the regex expression. For example, 2 . INT No Yes Examples EXAMPLE 1 regex:group('\\d\\d(.*)(WSO2.*)(WSO2.*)', '21 products are produced within 10 years by WSO2 currently by WSO2 employees', 3) Function returns 'WSO2 employees', the subsequence captured by the groupID 3 according to the regex pattern, \\d\\d(. )(WSO2. )(WSO2.*) .","title":"group (Function)"},{"location":"docs/api/5.1.0/#lookingat-function","text":"Matches the input.sequence from the beginning against the regex pattern, and unlike regex:matches() it does not require that the entire input.sequence be matched. Origin: siddhi-execution-regex:5.0.5 Syntax BOOL regex:lookingAt( STRING regex, STRING input.sequence) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic regex A regular expression. For example, \\d\\d(.*)WSO2 . STRING No Yes input.sequence The input sequence to be matched with the regular expression. For example, 21 products are produced by WSO2 . STRING No Yes Examples EXAMPLE 1 regex:lookingAt('\\d\\d(.*)(WSO2.*)', '21 products are produced by WSO2 currently in Sri Lanka') Function matches the input.sequence against the regex pattern, \\d\\d(. )(WSO2. ) from the beginning, and as it matches it returns true . EXAMPLE 2 regex:lookingAt('WSO2(.*)middleware(.*)', 'sample test string and WSO2 is situated in trace and it's a middleware company') Function matches the input.sequence against the regex pattern, WSO2(. )middleware(. ) from the beginning, and as it does not match it returns false .","title":"lookingAt (Function)"},{"location":"docs/api/5.1.0/#matches-function","text":"Matches the entire input.sequence against the regex pattern. Origin: siddhi-execution-regex:5.0.5 Syntax BOOL regex:matches( STRING regex, STRING input.sequence) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic regex A regular expression. For example, \\d\\d(.*)WSO2 . STRING No Yes input.sequence The input sequence to be matched with the regular expression. For example, 21 products are produced by WSO2 . STRING No Yes Examples EXAMPLE 1 regex:matches('WSO2(.*)middleware(.*)', 'WSO2 is situated in trace and its a middleware company') Function matches the entire input.sequence against WSO2(. )middleware(. ) regex pattern, and as it matches it returns true . EXAMPLE 2 regex:matches('WSO2(.*)middleware', 'WSO2 is situated in trace and its a middleware company') Function matches the entire input.sequence against WSO2(.*)middleware regex pattern. As it does not match it returns false .","title":"matches (Function)"},{"location":"docs/api/5.1.0/#reorder","text":"","title":"Reorder"},{"location":"docs/api/5.1.0/#akslack-stream-processor","text":"Stream processor performs reordering of out-of-order events optimized for a givenparameter using AQ-K-Slack algorithm . This is best for reordering events on attributes those are used for aggregations.data . Origin: siddhi-execution-reorder:5.0.3 Syntax reorder:akslack( LONG timestamp, INT|FLOAT|LONG|DOUBLE correlation.field) reorder:akslack( LONG timestamp, INT|FLOAT|LONG|DOUBLE correlation.field, LONG batch.size) reorder:akslack( LONG timestamp, INT|FLOAT|LONG|DOUBLE correlation.field, LONG batch.size, LONG timeout) reorder:akslack( LONG timestamp, INT|FLOAT|LONG|DOUBLE correlation.field, LONG batch.size, LONG timeout, LONG max.k) reorder:akslack( LONG timestamp, INT|FLOAT|LONG|DOUBLE correlation.field, LONG batch.size, LONG timeout, LONG max.k, BOOL discard.late.arrival) reorder:akslack( LONG timestamp, INT|FLOAT|LONG|DOUBLE correlation.field, LONG batch.size, LONG timeout, LONG max.k, BOOL discard.late.arrival, DOUBLE error.threshold, DOUBLE confidence.level) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic timestamp The event timestamp on which the events should be ordered. LONG No Yes correlation.field By monitoring the changes in this field Alpha K-Slack dynamically optimises its behavior. This field is used to calculate the runtime window coverage threshold, which represents the upper limit set for unsuccessfully handled late arrivals. INT FLOAT LONG DOUBLE No Yes batch.size The parameter 'batch.size' denotes the number of events that should be considered in the calculation of an alpha value. This should be greater than or equal to 15. 10,000 LONG Yes No timeout A timeout value in milliseconds, where the buffered events who are older than the given timeout period get flushed every second. -1 (timeout is infinite) LONG Yes No max.k The maximum K-Slack window threshold ('K' parameter). 9,223,372,036,854,775,807 (The maximum Long value) LONG Yes No discard.late.arrival If set to true the processor would discarded the out-of-order events arriving later than the K-Slack window, and in otherwise it allows the late arrivals to proceed. false BOOL Yes No error.threshold The error threshold to be applied in Alpha K-Slack algorithm. 0.03 (3%) DOUBLE Yes No confidence.level The confidence level to be applied in Alpha K-Slack algorithm. 0.95 (95%) DOUBLE Yes No Examples EXAMPLE 1 define stream StockStream (eventTime long, symbol string, volume long); @info(name = 'query1') from StockStream#reorder:akslack(eventTime, volume, 20)#window.time(5 min) select eventTime, symbol, sum(volume) as total insert into OutputStream; The query reorders events based on the 'eventTime' attribute value and optimises for aggregating 'volume' attribute considering last 20 events.","title":"akslack (Stream Processor)"},{"location":"docs/api/5.1.0/#kslack-stream-processor","text":"Stream processor performs reordering of out-of-order events using K-Slack algorithm . Origin: siddhi-execution-reorder:5.0.3 Syntax reorder:kslack( LONG timestamp) reorder:kslack( LONG timestamp, LONG timeout) reorder:kslack( LONG timestamp, BOOL discard.late.arrival) reorder:kslack( LONG timestamp, LONG timeout, LONG max.k) reorder:kslack( LONG timestamp, LONG timeout, BOOL discard.late.arrival) reorder:kslack( LONG timestamp, LONG timeout, LONG max.k, BOOL discard.late.arrival) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic timestamp The event timestamp on which the events should be ordered. LONG No Yes timeout A timeout value in milliseconds, where the buffered events who are older than the given timeout period get flushed every second. -1 (timeout is infinite) LONG Yes No max.k The maximum K-Slack window threshold ('K' parameter). 9,223,372,036,854,775,807 (The maximum Long value) LONG Yes No discard.late.arrival If set to true the processor would discarded the out-of-order events arriving later than the K-Slack window, and in otherwise it allows the late arrivals to proceed. false BOOL Yes No Examples EXAMPLE 1 define stream StockStream (eventTime long, symbol string, volume long); @info(name = 'query1') from StockStream#reorder:kslack(eventTime, 5000) select eventTime, symbol, volume insert into OutputStream; The query reorders events based on the 'eventTime' attribute value, and it forcefully flushes all the events who have arrived older than the given 'timeout' value ( 5000 milliseconds) every second.","title":"kslack (Stream Processor)"},{"location":"docs/api/5.1.0/#script","text":"","title":"Script"},{"location":"docs/api/5.1.0/#javascript-script","text":"This extension allows you to include JavaScript functions within the Siddhi Query Language. Origin: siddhi-script-js:5.0.2 Syntax define function FunctionName [javascript] return type { // Script code }; Examples EXAMPLE 1 define function concatJ[JavaScript] return string {\" var str1 = data[0]; var str2 = data[1]; var str3 = data[2]; var res = str1.concat(str2,str3); return res; }; This JS function will consume 3 var variables, concatenate them and will return as a string","title":"javascript (Script)"},{"location":"docs/api/5.1.0/#sink","text":"","title":"Sink"},{"location":"docs/api/5.1.0/#email-sink","text":"The email sink uses the 'smtp' server to publish events via emails. The events can be published in 'text', 'xml' or 'json' formats. The user can define email sink parameters in either the ' SP_HOME /conf/ PROFILE /deployment yaml' file or in the stream definition. The email sink first checks the stream definition for parameters, and if they are no configured there, it checks the 'deployment.yaml' file. If the parameters are not configured in either place, default values are considered for optional parameters. If you need to configure server system parameters that are not provided as options in the stream definition, then those parameters need to be defined them in the 'deployment.yaml' file under 'email sink properties'. For more information about the SMTP server parameters, see https://javaee.github.io/javamail/SMTP-Transport. Further, some email accounts are required to enable the 'access to less secure apps' option. For gmail accounts, you can enable this option via https://myaccount.google.com/lesssecureapps. Origin: siddhi-io-email:2.0.4 Syntax @sink(type=\"email\", username=\" STRING \", address=\" STRING \", password=\" STRING \", host=\" STRING \", port=\" INT \", ssl.enable=\" BOOL \", auth=\" BOOL \", content.type=\" STRING \", subject=\" STRING \", to=\" STRING \", cc=\" STRING \", bcc=\" STRING \", attachments=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic username The username of the email account that is used to send emails. e.g., 'abc' is the username of the 'abc@gmail.com' account. STRING No No address The address of the email account that is used to send emails. STRING No No password The password of the email account. STRING No No host The host name of the SMTP server. e.g., 'smtp.gmail.com' is a host name for a gmail account. The default value 'smtp.gmail.com' is only valid if the email account is a gmail account. smtp.gmail.com STRING Yes No port The port that is used to create the connection. '465' the default value is only valid is SSL is enabled. INT Yes No ssl.enable This parameter specifies whether the connection should be established via a secure connection or not. The value can be either 'true' or 'false'. If it is 'true', then the connection is establish via the 493 port which is a secure connection. true BOOL Yes No auth This parameter specifies whether to use the 'AUTH' command when authenticating or not. If the parameter is set to 'true', an attempt is made to authenticate the user using the 'AUTH' command. true BOOL Yes No content.type The content type can be either 'text/plain' or 'text/html'. text/plain STRING Yes No subject The subject of the mail to be send. STRING No Yes to The address of the 'to' recipient. If there are more than one 'to' recipients, then all the required addresses can be given as a comma-separated list. STRING No Yes cc The address of the 'cc' recipient. If there are more than one 'cc' recipients, then all the required addresses can be given as a comma-separated list. None STRING Yes No bcc The address of the 'bcc' recipient. If there are more than one 'bcc' recipients, then all the required addresses can be given as a comma-separated list. None STRING Yes No attachments File paths of the files that need to be attached to the email. These paths should be absolute paths. They can be either directories or files . If the path is to a directory, all the files located at the first level (i.e., not within another sub directory) are attached. None STRING Yes Yes System Parameters Name Description Default Value Possible Parameters mail.smtp.ssl.trust If this parameter is se, and a socket factory has not been specified, it enables the use of a MailSSLSocketFactory. If this parameter is set to \" \", all the hosts are trusted. If it is set to a whitespace-separated list of hosts, only those specified hosts are trusted. If not, the hosts trusted depends on the certificate presented by the server. String mail.smtp.connectiontimeout The socket connection timeout value in milliseconds. infinite timeout Any Integer mail.smtp.timeout The socket I/O timeout value in milliseconds. infinite timeout Any Integer mail.smtp.from The email address to use for the SMTP MAIL command. This sets the envelope return address. Defaults to msg.getFrom() or InternetAddress.getLocalAddress(). Any valid email address mail.smtp.localport The local port number to bind to when creating the SMTP socket. Defaults to the port number picked by the Socket class. Any Integer mail.smtp.ehlo If this parameter is set to 'false', you must not attempt to sign in with the EHLO command. true true or false mail.smtp.auth.login.disable If this is set to 'true', it is not allowed to use the 'AUTH LOGIN' command. false true or false mail.smtp.auth.plain.disable If this parameter is set to 'true', it is not allowed to use the 'AUTH PLAIN' command. false true or false mail.smtp.auth.digest-md5.disable If this parameter is set to 'true', it is not allowed to use the 'AUTH DIGEST-MD5' command. false true or false mail.smtp.auth.ntlm.disable If this parameter is set to 'true', it is not allowed to use the 'AUTH NTLM' command false true or false mail.smtp.auth.ntlm.domain The NTLM authentication domain. None The valid NTLM authentication domain name. mail.smtp.auth.ntlm.flags NTLM protocol-specific flags. For more details, see http://curl.haxx.se/rfc/ntlm.html#theNtlmFlags. None Valid NTLM protocol-specific flags. mail.smtp.dsn.notify The NOTIFY option to the RCPT command. None Either 'NEVER', or a combination of 'SUCCESS', 'FAILURE', and 'DELAY' (separated by commas). mail.smtp.dsn.ret The 'RET' option to the 'MAIL' command. None Either 'FULL' or 'HDRS'. mail.smtp.sendpartial If this parameter is set to 'true' and a message is addressed to both valid and invalid addresses, the message is sent with a log that reports the partial failure with a 'SendFailedException' error. If this parameter is set to 'false' (which is default), the message is not sent to any of the recipients when the recipient lists contain one or more invalid addresses. false true or false mail.smtp.sasl.enable If this parameter is set to 'true', the system attempts to use the 'javax.security.sasl' package to choose an authentication mechanism for the login. false true or false mail.smtp.sasl.mechanisms Enter a space or a comma-separated list of SASL mechanism names that the system shouldt try to use. None mail.smtp.sasl.authorizationid The authorization ID to be used in the SASL authentication. If no value is specified, the authentication ID (i.e., username) is used. username Valid ID mail.smtp.sasl.realm The realm to be used with the 'DIGEST-MD5' authentication. None mail.smtp.quitwait If this parameter is set to 'false', the 'QUIT' command is issued and the connection is immediately closed. If this parameter is set to 'true' (which is default), the transport waits for the response to the QUIT command. false true or false mail.smtp.reportsuccess If this parameter is set to 'true', the transport to includes an 'SMTPAddressSucceededException' for each address to which the message is successfully delivered. false true or false mail.smtp.socketFactory If this parameter is set to a class that implements the 'javax.net.SocketFactory' interface, this class is used to create SMTP sockets. None Socket Factory mail.smtp.socketFactory.class If this parameter is set, it specifies the name of a class that implements the 'javax.net.SocketFactory interface'. This class is used to create SMTP sockets. None mail.smtp.socketFactory.fallback If this parameter is set to 'true', the failure to create a socket using the specified socket factory class causes the socket to be created using the 'java.net.Socket' class. true true or false mail.smtp.socketFactory.port This specifies the port to connect to when using the specified socket factory. 25 Valid port number mail.smtp.ssl.protocols This specifies the SSL protocols that need to be enabled for the SSL connections. None This parameter specifies a whitespace separated list of tokens that are acceptable to the 'javax.net.ssl.SSLSocket.setEnabledProtocols' method. mail.smtp.starttls.enable If this parameter is set to 'true', it is possible to issue the 'STARTTLS' command (if supported by the server) to switch the connection to a TLS-protected connection before issuing any login commands. false true or false mail.smtp.starttls.required If this parameter is set to 'true', it is required to use the 'STARTTLS' command. If the server does not support the 'STARTTLS' command, or if the command fails, the connection method will fail. false true or false mail.smtp.socks.host This specifies the host name of a SOCKS5 proxy server to be used for the connections to the mail server. None mail.smtp.socks.port This specifies the port number for the SOCKS5 proxy server. This needs to be used only if the proxy server is not using the standard port number 1080. 1080 valid port number mail.smtp.auth.ntlm.disable If this parameter is set to 'true', the AUTH NTLM command cannot be issued. false true or false mail.smtp.mailextension The extension string to be appended to the MAIL command. None mail.smtp.userset If this parameter is set to 'true', you should use the 'RSET' command instead of the 'NOOP' command in the 'isConnected' method. In some scenarios, 'sendmail' responds slowly after many 'NOOP' commands. This is avoided by using 'RSET' instead. false true or false Examples EXAMPLE 1 @sink(type='email', @map(type ='json'), username='sender.account', address='sender.account@gmail.com',password='account.password',subject='Alerts from Wso2 Stream Processor',to='{{email}}',)define stream FooStream (email string, loginId int, name string); This example illustrates how to publish events via an email sink based on the values provided for the mandatory parameters. As shown in the example, it publishes events from the 'FooStream' in 'json' format as emails to the specified 'to' recipients via the email sink. The email is sent from the 'sender.account@gmail.com' email address via a secure connection. EXAMPLE 2 @sink(type='email', @map(type ='json'), subject='Alerts from Wso2 Stream Processor',to='{{email}}',)define stream FooStream (email string, loginId int, name string); This example illustrates how to configure the query parameters and the system parameters in the 'deployment.yaml' file. Corresponding parameters need to be configured under 'email', and namespace:'sink' as follows: siddhi: extensions: - extension: name:'email' namespace:'sink' properties: username: sender's email username address: sender's email address password: sender's email password As shown in the example, events from the FooStream are published in 'json' format via the email sink as emails to the given 'to' recipients. The email is sent from the 'sender.account@gmail.com' address via a secure connection. EXAMPLE 3 @sink(type='email', @map(type ='json'), username='sender.account', address='sender.account@gmail.com',password='account.password',host='smtp.gmail.com',port='465',ssl.enable='true',auth='true',content.type='text/html',subject='Alerts from Wso2 Stream Processor-{{name}}',to='to1.account@gmail.com, to2.account@gmail.com',cc='cc1.account@gmail.com, cc2.account@gmail.com',bcc='bcc1.account@gmail.com)define stream FooStream (name string, age int, country string); This example illustrates how to publish events via the email sink. Events from the 'FooStream' stream are published in 'xml' format via the email sink as a text/html message and sent to the specified 'to', 'cc', and 'bcc' recipients via a secure connection. The 'name' namespace in the 'subject' attribute is the value of the 'name' parameter in the corresponding output event. EXAMPLE 4 @sink(type='email', @map(type ='json'), username='sender.account', address='sender.account@gmail.com',password='account.password',host='smtp.gmail.com',port='465',ssl.enable='true',auth='true',content.type='text/html',subject='Alerts from Wso2 Stream Processor-{{name}}',to='to1.account@gmail.com, to2.account@gmail.com',cc='cc1.account@gmail.com, cc2.account@gmail.com',bcc='bcc1.account@gmail.comattachments= '{{attachments}}')define stream FooStream (name string, age int, country string, attachments string); This example illustrates how to publish events via the email sink. Here, the email also contains attachments. Events from the FooStream are published in 'xml' format via the email sink as a 'text/html' message to the specified 'to','cc', and 'bcc' recipients via a secure connection. The 'name' namespace in the 'subject' attribute is the value for the 'name' parameter in the corresponding output event. The attachments included in the email message are the local files available in the path specified as the value for the 'attachments' attribute.","title":"email (Sink)"},{"location":"docs/api/5.1.0/#file-sink","text":"File Sink can be used to publish (write) event data which is processed within siddhi to files. Siddhi-io-file sink provides support to write both textual and binary data into files Origin: siddhi-io-file:2.0.3 Syntax @sink(type=\"file\", file.uri=\" STRING \", append=\" BOOL \", add.line.separator=\" BOOL \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic file.uri Used to specify the file for data to be written. STRING No Yes append This parameter is used to specify whether the data should be append to the file or not. If append = 'true', data will be write at the end of the file without changing the existing content. If file does not exist, a new fill will be crated and then data will be written. If append append = 'false', If given file exists, existing content will be deleted and then data will be written back to the file. If given file does not exist, a new file will be created and then data will be written on it. true BOOL Yes No add.line.separator This parameter is used to specify whether events added to the file should be separated by a newline. If add.event.separator= 'true',then a newline will be added after data is added to the file. true. (However, if csv mapper is used, it is false) BOOL Yes No Examples EXAMPLE 1 @sink(type='file', @map(type='json'), append='false', file.uri='/abc/{{symbol}}.txt') define stream BarStream (symbol string, price float, volume long); Under above configuration, for each event, a file will be generated if there's no such a file,and then data will be written to that file as json messagesoutput will looks like below. { \"event\":{ \"symbol\":\"WSO2\", \"price\":55.6, \"volume\":100 } }","title":"file (Sink)"},{"location":"docs/api/5.1.0/#grpc-sink","text":"This extension publishes event data encoded into GRPC Classes as defined in the user input jar. This extension has a default gRPC service classes added. The default service is called \"EventService\". Please find the protobuf definition here . If we want to use our custom gRPC services, we have to pack auto-generated gRPC service classes and protobuf classes into a jar file and add it into the project classpath (or to the jars folder in the siddhi-tooling folder if we use it with siddhi-tooling ). Please find the custom protobuf definition that uses in examples here . This grpc sink is used for scenarios where we send a request and don't expect a response back. I.e getting a google.protobuf.Empty response back. Origin: siddhi-io-grpc:1.0.2 Syntax @sink(type=\"grpc\", publisher.url=\" STRING \", headers=\" STRING \", idle.timeout=\" LONG \", keep.alive.time=\" LONG \", keep.alive.timeout=\" LONG \", keep.alive.without.calls=\" BOOL \", enable.retry=\" BOOL \", max.retry.attempts=\" INT \", retry.buffer.size=\" LONG \", per.rpc.buffer.size=\" LONG \", channel.termination.waiting.time=\" LONG \", truststore.file=\" STRING \", truststore.password=\" STRING \", truststore.algorithm=\" STRING \", tls.store.type=\" STRING \", keystore.file=\" STRING \", keystore.password=\" STRING \", keystore.algorithm=\" STRING \", enable.ssl=\" BOOL \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic publisher.url The url to which the outgoing events should be published via this extension. This url should consist the host hostPort, port, fully qualified service name, method name in the following format. grpc://0.0.0.0:9763/ serviceName / methodName For example: grpc://0.0.0.0:9763/org.wso2.grpc.EventService/consume STRING No No headers GRPC Request headers in format \"' key : value ',' key : value '\" . If header parameter is not provided just the payload is sent - STRING Yes No idle.timeout Set the duration in seconds without ongoing RPCs before going to idle mode. 1800 LONG Yes No keep.alive.time Sets the time in seconds without read activity before sending a keepalive ping. Keepalives can increase the load on services so must be used with caution. By default set to Long.MAX_VALUE which disables keep alive pinging. Long.MAX_VALUE LONG Yes No keep.alive.timeout Sets the time in seconds waiting for read activity after sending a keepalive ping. 20 LONG Yes No keep.alive.without.calls Sets whether keepalive will be performed when there are no outstanding RPC on a connection. false BOOL Yes No enable.retry Enables the retry mechanism provided by the gRPC library. false BOOL Yes No max.retry.attempts Sets max number of retry attempts. The total number of retry attempts for each RPC will not exceed this number even if service config may allow a higher number. 5 INT Yes No retry.buffer.size Sets the retry buffer size in bytes. If the buffer limit is exceeded, no RPC could retry at the moment, and in hedging case all hedges but one of the same RPC will cancel. 16777216 LONG Yes No per.rpc.buffer.size Sets the per RPC buffer limit in bytes used for retry. The RPC is not retriable if its buffer limit is exceeded. 1048576 LONG Yes No channel.termination.waiting.time The time in seconds to wait for the channel to become terminated, giving up if the timeout is reached. 5 LONG Yes No truststore.file the file path of truststore. If this is provided then server authentication is enabled - STRING Yes No truststore.password the password of truststore. If this is provided then the integrity of the keystore is checked - STRING Yes No truststore.algorithm the encryption algorithm to be used for server authentication - STRING Yes No tls.store.type TLS store type - STRING Yes No keystore.file the file path of keystore. If this is provided then client authentication is enabled - STRING Yes No keystore.password the password of keystore - STRING Yes No keystore.algorithm the encryption algorithm to be used for client authentication - STRING Yes No enable.ssl to enable ssl. If set to true and truststore.file is not given then it will be set to default carbon jks by default FALSE BOOL Yes No Examples EXAMPLE 1 @sink(type='grpc', publisher.url = 'grpc://134.23.43.35:8080/org.wso2.grpc.EventService/consume', @map(type='json')) define stream FooStream (message String); Here a stream named FooStream is defined with grpc sink. A grpc server should be running at 194.23.98.100 listening to port 8080. sink.id is set to 1 here. So we can write a source with sink.id 1 so that it will listen to responses for requests published from this stream. Note that since we are using EventService/consume the sink will be operating in default mode EXAMPLE 2 @sink(type='grpc', publisher.url = 'grpc://134.23.43.35:8080/org.wso2.grpc.EventService/consume', headers='{{headers}}', @map(type='json'), @payload('{{message}}')) define stream FooStream (message String, headers String); A similar example to above but with headers. Headers are also send into the stream as a data. In the sink headers dynamic property reads the value and sends it as MetaData with the request EXAMPLE 3 @sink(type='grpc', publisher.url = 'grpc://134.23.43.35:8080/org.wso2.grpc.MyService/send', @map(type='protobuf'), define stream FooStream (stringValue string, intValue int,longValue long,booleanValue bool,floatValue float,doubleValue double); Here a stream named FooStream is defined with grpc sink. A grpc server should be running at 134.23.43.35 listening to port 8080 since there is no mapper provided, attributes of stream definition should be as same as the attributes of protobuf message definition. EXAMPLE 4 @sink(type='grpc', publisher.url = 'grpc://134.23.43.35:8080/org.wso2.grpc.MyService/testMap', @map(type='protobuf'), define stream FooStream (stringValue string, intValue int,map object); Here a stream named FooStream is defined with grpc sink. A grpc server should be running at 134.23.43.35 listening to port 8080. The 'map object' in the stream definition defines that this stream is going to use Map object with grpc service. We can use any map object that extends 'java.util.AbstractMap' class. EXAMPLE 5 @sink(type='grpc', publisher.url = 'grpc://134.23.43.35:8080/org.wso2.grpc.MyService/testMap', @map(type='protobuf', @payload(stringValue='a',longValue='b',intValue='c',booleanValue='d',floatValue = 'e', doubleValue = 'f'))) define stream FooStream (a string, b long, c int,d bool,e float,f double); Here a stream named FooStream is defined with grpc sink. A grpc server should be running at 194.23.98.100 listening to port 8080. @payload is provided in this stream, therefore we can use any name for the attributes in the stream definition, but we should correctly map those names with protobuf message attributes. If we are planning to send metadata within a stream we should use @payload to map attributes to identify the metadata attribute and the protobuf attributes separately. EXAMPLE 6 @sink(type='grpc', publisher.url = 'grpc://194.23.98.100:8888/org.wso2.grpc.test.StreamService/clientStream', @map(type='protobuf')) define stream FooStream (stringValue string, intValue int,longValue long,booleanValue bool,floatValue float,doubleValue double); Here in the grpc sink, we are sending a stream of requests to the server that runs on 194.23.98.100 and port 8888. When we need to send a stream of requests from the grpc sink we have to define a client stream RPC method.Then the siddhi will identify whether it's a unary method or a stream method and send requests according to the method type.","title":"grpc (Sink)"},{"location":"docs/api/5.1.0/#grpc-call-sink","text":"This extension publishes event data encoded into GRPC Classes as defined in the user input jar. This extension has a default gRPC service classes jar added. The default service is called \"EventService\". Please find the protobuf definition here . If we want to use our custom gRPC services, we have to pack auto-generated gRPC service classes and protobuf classes into a jar file and add it into the project classpath (or to the jars folder in the siddhi-tooling folder if we use it with siddhi-tooling ). Please find the custom protobuf definition that uses in examples here . This grpc-call sink is used for scenarios where we send a request out and expect a response back. In default mode this will use EventService process method. grpc-call-response source is used to receive the responses. A unique sink.id is used to correlate between the sink and its corresponding source. Origin: siddhi-io-grpc:1.0.2 Syntax @sink(type=\"grpc-call\", publisher.url=\" STRING \", sink.id=\" INT \", headers=\" STRING \", idle.timeout=\" LONG \", keep.alive.time=\" LONG \", keep.alive.timeout=\" LONG \", keep.alive.without.calls=\" BOOL \", enable.retry=\" BOOL \", max.retry.attempts=\" INT \", retry.buffer.size=\" LONG \", per.rpc.buffer.size=\" LONG \", channel.termination.waiting.time=\" LONG \", max.inbound.message.size=\" LONG \", max.inbound.metadata.size=\" LONG \", truststore.file=\" STRING \", truststore.password=\" STRING \", truststore.algorithm=\" STRING \", tls.store.type=\" STRING \", keystore.file=\" STRING \", keystore.password=\" STRING \", keystore.algorithm=\" STRING \", enable.ssl=\" BOOL \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic publisher.url The url to which the outgoing events should be published via this extension. This url should consist the host hostPort, port, fully qualified service name, method name in the following format. grpc://0.0.0.0:9763/ serviceName / methodName For example: grpc://0.0.0.0:9763/org.wso2.grpc.EventService/consume STRING No No sink.id a unique ID that should be set for each grpc-call-sink. There is a 1:1 mapping between grpc-call sinks and grpc-call-response sources. Each sink has one particular source listening to the responses to requests published from that sink. So the same sink.id should be given when writing the source also. INT No No headers GRPC Request headers in format \"' key : value ',' key : value '\" . If header parameter is not provided just the payload is sent - STRING Yes No idle.timeout Set the duration in seconds without ongoing RPCs before going to idle mode. 1800 LONG Yes No keep.alive.time Sets the time in seconds without read activity before sending a keepalive ping. Keepalives can increase the load on services so must be used with caution. By default set to Long.MAX_VALUE which disables keep alive pinging. Long.MAX_VALUE LONG Yes No keep.alive.timeout Sets the time in seconds waiting for read activity after sending a keepalive ping. 20 LONG Yes No keep.alive.without.calls Sets whether keepalive will be performed when there are no outstanding RPC on a connection. false BOOL Yes No enable.retry Enables the retry and hedging mechanism provided by the gRPC library. false BOOL Yes No max.retry.attempts Sets max number of retry attempts. The total number of retry attempts for each RPC will not exceed this number even if service config may allow a higher number. 5 INT Yes No retry.buffer.size Sets the retry buffer size in bytes. If the buffer limit is exceeded, no RPC could retry at the moment, and in hedging case all hedges but one of the same RPC will cancel. 16777216 LONG Yes No per.rpc.buffer.size Sets the per RPC buffer limit in bytes used for retry. The RPC is not retriable if its buffer limit is exceeded. 1048576 LONG Yes No channel.termination.waiting.time The time in seconds to wait for the channel to become terminated, giving up if the timeout is reached. 5 LONG Yes No max.inbound.message.size Sets the maximum message size allowed to be received on the channel in bytes 4194304 LONG Yes No max.inbound.metadata.size Sets the maximum size of metadata allowed to be received in bytes 8192 LONG Yes No truststore.file the file path of truststore. If this is provided then server authentication is enabled - STRING Yes No truststore.password the password of truststore. If this is provided then the integrity of the keystore is checked - STRING Yes No truststore.algorithm the encryption algorithm to be used for server authentication - STRING Yes No tls.store.type TLS store type - STRING Yes No keystore.file the file path of keystore. If this is provided then client authentication is enabled - STRING Yes No keystore.password the password of keystore - STRING Yes No keystore.algorithm the encryption algorithm to be used for client authentication - STRING Yes No enable.ssl to enable ssl. If set to true and truststore.file is not given then it will be set to default carbon jks by default FALSE BOOL Yes No Examples EXAMPLE 1 @sink(type='grpc-call', publisher.url = 'grpc://194.23.98.100:8080/EventService/process', sink.id= '1', @map(type='json')) define stream FooStream (message String); @source(type='grpc-call-response', sink.id= '1') define stream BarStream (message String); Here a stream named FooStream is defined with grpc sink. A grpc server should be running at 194.23.98.100 listening to port 8080. sink.id is set to 1 here. So we can write a source with sink.id 1 so that it will listen to responses for requests published from this stream. Note that since we are using EventService/process the sink will be operating in default mode EXAMPLE 2 @sink(type='grpc-call', publisher.url = 'grpc://194.23.98.100:8080/EventService/process', sink.id= '1', @map(type='json')) define stream FooStream (message String); @source(type='grpc-call-response', sink.id= '1') define stream BarStream (message String); Here with the same FooStream definition we have added a BarStream which has a grpc-call-response source with the same sink.id 1. So the responses for calls sent from the FooStream will be added to BarStream. EXAMPLE 3 @sink(type='grpc-call', publisher.url = 'grpc://194.23.98.100:8888/org.wso2.grpc.test.MyService/process', sink.id= '1', @map(type='protobuf')) define stream FooStream (stringValue string, intValue int,longValue long,booleanValue bool,floatValue float,doubleValue double); @source(type='grpc-call-response', receiver.url = 'grpc://localhost:8888/org.wso2.grpc.MyService/process', sink.id= '1', @map(type='protobuf'))define stream FooStream (stringValue string, intValue int,longValue long,booleanValue bool,floatValue float,doubleValue double); Here a stream named FooStream is defined with grpc sink. A grpc server should be running at 194.23.98.100 listening to port 8080. We have added another stream called BarStream which is a grpc-call-response source with the same sink.id 1 and as same as FooStream definition. So the responses for calls sent from the FooStream will be added to BarStream. Since there is no mapping available in the stream definition attributes names should be as same as the attributes of the protobuf message definition. (Here the only reason we provide receiver.url in the grpc-call-response source is for protobuf mapper to map Response into a siddhi event, we can give any address and any port number in the URL, but we should provide the service name and the method name correctly) EXAMPLE 4 @sink(type='grpc-call', publisher.url = 'grpc://194.23.98.100:8888/org.wso2.grpc.test.MyService/process', sink.id= '1', @map(type='protobuf', @payload(stringValue='a',longValue='c',intValue='b',booleanValue='d',floatValue = 'e', doubleValue = 'f')))define stream FooStream (a string, b int,c long,d bool,e float,f double); @source(type='grpc-call-response', receiver.url = 'grpc://localhost:8888/org.wso2.grpc.test.MyService/process', sink.id= '1', @map(type='protobuf',@attributes(a = 'stringValue', b = 'intValue', c = 'longValue',d = 'booleanValue', e ='floatValue', f ='doubleValue')))define stream FooStream (a string, b int,c long,d bool,e float,f double); Here with the same FooStream definition we have added a BarStream which has a grpc-call-response source with the same sink.id 1. So the responses for calls sent from the FooStream will be added to BarStream. In this stream we provided mapping for both the sink and the source. so we can use any name for the attributes in the stream definition, but we have to map those attributes with correct protobuf attributes. As same as the grpc-sink, if we are planning to use metadata we should map the attributes.","title":"grpc-call (Sink)"},{"location":"docs/api/5.1.0/#grpc-service-response-sink","text":"This extension is used to send responses back to a gRPC client after receiving requests through grpc-service source. This correlates with the particular source using a unique source.id Origin: siddhi-io-grpc:1.0.2 Syntax @sink(type=\"grpc-service-response\", source.id=\" INT \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic source.id A unique id to identify the correct source to which this sink is mapped. There is a 1:1 mapping between source and sink INT No No Examples EXAMPLE 1 @sink(type='grpc-service-response', source.id='1', @map(type='json')) define stream BarStream (messageId String, message String); @source(type='grpc-service', url='grpc://134.23.43.35:8080/org.wso2.grpc.EventService/process', source.id='1', @map(type='json', @attributes(messageId='trp:messageId', message='message'))) define stream FooStream (messageId String, message String); from FooStream select * insert into BarStream; The grpc requests are received through the grpc-service sink. Each received event is sent back through grpc-service-source. This is just a passthrough through Siddhi as we are selecting everything from FooStream and inserting into BarStream.","title":"grpc-service-response (Sink)"},{"location":"docs/api/5.1.0/#http-sink","text":"HTTP sink publishes messages via HTTP or HTTPS protocols using methods such as POST, GET, PUT, and DELETE on formats text , XML and JSON . It can also publish to endpoints protected by basic authentication or OAuth 2.0. Origin: siddhi-io-http:2.1.2 Syntax @sink(type=\"http\", publisher.url=\" STRING \", basic.auth.username=\" STRING \", basic.auth.password=\" STRING \", https.truststore.file=\" STRING \", https.truststore.password=\" STRING \", oauth.username=\" STRING \", oauth.password=\" STRING \", consumer.key=\" STRING \", consumer.secret=\" STRING \", token.url=\" STRING \", refresh.token=\" STRING \", headers=\" STRING \", method=\" STRING \", socket.idle.timeout=\" INT \", chunk.disabled=\" BOOL \", ssl.protocol=\" STRING \", ssl.verification.disabled=\" BOOL \", tls.store.type=\" STRING \", ssl.configurations=\" STRING \", proxy.host=\" STRING \", proxy.port=\" STRING \", proxy.username=\" STRING \", proxy.password=\" STRING \", client.bootstrap.configurations=\" STRING \", max.pool.active.connections=\" INT \", min.pool.idle.connections=\" INT \", max.pool.idle.connections=\" INT \", min.evictable.idle.time=\" STRING \", time.between.eviction.runs=\" STRING \", max.wait.time=\" STRING \", test.on.borrow=\" BOOL \", test.while.idle=\" BOOL \", exhausted.action=\" INT \", hostname.verification.enabled=\" BOOL \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic publisher.url The URL to which the outgoing events should be published. Examples: http://localhost:8080/endpoint , https://localhost:8080/endpoint STRING No No basic.auth.username The username to be included in the authentication header when calling endpoints protected by basic authentication. basic.auth.password property should be also set when using this property. - STRING Yes No basic.auth.password The password to be included in the authentication header when calling endpoints protected by basic authentication. basic.auth.username property should be also set when using this property. - STRING Yes No https.truststore.file The file path of the client truststore when sending messages through https protocol. ${carbon.home}/resources/security/client-truststore.jks STRING Yes No https.truststore.password The password for the client-truststore. wso2carbon STRING Yes No oauth.username The username to be included in the authentication header when calling endpoints protected by OAuth 2.0. oauth.password property should be also set when using this property. - STRING Yes No oauth.password The password to be included in the authentication header when calling endpoints protected by OAuth 2.0. oauth.username property should be also set when using this property. - STRING Yes No consumer.key Consumer key used for calling endpoints protected by OAuth 2.0 - STRING Yes No consumer.secret Consumer secret used for calling endpoints protected by OAuth 2.0 - STRING Yes No token.url Token URL to generate a new access tokens when calling endpoints protected by OAuth 2.0 - STRING Yes No refresh.token Refresh token used for generating new access tokens when calling endpoints protected by OAuth 2.0 - STRING Yes No headers HTTP request headers in format \"' key : value ',' key : value '\" . When Content-Type header is not provided the system derives the Content-Type based on the provided sink mapper as following: - @map(type='xml') : application/xml - @map(type='json') : application/json - @map(type='text') : plain/text - @map(type='keyvalue') : application/x-www-form-urlencoded - For all other cases system defaults to plain/text Also the Content-Length header need not to be provided, as the system automatically defines it by calculating the size of the payload. Content-Type and Content-Length headers STRING Yes No method The HTTP method used for calling the endpoint. POST STRING Yes No socket.idle.timeout Socket timeout in millis. 6000 INT Yes No chunk.disabled Disable chunked transfer encoding. false BOOL Yes No ssl.protocol SSL/TLS protocol. TLS STRING Yes No ssl.verification.disabled Disable SSL verification. false BOOL Yes No tls.store.type TLS store type. JKS STRING Yes No ssl.configurations SSL/TSL configurations in format \"' key : value ',' key : value '\" . Some supported parameters: - SSL/TLS protocols: 'sslEnabledProtocols:TLSv1.1,TLSv1.2' - List of ciphers: 'ciphers:TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256' - Enable session creation: 'client.enable.session.creation:true' - Supported server names: 'server.suported.server.names:server' - Add HTTP SNIMatcher: 'server.supported.snimatchers:SNIMatcher' - STRING Yes No proxy.host Proxy server host - STRING Yes No proxy.port Proxy server port - STRING Yes No proxy.username Proxy server username - STRING Yes No proxy.password Proxy server password - STRING Yes No client.bootstrap.configurations Client bootstrap configurations in format \"' key : value ',' key : value '\" . Some supported configurations : - Client connect timeout in millis: 'client.bootstrap.connect.timeout:15000' - Client socket timeout in seconds: 'client.bootstrap.socket.timeout:15' - Client socket reuse: 'client.bootstrap.socket.reuse:true' - Enable TCP no delay: 'client.bootstrap.nodelay:true' - Enable client keep alive: 'client.bootstrap.keepalive:true' - Send buffer size: 'client.bootstrap.sendbuffersize:1048576' - Receive buffer size: 'client.bootstrap.recievebuffersize:1048576' - STRING Yes No max.pool.active.connections Maximum possible number of active connection per client pool. -1 INT Yes No min.pool.idle.connections Minimum number of idle connections that can exist per client pool. 0 INT Yes No max.pool.idle.connections Maximum number of idle connections that can exist per client pool. 100 INT Yes No min.evictable.idle.time Minimum time (in millis) a connection may sit idle in the client pool before it become eligible for eviction. 300000 STRING Yes No time.between.eviction.runs Time between two eviction operations (in millis) on the client pool. 30000 STRING Yes No max.wait.time The maximum time (in millis) the pool will wait (when there are no available connections) for a connection to be returned to the pool. 60000 STRING Yes No test.on.borrow Enable connections to be validated before being borrowed from the client pool. true BOOL Yes No test.while.idle Enable connections to be validated during the eviction operation (if any). true BOOL Yes No exhausted.action Action that should be taken when the maximum number of active connections are being used. This action should be indicated as an int and possible action values are following. 0 - Fail the request. 1 - Block the request, until a connection returns to the pool. 2 - Grow the connection pool size. 1 (Block when exhausted) INT Yes No hostname.verification.enabled Enable hostname verification. true BOOL Yes No System Parameters Name Description Default Value Possible Parameters clientBootstrapClientGroupSize Number of client threads to perform non-blocking read and write to one or more channels. (Number of available processors) * 2 Any positive integer clientBootstrapBossGroupSize Number of boss threads to accept incoming connections. Number of available processors Any positive integer clientBootstrapWorkerGroupSize Number of worker threads to accept the connections from boss threads and perform non-blocking read and write from one or more channels. (Number of available processors) * 2 Any positive integer trustStoreLocation The default truststore file path. ${carbon.home}/resources/security/client-truststore.jks Path to client truststore .jks file trustStorePassword The default truststore password. wso2carbon Truststore password as string Examples EXAMPLE 1 @sink(type = 'http', publisher.url = 'http://stocks.com/stocks', @map(type = 'json')) define stream StockStream (symbol string, price float, volume long); Events arriving on the StockStream will be published to the HTTP endpoint http://stocks.com/stocks using POST method with Content-Type application/json by converting those events to the default JSON format as following: { \"event\": { \"symbol\": \"FB\", \"price\": 24.5, \"volume\": 5000 } } EXAMPLE 2 @sink(type='http', publisher.url = 'http://localhost:8009/foo', client.bootstrap.configurations = \"'client.bootstrap.socket.timeout:20'\", max.pool.active.connections = '1', headers = \"{{headers}}\", @map(type='xml', @payload(\"\"\" stock {{payloadBody}} /stock \"\"\"))) define stream FooStream (payloadBody String, headers string); Events arriving on FooStream will be published to the HTTP endpoint http://localhost:8009/foo using POST method with Content-Type application/xml and setting payloadBody and header attribute values. If the payloadBody contains symbol WSO2 /symbol price 55.6 /price volume 100 /volume and header contains 'topic:foobar' values, then the system will generate an output with the body: stock symbol WSO2 /symbol price 55.6 /price volume 100 /volume /stock and HTTP headers: Content-Length:xxx , Content-Location:'xxx' , Content-Type:'application/xml' , HTTP_METHOD:'POST'","title":"http (Sink)"},{"location":"docs/api/5.1.0/#http-call-sink","text":"The http-call sink publishes messages to endpoints via HTTP or HTTPS protocols using methods such as POST, GET, PUT, and DELETE on formats text , XML or JSON and consume responses through its corresponding http-call-response source. It also supports calling endpoints protected with basic authentication or OAuth 2.0. Origin: siddhi-io-http:2.1.2 Syntax @sink(type=\"http-call\", publisher.url=\" STRING \", sink.id=\" STRING \", basic.auth.username=\" STRING \", basic.auth.password=\" STRING \", https.truststore.file=\" STRING \", https.truststore.password=\" STRING \", oauth.username=\" STRING \", oauth.password=\" STRING \", consumer.key=\" STRING \", consumer.secret=\" STRING \", token.url=\" STRING \", refresh.token=\" STRING \", headers=\" STRING \", method=\" STRING \", downloading.enabled=\" BOOL \", download.path=\" STRING \", blocking.io=\" BOOL \", socket.idle.timeout=\" INT \", chunk.disabled=\" BOOL \", ssl.protocol=\" STRING \", ssl.verification.disabled=\" BOOL \", ssl.configurations=\" STRING \", proxy.host=\" STRING \", proxy.port=\" STRING \", proxy.username=\" STRING \", proxy.password=\" STRING \", client.bootstrap.configurations=\" STRING \", max.pool.active.connections=\" INT \", min.pool.idle.connections=\" INT \", max.pool.idle.connections=\" INT \", min.evictable.idle.time=\" STRING \", time.between.eviction.runs=\" STRING \", max.wait.time=\" STRING \", test.on.borrow=\" BOOL \", test.while.idle=\" BOOL \", exhausted.action=\" INT \", hostname.verification.enabled=\" BOOL \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic publisher.url The URL which should be called. Examples: http://localhost:8080/endpoint , https://localhost:8080/endpoint STRING No No sink.id Identifier to correlate the http-call sink to its corresponding http-call-response sources to retrieved the responses. STRING No No basic.auth.username The username to be included in the authentication header when calling endpoints protected by basic authentication. basic.auth.password property should be also set when using this property. - STRING Yes No basic.auth.password The password to be included in the authentication header when calling endpoints protected by basic authentication. basic.auth.username property should be also set when using this property. - STRING Yes No https.truststore.file The file path of the client truststore when sending messages through https protocol. ${carbon.home}/resources/security/client-truststore.jks STRING Yes No https.truststore.password The password for the client-truststore. wso2carbon STRING Yes No oauth.username The username to be included in the authentication header when calling endpoints protected by OAuth 2.0. oauth.password property should be also set when using this property. - STRING Yes No oauth.password The password to be included in the authentication header when calling endpoints protected by OAuth 2.0. oauth.username property should be also set when using this property. - STRING Yes No consumer.key Consumer key used for calling endpoints protected by OAuth 2.0 - STRING Yes No consumer.secret Consumer secret used for calling endpoints protected by OAuth 2.0 - STRING Yes No token.url Token URL to generate a new access tokens when calling endpoints protected by OAuth 2.0 - STRING Yes No refresh.token Refresh token used for generating new access tokens when calling endpoints protected by OAuth 2.0 - STRING Yes No headers HTTP request headers in format \"' key : value ',' key : value '\" . When the Content-Type header is not provided the system decides the Content-Type based on the provided sink mapper as following: - @map(type='xml') : application/xml - @map(type='json') : application/json - @map(type='text') : plain/text - @map(type='keyvalue') : application/x-www-form-urlencoded - For all other cases system defaults to plain/text Also the Content-Length header need not to be provided, as the system automatically defines it by calculating the size of the payload. Content-Type and Content-Length headers STRING Yes No method The HTTP method used for calling the endpoint. POST STRING Yes No downloading.enabled Enable response received by the http-call-response source to be written to a file. When this is enabled the download.path property should be also set. false BOOL Yes No download.path The absolute file path along with the file name where the downloads should be saved. - STRING Yes Yes blocking.io Blocks the request thread until a response it received from HTTP call-response source before sending any other request. false BOOL Yes No socket.idle.timeout Socket timeout in millis. 6000 INT Yes No chunk.disabled Disable chunked transfer encoding. false BOOL Yes No ssl.protocol SSL/TLS protocol. TLS STRING Yes No ssl.verification.disabled Disable SSL verification. false BOOL Yes No ssl.configurations SSL/TSL configurations. Expected format \"' key : value ',' key : value '\" . Some supported parameters: - SSL/TLS protocols: 'sslEnabledProtocols:TLSv1.1,TLSv1.2' - List of ciphers: 'ciphers:TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256' - Enable session creation: 'client.enable.session.creation:true' - Supported server names: 'server.suported.server.names:server' - Add HTTP SNIMatcher: 'server.supported.snimatchers:SNIMatcher' - STRING Yes No proxy.host Proxy server host - STRING Yes No proxy.port Proxy server port - STRING Yes No proxy.username Proxy server username - STRING Yes No proxy.password Proxy server password - STRING Yes No client.bootstrap.configurations Client bootstrap configurations in format \"' key : value ',' key : value '\" . Some supported configurations : - Client connect timeout in millis: 'client.bootstrap.connect.timeout:15000' - Client socket timeout in seconds: 'client.bootstrap.socket.timeout:15' - Client socket reuse: 'client.bootstrap.socket.reuse:true' - Enable TCP no delay: 'client.bootstrap.nodelay:true' - Enable client keep alive: 'client.bootstrap.keepalive:true' - Send buffer size: 'client.bootstrap.sendbuffersize:1048576' - Receive buffer size: 'client.bootstrap.recievebuffersize:1048576' - STRING Yes No max.pool.active.connections Maximum possible number of active connection per client pool. -1 INT Yes No min.pool.idle.connections Minimum number of idle connections that can exist per client pool. 0 INT Yes No max.pool.idle.connections Maximum number of idle connections that can exist per client pool. 100 INT Yes No min.evictable.idle.time Minimum time (in millis) a connection may sit idle in the client pool before it become eligible for eviction. 300000 STRING Yes No time.between.eviction.runs Time between two eviction operations (in millis) on the client pool. 30000 STRING Yes No max.wait.time The maximum time (in millis) the pool will wait (when there are no available connections) for a connection to be returned to the pool. 60000 STRING Yes No test.on.borrow Enable connections to be validated before being borrowed from the client pool. true BOOL Yes No test.while.idle Enable connections to be validated during the eviction operation (if any). true BOOL Yes No exhausted.action Action that should be taken when the maximum number of active connections are being used. This action should be indicated as an int and possible action values are following. 0 - Fail the request. 1 - Block the request, until a connection returns to the pool. 2 - Grow the connection pool size. 1 (Block when exhausted) INT Yes No hostname.verification.enabled Enable hostname verification true BOOL Yes No System Parameters Name Description Default Value Possible Parameters clientBootstrapClientGroupSize Number of client threads to perform non-blocking read and write to one or more channels. (Number of available processors) * 2 Any positive integer clientBootstrapBossGroupSize Number of boss threads to accept incoming connections. Number of available processors Any positive integer clientBootstrapWorkerGroupSize Number of worker threads to accept the connections from boss threads and perform non-blocking read and write from one or more channels. (Number of available processors) * 2 Any positive integer trustStoreLocation The default truststore file path. ${carbon.home}/resources/security/client-truststore.jks Path to client truststore .jks file trustStorePassword The default truststore password. wso2carbon Truststore password as string Examples EXAMPLE 1 @sink(type='http-call', sink.id='foo', publisher.url='http://localhost:8009/foo', @map(type='xml', @payload('{{payloadBody}}'))) define stream FooStream (payloadBody string); @source(type='http-call-response', sink.id='foo', @map(type='text', regex.A='((.|\\n)*)', @attributes(headers='trp:headers', message='A[1]'))) define stream ResponseStream(message string, headers string); When events arrive in FooStream , http-call sink makes calls to endpoint on url http://localhost:8009/foo with POST method and Content-Type application/xml . If the event payloadBody attribute contains following XML: item name apple /name price 55 /price quantity 5 /quantity /item the http-call sink maps that and sends it to the endpoint. When endpoint sends a response it will be consumed by the corresponding http-call-response source correlated via the same sink.id foo and that will map the response message and send it via ResponseStream steam by assigning the message body as message attribute and response headers as headers attribute of the event. EXAMPLE 2 @sink(type='http-call', publisher.url='http://localhost:8005/files/{{name}}' downloading.enabled='true', download.path='{{downloadPath}}{{name}}', method='GET', sink.id='download', @map(type='json')) define stream DownloadRequestStream(name String, id int, downloadPath string); @source(type='http-call-response', sink.id='download', http.status.code='2\\\\d+', @map(type='text', regex.A='((.|\\n)*)', @attributes(name='trp:name', id='trp:id', file='A[1]'))) define stream ResponseStream2xx(name string, id string, file string); @source(type='http-call-response', sink.id='download', http.status.code='4\\\\d+', @map(type='text', regex.A='((.|\\n)*)', @attributes(errorMsg='A[1]'))) define stream ResponseStream4xx(errorMsg string); When events arrive in DownloadRequestStream with name : foo.txt , id : 75 and downloadPath : /user/download/ the http-call sink sends a GET request to the url http://localhost:8005/files/foo.txt to download the file to the given path /user/download/foo.txt and capture the response via its corresponding http-call-response source based on the response status code. If the response status code is in the range of 200 the message will be received by the http-call-response source associated with the ResponseStream2xx stream which expects http.status.code with regex 2\\d+ while downloading the file to the local file system on the path /user/download/foo.txt and mapping the response message having the absolute file path to event's file attribute. If the response status code is in the range of 400 then the message will be received by the http-call-response source associated with the ResponseStream4xx stream which expects http.status.code with regex 4\\d+ while mapping the error response to the errorMsg attribute of the event.","title":"http-call (Sink)"},{"location":"docs/api/5.1.0/#http-request-sink","text":"Deprecated (Use http-call sink instead). The http-request sink publishes messages to endpoints via HTTP or HTTPS protocols using methods such as POST, GET, PUT, and DELETE on formats text , XML or JSON and consume responses through its corresponding http-response source. It also supports calling endpoints protected with basic authentication or OAuth 2.0. Origin: siddhi-io-http:2.1.2 Syntax @sink(type=\"http-request\", publisher.url=\" STRING \", sink.id=\" STRING \", basic.auth.username=\" STRING \", basic.auth.password=\" STRING \", https.truststore.file=\" STRING \", https.truststore.password=\" STRING \", oauth.username=\" STRING \", oauth.password=\" STRING \", consumer.key=\" STRING \", consumer.secret=\" STRING \", token.url=\" STRING \", refresh.token=\" STRING \", headers=\" STRING \", method=\" STRING \", downloading.enabled=\" BOOL \", download.path=\" STRING \", blocking.io=\" BOOL \", socket.idle.timeout=\" INT \", chunk.disabled=\" BOOL \", ssl.protocol=\" STRING \", ssl.verification.disabled=\" BOOL \", ssl.configurations=\" STRING \", proxy.host=\" STRING \", proxy.port=\" STRING \", proxy.username=\" STRING \", proxy.password=\" STRING \", client.bootstrap.configurations=\" STRING \", max.pool.active.connections=\" INT \", min.pool.idle.connections=\" INT \", max.pool.idle.connections=\" INT \", min.evictable.idle.time=\" STRING \", time.between.eviction.runs=\" STRING \", max.wait.time=\" STRING \", test.on.borrow=\" BOOL \", test.while.idle=\" BOOL \", exhausted.action=\" INT \", hostname.verification.enabled=\" BOOL \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic publisher.url The URL which should be called. Examples: http://localhost:8080/endpoint , https://localhost:8080/endpoint STRING No No sink.id Identifier to correlate the http-request sink to its corresponding http-response sources to retrieved the responses. STRING No No basic.auth.username The username to be included in the authentication header when calling endpoints protected by basic authentication. basic.auth.password property should be also set when using this property. - STRING Yes No basic.auth.password The password to be included in the authentication header when calling endpoints protected by basic authentication. basic.auth.username property should be also set when using this property. - STRING Yes No https.truststore.file The file path of the client truststore when sending messages through https protocol. ${carbon.home}/resources/security/client-truststore.jks STRING Yes No https.truststore.password The password for the client-truststore. wso2carbon STRING Yes No oauth.username The username to be included in the authentication header when calling endpoints protected by OAuth 2.0. oauth.password property should be also set when using this property. - STRING Yes No oauth.password The password to be included in the authentication header when calling endpoints protected by OAuth 2.0. oauth.username property should be also set when using this property. - STRING Yes No consumer.key Consumer key used for calling endpoints protected by OAuth 2.0 - STRING Yes No consumer.secret Consumer secret used for calling endpoints protected by OAuth 2.0 - STRING Yes No token.url Token URL to generate a new access tokens when calling endpoints protected by OAuth 2.0 - STRING Yes No refresh.token Refresh token used for generating new access tokens when calling endpoints protected by OAuth 2.0 - STRING Yes No headers HTTP request headers in format \"' key : value ',' key : value '\" . When the Content-Type header is not provided the system decides the Content-Type based on the provided sink mapper as following: - @map(type='xml') : application/xml - @map(type='json') : application/json - @map(type='text') : plain/text - @map(type='keyvalue') : application/x-www-form-urlencoded - For all other cases system defaults to plain/text Also the Content-Length header need not to be provided, as the system automatically defines it by calculating the size of the payload. Content-Type and Content-Length headers STRING Yes No method The HTTP method used for calling the endpoint. POST STRING Yes No downloading.enabled Enable response received by the http-response source to be written to a file. When this is enabled the download.path property should be also set. false BOOL Yes No download.path The absolute file path along with the file name where the downloads should be saved. - STRING Yes Yes blocking.io Blocks the request thread until a response it received from HTTP call-response source before sending any other request. false BOOL Yes No socket.idle.timeout Socket timeout in millis. 6000 INT Yes No chunk.disabled Disable chunked transfer encoding. false BOOL Yes No ssl.protocol SSL/TLS protocol. TLS STRING Yes No ssl.verification.disabled Disable SSL verification. false BOOL Yes No ssl.configurations SSL/TSL configurations in format \"' key : value ',' key : value '\" . Some supported parameters: - SSL/TLS protocols: 'sslEnabledProtocols:TLSv1.1,TLSv1.2' - List of ciphers: 'ciphers:TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256' - Enable session creation: 'client.enable.session.creation:true' - Supported server names: 'server.suported.server.names:server' - Add HTTP SNIMatcher: 'server.supported.snimatchers:SNIMatcher' - STRING Yes No proxy.host Proxy server host - STRING Yes No proxy.port Proxy server port - STRING Yes No proxy.username Proxy server username - STRING Yes No proxy.password Proxy server password - STRING Yes No client.bootstrap.configurations Client bootstrap configurations in format \"' key : value ',' key : value '\" . Some supported configurations : - Client connect timeout in millis: 'client.bootstrap.connect.timeout:15000' - Client socket timeout in seconds: 'client.bootstrap.socket.timeout:15' - Client socket reuse: 'client.bootstrap.socket.reuse:true' - Enable TCP no delay: 'client.bootstrap.nodelay:true' - Enable client keep alive: 'client.bootstrap.keepalive:true' - Send buffer size: 'client.bootstrap.sendbuffersize:1048576' - Receive buffer size: 'client.bootstrap.recievebuffersize:1048576' - STRING Yes No max.pool.active.connections Maximum possible number of active connection per client pool. -1 INT Yes No min.pool.idle.connections Minimum number of idle connections that can exist per client pool. 0 INT Yes No max.pool.idle.connections Maximum number of idle connections that can exist per client pool. 100 INT Yes No min.evictable.idle.time Minimum time (in millis) a connection may sit idle in the client pool before it become eligible for eviction. 300000 STRING Yes No time.between.eviction.runs Time between two eviction operations (in millis) on the client pool. 30000 STRING Yes No max.wait.time The maximum time (in millis) the pool will wait (when there are no available connections) for a connection to be returned to the pool. 60000 STRING Yes No test.on.borrow Enable connections to be validated before being borrowed from the client pool. true BOOL Yes No test.while.idle Enable connections to be validated during the eviction operation (if any). true BOOL Yes No exhausted.action Action that should be taken when the maximum number of active connections are being used. This action should be indicated as an int and possible action values are following. 0 - Fail the request. 1 - Block the request, until a connection returns to the pool. 2 - Grow the connection pool size. 1 (Block when exhausted) INT Yes No hostname.verification.enabled Enable hostname verification true BOOL Yes No System Parameters Name Description Default Value Possible Parameters clientBootstrapClientGroupSize Number of client threads to perform non-blocking read and write to one or more channels. (Number of available processors) * 2 Any positive integer clientBootstrapBossGroupSize Number of boss threads to accept incoming connections. Number of available processors Any positive integer clientBootstrapWorkerGroupSize Number of worker threads to accept the connections from boss threads and perform non-blocking read and write from one or more channels. (Number of available processors) * 2 Any positive integer trustStoreLocation The default truststore file path. ${carbon.home}/resources/security/client-truststore.jks Path to client truststore .jks file trustStorePassword The default truststore password. wso2carbon Truststore password as string Examples EXAMPLE 1 @sink(type='http-request', sink.id='foo', publisher.url='http://localhost:8009/foo', @map(type='xml', @payload('{{payloadBody}}'))) define stream FooStream (payloadBody string); @source(type='http-response', sink.id='foo', @map(type='text', regex.A='((.|\\n)*)', @attributes(headers='trp:headers', message='A[1]'))) define stream ResponseStream(message string, headers string); When events arrive in FooStream , http-request sink makes calls to endpoint on url http://localhost:8009/foo with POST method and Content-Type application/xml . If the event payloadBody attribute contains following XML: item name apple /name price 55 /price quantity 5 /quantity /item the http-request sink maps that and sends it to the endpoint. When endpoint sends a response it will be consumed by the corresponding http-response source correlated via the same sink.id foo and that will map the response message and send it via ResponseStream steam by assigning the message body as message attribute and response headers as headers attribute of the event. EXAMPLE 2 @sink(type='http-request', publisher.url='http://localhost:8005/files/{{name}}' downloading.enabled='true', download.path='{{downloadPath}}{{name}}', method='GET', sink.id='download', @map(type='json')) define stream DownloadRequestStream(name String, id int, downloadPath string); @source(type='http-response', sink.id='download', http.status.code='2\\\\d+', @map(type='text', regex.A='((.|\\n)*)', @attributes(name='trp:name', id='trp:id', file='A[1]'))) define stream ResponseStream2xx(name string, id string, file string); @source(type='http-response', sink.id='download', http.status.code='4\\\\d+', @map(type='text', regex.A='((.|\\n)*)', @attributes(errorMsg='A[1]'))) define stream ResponseStream4xx(errorMsg string); When events arrive in DownloadRequestStream with name : foo.txt , id : 75 and downloadPath : /user/download/ the http-request sink sends a GET request to the url http://localhost:8005/files/foo.txt to download the file to the given path /user/download/foo.txt and capture the response via its corresponding http-response source based on the response status code. If the response status code is in the range of 200 the message will be received by the http-response source associated with the ResponseStream2xx stream which expects http.status.code with regex 2\\d+ while downloading the file to the local file system on the path /user/download/foo.txt and mapping the response message having the absolute file path to event's file attribute. If the response status code is in the range of 400 then the message will be received by the http-response source associated with the ResponseStream4xx stream which expects http.status.code with regex 4\\d+ while mapping the error response to the errorMsg attribute of the event.","title":"http-request (Sink)"},{"location":"docs/api/5.1.0/#http-response-sink","text":"Deprecated (Use http-service-response sink instead). The http-response sink send responses of the requests consumed by its corresponding http-request source, by mapping the response messages to formats such as text , XML and JSON . Origin: siddhi-io-http:2.1.2 Syntax @sink(type=\"http-response\", source.id=\" STRING \", message.id=\" STRING \", headers=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic source.id Identifier to correlate the http-response sink to its corresponding http-request source which consumed the request. STRING No No message.id Identifier to correlate the response with the request received by http-request source. STRING No Yes headers HTTP request headers in format \"' key : value ',' key : value '\" . When the Content-Type header is not provided the system decides the Content-Type based on the provided sink mapper as following: - @map(type='xml') : application/xml - @map(type='json') : application/json - @map(type='text') : plain/text - @map(type='keyvalue') : application/x-www-form-urlencoded - For all other cases system defaults to plain/text Also the Content-Length header need not to be provided, as the system automatically defines it by calculating the size of the payload. Content-Type and Content-Length headers STRING Yes No Examples EXAMPLE 1 @source(type='http-request', receiver.url='http://localhost:5005/add', source.id='adder', @map(type='json, @attributes(messageId='trp:messageId', value1='$.event.value1', value2='$.event.value2'))) define stream AddStream (messageId string, value1 long, value2 long); @sink(type='http-response', source.id='adder', message.id='{{messageId}}', @map(type = 'json')) define stream ResultStream (messageId string, results long); @info(name = 'query1') from AddStream select messageId, value1 + value2 as results insert into ResultStream; The http-request source on stream AddStream listens on url http://localhost:5005/stocks for JSON messages with format: { \"event\": { \"value1\": 3, \"value2\": 4 } } and when events arrive it maps to AddStream events and pass them to query query1 for processing. The query results produced on ResultStream are sent as a response via http-response sink with format: { \"event\": { \"results\": 7 } } Here the request and response are correlated by passing the messageId produced by the http-request to the respective http-response sink.","title":"http-response (Sink)"},{"location":"docs/api/5.1.0/#http-service-response-sink","text":"The http-service-response sink send responses of the requests consumed by its corresponding http-service source, by mapping the response messages to formats such as text , XML and JSON . Origin: siddhi-io-http:2.1.2 Syntax @sink(type=\"http-service-response\", source.id=\" STRING \", message.id=\" STRING \", headers=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic source.id Identifier to correlate the http-service-response sink to its corresponding http-service source which consumed the request. STRING No No message.id Identifier to correlate the response with the request received by http-service source. STRING No Yes headers HTTP request headers in format \"' key : value ',' key : value '\" . When the Content-Type header is not provided the system decides the Content-Type based on the provided sink mapper as following: - @map(type='xml') : application/xml - @map(type='json') : application/json - @map(type='text') : plain/text - @map(type='keyvalue') : application/x-www-form-urlencoded - For all other cases system defaults to plain/text Also the Content-Length header need not to be provided, as the system automatically defines it by calculating the size of the payload. Content-Type and Content-Length headers STRING Yes No Examples EXAMPLE 1 @source(type='http-service', receiver.url='http://localhost:5005/add', source.id='adder', @map(type='json, @attributes(messageId='trp:messageId', value1='$.event.value1', value2='$.event.value2'))) define stream AddStream (messageId string, value1 long, value2 long); @sink(type='http-service-response', source.id='adder', message.id='{{messageId}}', @map(type = 'json')) define stream ResultStream (messageId string, results long); @info(name = 'query1') from AddStream select messageId, value1 + value2 as results insert into ResultStream; The http-service source on stream AddStream listens on url http://localhost:5005/stocks for JSON messages with format: { \"event\": { \"value1\": 3, \"value2\": 4 } } and when events arrive it maps to AddStream events and pass them to query query1 for processing. The query results produced on ResultStream are sent as a response via http-service-response sink with format: { \"event\": { \"results\": 7 } } Here the request and response are correlated by passing the messageId produced by the http-service to the respective http-service-response sink.","title":"http-service-response (Sink)"},{"location":"docs/api/5.1.0/#inmemory-sink","text":"In-memory sink publishes events to In-memory sources that are subscribe to the same topic to which the sink publishes. This provides a way to connect multiple Siddhi Apps deployed under the same Siddhi Manager (JVM). Here both the publisher and subscriber should have the same event schema (stream definition) for successful data transfer. Origin: siddhi-core:5.1.7 Syntax @sink(type=\"inMemory\", topic=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic topic Event are delivered to allthe subscribers subscribed on this topic. STRING No No Examples EXAMPLE 1 @sink(type='inMemory', topic='Stocks', @map(type='passThrough')) define stream StocksStream (symbol string, price float, volume long); Here the StocksStream uses inMemory sink to emit the Siddhi events to all the inMemory sources deployed in the same JVM and subscribed to the topic Stocks .","title":"inMemory (Sink)"},{"location":"docs/api/5.1.0/#jms-sink","text":"JMS Sink allows users to subscribe to a JMS broker and publish JMS messages. Origin: siddhi-io-jms:2.0.2 Syntax @sink(type=\"jms\", destination=\" STRING \", connection.factory.jndi.name=\" STRING \", factory.initial=\" STRING \", provider.url=\" STRING \", connection.factory.type=\" STRING \", connection.username=\" STRING \", connection.password=\" STRING \", connection.factory.nature=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic destination Queue/Topic name which JMS Source should subscribe to STRING No Yes connection.factory.jndi.name JMS Connection Factory JNDI name. This value will be used for the JNDI lookup to find the JMS Connection Factory. QueueConnectionFactory STRING Yes No factory.initial Naming factory initial value STRING No No provider.url Java naming provider URL. Property for specifying configuration information for the service provider to use. The value of the property should contain a URL string (e.g. \"ldap://somehost:389\") STRING No No connection.factory.type Type of the connection connection factory. This can be either queue or topic. queue STRING Yes No connection.username username for the broker. None STRING Yes No connection.password Password for the broker None STRING Yes No connection.factory.nature Connection factory nature for the broker(cached/pooled). default STRING Yes No Examples EXAMPLE 1 @sink(type='jms', @map(type='xml'), factory.initial='org.apache.activemq.jndi.ActiveMQInitialContextFactory', provider.url='vm://localhost',destination='DAS_JMS_OUTPUT_TEST', connection.factory.type='topic',connection.factory.jndi.name='TopicConnectionFactory') define stream inputStream (name string, age int, country string); This example shows how to publish to an ActiveMQ topic. EXAMPLE 2 @sink(type='jms', @map(type='xml'), factory.initial='org.apache.activemq.jndi.ActiveMQInitialContextFactory', provider.url='vm://localhost',destination='DAS_JMS_OUTPUT_TEST') define stream inputStream (name string, age int, country string); This example shows how to publish to an ActiveMQ queue. Note that we are not providing properties like connection factory type","title":"jms (Sink)"},{"location":"docs/api/5.1.0/#kafka-sink","text":"A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to use the Kafka transport, the type parameter should have kafka as its value. Origin: siddhi-io-kafka:5.0.4 Syntax @sink(type=\"kafka\", bootstrap.servers=\" STRING \", topic=\" STRING \", partition.no=\" INT \", sequence.id=\" STRING \", key=\" STRING \", is.binary.message=\" BOOL \", optional.configuration=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No is.binary.message In order to send the binary events via kafka sink, this parameter is set to 'True'. null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='topic_with_partitions', partition.no='0', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka sink configuration publishes to 0 th partition of the topic named topic_with_partitions . EXAMPLE 2 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='{{symbol}}', partition.no='{{volume}}', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes dynamic topic and partitions that are taken from the Siddhi event. The value for partition.no is taken from the volume attribute, and the topic value is taken from the symbol attribute.","title":"kafka (Sink)"},{"location":"docs/api/5.1.0/#kafkamultidc-sink","text":"A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to publish events via the Kafka transport, and using two Kafka brokers to publish events to the same topic, the type parameter must have kafkaMultiDC as its value. Origin: siddhi-io-kafka:5.0.4 Syntax @sink(type=\"kafkaMultiDC\", bootstrap.servers=\" STRING \", topic=\" STRING \", sequence.id=\" STRING \", key=\" STRING \", partition.no=\" INT \", is.binary.message=\" BOOL \", optional.configuration=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma -separated values. There must be at least two servers in this list. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No is.binary.message In order to send the binary events via kafkaMultiDCSink, it is required to set this parameter to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink(type='kafkaMultiDC', topic='myTopic', partition.no='0',bootstrap.servers='host1:9092, host2:9092', @map(type='xml'))Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes to the default (i.e., 0 th ) partition of the brokers in two data centers","title":"kafkaMultiDC (Sink)"},{"location":"docs/api/5.1.0/#log-sink","text":"This is a sink that can be used as a logger. This will log the output events in the output stream with user specified priority and a prefix Origin: siddhi-core:5.1.7 Syntax @sink(type=\"log\", priority=\" STRING \", prefix=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic priority This will set the logger priority i.e log level. Accepted values are INFO, DEBUG, WARN, FATAL, ERROR, OFF, TRACE INFO STRING Yes No prefix This will be the prefix to the output message. If the output stream has event [2,4] and the prefix is given as \"Hello\" then the log will show \"Hello : [2,4]\" default prefix will be : STRING Yes No Examples EXAMPLE 1 @sink(type='log', prefix='My Log', priority='DEBUG') define stream BarStream (symbol string, price float, volume long) In this example BarStream uses log sink and the prefix is given as My Log. Also the priority is set to DEBUG. EXAMPLE 2 @sink(type='log', priority='DEBUG') define stream BarStream (symbol string, price float, volume long) In this example BarStream uses log sink and the priority is set to DEBUG. User has not specified prefix so the default prefix will be in the form Siddhi App Name : Stream Name EXAMPLE 3 @sink(type='log', prefix='My Log') define stream BarStream (symbol string, price float, volume long) In this example BarStream uses log sink and the prefix is given as My Log. User has not given a priority so it will be set to default INFO. EXAMPLE 4 @sink(type='log') define stream BarStream (symbol string, price float, volume long) In this example BarStream uses log sink. The user has not given prefix or priority so they will be set to their default values.","title":"log (Sink)"},{"location":"docs/api/5.1.0/#nats-sink","text":"NATS Sink allows users to subscribe to a NATS broker and publish messages. Origin: siddhi-io-nats:2.0.6 Syntax @sink(type=\"nats\", destination=\" STRING \", bootstrap.servers=\" STRING \", client.id=\" STRING \", cluster.id=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic destination Subject name which NATS sink should publish to. STRING No Yes bootstrap.servers The NATS based url of the NATS server. nats://localhost:4222 STRING Yes No client.id The identifier of the client publishing/connecting to the NATS broker. Should be unique for each client connecting to the server/cluster. None STRING Yes No cluster.id The identifier of the NATS server/cluster. test-cluster STRING Yes No Examples EXAMPLE 1 @sink(type='nats', @map(type='xml'), destination='SP_NATS_OUTPUT_TEST', bootstrap.servers='nats://localhost:4222',client.id='nats_client',server.id='test-cluster') define stream outputStream (name string, age int, country string); This example shows how to publish to a NATS subject with all supporting configurations. With the following configuration the sink identified as 'nats-client' will publish to a subject named as 'SP_NATS_OUTPUT_TEST' which resides in a nats instance with a cluster id of 'test-cluster', running in localhost and listening to the port 4222 for client connection. EXAMPLE 2 @sink(type='nats', @map(type='xml'), destination='SP_NATS_OUTPUT_TEST') define stream outputStream (name string, age int, country string); This example shows how to publish to a NATS subject with mandatory configurations. With the following configuration the sink identified with an auto generated client id will publish to a subject named as 'SP_NATS_OUTPUT_TEST' which resides in a nats instance with a cluster id of 'test-cluster', running in localhost and listening to the port 4222 for client connection.","title":"nats (Sink)"},{"location":"docs/api/5.1.0/#prometheus-sink","text":"This sink publishes events processed by Siddhi into Prometheus metrics and exposes them to the Prometheus server at the specified URL. The created metrics can be published to Prometheus via 'server' or 'pushGateway', depending on your preference. The metric types that are supported by the Prometheus sink are 'counter', 'gauge', 'histogram', and 'summary'. The values and labels of the Prometheus metrics can be updated through the events. Origin: siddhi-io-prometheus:2.1.0 Syntax @sink(type=\"prometheus\", job=\" STRING \", publish.mode=\" STRING \", push.url=\" STRING \", server.url=\" STRING \", metric.type=\" STRING \", metric.help=\" STRING \", metric.name=\" STRING \", buckets=\" STRING \", quantiles=\" STRING \", quantile.error=\" DOUBLE \", value.attribute=\" STRING \", push.operation=\" STRING \", grouping.key=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic job This parameter specifies the job name of the metric. This must be the same job name that is defined in the Prometheus configuration file. siddhiJob STRING Yes No publish.mode The mode in which the metrics need to be exposed to the Prometheus server.The possible publishing modes are 'server' and 'pushgateway'.The server mode exposes the metrics through an HTTP server at the specified URL, and the 'pushGateway' mode pushes the metrics to the pushGateway that needs to be running at the specified URL. server STRING Yes No push.url This parameter specifies the target URL of the Prometheus pushGateway. This is the URL at which the pushGateway must be listening. This URL needs to be defined in the Prometheus configuration file as a target before it can be used here. http://localhost:9091 STRING Yes No server.url This parameter specifies the URL where the HTTP server is initiated to expose metrics in the 'server' publish mode. This URL needs to be defined in the Prometheus configuration file as a target before it can be used here. http://localhost:9080 STRING Yes No metric.type The type of Prometheus metric that needs to be created at the sink. The supported metric types are 'counter', 'gauge',c'histogram' and 'summary'. STRING No No metric.help A brief description of the metric and its purpose. STRING Yes No metric.name This parameter allows you to assign a preferred name for the metric. The metric name must match the regex format, i.e., [a-zA-Z_:][a-zA-Z0-9_:]*. STRING Yes No buckets The bucket values preferred by the user for histogram metrics. The bucket values must be in the 'string' format with each bucket value separated by a comma as shown in the example below. \"2,4,6,8\" null STRING Yes No quantiles This parameter allows you to specify quantile values for summary metrics as preferred. The quantile values must be in the 'string' format with each quantile value separated by a comma as shown in the example below. \"0.5,0.75,0.95\" null STRING Yes No quantile.error The error tolerance value for calculating quantiles in summary metrics. This must be a positive value, but less than 1. 0.001 DOUBLE Yes No value.attribute The name of the attribute in the stream definition that specifies the metric value. The defined 'value' attribute must be included in the stream definition. The system increases the metric value for the counter and gauge metric types by the value of the 'value attribute. The system observes the value of the 'value' attribute for the calculations of 'summary' and 'histogram' metric types. value STRING Yes No push.operation This parameter defines the mode for pushing metrics to the pushGateway. The available push operations are 'push' and 'pushadd'. The operations differ according to the existing metrics in pushGateway where 'push' operation replaces the existing metrics, and 'pushadd' operation only updates the newly created metrics. pushadd STRING Yes No grouping.key This parameter specifies the grouping key of created metrics in key-value pairs. The grouping key is used only in pushGateway mode in order to distinguish the metrics from already existing metrics. The expected format of the grouping key is as follows: \"'key1:value1','key2:value2'\" STRING Yes No System Parameters Name Description Default Value Possible Parameters jobName This property specifies the default job name for the metric. This job name must be the same as the job name defined in the Prometheus configuration file. siddhiJob Any string publishMode The default publish mode for the Prometheus sink for exposing metrics to the Prometheus server. The mode can be either 'server' or 'pushgateway'. server server or pushgateway serverURL This property configures the URL where the HTTP server is initiated to expose metrics. This URL needs to be defined in the Prometheus configuration file as a target to be identified by Prometheus before it can be used here. By default, the HTTP server is initiated at 'http://localhost:9080'. http://localhost:9080 Any valid URL pushURL This property configures the target URL of the Prometheus pushGateway (where the pushGateway needs to listen). This URL needs to be defined in the Prometheus configuration file as a target to be identified by Prometheus before it can be used here. http://localhost:9091 Any valid URL groupingKey This property configures the grouping key of created metrics in key-value pairs. Grouping key is used only in pushGateway mode in order to distinguish these metrics from already existing metrics under the same job. The expected format of the grouping key is as follows: \"'key1:value1','key2:value2'\" . null Any key value pairs in the supported format Examples EXAMPLE 1 @sink(type='prometheus',job='fooOrderCount', server.url ='http://localhost:9080', publish.mode='server', metric.type='counter', metric.help= 'Number of foo orders', @map(type='keyvalue')) define stream FooCountStream (Name String, quantity int, value int); In the above example, the Prometheus-sink creates a counter metric with the stream name and defined attributes as labels. The metric is exposed through an HTTP server at the target URL. EXAMPLE 2 @sink(type='prometheus',job='inventoryLevel', push.url='http://localhost:9080', publish.mode='pushGateway', metric.type='gauge', metric.help= 'Current level of inventory', @map(type='keyvalue')) define stream InventoryLevelStream (Name String, value int); In the above example, the Prometheus-sink creates a gauge metric with the stream name and defined attributes as labels.The metric is pushed to the Prometheus pushGateway at the target URL.","title":"prometheus (Sink)"},{"location":"docs/api/5.1.0/#rabbitmq-sink","text":"The rabbitmq sink pushes the events into a rabbitmq broker using the AMQP protocol Origin: siddhi-io-rabbitmq:3.0.2 Syntax @sink(type=\"rabbitmq\", uri=\" STRING \", heartbeat=\" INT \", exchange.name=\" STRING \", exchange.type=\" STRING \", exchange.durable.enabled=\" BOOL \", exchange.autodelete.enabled=\" BOOL \", delivery.mode=\" INT \", content.type=\" STRING \", content.encoding=\" STRING \", priority=\" INT \", correlation.id=\" STRING \", reply.to=\" STRING \", expiration=\" STRING \", message.id=\" STRING \", timestamp=\" STRING \", type=\" STRING \", user.id=\" STRING \", app.id=\" STRING \", routing.key=\" STRING \", headers=\" STRING \", tls.enabled=\" BOOL \", tls.truststore.path=\" STRING \", tls.truststore.password=\" STRING \", tls.truststore.type=\" STRING \", tls.version=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic uri The URI that used to connect to an AMQP server. If no URI is specified, an error is logged in the CLI.e.g., amqp://guest:guest , amqp://guest:guest@localhost:5672 STRING No No heartbeat The period of time (in seconds) after which the peer TCP connection should be considered unreachable (down) by RabbitMQ and client libraries. 60 INT Yes No exchange.name The name of the exchange that decides what to do with a message it sends.If the exchange.name already exists in the RabbitMQ server, then the system uses that exchange.name instead of redeclaring. STRING No Yes exchange.type The type of the exchange.name. The exchange types available are direct , fanout , topic and headers . For a detailed description of each type, see RabbitMQ - AMQP Concepts direct STRING Yes Yes exchange.durable.enabled If this is set to true , the exchange remains declared even if the broker restarts. false BOOL Yes Yes exchange.autodelete.enabled If this is set to true , the exchange is automatically deleted when it is not used anymore. false BOOL Yes Yes delivery.mode This determines whether the connection should be persistent or not. The value must be either 1 or 2 .If the delivery.mode = 1, then the connection is not persistent. If the delivery.mode = 2, then the connection is persistent. 1 INT Yes No content.type The message content type. This should be the MIME content type. null STRING Yes No content.encoding The message content encoding. The value should be MIME content encoding. null STRING Yes No priority Specify a value within the range 0 to 9 in this parameter to indicate the message priority. 0 INT Yes Yes correlation.id The message correlated to the current message. e.g., The request to which this message is a reply. When a request arrives, a message describing the task is pushed to the queue by the front end server. After that the frontend server blocks to wait for a response message with the same correlation ID. A pool of worker machines listen on queue, and one of them picks up the task, performs it, and returns the result as message. Once a message with right correlation ID arrives, thefront end server continues to return the response to the caller. null STRING Yes Yes reply.to This is an anonymous exclusive callback queue. When the RabbitMQ receives a message with the reply.to property, it sends the response to the mentioned queue. This is commonly used to name a reply queue (or any other identifier that helps a consumer application to direct its response). null STRING Yes No expiration The expiration time after which the message is deleted. The value of the expiration field describes the TTL (Time To Live) period in milliseconds. null STRING Yes No message.id The message identifier. If applications need to identify messages, it is recommended that they use this attribute instead of putting it into the message payload. null STRING Yes Yes timestamp Timestamp of the moment when the message was sent. If you do not specify a value for this parameter, the system automatically generates the current date and time as the timestamp value. The format of the timestamp value is dd/mm/yyyy . current timestamp STRING Yes No type The type of the message. e.g., The type of the event or the command represented by the message. null STRING Yes No user.id The user ID specified here is verified by RabbitMQ against theuser name of the actual connection. This is an optional parameter. null STRING Yes No app.id The identifier of the application that produced the message. null STRING Yes No routing.key The key based on which the excahnge determines how to route the message to the queue. The routing key is similar to an address for the message. empty STRING Yes Yes headers The headers of the message. The attributes used for routing are taken from the this paremeter. A message is considered matching if the value of the header equals the value specified upon binding. null STRING Yes Yes tls.enabled This parameter specifies whether an encrypted communication channel should be established or not. When this parameter is set to true , the tls.truststore.path and tls.truststore.password parameters are initialized. false BOOL Yes No tls.truststore.path The file path to the location of the truststore of the client that sends the RabbitMQ events via the AMQP protocol. A custom client-truststore can be specified if required. If a custom truststore is not specified, then the system uses the default client-trustore in the {carbon.home}/resources/security /code directory. /p /td td style=\"vertical-align: top\" {carbon.home}/resources/security</code> directory.</p></td> <td style=\"vertical-align: top\"> /resources/security/client-truststore.jks STRING Yes No tls.truststore.password The password for the client-truststore. A custom password can be specified if required. If no custom password is specified, then the system uses wso2carbon as the default password. wso2carbon STRING Yes No tls.truststore.type The type of the truststore. JKS STRING Yes No tls.version The version of the tls/ssl. SSL STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink(type ='rabbitmq', uri = 'amqp://guest:guest@localhost:5672', exchange.name = 'direct', routing.key= 'direct', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes events to the direct exchange with the direct exchange type and the directTest routing key.","title":"rabbitmq (Sink)"},{"location":"docs/api/5.1.0/#tcp-sink","text":"A Siddhi application can be configured to publish events via the TCP transport by adding the @Sink(type = 'tcp') annotation at the top of an event stream definition. Origin: siddhi-io-tcp:3.0.4 Syntax @sink(type=\"tcp\", url=\" STRING \", sync=\" STRING \", tcp.no.delay=\" BOOL \", keep.alive=\" BOOL \", worker.threads=\" INT|LONG \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic url The URL to which outgoing events should be published via TCP. The URL should adhere to tcp:// host : port / context format. STRING No No sync This parameter defines whether the events should be published in a synchronized manner or not. If sync = 'true', then the worker will wait for the ack after sending the message. Else it will not wait for an ack. false STRING Yes Yes tcp.no.delay This is to specify whether to disable Nagle algorithm during message passing. If tcp.no.delay = 'true', the execution of Nagle algorithm will be disabled in the underlying TCP logic. Hence there will be no delay between two successive writes to the TCP connection. Else there can be a constant ack delay. true BOOL Yes No keep.alive This property defines whether the server should be kept alive when there are no connections available. true BOOL Yes No worker.threads Number of threads to publish events. 10 INT LONG Yes No Examples EXAMPLE 1 @Sink(type = 'tcp', url='tcp://localhost:8080/abc', sync='true' @map(type='binary')) define stream Foo (attribute1 string, attribute2 int); A sink of type 'tcp' has been defined. All events arriving at Foo stream via TCP transport will be sent to the url tcp://localhost:8080/abc in a synchronous manner.","title":"tcp (Sink)"},{"location":"docs/api/5.1.0/#sinkmapper","text":"","title":"Sinkmapper"},{"location":"docs/api/5.1.0/#avro-sink-mapper","text":"This extension is a Siddhi Event to Avro Message output mapper.Transports that publish messages to Avro sink can utilize this extension to convert Siddhi events to Avro messages. You can either specify the Avro schema or provide the schema registry URL and the schema reference ID as parameters in the stream definition. If no Avro schema is specified, a flat Avro schema of the 'record' type is generated with the stream attributes as schema fields. Origin: siddhi-map-avro:2.0.5 Syntax @sink(..., @map(type=\"avro\", schema.def=\" STRING \", schema.registry=\" STRING \", schema.id=\" STRING \") QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic schema.def This specifies the required Avro schema to be used to convert Siddhi events to Avro messages. The schema needs to be specified as a quoted JSON string. STRING No No schema.registry This specifies the URL of the schema registry. STRING No No schema.id This specifies the ID of the avro schema. This ID is the global ID that is returned from the schema registry when posting the schema to the registry. The specified ID is used to retrieve the schema from the schema registry. STRING No No Examples EXAMPLE 1 @sink(type='inMemory', topic='stock', @map(type='avro',schema.def = \"\"\"{\"type\":\"record\",\"name\":\"stock\",\"namespace\":\"stock.example\",\"fields\":[{\"name\":\"symbol\",\"type\":\"string\"},{\"name\":\"price\",\"type\":\"float\"},{\"name\":\"volume\",\"type\":\"long\"}]}\"\"\")) define stream StockStream (symbol string, price float, volume long); The above configuration performs a default Avro mapping that generates an Avro message as an output ByteBuffer. EXAMPLE 2 @sink(type='inMemory', topic='stock', @map(type='avro',schema.registry = 'http://localhost:8081', schema.id ='22',@payload(\"\"\"{\"Symbol\":{{symbol}},\"Price\":{{price}},\"Volume\":{{volume}}}\"\"\" ))) define stream StockStream (symbol string, price float, volume long); The above configuration performs a custom Avro mapping that generates an Avro message as an output ByteBuffer. The Avro schema is retrieved from the given schema registry (localhost:8081) using the schema ID provided.","title":"avro (Sink Mapper)"},{"location":"docs/api/5.1.0/#binary-sink-mapper","text":"This section explains how to map events processed via Siddhi in order to publish them in the binary format. Origin: siddhi-map-binary:2.0.4 Syntax @sink(..., @map(type=\"binary\") Examples EXAMPLE 1 @sink(type='inMemory', topic='WSO2', @map(type='binary')) define stream FooStream (symbol string, price float, volume long); This will publish Siddhi event in binary format.","title":"binary (Sink Mapper)"},{"location":"docs/api/5.1.0/#csv-sink-mapper","text":"This output mapper extension allows you to convert Siddhi events processed by the WSO2 SP to CSV message before publishing them. You can either use custom placeholder to map a custom CSV message or use pre-defined CSV format where event conversion takes place without extra configurations. Origin: siddhi-map-csv:2.0.3 Syntax @sink(..., @map(type=\"csv\", delimiter=\" STRING \", header=\" BOOL \", event.grouping.enabled=\" BOOL \") QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic delimiter This parameter used to separate the output CSV data, when converting a Siddhi event to CSV format, , STRING Yes No header This parameter specifies whether the CSV messages will be generated with header or not. If this parameter is set to true, message will be generated with header false BOOL Yes No event.grouping.enabled If this parameter is set to true , events are grouped via a line.separator when multiple events are received. It is required to specify a value for the System.lineSeparator() when the value for this parameter is true . false BOOL Yes No Examples EXAMPLE 1 @sink(type='inMemory', topic='{{symbol}}', @map(type='csv')) define stream BarStream (symbol string, price float, volume long); Above configuration will perform a default CSV output mapping, which will generate output as follows: WSO2,55.6,100 OS supported line separator If header is true and delimiter is \"-\", then the output will be as follows: symbol-price-volume OS supported line separator WSO2-55.6-100 OS supported line separator EXAMPLE 2 @sink(type='inMemory', topic='{{symbol}}', @map(type='csv',header='true',delimiter='-',@payload(symbol='0',price='2',volume='1')))define stream BarStream (symbol string, price float,volume long); Above configuration will perform a custom CSV mapping. Here, user can add custom place order in the @payload. The place order indicates that where the attribute name's value will be appear in the output message, The output will be produced output as follows: WSO2,100,55.6 If header is true and delimiter is \"-\", then the output will be as follows: symbol-price-volume WSO2-55.6-100 OS supported line separator If event grouping is enabled, then the output is as follows: WSO2-55.6-100 OS supported line separator WSO2-55.6-100 OS supported line separator WSO2-55.6-100 OS supported line separator","title":"csv (Sink Mapper)"},{"location":"docs/api/5.1.0/#json-sink-mapper","text":"This extension is an Event to JSON output mapper. Transports that publish messages can utilize this extension to convert Siddhi events to JSON messages. You can either send a pre-defined JSON format or a custom JSON message. Origin: siddhi-map-json:5.0.4 Syntax @sink(..., @map(type=\"json\", validate.json=\" BOOL \", enclosing.element=\" STRING \") QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic validate.json If this property is set to true , it enables JSON validation for the JSON messages generated. When validation is carried out, messages that do not adhere to proper JSON standards are dropped. This property is set to 'false' by default. false BOOL Yes No enclosing.element This specifies the enclosing element to be used if multiple events are sent in the same JSON message. Siddhi treats the child elements of the given enclosing element as events and executes JSON expressions on them. If an enclosing.element is not provided, the multiple event scenario is disregarded and JSON path is evaluated based on the root element. $ STRING Yes No Examples EXAMPLE 1 @sink(type='inMemory', topic='stock', @map(type='json')) define stream FooStream (symbol string, price float, volume long); Above configuration does a default JSON input mapping that generates the output given below. { \"event\":{ \"symbol\":WSO2, \"price\":55.6, \"volume\":100 } } EXAMPLE 2 @sink(type='inMemory', topic='{{symbol}}', @map(type='json', enclosing.element='$.portfolio', validate.json='true', @payload( \"\"\"{\"StockData\":{\"Symbol\":\"{{symbol}}\",\"Price\":{{price}}}\"\"\"))) define stream BarStream (symbol string, price float, volume long); The above configuration performs a custom JSON mapping that generates the following JSON message as the output. {\"portfolio\":{ \"StockData\":{ \"Symbol\":WSO2, \"Price\":55.6 } } }","title":"json (Sink Mapper)"},{"location":"docs/api/5.1.0/#keyvalue-sink-mapper","text":"The Event to Key-Value Map output mapper extension allows you to convert Siddhi events processed by WSO2 SP to key-value map events before publishing them. You can either use pre-defined keys where conversion takes place without extra configurations, or use custom keys with which the messages can be published. Origin: siddhi-map-keyvalue:2.0.4 Syntax @sink(..., @map(type=\"keyvalue\") Examples EXAMPLE 1 @sink(type='inMemory', topic='stock', @map(type='keyvalue')) define stream FooStream (symbol string, price float, volume long); This query performs a default Key-Value output mapping. The expected output is something similar to the following: symbol:'WSO2' price : 55.6f volume: 100L EXAMPLE 2 @sink(type='inMemory', topic='stock', @map(type='keyvalue', @payload(a='symbol',b='price',c='volume'))) define stream FooStream (symbol string, price float, volume long); This query performs a custom Key-Value output mapping where values are passed as objects. Values for symbol , price , and volume attributes are published with the keys a , b and c respectively. The expected output is a map similar to the following: a:'WSO2' b : 55.6f c: 100L EXAMPLE 3 @sink(type='inMemory', topic='stock', @map(type='keyvalue', @payload(a='{{symbol}} is here',b='`price`',c='volume'))) define stream FooStream (symbol string, price float, volume long); This query performs a custom Key-Value output mapping where the values of the a and b attributes are strings and c is object. The expected output should be a Map similar to the following: a:'WSO2 is here' b : 'price' c: 100L","title":"keyvalue (Sink Mapper)"},{"location":"docs/api/5.1.0/#passthrough-sink-mapper","text":"Pass-through mapper passed events (Event[]) through without any mapping or modifications. Origin: siddhi-core:5.1.7 Syntax @sink(..., @map(type=\"passThrough\") Examples EXAMPLE 1 @sink(type='inMemory', @map(type='passThrough')) define stream BarStream (symbol string, price float, volume long); In the following example BarStream uses passThrough outputmapper which emit Siddhi event directly without any transformation into sink.","title":"passThrough (Sink Mapper)"},{"location":"docs/api/5.1.0/#protobuf-sink-mapper","text":"This output mapper allows you to convert Events to protobuf messages before publishing them. To work with this mapper you have to add auto-generated protobuf classes to the project classpath. When you use this output mapper, you can either define stream attributes as the same names as the protobuf message attributes or you can use custom mapping to map stream definition attributes with the protobuf attributes..Please find the sample proto definition here Origin: siddhi-map-protobuf:1.0.1 Syntax @sink(..., @map(type=\"protobuf\", class=\" STRING \") QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic class This specifies the class name of the protobuf message class, If sink type is grpc then it's not necessary to provide this parameter. - STRING Yes No Examples EXAMPLE 1 @sink(type='grpc', url = 'grpc://localhost:2000/org.wso2.grpc.test.MyService/process @map(type='protobuf')) define stream BarStream (stringValue string, intValue int,longValue long,booleanValue bool,floatValue float,doubleValue double) Above definition will map BarStream values into the protobuf message type of the 'process' method in 'MyService' service EXAMPLE 2 @sink(type='grpc', url = 'grpc://localhost:2000/org.wso2.grpc.test.MyService/process @map(type='protobuf'), @payload(stringValue='a',longValue='b',intValue='c',booleanValue='d',floatValue = 'e', doubleValue = 'f'))) define stream BarStream (a string, b long, c int,d bool,e float,f double); The above definition will map BarStream values to request message type of the 'process' method in 'MyService' service. and stream values will map like this, - value of 'a' will be assign 'stringValue' variable in the message class - value of 'b' will be assign 'longValue' variable in the message class - value of 'c' will be assign 'intValue' variable in the message class - value of 'd' will be assign 'booleanValue' variable in the message class - value of 'e' will be assign 'floatValue' variable in the message class - value of 'f' will be assign 'doubleValue' variable in the message class EXAMPLE 3 @sink(type='grpc', url = 'grpc://localhost:2000/org.wso2.grpc.test.MyService/testMap' @map(type='protobuf')) define stream BarStream (stringValue string,intValue int,map object); The above definition will map BarStream values to request message type of the 'testMap' method in 'MyService' service and since there is an object data type is inthe stream(map object) , mapper will assume that 'map' is an instance of 'java.util.Map' class, otherwise it will throws and error. EXAMPLE 4 @sink(type='inMemory', topic='test01', @map(type='protobuf', class='org.wso2.grpc.test.Request')) define stream BarStream (stringValue string, intValue int,longValue long,booleanValue bool,floatValue float,doubleValue double); The above definition will map BarStream values to 'org.wso2.grpc.test.Request'protobuf class type. If sink type is not a grpc, sink is expecting to get the mapping protobuf class from the 'class' parameter in the @map extension","title":"protobuf (Sink Mapper)"},{"location":"docs/api/5.1.0/#text-sink-mapper","text":"This extension is a Event to Text output mapper. Transports that publish text messages can utilize this extension to convert the Siddhi events to text messages. Users can use a pre-defined text format where event conversion is carried out without any additional configurations, or use custom placeholder(using {{ and }} ) to map custom text messages. Again, you can also enable mustache based custom mapping. In mustache based custom mapping you can use custom placeholder (using {{ and }} or {{{ and }}} ) to map custom text. In mustache based custom mapping, all variables are HTML escaped by default. For example: is replaced with amp; \" is replaced with quot; = is replaced with #61; If you want to return unescaped HTML, use the triple mustache {{{ instead of double {{ . Origin: siddhi-map-text:2.0.4 Syntax @sink(..., @map(type=\"text\", event.grouping.enabled=\" BOOL \", delimiter=\" STRING \", new.line.character=\" STRING \", mustache.enabled=\" BOOL \") QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic event.grouping.enabled If this parameter is set to true , events are grouped via a delimiter when multiple events are received. It is required to specify a value for the delimiter parameter when the value for this parameter is true . false BOOL Yes No delimiter This parameter specifies how events are separated when a grouped event is received. This must be a whole line and not a single character. ~ ~ ~ ~ STRING Yes No new.line.character This attribute indicates the new line character of the event that is expected to be received. This is used mostly when communication between 2 types of operating systems is expected. For example, Linux uses \\n whereas Windows uses \\r\\n as the end of line character. \\n STRING Yes No mustache.enabled If this parameter is set to true , then mustache mapping gets enabled forcustom text mapping. false BOOL Yes No Examples EXAMPLE 1 @sink(type='inMemory', topic='stock', @map(type='text')) define stream FooStream (symbol string, price float, volume long); This query performs a default text input mapping. The expected output is as follows: symbol:\"WSO2\", price:55.6, volume:100 EXAMPLE 2 @sink(type='inMemory', topic='stock', @map(type='text', event.grouping.enabled='true')) define stream FooStream (symbol string, price float, volume long); This query performs a default text input mapping with event grouping. The expected output is as follows: symbol:\"WSO2\", price:55.6, volume:100 ~ ~ ~ ~ symbol:\"WSO2\", price:55.6, volume:100 EXAMPLE 3 @sink(type='inMemory', topic='stock', @map(type='text', @payload(\"SensorID : {{symbol}}/{{volume}}, SensorPrice : Rs{{price}}/=, Value : {{volume}}ml\"))) define stream FooStream (symbol string, price float, volume long); This query performs a custom text mapping. The expected output is as follows: SensorID : wso2/100, SensorPrice : Rs1000/=, Value : 100ml for the following siddhi event. {wso2,1000,100} EXAMPLE 4 @sink(type='inMemory', topic='stock', @map(type='text', event.grouping.enabled='true', @payload(\"Stock price of {{symbol}} is {{price}}\"))) define stream FooStream (symbol string, price float, volume long); This query performs a custom text mapping with event grouping. The expected output is as follows: Stock price of WSO2 is 55.6 ~ ~ ~ ~ Stock price of WSO2 is 55.6 ~ ~ ~ ~ Stock price of WSO2 is 55.6 for the following siddhi event. {WSO2,55.6,10} EXAMPLE 5 @sink(type='inMemory', topic='stock', @map(type='text', mustache.enabled='true', @payload(\"SensorID : {{{symbol}}}/{{{volume}}}, SensorPrice : Rs{{{price}}}/=, Value : {{{volume}}}ml\"))) define stream FooStream (symbol string, price float, volume long); This query performs a custom text mapping to return unescaped HTML. The expected output is as follows: SensorID : a b/100, SensorPrice : Rs1000/=, Value : 100ml for the following siddhi event. {a b,1000,100}","title":"text (Sink Mapper)"},{"location":"docs/api/5.1.0/#xml-sink-mapper","text":"This mapper converts Siddhi output events to XML before they are published via transports that publish in XML format. Users can either send a pre-defined XML format or a custom XML message containing event data. Origin: siddhi-map-xml:5.0.3 Syntax @sink(..., @map(type=\"xml\", validate.xml=\" BOOL \", enclosing.element=\" STRING \") QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic validate.xml This parameter specifies whether the XML messages generated should be validated or not. If this parameter is set to true, messages that do not adhere to proper XML standards are dropped. false BOOL Yes No enclosing.element When an enclosing element is specified, the child elements (e.g., the immediate child elements) of that element are considered as events. This is useful when you need to send multiple events in a single XML message. When an enclosing element is not specified, one XML message per every event will be emitted without enclosing. None in custom mapping and events in default mapping STRING Yes No Examples EXAMPLE 1 @sink(type='inMemory', topic='stock', @map(type='xml')) define stream FooStream (symbol string, price float, volume long); Above configuration will do a default XML input mapping which will generate below output events event symbol WSO2 /symbol price 55.6 /price volume 100 /volume /event /events EXAMPLE 2 @sink(type='inMemory', topic='{{symbol}}', @map(type='xml', enclosing.element=' portfolio ', validate.xml='true', @payload( \" StockData Symbol {{symbol}} /Symbol Price {{price}} /Price /StockData \"))) define stream BarStream (symbol string, price float, volume long); Above configuration will perform a custom XML mapping. Inside @payload you can specify the custom template that you want to send the messages out and addd placeholders to places where you need to add event attributes.Above config will produce below output XML message portfolio StockData Symbol WSO2 /Symbol Price 55.6 /Price /StockData /portfolio","title":"xml (Sink Mapper)"},{"location":"docs/api/5.1.0/#source","text":"","title":"Source"},{"location":"docs/api/5.1.0/#cdc-source","text":"The CDC source receives events when change events (i.e., INSERT, UPDATE, DELETE) are triggered for a database table. Events are received in the 'key-value' format. There are two modes you could perform CDC: Listening mode and Polling mode. In polling mode, the datasource is periodically polled for capturing the changes. The polling period can be configured. In polling mode, you can only capture INSERT and UPDATE changes. On listening mode, the Source will keep listening to the Change Log of the database and notify in case a change has taken place. Here, you are immediately notified about the change, compared to polling mode. The key values of the map of a CDC change event are as follows. For 'listening' mode: For insert: Keys are specified as columns of the table. For delete: Keys are followed by the specified table columns. This is achieved via 'before_'. e.g., specifying 'before_X' results in the key being added before the column named 'X'. For update: Keys are followed followed by the specified table columns. This is achieved via 'before_'. e.g., specifying 'before_X' results in the key being added before the column named 'X'. For 'polling' mode: Keys are specified as the columns of the table. See parameter: mode for supported databases and change events. Origin: siddhi-io-cdc:2.0.3 Syntax @source(type=\"cdc\", url=\" STRING \", mode=\" STRING \", jdbc.driver.name=\" STRING \", username=\" STRING \", password=\" STRING \", pool.properties=\" STRING \", datasource.name=\" STRING \", table.name=\" STRING \", polling.column=\" STRING \", polling.interval=\" INT \", operation=\" STRING \", connector.properties=\" STRING \", database.server.id=\" STRING \", database.server.name=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic url The connection URL to the database. F=The format used is: 'jdbc:mysql:// host : port / database_name ' STRING No No mode Mode to capture the change data. The type of events that can be received, and the required parameters differ based on the mode. The mode can be one of the following: 'polling': This mode uses a column named 'polling.column' to monitor the given table. It captures change events of the 'RDBMS', 'INSERT, and 'UPDATE' types. 'listening': This mode uses logs to monitor the given table. It currently supports change events only of the 'MySQL', 'INSERT', 'UPDATE', and 'DELETE' types. listening STRING Yes No jdbc.driver.name The driver class name for connecting the database. It is required to specify a value for this parameter when the mode is 'polling'. STRING Yes No username The username to be used for accessing the database. This user needs to have the 'SELECT', 'RELOAD', 'SHOW DATABASES', 'REPLICATION SLAVE', and 'REPLICATION CLIENT'privileges for the change data capturing table (specified via the 'table.name' parameter). To operate in the polling mode, the user needs 'SELECT' privileges. STRING No No password The password of the username you specified for accessing the database. STRING No No pool.properties The pool parameters for the database connection can be specified as key-value pairs. STRING Yes No datasource.name Name of the wso2 datasource to connect to the database. When datasource name is provided, the URL, username and password are not needed. A datasource based connection is given more priority over the URL based connection. This parameter is applicable only when the mode is set to 'polling', and it can be applied only when you use this extension with WSO2 Stream Processor. STRING Yes No table.name The name of the table that needs to be monitored for data changes. STRING No No polling.column The column name that is polled to capture the change data. It is recommended to have a TIMESTAMP field as the 'polling.column' in order to capture the inserts and updates. Numeric auto-incremental fields and char fields can also be used as 'polling.column'. However, note that fields of these types only support insert change capturing, and the possibility of using a char field also depends on how the data is input. It is required to enter a value for this parameter when the mode is 'polling'. STRING Yes No polling.interval The time interval (specified in seconds) to poll the given table for changes. This parameter is applicable only when the mode is set to 'polling'. 1 INT Yes No operation The change event operation you want to carry out. Possible values are 'insert', 'update' or 'delete'. It is required to specify a value when the mode is 'listening'. This parameter is not case sensitive. STRING No No connector.properties Here, you can specify Debezium connector properties as a comma-separated string. The properties specified here are given more priority over the parameters. This parameter is applicable only for the 'listening' mode. Empty_String STRING Yes No database.server.id An ID to be used when joining MySQL database cluster to read the bin log. This should be a unique integer between 1 to 2^32. This parameter is applicable only when the mode is 'listening'. Random integer between 5400 and 6400 STRING Yes No database.server.name A logical name that identifies and provides a namespace for the database server. This parameter is applicable only when the mode is 'listening'. {host}_{port} STRING Yes No Examples EXAMPLE 1 @source(type = 'cdc' , url = 'jdbc:mysql://localhost:3306/SimpleDB', username = 'cdcuser', password = 'pswd4cdc', table.name = 'students', operation = 'insert', @map(type='keyvalue', @attributes(id = 'id', name = 'name'))) define stream inputStream (id string, name string); In this example, the CDC source listens to the row insertions that are made in the 'students' table with the column name, and the ID. This table belongs to the 'SimpleDB' MySQL database that can be accessed via the given URL. EXAMPLE 2 @source(type = 'cdc' , url = 'jdbc:mysql://localhost:3306/SimpleDB', username = 'cdcuser', password = 'pswd4cdc', table.name = 'students', operation = 'update', @map(type='keyvalue', @attributes(id = 'id', name = 'name', before_id = 'before_id', before_name = 'before_name'))) define stream inputStream (before_id string, id string, before_name string , name string); In this example, the CDC source listens to the row updates that are made in the 'students' table. This table belongs to the 'SimpleDB' MySQL database that can be accessed via the given URL. EXAMPLE 3 @source(type = 'cdc' , url = 'jdbc:mysql://localhost:3306/SimpleDB', username = 'cdcuser', password = 'pswd4cdc', table.name = 'students', operation = 'delete', @map(type='keyvalue', @attributes(before_id = 'before_id', before_name = 'before_name'))) define stream inputStream (before_id string, before_name string); In this example, the CDC source listens to the row deletions made in the 'students' table. This table belongs to the 'SimpleDB' database that can be accessed via the given URL. EXAMPLE 4 @source(type = 'cdc', mode='polling', polling.column = 'id', jdbc.driver.name = 'com.mysql.jdbc.Driver', url = 'jdbc:mysql://localhost:3306/SimpleDB', username = 'cdcuser', password = 'pswd4cdc', table.name = 'students', @map(type='keyvalue'), @attributes(id = 'id', name = 'name')) define stream inputStream (id int, name string); In this example, the CDC source polls the 'students' table for inserts. 'id' that is specified as the polling colum' is an auto incremental field. The connection to the database is made via the URL, username, password, and the JDBC driver name. EXAMPLE 5 @source(type = 'cdc', mode='polling', polling.column = 'id', datasource.name = 'SimpleDB', table.name = 'students', @map(type='keyvalue'), @attributes(id = 'id', name = 'name')) define stream inputStream (id int, name string); In this example, the CDC source polls the 'students' table for inserts. The given polling column is a char column with the 'S001, S002, ... .' pattern. The connection to the database is made via a data source named 'SimpleDB'. Note that the 'datasource.name' parameter works only with the Stream Processor. EXAMPLE 6 @source(type = 'cdc', mode='polling', polling.column = 'last_updated', datasource.name = 'SimpleDB', table.name = 'students', @map(type='keyvalue')) define stream inputStream (name string); In this example, the CDC source polls the 'students' table for inserts and updates. The polling column is a timestamp field.","title":"cdc (Source)"},{"location":"docs/api/5.1.0/#email-source","text":"The 'Email' source allows you to receive events via emails. An 'Email' source can be configured using the 'imap' or 'pop3' server to receive events. This allows you to filter the messages that satisfy the criteria specified under the 'search term' option. The email source parameters can be defined in either the ' SP_HOME /conf/ PROFILE /deployment yaml' file or the stream definition. If the parameter configurations are not available in either place, the default values are considered (i.e., if default values are available). If you need to configure server system parameters that are not provided as options in the stream definition, they need to be defined in the 'deployment yaml' file under 'email source properties'. For more information about 'imap' and 'pop3' server system parameters, see the following. JavaMail Reference Implementation - IMAP Store JavaMail Reference Implementation - POP3 Store Store Origin: siddhi-io-email:2.0.4 Syntax @source(type=\"email\", username=\" STRING \", password=\" STRING \", store=\" STRING \", host=\" STRING \", port=\" INT \", folder=\" STRING \", search.term=\" STRING \", polling.interval=\" LONG \", action.after.processed=\" STRING \", folder.to.move=\" STRING \", content.type=\" STRING \", ssl.enable=\" BOOL \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic username The user name of the email account. e.g., 'wso2mail' is the username of the 'wso2mail@gmail.com' mail account. STRING No No password The password of the email account STRING No No store The store type that used to receive emails. Possible values are 'imap' and 'pop3'. imap STRING Yes No host The host name of the server (e.g., 'imap.gmail.com' is the host name for a gmail account with an IMAP store.). The default value 'imap.gmail.com' is only valid if the email account is a gmail account with IMAP enabled. If store type is 'imap', then the default value is 'imap.gmail.com'. If the store type is 'pop3', then thedefault value is 'pop3.gmail.com'. STRING Yes No port The port that is used to create the connection. '993', the default value is valid only if the store is 'imap' and ssl-enabled. INT Yes No folder The name of the folder to which the emails should be fetched. INBOX STRING Yes No search.term The option that includes conditions such as key-value pairs to search for emails. In a string search term, the key and the value should be separated by a semicolon (';'). Each key-value pair must be within inverted commas (' '). The string search term can define multiple comma-separated key-value pairs. This string search term currently supports only the 'subject', 'from', 'to', 'bcc', and 'cc' keys. e.g., if you enter 'subject:DAS, from:carbon, bcc:wso2', the search term creates a search term instance that filters emails that contain 'DAS' in the subject, 'carbon' in the 'from' address, and 'wso2' in one of the 'bcc' addresses. The string search term carries out sub string matching that is case-sensitive. If '@' in included in the value for any key other than the 'subject' key, it checks for an address that is equal to the value given. e.g., If you search for 'abc@', the string search terms looks for an address that contains 'abc' before the '@' symbol. None STRING Yes No polling.interval This defines the time interval in seconds at which th email source should poll the account to check for new mail arrivals.in seconds. 600 LONG Yes No action.after.processed The action to be performed by the email source for the processed mail. Possible values are as follows: 'FLAGGED': Sets the flag as 'flagged'. 'SEEN': Sets the flag as 'read'. 'ANSWERED': Sets the flag as 'answered'. 'DELETE': Deletes tha mail after the polling cycle. 'MOVE': Moves the mail to the folder specified in the 'folder.to.move' parameter. If the folder specified is 'pop3', then the only option available is 'DELETE'. NONE STRING Yes No folder.to.move The name of the folder to which the mail must be moved once it is processed. If the action after processing is 'MOVE', it is required to specify a value for this parameter. STRING No No content.type The content type of the email. It can be either 'text/plain' or 'text/html.' text/plain STRING Yes No ssl.enable If this is set to 'true', a secure port is used to establish the connection. The possible values are 'true' and 'false'. true BOOL Yes No System Parameters Name Description Default Value Possible Parameters mail.imap.partialfetch This determines whether the IMAP partial-fetch capability should be used. true true or false mail.imap.fetchsize The partial fetch size in bytes. 16K value in bytes mail.imap.peek If this is set to 'true', the IMAP PEEK option should be used when fetching body parts to avoid setting the 'SEEN' flag on messages. The default value is 'false'. This can be overridden on a per-message basis by the 'setPeek method' in 'IMAPMessage'. false true or false mail.imap.connectiontimeout The socket connection timeout value in milliseconds. This timeout is implemented by 'java.net.Socket'. infinity timeout Any Integer value mail.imap.timeout The socket read timeout value in milliseconds. This timeout is implemented by 'java.net.Socket'. infinity timeout Any Integer value mail.imap.writetimeout The socket write timeout value in milliseconds. This timeout is implemented by using a 'java.util.concurrent.ScheduledExecutorService' per connection that schedules a thread to close the socket if the timeout period elapses. Therefore, the overhead of using this timeout is one thread per connection. infinity timeout Any Integer value mail.imap.statuscachetimeout The timeout value in milliseconds for the cache of 'STATUS' command response. 1000ms Time out in miliseconds mail.imap.appendbuffersize The maximum size of a message to buffer in memory when appending to an IMAP folder. None Any Integer value mail.imap.connectionpoolsize The maximum number of available connections in the connection pool. 1 Any Integer value mail.imap.connectionpooltimeout The timeout value in milliseconds for connection pool connections. 45000ms Any Integer mail.imap.separatestoreconnection If this parameter is set to 'true', it indicates that a dedicated store connection needs to be used for store commands. true true or false mail.imap.auth.login.disable If this is set to 'true', it is not possible to use the non-standard 'AUTHENTICATE LOGIN' command instead of the plain 'LOGIN' command. false true or false mail.imap.auth.plain.disable If this is set to 'true', the 'AUTHENTICATE PLAIN' command cannot be used. false true or false mail.imap.auth.ntlm.disable If true, prevents use of the AUTHENTICATE NTLM command. false true or false mail.imap.proxyauth.user If the server supports the PROXYAUTH extension, this property specifies the name of the user to act as. Authentication to log in to the server is carried out using the administrator's credentials. After authentication, the IMAP provider issues the 'PROXYAUTH' command with the user name specified in this property. None Valid string value mail.imap.localaddress The local address (host name) to bind to when creating the IMAP socket. Defaults to the address picked by the Socket class. Valid string value mail.imap.localport The local port number to bind to when creating the IMAP socket. Defaults to the port number picked by the Socket class. Valid String value mail.imap.sasl.enable If this parameter is set to 'true', the system attempts to use the 'javax.security.sasl' package to choose an authentication mechanism for the login. false true or false mail.imap.sasl.mechanisms A list of SASL mechanism names that the system should to try to use. The names can be separated by spaces or commas. None Valid string value mail.imap.sasl.authorizationid The authorization ID to use in the SASL authentication. If this parameter is not set, the authentication ID (username) is used. Valid string value mail.imap.sasl.realm The realm to use with SASL authentication mechanisms that require a realm, such as 'DIGEST-MD5'. None Valid string value mail.imap.auth.ntlm.domain The NTLM authentication domain. None Valid string value The NTLM authentication domain. NTLM protocol-specific flags. None Valid integer value mail.imap.socketFactory If this parameter is set to a class that implements the 'javax.net.SocketFactory' interface, this class is used to create IMAP sockets. None Valid SocketFactory mail.imap.socketFactory.class If this parameter is set, it specifies the name of a class that implements the 'javax.net.SocketFactory' interface. This class is used to create IMAP sockets. None Valid string mail.imap.socketFactory.fallback If this parameter is set to 'true', failure to create a socket using the specified socket factory class results in the socket being created using the 'java.net.Socket' class. true true or false mail.imap.socketFactory.port This specifies the port to connect to when using the specified socket factory. If this parameter is not set, the default port is used. 143 Valid Integer mail.imap.ssl.checkserveridentity If this parameter is set to 'true', the system checks the server identity as specified by RFC 2595. false true or false mail.imap.ssl.trust If this parameter is set and a socket factory has not been specified, it enables the use of a 'MailSSLSocketFactory'. If this parameter is set to ' ', all the hosts are trusted. If this parameter specifies list of hosts separated by white spaces, only those hosts are trusted. If the parameter is not set to any of the values mentioned above, trust depends on the certificate presented by the server. Valid String mail.imap.ssl.socketFactory If this parameter is set to a class that extends the 'javax.net.ssl.SSLSocketFactory' class this class is used to create IMAP SSL sockets. None SSL Socket Factory mail.imap.ssl.socketFactory.class If this parameter is set, it specifies the name of a class that extends the 'javax.net.ssl.SSLSocketFactory' class. This class is used to create IMAP SSL sockets. None Valid String mail.imap.ssl.socketFactory.port This specifies the port to connect to when using the specified socket factory. the default port 993 is used. valid port number mail.imap.ssl.protocols This specifies the SSL protocols that are enabled for SSL connections. The property value is a whitespace-separated list of tokens acceptable to the 'javax.net.ssl.SSLSocket.setEnabledProtocols' method. None Valid string mail.imap.starttls.enable If this parameter is set to 'true', it is possible to use the 'STARTTLS' command (if supported by the server) to switch the connection to a TLS-protected connection before issuing any login commands. false true or false mail.imap.socks.host This specifies the host name of a 'SOCKS5' proxy server that is used to connect to the mail server. None Valid String mail.imap.socks.port This specifies the port number for the 'SOCKS5' proxy server. This is needed if the proxy server is not using the standard port number 1080. 1080 Valid String mail.imap.minidletime This property sets the delay in milliseconds. 10 milliseconds time in seconds (Integer) mail.imap.enableimapevents If this property is set to 'true', it enables special IMAP-specific events to be delivered to the 'ConnectionListener' of the store. The unsolicited responses received during the idle method of the store are sent as connection events with 'IMAPStore.RESPONSE' as the type. The event's message is the raw IMAP response string. false true or false mail.imap.folder.class The class name of a subclass of 'com.sun.mail.imap.IMAPFolder'. The subclass can be used to provide support for additional IMAP commands. The subclass must have public constructors of the form 'public MyIMAPFolder'(String fullName, char separator, IMAPStore store, Boolean isNamespace) and public 'MyIMAPFolder'(ListInfo li, IMAPStore store) None Valid String mail.pop3.connectiontimeout The socket connection timeout value in milliseconds. Infinite timeout Integer value mail.pop3.timeout The socket I/O timeout value in milliseconds. Infinite timeout Integer value mail.pop3.message.class The class name of a subclass of 'com.sun.mail.pop3.POP3Message'. None Valid String mail.pop3.localaddress The local address (host name) to bind to when creating the POP3 socket. Defaults to the address picked by the Socket class. Valid String mail.pop3.localport The local port number to bind to when creating the POP3 socket. Defaults to the port number picked by the Socket class. Valid port number mail.pop3.apop.enable If this parameter is set to 'true', use 'APOP' instead of 'USER/PASS' to log in to the 'POP3' server (if the 'POP3' server supports 'APOP'). APOP sends a digest of the password instead of clearing the text password. false true or false mail.pop3.socketFactory If this parameter is set to a class that implements the 'javax.net.SocketFactory' interface, this class is used to create 'POP3' sockets. None Socket Factory mail.pop3.socketFactory.class If this parameter is set, it specifies the name of a class that implements the 'javax.net.SocketFactory' interface. This class is used to create 'POP3' sockets. None Valid String mail.pop3.socketFactory.fallback If this parameter is set to 'true', failure to create a socket using the specified socket factory class results in the socket being created using the 'java.net.Socket' class. false true or false mail.pop3.socketFactory.port This specifies the port to connect to when using the specified socket factory. Default port Valid port number mail.pop3.ssl.checkserveridentity If this parameter is set to 'true', check the server identity as specified by RFC 2595. false true or false mail.pop3.ssl.trust If this parameter is set and a socket factory has not been specified, it is possible to use a 'MailSSLSocketFactory'. If this parameter is set to ' ', all the hosts are trusted. If the parameter is set to a whitespace-separated list of hosts, only those hosts are trusted. If the parameter is not set to any of the values mentioned above, trust depends on the certificate presented by the server. Valid String mail.pop3.ssl.socketFactory If this parameter is set to a class that extends the 'javax.net.ssl.SSLSocketFactory' class, this class is used to create 'POP3' SSL sockets. None SSL Socket Factory mail.pop3.ssl.checkserveridentity If this parameter is set to 'true', the system checks the server identity as specified by 'RFC 2595'. false true or false mail.pop3.ssl.trust If this parameter is set and a socket factory has not been specified, it is possible to use a 'MailSSLSocketFactory'. If this parameter is set to '*', all the hosts are trusted. If the parameter is set to a whitespace-separated list of hosts, only those hosts are trusted. Trust depends on the certificate presented by the server. Valid String mail.pop3.ssl.socketFactory If this parameter is set to a class that extends the 'javax.net.ssl.SSLSocketFactory' class, this class is used to create 'POP3 SSL' sockets. None SSL Socket Factory mail.pop3.ssl.socketFactory.class If this parameter is set, it specifies the name of a class that extends the 'javax.net.ssl.SSLSocketFactory' class. This class is used to create 'POP3 SSL' sockets. None Valid String mail.pop3.ssl.socketFactory.p This parameter pecifies the port to connect to when using the specified socket factory. 995 Valid Integer mail.pop3.ssl.protocols This parameter specifies the SSL protocols that are enabled for SSL connections. The property value is a whitespace-separated list of tokens acceptable to the 'javax.net.ssl.SSLSocket.setEnabledProtocols' method. None Valid String mail.pop3.starttls.enable If this parameter is set to 'true', it is possible to use the 'STLS' command (if supported by the server) to switch the connection to a TLS-protected connection before issuing any login commands. false true or false mail.pop3.starttls.required If this parameter is set to 'true', it is required to use the 'STLS' command. The connect method fails if the server does not support the 'STLS' command or if the command fails. false true or false mail.pop3.socks.host This parameter specifies the host name of a 'SOCKS5' proxy server that can be used to connect to the mail server. None Valid String mail.pop3.socks.port This parameter specifies the port number for the 'SOCKS5' proxy server. None Valid String mail.pop3.disabletop If this parameter is set to 'true', the 'POP3 TOP' command is not used to fetch message headers. false true or false mail.pop3.forgettopheaders If this parameter is set to 'true', the headers that might have been retrieved using the 'POP3 TOP' command is forgotten and replaced by the headers retrieved when the 'POP3 RETR' command is executed. false true or false mail.pop3.filecache.enable If this parameter is set to 'true', the 'POP3' provider caches message data in a temporary file instead of caching them in memory. Messages are only added to the cache when accessing the message content. Message headers are always cached in memory (on demand). The file cache is removed when the folder is closed or the JVM terminates. false true or false mail.pop3.filecache.dir If the file cache is enabled, this property is used to override the default directory used by the JDK for temporary files. None Valid String mail.pop3.cachewriteto This parameter controls the behavior of the 'writeTo' method on a 'POP3' message object. If the parameter is set to 'true', the message content has not been cached yet, and the 'ignoreList' is null, the message is cached before being written. If not, the message is streamed directly to the output stream without being cached. false true or false mail.pop3.keepmessagecontent If this property is set to 'true', a hard reference to the cached content is retained, preventing the memory from being reused until the folder is closed, or until the cached content is explicitly invalidated (using the 'invalidate' method). false true or false Examples EXAMPLE 1 @source(type='email', @map(type='xml'), username='receiver.account', password='account.password',)define stream inputStream (name string, age int, country string); This example illustrates how to receive events in 'xml' format via the email source. In this example, only the required parameters are defined in the stream definition. The default values are taken for the other parameters. The search term is not defined, and therefore, all the new messages in the inbox folder are polled and taken. EXAMPLE 2 @source(type='email', @map(type='xml'), username='receiver.account', password='account.password',store = 'imap',host = 'imap.gmail.com',port = '993',searchTerm = 'subject:Stream Processor, from: from.account@ , cc: cc.account',polling.interval='500',action.after.processed='DELETE',content.type='text/html,)define stream inputStream (name string, age int, country string); This example illustrates how to receive events in 'xml' format via the email source. The email source polls the mail account every 500 seconds to check whether any new mails have arrived. It processes new mails only if they satisfy the conditions specified for the email search term (the value for 'from' of the email message should be 'from.account@. host name ', and the message should contain 'cc.account' in the cc receipient list and the word 'Stream Processor' in the mail subject). in this example, the action after processing is 'DELETE'. Therefore,after processing the event, corresponding mail is deleted from the mail folder.","title":"email (Source)"},{"location":"docs/api/5.1.0/#file-source","text":"File Source provides the functionality for user to feed data to siddhi from files. Both text and binary files are supported by file source. Origin: siddhi-io-file:2.0.3 Syntax @source(type=\"file\", dir.uri=\" STRING \", file.uri=\" STRING \", mode=\" STRING \", tailing=\" BOOL \", action.after.process=\" STRING \", action.after.failure=\" STRING \", move.after.process=\" STRING \", move.after.failure=\" STRING \", begin.regex=\" STRING \", end.regex=\" STRING \", file.polling.interval=\" STRING \", dir.polling.interval=\" STRING \", timeout=\" STRING \", file.read.wait.timeout=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic dir.uri Used to specify a directory to be processed. All the files inside this directory will be processed. Only one of 'dir.uri' and 'file.uri' should be provided. This uri MUST have the respective protocol specified. STRING No No file.uri Used to specify a file to be processed. Only one of 'dir.uri' and 'file.uri' should be provided. This uri MUST have the respective protocol specified. STRING No No mode This parameter is used to specify how files in given directory should.Possible values for this parameter are, 1. TEXT.FULL : to read a text file completely at once. 2. BINARY.FULL : to read a binary file completely at once. 3. LINE : to read a text file line by line. 4. REGEX : to read a text file and extract data using a regex. line STRING Yes No tailing This can either have value true or false. By default it will be true. This attribute allows user to specify whether the file should be tailed or not. If tailing is enabled, the first file of the directory will be tailed. Also tailing should not be enabled in 'binary.full' or 'text.full' modes. true BOOL Yes No action.after.process This parameter is used to specify the action which should be carried out after processing a file in the given directory. It can be either DELETE or MOVE and default value will be 'DELETE'. If the action.after.process is MOVE, user must specify the location to move consumed files using 'move.after.process' parameter. delete STRING Yes No action.after.failure This parameter is used to specify the action which should be carried out if a failure occurred during the process. It can be either DELETE or MOVE and default value will be 'DELETE'. If the action.after.failure is MOVE, user must specify the location to move consumed files using 'move.after.failure' parameter. delete STRING Yes No move.after.process If action.after.process is MOVE, user must specify the location to move consumed files using 'move.after.process' parameter. This should be the absolute path of the file that going to be created after moving is done. This uri MUST have the respective protocol specified. STRING No No move.after.failure If action.after.failure is MOVE, user must specify the location to move consumed files using 'move.after.failure' parameter. This should be the absolute path of the file that going to be created after moving is done. This uri MUST have the respective protocol specified. STRING No No begin.regex This will define the regex to be matched at the beginning of the retrieved content. None STRING Yes No end.regex This will define the regex to be matched at the end of the retrieved content. None STRING Yes No file.polling.interval This parameter is used to specify the time period (in milliseconds) of a polling cycle for a file. 1000 STRING Yes No dir.polling.interval This parameter is used to specify the time period (in milliseconds) of a polling cycle for a directory. 1000 STRING Yes No timeout This parameter is used to specify the maximum time period (in milliseconds) for waiting until a file is processed. 5000 STRING Yes No file.read.wait.timeout This parameter is used to specify the maximum time period (in milliseconds) till it waits before retrying to read the full file content. 1000 STRING Yes No Examples EXAMPLE 1 @source(type='file', mode='text.full', tailing='false' dir.uri='file://abc/xyz', action.after.process='delete', @map(type='json')) define stream FooStream (symbol string, price float, volume long); Under above configuration, all the files in directory will be picked and read one by one. In this case, it's assumed that all the files contains json valid json strings with keys 'symbol','price' and 'volume'. Once a file is read, its content will be converted to an event using siddhi-map-json extension and then, that event will be received to the FooStream. Finally, after reading is finished, the file will be deleted. EXAMPLE 2 @source(type='file', mode='files.repo.line', tailing='true', dir.uri='file://abc/xyz', @map(type='json')) define stream FooStream (symbol string, price float, volume long); Under above configuration, the first file in directory '/abc/xyz' will be picked and read line by line. In this case, it is assumed that the file contains lines json strings. For each line, line content will be converted to an event using siddhi-map-json extension and then, that event will be received to the FooStream. Once file content is completely read, it will keep checking whether a new entry is added to the file or not. If such entry is added, it will be immediately picked up and processed.","title":"file (Source)"},{"location":"docs/api/5.1.0/#grpc-source","text":"This extension starts a grpc server during initialization time. The server listens to requests from grpc stubs. This source has a default mode of operation and custom user defined grpc service mode. By default this uses EventService. Please find the proto definition here . In the default mode this source will use EventService consume method. If we want to use our custom gRPC services, we have to pack auto-generated gRPC service classes and protobuf classes into a jar file and add it into the project classpath (or to the jars folder in the siddhi-tooling folder if we use it with siddhi-tooling ). Please find the custom protobuf definition that uses in examples here . This method will receive requests and injects them into stream through a mapper. Origin: siddhi-io-grpc:1.0.2 Syntax @source(type=\"grpc\", receiver.url=\" STRING \", max.inbound.message.size=\" INT \", max.inbound.metadata.size=\" INT \", server.shutdown.waiting.time=\" LONG \", truststore.file=\" STRING \", truststore.password=\" STRING \", truststore.algorithm=\" STRING \", tls.store.type=\" STRING \", keystore.file=\" STRING \", keystore.password=\" STRING \", keystore.algorithm=\" STRING \", enable.ssl=\" BOOL \", threadpool.size=\" INT \", threadpool.buffer.size=\" INT \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic receiver.url The url which can be used by a client to access the grpc server in this extension. This url should consist the host hostPort, port, fully qualified service name, method name in the following format. grpc://0.0.0.0:9763/ serviceName / methodName For example: grpc://0.0.0.0:9763/org.wso2.grpc.EventService/consume STRING No No max.inbound.message.size Sets the maximum message size in bytes allowed to be received on the server. 4194304 INT Yes No max.inbound.metadata.size Sets the maximum size of metadata in bytes allowed to be received. 8192 INT Yes No server.shutdown.waiting.time The time in seconds to wait for the server to shutdown, giving up if the timeout is reached. 5 LONG Yes No truststore.file the file path of truststore. If this is provided then server authentication is enabled - STRING Yes No truststore.password the password of truststore. If this is provided then the integrity of the keystore is checked - STRING Yes No truststore.algorithm the encryption algorithm to be used for server authentication - STRING Yes No tls.store.type TLS store type - STRING Yes No keystore.file the file path of keystore. If this is provided then client authentication is enabled - STRING Yes No keystore.password the password of keystore - STRING Yes No keystore.algorithm the encryption algorithm to be used for client authentication - STRING Yes No enable.ssl to enable ssl. If set to true and truststore.file is not given then it will be set to default carbon jks by default FALSE BOOL Yes No threadpool.size Sets the maximum size of threadpool dedicated to serve requests at the gRPC server 100 INT Yes No threadpool.buffer.size Sets the maximum size of threadpool buffer server 100 INT Yes No Examples EXAMPLE 1 @source(type='grpc', receiver.url='grpc://localhost:8888/org.wso2.grpc.EventService/consume', @map(type='json')) define stream BarStream (message String); Here the port is given as 8888. So a grpc server will be started on port 8888 and the server will expose EventService. This is the default service packed with the source. In EventService the consume method is EXAMPLE 2 @source(type='grpc', receiver.url='grpc://localhost:8888/org.wso2.grpc.EventService/consume', @map(type='json', @attributes(name='trp:name', age='trp:age', message='message'))) define stream BarStream (message String, name String, age int); Here we are getting headers sent with the request as transport properties and injecting them into the stream. With each request a header will be sent in MetaData in the following format: 'Name:John', 'Age:23' EXAMPLE 3 @source(type='grpc', receiver.url='grpc://localhost:8888/org.wso2.grpc.MyService/send', @map(type='protobuf')) define stream BarStream (stringValue string, intValue int,longValue long,booleanValue bool,floatValue float,doubleValue double); Here the port is given as 8888. So a grpc server will be started on port 8888 and sever will keep listening to the 'send' RPC method in the 'MyService' service. EXAMPLE 4 @source(type='grpc', receiver.url='grpc://localhost:8888/org.wso2.grpc.MyService/send', @map(type='protobuf', @attributes(a = 'stringValue', b = 'intValue', c = 'longValue',d = 'booleanValue', e ='floatValue', f ='doubleValue'))) define stream BarStream (a string ,c long,b int, d bool,e float,f double); Here the port is given as 8888. So a grpc server will be started on port 8888 and sever will keep listening to the 'send' method in the 'MyService' service. Since we provide mapping in the stream we can use any names for stream attributes, but we have to map those names with correct protobuf message attributes' names. If we want to send metadata, we should map the attributes. EXAMPLE 5 @source(type='grpc', receiver.url='grpc://localhost:8888/org.wso2.grpc.StreamService/clientStream', @map(type='protobuf')) define stream BarStream (stringValue string, intValue int,longValue long,booleanValue bool,floatValue float,doubleValue double); Here we receive a stream of requests to the grpc source. Whenever we want to use streaming with grpc source, we have to define the RPC method as client streaming method (look at the sample proto file provided in the resource folder here ), when we define a stream method siddhi will identify it as a stream RPC method and ready to accept stream of request from the client.","title":"grpc (Source)"},{"location":"docs/api/5.1.0/#grpc-call-response-source","text":"This grpc source receives responses received from gRPC server for requests sent from a grpc-call sink. The source will receive responses for sink with the same sink.id. For example if you have a gRPC sink with sink.id 15 then we need to set the sink.id as 15 in the source to receives responses. Sinks and sources have 1:1 mapping Origin: siddhi-io-grpc:1.0.2 Syntax @source(type=\"grpc-call-response\", sink.id=\" INT \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic sink.id a unique ID that should be set for each grpc-call source. There is a 1:1 mapping between grpc-call sinks and grpc-call-response sources. Each sink has one particular source listening to the responses to requests published from that sink. So the same sink.id should be given when writing the sink also. INT No No Examples EXAMPLE 1 @source(type='grpc-call-response', sink.id= '1') define stream BarStream (message String);@sink(type='grpc-call', publisher.url = 'grpc://194.23.98.100:8080/EventService/process', sink.id= '1', @map(type='json')) define stream FooStream (message String); Here we are listening to responses for requests sent from the sink with sink.id 1 will be received here. The results will be injected into BarStream","title":"grpc-call-response (Source)"},{"location":"docs/api/5.1.0/#grpc-service-source","text":"This extension implements a grpc server for receiving and responding to requests. During initialization time a grpc server is started on the user specified port exposing the required service as given in the url. This source also has a default mode and a user defined grpc service mode. By default this uses EventService. Please find the proto definition here In the default mode this will use the EventService process method. If we want to use our custom gRPC services, we have to pack auto-generated gRPC service classes and protobuf classes into a jar file and add it into the project classpath (or to the jars folder in the siddhi-tooling folder if we use it with siddhi-tooling ). Please find the custom protobuf definition that uses in examples here . This accepts grpc message class Event as defined in the EventService proto. This uses GrpcServiceResponse sink to send reponses back in the same Event message format. Origin: siddhi-io-grpc:1.0.2 Syntax @source(type=\"grpc-service\", receiver.url=\" STRING \", max.inbound.message.size=\" INT \", max.inbound.metadata.size=\" INT \", service.timeout=\" INT \", server.shutdown.waiting.time=\" LONG \", truststore.file=\" STRING \", truststore.password=\" STRING \", truststore.algorithm=\" STRING \", tls.store.type=\" STRING \", keystore.file=\" STRING \", keystore.password=\" STRING \", keystore.algorithm=\" STRING \", enable.ssl=\" BOOL \", threadpool.size=\" INT \", threadpool.buffer.size=\" INT \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic receiver.url The url which can be used by a client to access the grpc server in this extension. This url should consist the host hostPort, port, fully qualified service name, method name in the following format. grpc://0.0.0.0:9763/ serviceName / methodName For example: grpc://0.0.0.0:9763/org.wso2.grpc.EventService/consume STRING No No max.inbound.message.size Sets the maximum message size in bytes allowed to be received on the server. 4194304 INT Yes No max.inbound.metadata.size Sets the maximum size of metadata in bytes allowed to be received. 8192 INT Yes No service.timeout The period of time in milliseconds to wait for siddhi to respond to a request received. After this time period of receiving a request it will be closed with an error message. 10000 INT Yes No server.shutdown.waiting.time The time in seconds to wait for the server to shutdown, giving up if the timeout is reached. 5 LONG Yes No truststore.file the file path of truststore. If this is provided then server authentication is enabled - STRING Yes No truststore.password the password of truststore. If this is provided then the integrity of the keystore is checked - STRING Yes No truststore.algorithm the encryption algorithm to be used for server authentication - STRING Yes No tls.store.type TLS store type - STRING Yes No keystore.file the file path of keystore. If this is provided then client authentication is enabled - STRING Yes No keystore.password the password of keystore - STRING Yes No keystore.algorithm the encryption algorithm to be used for client authentication - STRING Yes No enable.ssl to enable ssl. If set to true and truststore.file is not given then it will be set to default carbon jks by default FALSE BOOL Yes No threadpool.size Sets the maximum size of threadpool dedicated to serve requests at the gRPC server 100 INT Yes No threadpool.buffer.size Sets the maximum size of threadpool buffer server 100 INT Yes No Examples EXAMPLE 1 @source(type='grpc-service', receiver.url='grpc://localhost:8888/org.wso2.grpc.EventService/process', source.id='1', @map(type='json', @attributes(messageId='trp:messageId', message='message'))) define stream FooStream (messageId String, message String); Here a grpc server will be started at port 8888. The process method of EventService will be exposed for clients. source.id is set as 1. So a grpc-service-response sink with source.id = 1 will send responses back for requests received to this source. Note that it is required to specify the transport property messageId since we need to correlate the request message with the response. EXAMPLE 2 @sink(type='grpc-service-response', source.id='1', @map(type='json')) define stream BarStream (messageId String, message String); @source(type='grpc-service', receiver.url='grpc://134.23.43.35:8080/org.wso2.grpc.EventService/process', source.id='1', @map(type='json', @attributes(messageId='trp:messageId', message='message'))) define stream FooStream (messageId String, message String); from FooStream select * insert into BarStream; The grpc requests are received through the grpc-service sink. Each received event is sent back through grpc-service-source. This is just a passthrough through Siddhi as we are selecting everything from FooStream and inserting into BarStream. EXAMPLE 3 @source(type='grpc-service', source.id='1' receiver.url='grpc://locanhost:8888/org.wso2.grpc.EventService/consume', @map(type='json', @attributes(name='trp:name', age='trp:age', message='message'))) define stream BarStream (message String, name String, age int); Here we are getting headers sent with the request as transport properties and injecting them into the stream. With each request a header will be sent in MetaData in the following format: 'Name:John', 'Age:23' EXAMPLE 4 @sink(type='grpc-service-response', source.id='1', message.id='{{messageId}}', @map(type='protobuf', @payload(stringValue='a',intValue='b',longValue='c',booleanValue='d',floatValue = 'e', doubleValue ='f'))) define stream BarStream (a string,messageId string, b int,c long,d bool,e float,f double); @source(type='grpc-service', receiver.url='grpc://134.23.43.35:8888/org.wso2.grpc.test.MyService/process', source.id='1', @map(type='protobuf', @attributes(messageId='trp:message.id', a = 'stringValue', b = 'intValue', c = 'longValue',d = 'booleanValue', e = 'floatValue', f ='doubleValue'))) define stream FooStream (a string,messageId string, b int,c long,d bool,e float,f double); from FooStream select * insert into BarStream; Here a grpc server will be started at port 8888. The process method of the MyService will be exposed to the clients. 'source.id' is set as 1. So a grpc-service-response sink with source.id = 1 will send responses back for requests received to this source. Note that it is required to specify the transport property messageId since we need to correlate the request message with the response and also we should map stream attributes with correct protobuf message attributes even they define using the same name as protobuf message attributes.","title":"grpc-service (Source)"},{"location":"docs/api/5.1.0/#http-source","text":"HTTP source receives POST requests via HTTP and HTTPS protocols in format such as text , XML and JSON . It also supports basic authentication to ensure events are received from authorized users/systems. The request headers and properties can be accessed via transport properties in the format trp: header . Origin: siddhi-io-http:2.1.2 Syntax @source(type=\"http\", receiver.url=\" STRING \", basic.auth.enabled=\" STRING \", worker.count=\" INT \", socket.idle.timeout=\" INT \", ssl.verify.client=\" STRING \", ssl.protocol=\" STRING \", tls.store.type=\" STRING \", ssl.configurations=\" STRING \", request.size.validation.configurations=\" STRING \", header.validation.configurations=\" STRING \", server.bootstrap.configurations=\" STRING \", trace.log.enabled=\" BOOL \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic receiver.url The URL on which events should be received. To enable SSL use https protocol in the url. http://0.0.0.0:9763/ appNAme / streamName STRING Yes No basic.auth.enabled This only works in VM, Docker and Kubernetes. Where when enabled it authenticates each request using the Authorization:'Basic encodeBase64(username:Password)' header. false STRING Yes No worker.count The number of active worker threads to serve the incoming events. By default the value is set to 1 to ensure events are processed in the same order they arrived. By increasing this value, higher performance can be achieved in the expense of loosing event ordering. 1 INT Yes No socket.idle.timeout Idle timeout for HTTP connection in millis. 120000 INT Yes No ssl.verify.client The type of client certificate verification. Supported values are require , optional . - STRING Yes No ssl.protocol SSL/TLS protocol. TLS STRING Yes No tls.store.type TLS store type. JKS STRING Yes No ssl.configurations SSL/TSL configurations in format \"' key : value ',' key : value '\" . Some supported parameters: - SSL/TLS protocols: 'sslEnabledProtocols:TLSv1.1,TLSv1.2' - List of ciphers: 'ciphers:TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256' - Enable session creation: 'client.enable.session.creation:true' - Supported server names: 'server.suported.server.names:server' - Add HTTP SNIMatcher: 'server.supported.snimatchers:SNIMatcher' - STRING Yes No request.size.validation.configurations Configurations to validate the HTTP request size. Expected format \"' key : value ',' key : value '\" . Some supported configurations : - Enable request size validation: 'request.size.validation:true' If request size is validated - Maximum request size: 'request.size.validation.maximum.value:2048' - Response status code when request size validation fails: 'request.size.validation.reject.status.code:401' - Response message when request size validation fails: 'request.size.validation.reject.message:Message is bigger than the valid size' - Response Content-Type when request size validation fails: 'request.size.validation.reject.message.content.type:plain/text' - STRING Yes No header.validation.configurations Configurations to validate HTTP headers. Expected format \"' key : value ',' key : value '\" . Some supported configurations : - Enable header size validation: 'header.size.validation:true' If header size is validated - Maximum length of initial line: 'header.validation.maximum.request.line:4096' - Maximum length of all headers: 'header.validation.maximum.size:8192' - Maximum length of the content or each chunk: 'header.validation.maximum.chunk.size:8192' - Response status code when header validation fails: 'header.validation.reject.status.code:401' - Response message when header validation fails: 'header.validation.reject.message:Message header is bigger than the valid size' - Response Content-Type when header validation fails: 'header.validation.reject.message.content.type:plain/text' - STRING Yes No server.bootstrap.configurations Server bootstrap configurations in format \"' key : value ',' key : value '\" . Some supported configurations : - Server connect timeout in millis: 'server.bootstrap.connect.timeout:15000' - Server socket timeout in seconds: 'server.bootstrap.socket.timeout:15' - Enable TCP no delay: 'server.bootstrap.nodelay:true' - Enable server keep alive: 'server.bootstrap.keepalive:true' - Send buffer size: 'server.bootstrap.sendbuffersize:1048576' - Receive buffer size: 'server.bootstrap.recievebuffersize:1048576' - Number of connections queued: 'server.bootstrap.socket.backlog:100' - STRING Yes No trace.log.enabled Enable trace log for traffic monitoring. false BOOL Yes No System Parameters Name Description Default Value Possible Parameters serverBootstrapBossGroupSize Number of boss threads to accept incoming connections. Number of available processors Any positive integer serverBootstrapWorkerGroupSize Number of worker threads to accept the connections from boss threads and perform non-blocking read and write from one or more channels. (Number of available processors) * 2 Any positive integer serverBootstrapClientGroupSize Number of client threads to perform non-blocking read and write to one or more channels. (Number of available processors) * 2 Any positive integer defaultHost The default host of the transport. 0.0.0.0 Any valid host defaultScheme The default protocol. http http https defaultHttpPort The default HTTP port when default scheme is http . 8280 Any valid port defaultHttpsPort The default HTTPS port when default scheme is https . 8243 Any valid port keyStoreLocation The default keystore file path. ${carbon.home}/resources/security/wso2carbon.jks Path to .jks file keyStorePassword The default keystore password. wso2carbon Keystore password as string Examples EXAMPLE 1 @app.name('StockProcessor') @source(type='http', @map(type = 'json')) define stream StockStream (symbol string, price float, volume long); Above HTTP source listeners on url http://0.0.0.0:9763/StockProcessor/StockStream for JSON messages on the format: { \"event\": { \"symbol\": \"FB\", \"price\": 24.5, \"volume\": 5000 } } It maps the incoming messages and sends them to StockStream for processing. EXAMPLE 2 @source(type='http', receiver.url='http://localhost:5005/stocks', @map(type = 'xml')) define stream StockStream (symbol string, price float, volume long); Above HTTP source listeners on url http://localhost:5005/stocks for JSON messages on the format: events event symbol Fb /symbol price 55.6 /price volume 100 /volume /event /events It maps the incoming messages and sends them to StockStream for processing.","title":"http (Source)"},{"location":"docs/api/5.1.0/#http-call-response-source","text":"The http-call-response source receives the responses for the calls made by its corresponding http-call sink, and maps them from formats such as text , XML and JSON . To handle messages with different http status codes having different formats, multiple http-call-response sources are allowed to associate with a single http-call sink. It allows accessing the attributes of the event that initiated the call, and the response headers and properties via transport properties in the format trp: attribute name and trp: header/property respectively. Origin: siddhi-io-http:2.1.2 Syntax @source(type=\"http-call-response\", sink.id=\" STRING \", http.status.code=\" STRING \", allow.streaming.responses=\" BOOL \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic sink.id Identifier to correlate the http-call-response source with its corresponding http-call sink that published the messages. STRING No No http.status.code The matching http responses status code regex, that is used to filter the the messages which will be processed by the source.Eg: http.status.code = '200' , http.status.code = '4\\d+' 200 STRING Yes No allow.streaming.responses Enable consuming responses on a streaming manner. false BOOL Yes No Examples EXAMPLE 1 @sink(type='http-call', method='POST', publisher.url='http://localhost:8005/registry/employee', sink.id='employee-info', @map(type='json')) define stream EmployeeRequestStream (name string, id int); @source(type='http-call-response', sink.id='employee-info', http.status.code='2\\\\d+', @map(type='json', @attributes(name='trp:name', id='trp:id', location='$.town', age='$.age'))) define stream EmployeeResponseStream(name string, id int, location string, age int); @source(type='http-call-response', sink.id='employee-info', http.status.code='4\\\\d+', @map(type='text', regex.A='((.|\\n)*)', @attributes(error='A[1]'))) define stream EmployeeErrorStream(error string); When events arrive in EmployeeRequestStream , http-call sink makes calls to endpoint on url http://localhost:8005/registry/employee with POST method and Content-Type application/json . If the arriving event has attributes name : John and id : 1423 it will send a message with default JSON mapping as follows: { \"event\": { \"name\": \"John\", \"id\": 1423 } } When the endpoint responds with status code in the range of 200 the message will be received by the http-call-response source associated with the EmployeeResponseStream stream, because it is correlated with the sink by the same sink.id employee-info and as that expects messages with http.status.code in regex format 2\\d+ . If the response message is in the format { \"town\": \"NY\", \"age\": 24 } the source maps the location and age attributes by executing JSON path on the message and maps the name and id attributes by extracting them from the request event via as transport properties. If the response status code is in the range of 400 then the message will be received by the http-call-response source associated with the EmployeeErrorStream stream, because it is correlated with the sink by the same sink.id employee-info and it expects messages with http.status.code in regex format 4\\d+ , and maps the error response to the error attribute of the event.","title":"http-call-response (Source)"},{"location":"docs/api/5.1.0/#http-request-source","text":"Deprecated (Use http-service source instead). The http-request source receives POST requests via HTTP and HTTPS protocols in format such as text , XML and JSON and sends responses via its corresponding http-response sink correlated through a unique source.id . For request and response correlation, it generates a messageId upon each incoming request and expose it via transport properties in the format trp:messageId to correlate them with the responses at the http-response sink. The request headers and properties can be accessed via transport properties in the format trp: header . It also supports basic authentication to ensure events are received from authorized users/systems. Origin: siddhi-io-http:2.1.2 Syntax @source(type=\"http-request\", receiver.url=\" STRING \", source.id=\" STRING \", connection.timeout=\" INT \", basic.auth.enabled=\" STRING \", worker.count=\" INT \", socket.idle.timeout=\" INT \", ssl.verify.client=\" STRING \", ssl.protocol=\" STRING \", tls.store.type=\" STRING \", ssl.configurations=\" STRING \", request.size.validation.configurations=\" STRING \", header.validation.configurations=\" STRING \", server.bootstrap.configurations=\" STRING \", trace.log.enabled=\" BOOL \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic receiver.url The URL on which events should be received. To enable SSL use https protocol in the url. http://0.0.0.0:9763/ appNAme / streamName STRING Yes No source.id Identifier to correlate the http-request source to its corresponding http-response sinks to send responses. STRING No No connection.timeout Connection timeout in millis. The system will send a timeout, if a corresponding response is not sent by an associated http-response sink within the given time. 120000 INT Yes No basic.auth.enabled This only works in VM, Docker and Kubernetes. Where when enabled it authenticates each request using the Authorization:'Basic encodeBase64(username:Password)' header. false STRING Yes No worker.count The number of active worker threads to serve the incoming events. By default the value is set to 1 to ensure events are processed in the same order they arrived. By increasing this value, higher performance can be achieved in the expense of loosing event ordering. 1 INT Yes No socket.idle.timeout Idle timeout for HTTP connection in millis. 120000 INT Yes No ssl.verify.client The type of client certificate verification. Supported values are require , optional . - STRING Yes No ssl.protocol SSL/TLS protocol. TLS STRING Yes No tls.store.type TLS store type. JKS STRING Yes No ssl.configurations SSL/TSL configurations in format \"' key : value ',' key : value '\" . Some supported parameters: - SSL/TLS protocols: 'sslEnabledProtocols:TLSv1.1,TLSv1.2' - List of ciphers: 'ciphers:TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256' - Enable session creation: 'client.enable.session.creation:true' - Supported server names: 'server.suported.server.names:server' - Add HTTP SNIMatcher: 'server.supported.snimatchers:SNIMatcher' - STRING Yes No request.size.validation.configurations Configurations to validate the HTTP request size. Expected format \"' key : value ',' key : value '\" . Some supported configurations : - Enable request size validation: 'request.size.validation:true' If request size is validated - Maximum request size: 'request.size.validation.maximum.value:2048' - Response status code when request size validation fails: 'request.size.validation.reject.status.code:401' - Response message when request size validation fails: 'request.size.validation.reject.message:Message is bigger than the valid size' - Response Content-Type when request size validation fails: 'request.size.validation.reject.message.content.type:plain/text' - STRING Yes No header.validation.configurations Configurations to validate HTTP headers. Expected format \"' key : value ',' key : value '\" . Some supported configurations : - Enable header size validation: 'header.size.validation:true' If header size is validated - Maximum length of initial line: 'header.validation.maximum.request.line:4096' - Maximum length of all headers: 'header.validation.maximum.size:8192' - Maximum length of the content or each chunk: 'header.validation.maximum.chunk.size:8192' - Response status code when header validation fails: 'header.validation.reject.status.code:401' - Response message when header validation fails: 'header.validation.reject.message:Message header is bigger than the valid size' - Response Content-Type when header validation fails: 'header.validation.reject.message.content.type:plain/text' - STRING Yes No server.bootstrap.configurations Server bootstrap configurations in format \"' key : value ',' key : value '\" . Some supported configurations : - Server connect timeout in millis: 'server.bootstrap.connect.timeout:15000' - Server socket timeout in seconds: 'server.bootstrap.socket.timeout:15' - Enable TCP no delay: 'server.bootstrap.nodelay:true' - Enable server keep alive: 'server.bootstrap.keepalive:true' - Send buffer size: 'server.bootstrap.sendbuffersize:1048576' - Receive buffer size: 'server.bootstrap.recievebuffersize:1048576' - Number of connections queued: 'server.bootstrap.socket.backlog:100' - STRING Yes No trace.log.enabled Enable trace log for traffic monitoring. false BOOL Yes No System Parameters Name Description Default Value Possible Parameters serverBootstrapBossGroupSize Number of boss threads to accept incoming connections. Number of available processors Any positive integer serverBootstrapWorkerGroupSize Number of worker threads to accept the connections from boss threads and perform non-blocking read and write from one or more channels. (Number of available processors) * 2 Any positive integer serverBootstrapClientGroupSize Number of client threads to perform non-blocking read and write to one or more channels. (Number of available processors) * 2 Any positive integer defaultHost The default host of the transport. 0.0.0.0 Any valid host defaultScheme The default protocol. http http https defaultHttpPort The default HTTP port when default scheme is http . 8280 Any valid port defaultHttpsPort The default HTTPS port when default scheme is https . 8243 Any valid port keyStoreLocation The default keystore file path. ${carbon.home}/resources/security/wso2carbon.jks Path to .jks file keyStorePassword The default keystore password. wso2carbon Keystore password as string Examples EXAMPLE 1 @source(type='http-request', receiver.url='http://localhost:5005/add', source.id='adder', @map(type='json, @attributes(messageId='trp:messageId', value1='$.event.value1', value2='$.event.value2'))) define stream AddStream (messageId string, value1 long, value2 long); @sink(type='http-response', source.id='adder', message.id='{{messageId}}', @map(type = 'json')) define stream ResultStream (messageId string, results long); @info(name = 'query1') from AddStream select messageId, value1 + value2 as results insert into ResultStream; Above sample listens events on http://localhost:5005/stocks url for JSON messages on the format: { \"event\": { \"value1\": 3, \"value2\": 4 } } Map the vents into AddStream, process the events through query query1 , and sends the results produced on ResultStream via http-response sink on the message format: { \"event\": { \"results\": 7 } }","title":"http-request (Source)"},{"location":"docs/api/5.1.0/#http-response-source","text":"Deprecated (Use http-call-response source instead). The http-response source receives the responses for the calls made by its corresponding http-request sink, and maps them from formats such as text , XML and JSON . To handle messages with different http status codes having different formats, multiple http-response sources are allowed to associate with a single http-request sink. It allows accessing the attributes of the event that initiated the call, and the response headers and properties via transport properties in the format trp: attribute name and trp: header/property respectively. Origin: siddhi-io-http:2.1.2 Syntax @source(type=\"http-response\", sink.id=\" STRING \", http.status.code=\" STRING \", allow.streaming.responses=\" BOOL \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic sink.id Identifier to correlate the http-response source with its corresponding http-request sink that published the messages. STRING No No http.status.code The matching http responses status code regex, that is used to filter the the messages which will be processed by the source.Eg: http.status.code = '200' , http.status.code = '4\\d+' 200 STRING Yes No allow.streaming.responses Enable consuming responses on a streaming manner. false BOOL Yes No Examples EXAMPLE 1 @sink(type='http-request', method='POST', publisher.url='http://localhost:8005/registry/employee', sink.id='employee-info', @map(type='json')) define stream EmployeeRequestStream (name string, id int); @source(type='http-response', sink.id='employee-info', http.status.code='2\\\\d+', @map(type='json', @attributes(name='trp:name', id='trp:id', location='$.town', age='$.age'))) define stream EmployeeResponseStream(name string, id int, location string, age int); @source(type='http-response', sink.id='employee-info', http.status.code='4\\\\d+', @map(type='text', regex.A='((.|\\n)*)', @attributes(error='A[1]'))) define stream EmployeeErrorStream(error string); When events arrive in EmployeeRequestStream , http-request sink makes calls to endpoint on url http://localhost:8005/registry/employee with POST method and Content-Type application/json . If the arriving event has attributes name : John and id : 1423 it will send a message with default JSON mapping as follows: { \"event\": { \"name\": \"John\", \"id\": 1423 } } When the endpoint responds with status code in the range of 200 the message will be received by the http-response source associated with the EmployeeResponseStream stream, because it is correlated with the sink by the same sink.id employee-info and as that expects messages with http.status.code in regex format 2\\d+ . If the response message is in the format { \"town\": \"NY\", \"age\": 24 } the source maps the location and age attributes by executing JSON path on the message and maps the name and id attributes by extracting them from the request event via as transport properties. If the response status code is in the range of 400 then the message will be received by the http-response source associated with the EmployeeErrorStream stream, because it is correlated with the sink by the same sink.id employee-info and it expects messages with http.status.code in regex format 4\\d+ , and maps the error response to the error attribute of the event.","title":"http-response (Source)"},{"location":"docs/api/5.1.0/#http-service-source","text":"The http-service source receives POST requests via HTTP and HTTPS protocols in format such as text , XML and JSON and sends responses via its corresponding http-service-response sink correlated through a unique source.id . For request and response correlation, it generates a messageId upon each incoming request and expose it via transport properties in the format trp:messageId to correlate them with the responses at the http-service-response sink. The request headers and properties can be accessed via transport properties in the format trp: header . It also supports basic authentication to ensure events are received from authorized users/systems. Origin: siddhi-io-http:2.1.2 Syntax @source(type=\"http-service\", receiver.url=\" STRING \", source.id=\" STRING \", connection.timeout=\" INT \", basic.auth.enabled=\" STRING \", worker.count=\" INT \", socket.idle.timeout=\" INT \", ssl.verify.client=\" STRING \", ssl.protocol=\" STRING \", tls.store.type=\" STRING \", ssl.configurations=\" STRING \", request.size.validation.configurations=\" STRING \", header.validation.configurations=\" STRING \", server.bootstrap.configurations=\" STRING \", trace.log.enabled=\" BOOL \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic receiver.url The URL on which events should be received. To enable SSL use https protocol in the url. http://0.0.0.0:9763/ appNAme / streamName STRING Yes No source.id Identifier to correlate the http-service source to its corresponding http-service-response sinks to send responses. STRING No No connection.timeout Connection timeout in millis. The system will send a timeout, if a corresponding response is not sent by an associated http-service-response sink within the given time. 120000 INT Yes No basic.auth.enabled This only works in VM, Docker and Kubernetes. Where when enabled it authenticates each request using the Authorization:'Basic encodeBase64(username:Password)' header. false STRING Yes No worker.count The number of active worker threads to serve the incoming events. By default the value is set to 1 to ensure events are processed in the same order they arrived. By increasing this value, higher performance can be achieved in the expense of loosing event ordering. 1 INT Yes No socket.idle.timeout Idle timeout for HTTP connection in millis. 120000 INT Yes No ssl.verify.client The type of client certificate verification. Supported values are require , optional . - STRING Yes No ssl.protocol SSL/TLS protocol. TLS STRING Yes No tls.store.type TLS store type. JKS STRING Yes No ssl.configurations SSL/TSL configurations in format \"' key : value ',' key : value '\" . Some supported parameters: - SSL/TLS protocols: 'sslEnabledProtocols:TLSv1.1,TLSv1.2' - List of ciphers: 'ciphers:TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256' - Enable session creation: 'client.enable.session.creation:true' - Supported server names: 'server.suported.server.names:server' - Add HTTP SNIMatcher: 'server.supported.snimatchers:SNIMatcher' - STRING Yes No request.size.validation.configurations Configurations to validate the HTTP request size. Expected format \"' key : value ',' key : value '\" . Some supported configurations : - Enable request size validation: 'request.size.validation:true' If request size is validated - Maximum request size: 'request.size.validation.maximum.value:2048' - Response status code when request size validation fails: 'request.size.validation.reject.status.code:401' - Response message when request size validation fails: 'request.size.validation.reject.message:Message is bigger than the valid size' - Response Content-Type when request size validation fails: 'request.size.validation.reject.message.content.type:plain/text' - STRING Yes No header.validation.configurations Configurations to validate HTTP headers. Expected format \"' key : value ',' key : value '\" . Some supported configurations : - Enable header size validation: 'header.size.validation:true' If header size is validated - Maximum length of initial line: 'header.validation.maximum.request.line:4096' - Maximum length of all headers: 'header.validation.maximum.size:8192' - Maximum length of the content or each chunk: 'header.validation.maximum.chunk.size:8192' - Response status code when header validation fails: 'header.validation.reject.status.code:401' - Response message when header validation fails: 'header.validation.reject.message:Message header is bigger than the valid size' - Response Content-Type when header validation fails: 'header.validation.reject.message.content.type:plain/text' - STRING Yes No server.bootstrap.configurations Server bootstrap configurations in format \"' key : value ',' key : value '\" . Some supported configurations : - Server connect timeout in millis: 'server.bootstrap.connect.timeout:15000' - Server socket timeout in seconds: 'server.bootstrap.socket.timeout:15' - Enable TCP no delay: 'server.bootstrap.nodelay:true' - Enable server keep alive: 'server.bootstrap.keepalive:true' - Send buffer size: 'server.bootstrap.sendbuffersize:1048576' - Receive buffer size: 'server.bootstrap.recievebuffersize:1048576' - Number of connections queued: 'server.bootstrap.socket.backlog:100' - STRING Yes No trace.log.enabled Enable trace log for traffic monitoring. false BOOL Yes No System Parameters Name Description Default Value Possible Parameters serverBootstrapBossGroupSize Number of boss threads to accept incoming connections. Number of available processors Any positive integer serverBootstrapWorkerGroupSize Number of worker threads to accept the connections from boss threads and perform non-blocking read and write from one or more channels. (Number of available processors) * 2 Any positive integer serverBootstrapClientGroupSize Number of client threads to perform non-blocking read and write to one or more channels. (Number of available processors) * 2 Any positive integer defaultHost The default host of the transport. 0.0.0.0 Any valid host defaultScheme The default protocol. http http https defaultHttpPort The default HTTP port when default scheme is http . 8280 Any valid port defaultHttpsPort The default HTTPS port when default scheme is https . 8243 Any valid port keyStoreLocation The default keystore file path. ${carbon.home}/resources/security/wso2carbon.jks Path to .jks file keyStorePassword The default keystore password. wso2carbon Keystore password as string Examples EXAMPLE 1 @source(type='http-service', receiver.url='http://localhost:5005/add', source.id='adder', @map(type='json, @attributes(messageId='trp:messageId', value1='$.event.value1', value2='$.event.value2'))) define stream AddStream (messageId string, value1 long, value2 long); @sink(type='http-service-response', source.id='adder', message.id='{{messageId}}', @map(type = 'json')) define stream ResultStream (messageId string, results long); @info(name = 'query1') from AddStream select messageId, value1 + value2 as results insert into ResultStream; Above sample listens events on http://localhost:5005/stocks url for JSON messages on the format: { \"event\": { \"value1\": 3, \"value2\": 4 } } Map the vents into AddStream, process the events through query query1 , and sends the results produced on ResultStream via http-service-response sink on the message format: { \"event\": { \"results\": 7 } }","title":"http-service (Source)"},{"location":"docs/api/5.1.0/#inmemory-source","text":"In-memory source subscribes to a topic to consume events which are published on the same topic by In-memory sinks. This provides a way to connect multiple Siddhi Apps deployed under the same Siddhi Manager (JVM). Here both the publisher and subscriber should have the same event schema (stream definition) for successful data transfer. Origin: siddhi-core:5.1.7 Syntax @source(type=\"inMemory\", topic=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic topic Subscribes to the events sent on the given topic. STRING No No Examples EXAMPLE 1 @source(type='inMemory', topic='Stocks', @map(type='passThrough')) define stream StocksStream (symbol string, price float, volume long); Here the StocksStream uses inMemory source to consume events published on the topic Stocks by the inMemory sinks deployed in the same JVM.","title":"inMemory (Source)"},{"location":"docs/api/5.1.0/#jms-source","text":"JMS Source allows users to subscribe to a JMS broker and receive JMS messages. It has the ability to receive Map messages and Text messages. Origin: siddhi-io-jms:2.0.2 Syntax @source(type=\"jms\", destination=\" STRING \", connection.factory.jndi.name=\" STRING \", factory.initial=\" STRING \", provider.url=\" STRING \", connection.factory.type=\" STRING \", worker.count=\" INT \", connection.username=\" STRING \", connection.password=\" STRING \", retry.interval=\" INT \", retry.count=\" INT \", use.receiver=\" BOOL \", subscription.durable=\" BOOL \", connection.factory.nature=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic destination Queue/Topic name which JMS Source should subscribe to STRING No No connection.factory.jndi.name JMS Connection Factory JNDI name. This value will be used for the JNDI lookup to find the JMS Connection Factory. QueueConnectionFactory STRING Yes No factory.initial Naming factory initial value STRING No No provider.url Java naming provider URL. Property for specifying configuration information for the service provider to use. The value of the property should contain a URL string (e.g. \"ldap://somehost:389\") STRING No No connection.factory.type Type of the connection connection factory. This can be either queue or topic. queue STRING Yes No worker.count Number of worker threads listening on the given queue/topic. 1 INT Yes No connection.username username for the broker. None STRING Yes No connection.password Password for the broker None STRING Yes No retry.interval Interval between each retry attempt in case of connection failure in milliseconds. 10000 INT Yes No retry.count Number of maximum reties that will be attempted in case of connection failure with broker. 5 INT Yes No use.receiver Implementation to be used when consuming JMS messages. By default transport will use MessageListener and tweaking this property will make make use of MessageReceiver false BOOL Yes No subscription.durable Property to enable durable subscription. false BOOL Yes No connection.factory.nature Connection factory nature for the broker. default STRING Yes No Examples EXAMPLE 1 @source(type='jms', @map(type='json'), factory.initial='org.apache.activemq.jndi.ActiveMQInitialContextFactory', provider.url='tcp://localhost:61616',destination='DAS_JMS_TEST', connection.factory.type='topic',connection.factory.jndi.name='TopicConnectionFactory') define stream inputStream (name string, age int, country string); This example shows how to connect to an ActiveMQ topic and receive messages. EXAMPLE 2 @source(type='jms', @map(type='json'), factory.initial='org.apache.activemq.jndi.ActiveMQInitialContextFactory', provider.url='tcp://localhost:61616',destination='DAS_JMS_TEST' ) define stream inputStream (name string, age int, country string); This example shows how to connect to an ActiveMQ queue and receive messages. Note that we are not providing properties like connection factory type","title":"jms (Source)"},{"location":"docs/api/5.1.0/#kafka-source","text":"A Kafka source receives events to be processed by WSO2 SP from a topic with a partition for a Kafka cluster. The events received can be in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. Origin: siddhi-io-kafka:5.0.4 Syntax @source(type=\"kafka\", bootstrap.servers=\" STRING \", topic.list=\" STRING \", group.id=\" STRING \", threading.option=\" STRING \", partition.no.list=\" STRING \", seq.enabled=\" BOOL \", is.binary.message=\" BOOL \", topic.offsets.map=\" STRING \", optional.configuration=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This specifies the list of Kafka servers to which the Kafka source must listen. This list can be provided as a set of comma-separated values. e.g., localhost:9092,localhost:9093 STRING No No topic.list This specifies the list of topics to which the source must listen. This list can be provided as a set of comma-separated values. e.g., topic_one,topic_two STRING No No group.id This is an ID to identify the Kafka source group. The group ID ensures that sources with the same topic and partition that are in the same group do not receive the same event. STRING No No threading.option This specifies whether the Kafka source is to be run on a single thread, or in multiple threads based on a condition. Possible values are as follows: single.thread : To run the Kafka source on a single thread. topic.wise : To use a separate thread per topic. partition.wise : To use a separate thread per partition. STRING No No partition.no.list The partition number list for the given topic. This is provided as a list of comma-separated values. e.g., 0,1,2, . 0 STRING Yes No seq.enabled If this parameter is set to true , the sequence of the events received via the source is taken into account. Therefore, each event should contain a sequence number as an attribute value to indicate the sequence. false BOOL Yes No is.binary.message In order to receive binary events via the Kafka source,it is required to setthis parameter to 'True'. false BOOL Yes No topic.offsets.map This parameter specifies reading offsets for each topic and partition. The value for this parameter is specified in the following format: topic = offset , topic = offset , When an offset is defined for a topic, the Kafka source skips reading the message with the number specified as the offset as well as all the messages sent previous to that message. If the offset is not defined for a specific topic it reads messages from the beginning. e.g., stocks=100,trades=50 reads from the 101th message of the stocks topic, and from the 51 st message of the trades topic. null STRING Yes No optional.configuration This parameter contains all the other possible configurations that the consumer is created with. e.g., ssl.keystore.type:JKS,batch.size:200 . null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic,kafka_topic2', group.id='test', threading.option='partition.wise', bootstrap.servers='localhost:9092', partition.no.list='0,1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This kafka source configuration listens to the kafka_topic and kafka_topic2 topics with 0 and 1 partitions. A thread is created for each topic and partition combination. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . EXAMPLE 2 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic', group.id='test', threading.option='single.thread', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka source configuration listens to the kafka_topic topic for the default partition because no partition.no.list is defined. Only one thread is created for the topic. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream .","title":"kafka (Source)"},{"location":"docs/api/5.1.0/#kafkamultidc-source","text":"The Kafka Multi-Datacenter(DC) source receives records from the same topic in brokers deployed in two different kafka clusters. It filters out all the duplicate messages and ensuresthat the events are received in the correct order using sequential numbering. It receives events in formats such as TEXT , XML JSON and Binary`.The Kafka Source creates the default partition '0' for a given topic, if the topic has not yet been created in the Kafka cluster. Origin: siddhi-io-kafka:5.0.4 Syntax @source(type=\"kafkaMultiDC\", bootstrap.servers=\" STRING \", topic=\" STRING \", partition.no=\" INT \", is.binary.message=\" BOOL \", optional.configuration=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This contains the kafka server list which the kafka source listens to. This is given using comma-separated values. eg: 'localhost:9092,localhost:9093' STRING No No topic This is the topic that the source listens to. eg: 'topic_one' STRING No No partition.no This is the partition number of the given topic. 0 INT Yes No is.binary.message In order to receive the binary events via the Kafka Multi-DC source, the value of this parameter needs to be set to 'True'. false BOOL Yes No optional.configuration This contains all the other possible configurations with which the consumer can be created.eg: producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source(type='kafkaMultiDC', topic='kafka_topic', bootstrap.servers='host1:9092,host1:9093', partition.no='1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; The following query listens to 'kafka_topic' topic, deployed in the broker host1:9092 and host1:9093, with partition 1. A thread is created for each broker. The receiving xml events are mapped to a siddhi event and sent to the FooStream.","title":"kafkaMultiDC (Source)"},{"location":"docs/api/5.1.0/#nats-source","text":"NATS Source allows users to subscribe to a NATS broker and receive messages. It has the ability to receive all the message types supported by NATS. Origin: siddhi-io-nats:2.0.6 Syntax @source(type=\"nats\", destination=\" STRING \", bootstrap.servers=\" STRING \", client.id=\" STRING \", cluster.id=\" STRING \", queue.group.name=\" STRING \", durable.name=\" STRING \", subscription.sequence=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic destination Subject name which NATS Source should subscribe to. STRING No No bootstrap.servers The NATS based url of the NATS server. nats://localhost:4222 STRING Yes No client.id The identifier of the client subscribing/connecting to the NATS broker. None STRING Yes No cluster.id The identifier of the NATS server/cluster. test-cluster STRING Yes No queue.group.name This can be used when there is a requirement to share the load of a NATS subject. Clients belongs to the same queue group share the subscription load. None STRING Yes No durable.name This can be used to subscribe to a subject from the last acknowledged message when a client or connection failure happens. The client can be uniquely identified using the tuple (client.id, durable.name). None STRING Yes No subscription.sequence This can be used to subscribe to a subject from a given number of message sequence. All the messages from the given point of sequence number will be passed to the client. If not provided then the either the persisted value or 0 will be used. None STRING Yes No Examples EXAMPLE 1 @source(type='nats', @map(type='text'), destination='SP_NATS_INPUT_TEST', bootstrap.servers='nats://localhost:4222',client.id='nats_client',server.id='test-cluster',queue.group.name = 'group_nats',durable.name = 'nats-durable',subscription.sequence = '100') define stream inputStream (name string, age int, country string); This example shows how to subscribe to a NATS subject with all supporting configurations.With the following configuration the source identified as 'nats-client' will subscribes to a subject named as 'SP_NATS_INPUT_TEST' which resides in a nats instance with a cluster id of 'test-cluster', running in localhost and listening to the port 4222 for client connection. This subscription will receive all the messages from 100 th in the subject. EXAMPLE 2 @source(type='nats', @map(type='text'), destination='SP_NATS_INPUT_TEST', ) define stream inputStream (name string, age int, country string); This example shows how to subscribe to a NATS subject with mandatory configurations.With the following configuration the source identified with an auto generated client id will subscribes to a subject named as 'SP_NATS_INTPUT_TEST' which resides in a nats instance with a cluster id of 'test-cluster', running in localhost and listening to the port 4222 for client connection. This will receive all available messages in the subject","title":"nats (Source)"},{"location":"docs/api/5.1.0/#prometheus-source","text":"This source consumes Prometheus metrics that are exported from a specified URL as Siddhi events by sending HTTP requests to the URL. Based on the source configuration, it analyzes metrics from the text response and sends them as Siddhi events through key-value mapping.The user can retrieve metrics of the 'including', 'counter', 'gauge', 'histogram', and 'summary' types. The source retrieves the metrics from a text response of the target. Therefore, it is you need to use 'string' as the attribute type for the attributes that correspond with the Prometheus metric labels. Further, the Prometheus metric value is passed through the event as 'value'. This requires you to include an attribute named 'value' in the stream definition. The supported types for the 'value' attribute are 'INT', 'LONG', 'FLOAT', and 'DOUBLE'. Origin: siddhi-io-prometheus:2.1.0 Syntax @source(type=\"prometheus\", target.url=\" STRING \", scrape.interval=\" INT \", scrape.timeout=\" INT \", scheme=\" STRING \", metric.name=\" STRING \", metric.type=\" STRING \", username=\" STRING \", password=\" STRING \", client.truststore.file=\" STRING \", client.truststore.password=\" STRING \", headers=\" STRING \", job=\" STRING \", instance=\" STRING \", grouping.key=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic target.url This property specifies the target URL to which the Prometheus metrics are exported in the 'TEXT' format. STRING No No scrape.interval This property specifies the time interval in seconds within which the source should send an HTTP request to the specified target URL. 60 INT Yes No scrape.timeout This property is the time duration in seconds for a scrape request to get timed-out if the server at the URL does not respond. 10 INT Yes No scheme This property specifies the scheme of the target URL. The supported schemes are 'HTTP' and 'HTTPS'. HTTP STRING Yes No metric.name This property specifies the name of the metrics that are to be fetched. The metric name must match the regex format, i.e., '[a-zA-Z_:][a-zA-Z0-9_:]* '. Stream name STRING Yes No metric.type This property specifies the type of the Prometheus metric that is required to be fetched. The supported metric types are 'counter', 'gauge',\" 'histogram', and 'summary'. STRING No No username This property specifies the username that needs to be added in the authorization header of the HTTP request if basic authentication is enabled at the target. It is required to specify both the username and the password to enable basic authentication. If you do not provide a value for one or both of these parameters, an error is logged in the console. STRING Yes No password This property specifies the password that needs to be added in the authorization header of the HTTP request if basic authentication is enabled at the target. It is required to specify both the username and the password to enable basic authentication. If you do not provide a value for one or both of these parameters, an error is logged in the console. STRING Yes No client.truststore.file The file path to the location of the truststore to which the client needs to send HTTPS requests via the 'HTTPS' protocol. STRING Yes No client.truststore.password The password for the client-truststore. This is required to send HTTPS requests. A custom password can be specified if required. STRING Yes No headers Headers that need to be included as HTTP request headers in the request. The format of the supported input is as follows, \"'header1:value1','header2:value2'\" STRING Yes No job This property defines the job name of the exported Prometheus metrics that needs to be fetched. STRING Yes No instance This property defines the instance of the exported Prometheus metrics that needs to be fetched. STRING Yes No grouping.key This parameter specifies the grouping key of the required metrics in key-value pairs. The grouping key is used if the metrics are exported by Prometheus 'pushGateway' in order to distinguish those metrics from already existing metrics. The expected format of the grouping key is as follows: \"'key1:value1','key2:value2'\" STRING Yes No System Parameters Name Description Default Value Possible Parameters scrapeInterval The default time interval in seconds for the Prometheus source to send HTTP requests to the target URL. 60 Any integer value scrapeTimeout The default time duration (in seconds) for an HTTP request to time-out if the server at the URL does not respond. 10 Any integer value scheme The scheme of the target for the Prometheus source to send HTTP requests. The supported schemes are 'HTTP' and 'HTTPS'. HTTP HTTP or HTTPS username The username that needs to be added in the authorization header of the HTTP request if basic authentication is enabled at the target. It is required to specify both the username and password to enable basic authentication. If you do not specify a value for one or both of these parameters, an error is logged in the console. Any string password The password that needs to be added in the authorization header of the HTTP request if basic authentication is enabled at the target. It is required to specify both the username and password to enable basic authentication. If you do not specify a value for one or both of these parameters, an error is logged in the console. Any string trustStoreFile The default file path to the location of truststore that the client needs to access in order to send HTTPS requests through 'HTTPS' protocol. ${carbon.home}/resources/security/client-truststore.jks Any valid path for the truststore file trustStorePassword The default password for the client-truststore that the client needs to access in order to send HTTPS requests through 'HTTPS' protocol. wso2carbon Any string headers The headers that need to be included as HTTP request headers in the scrape request. The format of the supported input is as follows, \"'header1:value1','header2:value2'\" Any valid http headers job The default job name of the exported Prometheus metrics that needs to be fetched. Any valid job name instance The default instance of the exported Prometheus metrics that needs to be fetched. Any valid instance name groupingKey The default grouping key of the required Prometheus metrics in key-value pairs. The grouping key is used if the metrics are exported by the Prometheus pushGateway in order to distinguish these metrics from already existing metrics. The expected format of the grouping key is as follows: \"'key1:value1','key2:value2'\" Any valid grouping key pairs Examples EXAMPLE 1 @source(type= 'prometheus', target.url= 'http://localhost:9080/metrics', metric.type= 'counter', metric.name= 'sweet_production_counter', @map(type= 'keyvalue')) define stream FooStream1(metric_name string, metric_type string, help string, subtype string, name string, quantity string, value double); In this example, the Prometheus source sends an HTTP request to the 'target.url' and analyzes the response. From the analyzed response, the source retrieves the Prometheus counter metrics with the 'sweet_production_counter' nameand converts the filtered metrics into Siddhi events using the key-value mapper. The generated maps have keys and values as follows: metric_name - sweet_production_counter metric_type - counter help - help_string_of_metric subtype - null name - value_of_label_name quantity - value_of_label_quantity value - value_of_metric EXAMPLE 2 @source(type= 'prometheus', target.url= 'http://localhost:9080/metrics', metric.type= 'summary', metric.name= 'sweet_production_summary', @map(type= 'keyvalue')) define stream FooStream2(metric_name string, metric_type string, help string, subtype string, name string, quantity string, quantile string, value double); In this example, the Prometheus source sends an HTTP request to the 'target.url' and analyzes the response. From the analysed response, the source retrieves the Prometheus summary metrics with the 'sweet_production_summary' nameand converts the filtered metrics into Siddhi events using the key-value mapper. The generated maps have keys and values as follows: metric_name - sweet_production_summary metric_type - summary help - help_string_of_metric subtype - 'sum'/'count'/'null' name - value_of_label_name quantity - value_of_label_quantity quantile - value of the quantile value - value_of_metric EXAMPLE 3 @source(type= 'prometheus', target.url= 'http://localhost:9080/metrics', metric.type= 'histogram', metric.name= 'sweet_production_histogram', @map(type= 'keyvalue')) define stream FooStream3(metric_name string, metric_type string, help string, subtype string, name string, quantity string, le string, value double); In this example, the prometheus source sends an HTTP request to the 'target.url' and analyzes the response. From the analyzed response, the source retrieves the Prometheus histogram metrics with the 'sweet_production_histogram' name and converts the filtered metrics into Siddhi events using the key-value mapper. The generated maps have keys and values as follows, metric_name - sweet_production_histogram metric_type - histogram help - help_string_of_metric subtype - 'sum'/'count'/'bucket' name - value_of_label_name quantity - value_of_label_quantity le - value of the bucket value - value_of_metric","title":"prometheus (Source)"},{"location":"docs/api/5.1.0/#rabbitmq-source","text":"The rabbitmq source receives the events from the rabbitmq broker via the AMQP protocol. Origin: siddhi-io-rabbitmq:3.0.2 Syntax @source(type=\"rabbitmq\", uri=\" STRING \", heartbeat=\" INT \", exchange.name=\" STRING \", exchange.type=\" STRING \", exchange.durable.enabled=\" BOOL \", exchange.autodelete.enabled=\" BOOL \", routing.key=\" STRING \", headers=\" STRING \", queue.name=\" STRING \", queue.durable.enabled=\" BOOL \", queue.exclusive.enabled=\" BOOL \", queue.autodelete.enabled=\" BOOL \", tls.enabled=\" BOOL \", tls.truststore.path=\" STRING \", tls.truststore.password=\" STRING \", tls.truststore.type=\" STRING \", tls.version=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic uri The URI that is used to connect to an AMQP server. If no URI is specified,an error is logged in the CLI.e.g., amqp://guest:guest , amqp://guest:guest@localhost:5672 STRING No No heartbeat The period of time (in seconds) after which the peer TCP connection should be considered unreachable (down) by RabbitMQ and client libraries. 60 INT Yes No exchange.name The name of the exchange that decides what to do with a message it receives.If the exchange.name already exists in the RabbitMQ server, then the system uses that exchange.name instead of redeclaring. STRING No No exchange.type The type of the exchange name. The exchange types available are direct , fanout , topic and headers . For a detailed description of each type, see RabbitMQ - AMQP Concepts . direct STRING Yes No exchange.durable.enabled If this is set to true , the exchange remains declared even if the broker restarts. false BOOL Yes No exchange.autodelete.enabled If this is set to true , the exchange is automatically deleted when it is not used anymore. false BOOL Yes No routing.key The key based on which the exchange determines how to route the message to queues. The routing key is like an address for the message. The routing.key must be initialized when the value for the exchange.type parameter is direct or topic . empty STRING Yes No headers The headers of the message. The attributes used for routing are taken from the this paremeter. A message is considered matching if the value of the header equals the value specified upon binding. null STRING Yes No queue.name A queue is a buffer that stores messages. If the queue name already exists in the RabbitMQ server, then the system usees that queue name instead of redeclaring it. If no value is specified for this parameter, the system uses the unique queue name that is automatically generated by the RabbitMQ server. system generated queue name STRING Yes No queue.durable.enabled If this parameter is set to true , the queue remains declared even if the broker restarts false BOOL Yes No queue.exclusive.enabled If this parameter is set to true , the queue is exclusive for the current connection. If it is set to false , it is also consumable by other connections. false BOOL Yes No queue.autodelete.enabled If this parameter is set to true , the queue is automatically deleted when it is not used anymore. false BOOL Yes No tls.enabled This parameter specifies whether an encrypted communication channel should be established or not. When this parameter is set to true , the tls.truststore.path and tls.truststore.password parameters are initialized. false BOOL Yes No tls.truststore.path The file path to the location of the truststore of the client that receives the RabbitMQ events via the AMQP protocol. A custom client-truststore can be specified if required. If a custom truststore is not specified, then the system uses the default client-trustore in the {carbon.home}/resources/security /code directory. /p /td td style=\"vertical-align: top\" {carbon.home}/resources/security</code> directory.</p></td> <td style=\"vertical-align: top\"> /resources/security/client-truststore.jks STRING Yes No tls.truststore.password The password for the client-truststore. A custom password can be specified if required. If no custom password is specified, then the system uses wso2carbon as the default password. wso2carbon STRING Yes No tls.truststore.type The type of the truststore. JKS STRING Yes No tls.version The version of the tls/ssl. SSL STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @source(type ='rabbitmq', uri = 'amqp://guest:guest@localhost:5672', exchange.name = 'direct', routing.key= 'direct', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query receives events from the direct exchange with the direct exchange type, and the directTest routing key.","title":"rabbitmq (Source)"},{"location":"docs/api/5.1.0/#tcp-source","text":"A Siddhi application can be configured to receive events via the TCP transport by adding the @Source(type = 'tcp') annotation at the top of an event stream definition. When this is defined the associated stream will receive events from the TCP transport on the host and port defined in the system. Origin: siddhi-io-tcp:3.0.4 Syntax @source(type=\"tcp\", context=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic context The URL 'context' that should be used to receive the events. / STRING Yes No System Parameters Name Description Default Value Possible Parameters host Tcp server host. 0.0.0.0 Any valid host or IP port Tcp server port. 9892 Any integer representing valid port receiver.threads Number of threads to receive connections. 10 Any positive integer worker.threads Number of threads to serve events. 10 Any positive integer tcp.no.delay This is to specify whether to disable Nagle algorithm during message passing. If tcp.no.delay = 'true', the execution of Nagle algorithm will be disabled in the underlying TCP logic. Hence there will be no delay between two successive writes to the TCP connection. Else there can be a constant ack delay. true true false keep.alive This property defines whether the server should be kept alive when there are no connections available. true true false Examples EXAMPLE 1 @Source(type = 'tcp', context='abc', @map(type='binary')) define stream Foo (attribute1 string, attribute2 int ); Under this configuration, events are received via the TCP transport on default host,port, abc context, and they are passed to Foo stream for processing.","title":"tcp (Source)"},{"location":"docs/api/5.1.0/#sourcemapper","text":"","title":"Sourcemapper"},{"location":"docs/api/5.1.0/#avro-source-mapper","text":"This extension is an Avro to Event input mapper. Transports that accept Avro messages can utilize this extension to convert the incoming Avro messages to Siddhi events. The Avro schema to be used for creating Avro messages can be specified as a parameter in the stream definition. If no Avro schema is specified, a flat avro schema of the 'record' type is generated with the stream attributes as schema fields. The generated/specified Avro schema is used to convert Avro messages to Siddhi events. Origin: siddhi-map-avro:2.0.5 Syntax @source(..., @map(type=\"avro\", schema.def=\" STRING \", schema.registry=\" STRING \", schema.id=\" STRING \", fail.on.missing.attribute=\" BOOL \") QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic schema.def This specifies the schema of the Avro message. The full schema used to create the Avro message needs to be specified as a quoted JSON string. STRING No No schema.registry This specifies the URL of the schema registry. STRING No No schema.id This specifies the ID of the Avro schema. This ID is the global ID that is returned from the schema registry when posting the schema to the registry. The schema is retrieved from the schema registry via the specified ID. STRING No No fail.on.missing.attribute If this parameter is set to 'true', a JSON execution failing or returning a null value results in that message being dropped by the system. If this parameter is set to 'false', a JSON execution failing or returning a null value results in the system being prompted to send the event with a null value to Siddhi so that the user can handle it as required (i.e., by assigning a default value. true BOOL Yes No Examples EXAMPLE 1 @source(type='inMemory', topic='user', @map(type='avro', schema .def = \"\"\"{\"type\":\"record\",\"name\":\"userInfo\",\"namespace\":\"user.example\",\"fields\":[{\"name\":\"name\",\"type\":\"string\"}, {\"name\":\"age\",\"type\":\"int\"}]}\"\"\")) define stream UserStream (name string, age int ); The above Siddhi query performs a default Avro input mapping. The input Avro message that contains user information is converted to a Siddhi event. The expected input is a byte array or ByteBuffer. EXAMPLE 2 @source(type='inMemory', topic='user', @map(type='avro', schema .def = \"\"\"{\"type\":\"record\",\"name\":\"userInfo\",\"namespace\":\"avro.userInfo\",\"fields\":[{\"name\":\"username\",\"type\":\"string\"}, {\"name\":\"age\",\"type\":\"int\"}]}\"\"\",@attributes(name=\"username\",age=\"age\"))) define stream userStream (name string, age int ); The above Siddhi query performs a custom Avro input mapping. The input Avro message that contains user information is converted to a Siddhi event. The expected input is a byte array or ByteBuffer. EXAMPLE 3 @source(type='inMemory', topic='user', @map(type='avro',schema.registry='http://192.168.2.5:9090', schema.id='1',@attributes(name=\"username\",age=\"age\"))) define stream UserStream (name string, age int ); The above Siddhi query performs a custom Avro input mapping. The input Avro message that contains user information is converted to a Siddhi event via the schema retrieved from the given schema registry(localhost:8081). The expected input is a byte array or ByteBuffer.","title":"avro (Source Mapper)"},{"location":"docs/api/5.1.0/#binary-source-mapper","text":"This extension is a binary input mapper that converts events received in binary format to Siddhi events before they are processed. Origin: siddhi-map-binary:2.0.4 Syntax @source(..., @map(type=\"binary\") Examples EXAMPLE 1 @source(type='inMemory', topic='WSO2', @map(type='binary'))define stream FooStream (symbol string, price float, volume long); This query performs a mapping to convert an event of the binary format to a Siddhi event.","title":"binary (Source Mapper)"},{"location":"docs/api/5.1.0/#csv-source-mapper","text":"This extension is used to convert CSV message to Siddhi event input mapper. You can either receive pre-defined CSV message where event conversion takes place without extra configurations,or receive custom CSV message where a custom place order to map from custom CSV message. Origin: siddhi-map-csv:2.0.3 Syntax @source(..., @map(type=\"csv\", delimiter=\" STRING \", header.present=\" BOOL \", fail.on.unknown.attribute=\" BOOL \", event.grouping.enabled=\" BOOL \") QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic delimiter When converting a CSV format message to Siddhi event, this parameter indicatesinput CSV message's data should be split by this parameter , STRING Yes No header.present When converting a CSV format message to Siddhi event, this parameter indicates whether CSV message has header or not. This can either have value true or false.If it's set to false then it indicates that CSV message has't header. false BOOL Yes No fail.on.unknown.attribute This parameter specifies how unknown attributes should be handled. If it's set to true and one or more attributes don't havevalues, then SP will drop that message. If this parameter is set to false , the Stream Processor adds the required attribute's values to such events with a null value and the event is converted to a Siddhi event. true BOOL Yes No event.grouping.enabled This parameter specifies whether event grouping is enabled or not. To receive a group of events together and generate multiple events, this parameter must be set to true . false BOOL Yes No Examples EXAMPLE 1 @source(type='inMemory', topic='stock', @map(type='csv')) define stream FooStream (symbol string, price float, volume int); Above configuration will do a default CSV input mapping. Expected input will look like below: WSO2 ,55.6 , 100OR \"WSO2,No10,Palam Groove Rd,Col-03\" ,55.6 , 100If header.present is true and delimiter is \"-\", then the input is as follows: symbol-price-volumeWSO2-55.6-100 EXAMPLE 2 @source(type='inMemory', topic='stock', @map(type='csv',header='true', @attributes(symbol = \"2\", price = \"0\", volume = \"1\"))) define stream FooStream (symbol string, price float, volume long); Above configuration will perform a custom CSV mapping. Here, user can add place order of each attribute in the @attribute. The place order indicates where the attribute name's value has appeared in the input.Expected input will look like below: 55.6,100,WSO2 OR55.6,100,\"WSO2,No10,Palm Groove Rd,Col-03\" If header is true and delimiter is \"-\", then the output is as follows: price-volume-symbol 55.6-100-WSO2 If group events is enabled then input should be as follows: price-volume-symbol 55.6-100-WSO2System.lineSeparator() 55.6-100-IBMSystem.lineSeparator() 55.6-100-IFSSystem.lineSeparator()","title":"csv (Source Mapper)"},{"location":"docs/api/5.1.0/#json-source-mapper","text":"This extension is a JSON-to-Event input mapper. Transports that accept JSON messages can utilize this extension to convert an incoming JSON message into a Siddhi event. Users can either send a pre-defined JSON format, where event conversion happens without any configurations, or use the JSON path to map from a custom JSON message. In default mapping, the JSON string of the event can be enclosed by the element \"event\", though optional. Origin: siddhi-map-json:5.0.4 Syntax @source(..., @map(type=\"json\", enclosing.element=\" STRING \", fail.on.missing.attribute=\" BOOL \") QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic enclosing.element This is used to specify the enclosing element when sending multiple events in the same JSON message. Mapper treats the child elements of a given enclosing element as events and executes the JSON path expressions on these child elements. If the enclosing.element is not provided then the multiple-event scenario is disregarded and the JSON path is evaluated based on the root element. $ STRING Yes No fail.on.missing.attribute This parameter allows users to handle unknown attributes.The value of this can either be true or false. By default it is true. If a JSON execution fails or returns null, mapper drops that message. However, setting this property to false prompts mapper to send an event with a null value to Siddhi, where users can handle it as required, ie., assign a default value.) true BOOL Yes No Examples EXAMPLE 1 @source(type='inMemory', topic='stock', @map(type='json')) define stream FooStream (symbol string, price float, volume long); This configuration performs a default JSON input mapping. For a single event, the input is required to be in one of the following formats: { \"event\":{ \"symbol\":\"WSO2\", \"price\":55.6, \"volume\":100 } } or { \"symbol\":\"WSO2\", \"price\":55.6, \"volume\":100 } EXAMPLE 2 @source(type='inMemory', topic='stock', @map(type='json')) define stream FooStream (symbol string, price float, volume long); This configuration performs a default JSON input mapping. For multiple events, the input is required to be in one of the following formats: [ {\"event\":{\"symbol\":\"WSO2\",\"price\":55.6,\"volume\":100}}, {\"event\":{\"symbol\":\"WSO2\",\"price\":56.6,\"volume\":99}}, {\"event\":{\"symbol\":\"WSO2\",\"price\":57.6,\"volume\":80}} ] or [ {\"symbol\":\"WSO2\",\"price\":55.6,\"volume\":100}, {\"symbol\":\"WSO2\",\"price\":56.6,\"volume\":99}, {\"symbol\":\"WSO2\",\"price\":57.6,\"volume\":80} ] EXAMPLE 3 @source(type='inMemory', topic='stock', @map(type='json', enclosing.element=\"$.portfolio\", @attributes(symbol = \"company.symbol\", price = \"price\", volume = \"volume\"))) This configuration performs a custom JSON mapping. For a single event, the expected input is similar to the one shown below: { \"portfolio\":{ \"stock\":{ \"volume\":100, \"company\":{ \"symbol\":\"WSO2\" }, \"price\":55.6 } } } EXAMPLE 4 @source(type='inMemory', topic='stock', @map(type='json', enclosing.element=\"$.portfolio\", @attributes(symbol = \"stock.company.symbol\", price = \"stock.price\", volume = \"stock.volume\"))) define stream FooStream (symbol string, price float, volume long); The configuration performs a custom JSON mapping. For multiple events, expected input looks as follows. .{\"portfolio\": [ {\"stock\":{\"volume\":100,\"company\":{\"symbol\":\"wso2\"},\"price\":56.6}}, {\"stock\":{\"volume\":200,\"company\":{\"symbol\":\"wso2\"},\"price\":57.6}} ] }","title":"json (Source Mapper)"},{"location":"docs/api/5.1.0/#keyvalue-source-mapper","text":"Key-Value Map to Event input mapper extension allows transports that accept events as key value maps to convert those events to Siddhi events. You can either receive pre-defined keys where conversion takes place without extra configurations, or use custom keys to map from the message. Origin: siddhi-map-keyvalue:2.0.4 Syntax @source(..., @map(type=\"keyvalue\", fail.on.missing.attribute=\" BOOL \") QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic fail.on.missing.attribute If this parameter is set to true , if an event arrives without a matching key for a specific attribute in the connected stream, it is dropped and not processed by the Stream Processor. If this parameter is set to false the Stream Processor adds the required key to such events with a null value, and the event is converted to a Siddhi event so that you could handle them as required before they are further processed. true BOOL Yes No Examples EXAMPLE 1 @source(type='inMemory', topic='stock', @map(type='keyvalue')) define stream FooStream (symbol string, price float, volume long); This query performs a default key value input mapping. The expected input is a map similar to the following: symbol: 'WSO2' price: 55.6f volume: 100 EXAMPLE 2 @source(type='inMemory', topic='stock', @map(type='keyvalue', fail.on.missing.attribute='true', @attributes(symbol = 's', price = 'p', volume = 'v')))define stream FooStream (symbol string, price float, volume long); This query performs a custom key value input mapping. The matching keys for the symbol , price and volume attributes are be s , p , and v respectively. The expected input is a map similar to the following: s: 'WSO2' p: 55.6 v: 100","title":"keyvalue (Source Mapper)"},{"location":"docs/api/5.1.0/#passthrough-source-mapper","text":"Pass-through mapper passed events (Event[]) through without any mapping or modifications. Origin: siddhi-core:5.1.7 Syntax @source(..., @map(type=\"passThrough\") Examples EXAMPLE 1 @source(type='tcp', @map(type='passThrough')) define stream BarStream (symbol string, price float, volume long); In this example BarStream uses passThrough inputmapper which passes the received Siddhi event directly without any transformation into source.","title":"passThrough (Source Mapper)"},{"location":"docs/api/5.1.0/#protobuf-source-mapper","text":"This input mapper allows you to convert protobuf messages into Events. To work with this input mapper you have to add auto-generated protobuf classes to the project classpath. When you use this input mapper, you can either define stream attributes as the same names as the protobuf message attributes or you can use custom mapping to map stream definition attributes with the protobuf attributes..Please find the sample proto definition here Origin: siddhi-map-protobuf:1.0.1 Syntax @source(..., @map(type=\"protobuf\", class=\" STRING \") QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic class This specifies the class name of the protobuf message class, If sink type is grpc then it's not necessary to provide this field. - STRING Yes No Examples EXAMPLE 1 source(type='grpc', receiver.url = 'grpc://localhost: 2000/org.wso2.grpc.test.MyService/process', @map(type='protobuf')) define stream FooStream (stringValue string, intValue int,longValue long,booleanValue bool,floatValue float,doubleValue double); Above definition will convert the protobuf messages that are received to this source into siddhi events. EXAMPLE 2 source(type='grpc', receiver.url = 'grpc://localhost: 2000/org.wso2.grpc.test.MyService/process', @map(type='protobuf', @attributes(a = 'stringValue', b = 'intValue', c = 'longValue',d = 'booleanValue',' e = floatValue', f ='doubleValue'))) define stream FooStream (a string ,c long,b int, d bool,e float,f double); Above definition will convert the protobuf messages that are received to this source into siddhi events. since there's a mapping available for the stream, protobuf message object will be map like this, -'stringValue' of the protobuf message will be assign to the 'a' attribute of the stream - 'intValue' of the protobuf message will be assign to the 'b' attribute of the stream - 'longValue' of the protobuf message will be assign to the 'c' attribute of the stream - 'booleanValue' of the protobuf message will be assign to the 'd' attribute of the stream - 'floatValue' of the protobuf message will be assign to the 'e' attribute of the stream - 'doubleValue' of the protobuf message will be assign to the 'f' attribute of the stream EXAMPLE 3 source(type='grpc', receiver.url = 'grpc://localhost: 2000/org.wso2.grpc.test.MyService/testMap', @map(type='protobuf')) define stream FooStream (stringValue string ,intValue int,map object); Above definition will convert the protobuf messages that are received to this source into siddhi events. since there's an object type attribute available in the stream (map object), mapper will assume that object is an instance of 'java.util.Map' class. otherwise mapper will throws an exception EXAMPLE 4 @source(type='inMemory', topic='test01', @map(type='protobuf', class='org.wso2.grpc.test.Request')) define stream FooStream (stringValue string, intValue int,longValue long,booleanValue bool,floatValue float,doubleValue double); The above definition will convert the 'org.wso2.grpc.test.Request' type protobuf messages into siddhi events. If we did not provide the 'receiver.url' in the stream definition we have to provide the protobuf class name in the 'class' parameter inside @map.","title":"protobuf (Source Mapper)"},{"location":"docs/api/5.1.0/#text-source-mapper","text":"This extension is a text to Siddhi event input mapper. Transports that accept text messages can utilize this extension to convert the incoming text message to Siddhi event. Users can either use a pre-defined text format where event conversion happens without any additional configurations, or specify a regex to map a text message using custom configurations. Origin: siddhi-map-text:2.0.4 Syntax @source(..., @map(type=\"text\", regex.groupid=\" STRING \", fail.on.missing.attribute=\" BOOL \", event.grouping.enabled=\" BOOL \", delimiter=\" STRING \", new.line.character=\" STRING \") QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic regex.groupid This parameter specifies a regular expression group. The groupid can be any capital letter (e.g., regex.A,regex.B .. etc). You can specify any number of regular expression groups. In the attribute annotation, you need to map all attributes to the regular expression group with the matching group index. If you need to to enable custom mapping, it is required to specifythe matching group for each and every attribute. STRING No No fail.on.missing.attribute This parameter specifies how unknown attributes should be handled. If it is set to true a message is dropped if its execution fails, or if one or more attributes do not have values. If this parameter is set to false , null values are assigned to attributes with missing values, and messages with such attributes are not dropped. true BOOL Yes No event.grouping.enabled This parameter specifies whether event grouping is enabled or not. To receive a group of events together and generate multiple events, this parameter must be set to true . false BOOL Yes No delimiter This parameter specifies how events must be separated when multiple events are received. This must be whole line and not a single character. ~ ~ ~ ~ STRING Yes No new.line.character This attribute indicates the new line character of the event that is expected to be received. This is used mostly when communication between 2 types of operating systems is expected. For example, Linux uses \\n as the end of line character whereas windows uses \\r\\n . \\n STRING Yes No Examples EXAMPLE 1 @source(type='inMemory', topic='stock', @map(type='text')) define stream FooStream (symbol string, price float, volume long); This query performs a default text input mapping. The expected input is as follows: symbol:\"WSO2\", price:55.6, volume:100 OR symbol:'WSO2', price:55.6, volume:100 If group events is enabled then input should be as follows: symbol:\"WSO2\", price:55.6, volume:100 ~ ~ ~ ~ symbol:\"WSO2\", price:55.6, volume:100 EXAMPLE 2 @source(type='inMemory', topic='stock', @map(type='text', fail.on.missing.attribute = 'true', regex.A='(\\w+)\\s([-0-9]+)',regex.B='volume\\s([-0-9]+)', @attributes(symbol = 'A[1]',price = 'A[2]',volume = 'B'))) define stream FooStream (symbol string, price float, volume long); This query performs a custom text mapping. The expected input is as follows: wos2 550 volume 100 If group events is enabled then input should be as follows: wos2 550 volume 100 ~ ~ ~ ~ wos2 550 volume 100 ~ ~ ~ ~ wos2 550 volume 100","title":"text (Source Mapper)"},{"location":"docs/api/5.1.0/#xml-source-mapper","text":"This mapper converts XML input to Siddhi event. Transports which accepts XML messages can utilize this extension to convert the incoming XML message to Siddhi event. Users can either send a pre-defined XML format where event conversion will happen without any configs or can use xpath to map from a custom XML message. Origin: siddhi-map-xml:5.0.3 Syntax @source(..., @map(type=\"xml\", namespaces=\" STRING \", enclosing.element=\" STRING \", fail.on.missing.attribute=\" BOOL \") QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic namespaces Used to provide namespaces used in the incoming XML message beforehand to configure xpath expressions. User can provide a comma separated list. If these are not provided xpath evaluations will fail None STRING Yes No enclosing.element Used to specify the enclosing element in case of sending multiple events in same XML message. WSO2 DAS will treat the child element of given enclosing element as events and execute xpath expressions on child elements. If enclosing.element is not provided multiple event scenario is disregarded and xpaths will be evaluated with respect to root element. Root element STRING Yes No fail.on.missing.attribute This can either have value true or false. By default it will be true. This attribute allows user to handle unknown attributes. By default if an xpath execution fails or returns null DAS will drop that message. However setting this property to false will prompt DAS to send and event with null value to Siddhi where user can handle it accordingly(ie. Assign a default value) True BOOL Yes No Examples EXAMPLE 1 @source(type='inMemory', topic='stock', @map(type='xml')) define stream FooStream (symbol string, price float, volume long); Above configuration will do a default XML input mapping. Expected input will look like below. events event symbol WSO2 /symbol price 55.6 /price volume 100 /volume /event /events EXAMPLE 2 @source(type='inMemory', topic='stock', @map(type='xml', namespaces = \"dt=urn:schemas-microsoft-com:datatypes\", enclosing.element=\"//portfolio\", @attributes(symbol = \"company/symbol\", price = \"price\", volume = \"volume\"))) define stream FooStream (symbol string, price float, volume long); Above configuration will perform a custom XML mapping. In the custom mapping user can add xpath expressions representing each event attribute using @attribute annotation. Expected input will look like below. portfolio xmlns:dt=\"urn:schemas-microsoft-com:datatypes\" stock exchange=\"nasdaq\" volume 100 /volume company symbol WSO2 /symbol /company price dt:type=\"number\" 55.6 /price /stock /portfolio","title":"xml (Source Mapper)"},{"location":"docs/api/5.1.0/#store","text":"","title":"Store"},{"location":"docs/api/5.1.0/#mongodb-store","text":"Using this extension a MongoDB Event Table can be configured to persist events in a MongoDB of user's choice. Origin: siddhi-store-mongodb:2.0.3 Syntax @Store(type=\"mongodb\", mongodb.uri=\" STRING \", collection.name=\" STRING \", secure.connection=\" STRING \", trust.store=\" STRING \", trust.store.password=\" STRING \", key.store=\" STRING \", key.store.password=\" STRING \") @PrimaryKey(\"PRIMARY_KEY\") @Index(\"INDEX\") QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic mongodb.uri The MongoDB URI for the MongoDB data store. The uri must be of the format mongodb://[username:password@]host1[:port1][,hostN[:portN]][/[database][?options]] The options specified in the uri will override any connection options specified in the deployment yaml file. Note: The user should have read permissions to the admindb as well as read/write permissions to the database accessed. STRING No No collection.name The name of the collection in the store this Event Table should be persisted as. Name of the siddhi event table. STRING Yes No secure.connection Describes enabling the SSL for the mongodb connection false STRING Yes No trust.store File path to the trust store. {carbon.home}/resources/security/client-truststore.jks /td td style=\"vertical-align: top\" STRING /td td style=\"vertical-align: top\" Yes /td td style=\"vertical-align: top\" No /td /tr tr td style=\"vertical-align: top\" trust.store.password /td td style=\"vertical-align: top; word-wrap: break-word\" p style=\"word-wrap: break-word;margin: 0;\" Password to access the trust store /p /td td style=\"vertical-align: top\" wso2carbon /td td style=\"vertical-align: top\" STRING /td td style=\"vertical-align: top\" Yes /td td style=\"vertical-align: top\" No /td /tr tr td style=\"vertical-align: top\" key.store /td td style=\"vertical-align: top; word-wrap: break-word\" p style=\"word-wrap: break-word;margin: 0;\" File path to the keystore. /p /td td style=\"vertical-align: top\" {carbon.home}/resources/security/client-truststore.jks</td> <td style=\"vertical-align: top\">STRING</td> <td style=\"vertical-align: top\">Yes</td> <td style=\"vertical-align: top\">No</td> </tr> <tr> <td style=\"vertical-align: top\">trust.store.password</td> <td style=\"vertical-align: top; word-wrap: break-word\"><p style=\"word-wrap: break-word;margin: 0;\">Password to access the trust store</p></td> <td style=\"vertical-align: top\">wso2carbon</td> <td style=\"vertical-align: top\">STRING</td> <td style=\"vertical-align: top\">Yes</td> <td style=\"vertical-align: top\">No</td> </tr> <tr> <td style=\"vertical-align: top\">key.store</td> <td style=\"vertical-align: top; word-wrap: break-word\"><p style=\"word-wrap: break-word;margin: 0;\">File path to the keystore.</p></td> <td style=\"vertical-align: top\"> /resources/security/client-truststore.jks STRING Yes No key.store.password Password to access the keystore wso2carbon STRING Yes No System Parameters Name Description Default Value Possible Parameters applicationName Sets the logical name of the application using this MongoClient. The application name may be used by the client to identify the application to the server, for use in server logs, slow query logs, and profile collection. null the logical name of the application using this MongoClient. The UTF-8 encoding may not exceed 128 bytes. cursorFinalizerEnabled Sets whether cursor finalizers are enabled. true true false requiredReplicaSetName The name of the replica set null the logical name of the replica set sslEnabled Sets whether to initiate connection with TSL/SSL enabled. true: Initiate the connection with TLS/SSL. false: Initiate the connection without TLS/SSL. false true false trustStore File path to the trust store. {carbon.home}/resources/security/client-truststore.jks /td td style=\"vertical-align: top\" Any valid file path. /td /tr tr td style=\"vertical-align: top\" trustStorePassword /td td style=\"vertical-align: top;\" p style=\"word-wrap: break-word;margin: 0;\" Password to access the trust store /p /td td style=\"vertical-align: top\" wso2carbon /td td style=\"vertical-align: top\" Any valid password. /td /tr tr td style=\"vertical-align: top\" keyStore /td td style=\"vertical-align: top;\" p style=\"word-wrap: break-word;margin: 0;\" File path to the keystore. /p /td td style=\"vertical-align: top\" {carbon.home}/resources/security/client-truststore.jks</td> <td style=\"vertical-align: top\">Any valid file path.</td> </tr> <tr> <td style=\"vertical-align: top\">trustStorePassword</td> <td style=\"vertical-align: top;\"><p style=\"word-wrap: break-word;margin: 0;\">Password to access the trust store</p></td> <td style=\"vertical-align: top\">wso2carbon</td> <td style=\"vertical-align: top\">Any valid password.</td> </tr> <tr> <td style=\"vertical-align: top\">keyStore</td> <td style=\"vertical-align: top;\"><p style=\"word-wrap: break-word;margin: 0;\">File path to the keystore.</p></td> <td style=\"vertical-align: top\"> /resources/security/client-truststore.jks Any valid file path. keyStorePassword Password to access the keystore wso2carbon Any valid password. connectTimeout The time in milliseconds to attempt a connection before timing out. 10000 Any positive integer connectionsPerHost The maximum number of connections in the connection pool. 100 Any positive integer minConnectionsPerHost The minimum number of connections in the connection pool. 0 Any natural number maxConnectionIdleTime The maximum number of milliseconds that a connection can remain idle in the pool before being removed and closed. A zero value indicates no limit to the idle time. A pooled connection that has exceeded its idle time will be closed and replaced when necessary by a new connection. 0 Any positive integer maxWaitTime The maximum wait time in milliseconds that a thread may wait for a connection to become available. A value of 0 means that it will not wait. A negative value means to wait indefinitely 120000 Any integer threadsAllowedToBlockForConnectionMultiplier The maximum number of connections allowed per host for this MongoClient instance. Those connections will be kept in a pool when idle. Once the pool is exhausted, any operation requiring a connection will block waiting for an available connection. 100 Any natural number maxConnectionLifeTime The maximum life time of a pooled connection. A zero value indicates no limit to the life time. A pooled connection that has exceeded its life time will be closed and replaced when necessary by a new connection. 0 Any positive integer socketKeepAlive Sets whether to keep a connection alive through firewalls false true false socketTimeout The time in milliseconds to attempt a send or receive on a socket before the attempt times out. Default 0 means never to timeout. 0 Any natural integer writeConcern The write concern to use. acknowledged acknowledged w1 w2 w3 unacknowledged fsynced journaled replica_acknowledged normal safe majority fsync_safe journal_safe replicas_safe readConcern The level of isolation for the reads from replica sets. default local majority linearizable readPreference Specifies the replica set read preference for the connection. primary primary secondary secondarypreferred primarypreferred nearest localThreshold The size (in milliseconds) of the latency window for selecting among multiple suitable MongoDB instances. 15 Any natural number serverSelectionTimeout Specifies how long (in milliseconds) to block for server selection before throwing an exception. A value of 0 means that it will timeout immediately if no server is available. A negative value means to wait indefinitely. 30000 Any integer heartbeatSocketTimeout The socket timeout for connections used for the cluster heartbeat. A value of 0 means that it will timeout immediately if no cluster member is available. A negative value means to wait indefinitely. 20000 Any integer heartbeatConnectTimeout The connect timeout for connections used for the cluster heartbeat. A value of 0 means that it will timeout immediately if no cluster member is available. A negative value means to wait indefinitely. 20000 Any integer heartbeatFrequency Specify the interval (in milliseconds) between checks, counted from the end of the previous check until the beginning of the next one. 10000 Any positive integer minHeartbeatFrequency Sets the minimum heartbeat frequency. In the event that the driver has to frequently re-check a server's availability, it will wait at least this long since the previous check to avoid wasted effort. 500 Any positive integer Examples EXAMPLE 1 @Store(type=\"mongodb\",mongodb.uri=\"mongodb://admin:admin@localhost/Foo\") @PrimaryKey(\"symbol\") @Index(\"volume:1\", {background:true,unique:true}\") define table FooTable (symbol string, price float, volume long); This will create a collection called FooTable for the events to be saved with symbol as Primary Key(unique index at mongoDB level) and index for the field volume will be created in ascending order with the index option to create the index in the background. Note: @PrimaryKey: This specifies a list of comma-separated values to be treated as unique fields in the table. Each record in the table must have a unique combination of values for the fields specified here. @Index: This specifies the fields that must be indexed at the database level. You can specify multiple values as a come-separated list. A single value to be in the format, FieldName : SortOrder . The last element is optional through which a valid index options can be passed. SortOrder : 1 for Ascending -1 for Descending. Optional, with default value as 1. IndexOptions : Index Options must be defined inside curly brackets. Options must follow the standard mongodb index options format. https://docs.mongodb.com/manual/reference/method/db.collection.createIndex/ Example 1: @Index( 'symbol:1' , '{\"unique\":true}' ) Example 2: @Index( 'symbol' , '{\"unique\":true}' ) Example 3: @Index( 'symbol:1' , 'volume:-1' , '{\"unique\":true}' )","title":"mongodb (Store)"},{"location":"docs/api/5.1.0/#rdbms-store","text":"This extension assigns data sources and connection instructions to event tables. It also implements read-write operations on connected data sources. Origin: siddhi-store-rdbms:7.0.1 Syntax @Store(type=\"rdbms\", jdbc.url=\" STRING \", username=\" STRING \", password=\" STRING \", jdbc.driver.name=\" STRING \", pool.properties=\" STRING \", jndi.resource=\" STRING \", datasource=\" STRING \", table.name=\" STRING \", field.length=\" STRING \", table.check.query=\" STRING \") @PrimaryKey(\"PRIMARY_KEY\") @Index(\"INDEX\") QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic jdbc.url The JDBC URL via which the RDBMS data store is accessed. STRING No No username The username to be used to access the RDBMS data store. STRING No No password The password to be used to access the RDBMS data store. STRING No No jdbc.driver.name The driver class name for connecting the RDBMS data store. STRING No No pool.properties Any pool parameters for the database connection must be specified as key-value pairs. null STRING Yes No jndi.resource The name of the JNDI resource through which the connection is attempted. If this is found, the pool properties described above are not taken into account and the connection is attempted via JNDI lookup instead. null STRING Yes No datasource The name of the Carbon datasource that should be used for creating the connection with the database. If this is found, neither the pool properties nor the JNDI resource name described above are taken into account and the connection is attempted via Carbon datasources instead. Only works in Siddhi Distribution null STRING Yes No table.name The name with which the event table should be persisted in the store. If no name is specified via this parameter, the event table is persisted with the same name as the Siddhi table. The table name defined in the Siddhi App query. STRING Yes No field.length The number of characters that the values for fields of the 'STRING' type in the table definition must contain. Each required field must be provided as a comma-separated list of key-value pairs in the ' field.name : length ' format. If this is not specified, the default number of characters specific to the database type is considered. null STRING Yes No table.check.query This query will be used to check whether the table is exist in the given database. But the provided query should return an SQLException if the table does not exist in the database. Furthermore if the provided table is a database view, and it is not exists in the database a table from given name will be created in the database The tableCheckQuery which define in store rdbms configs STRING Yes No System Parameters Name Description Default Value Possible Parameters {{RDBMS-Name}}.maxVersion The latest version supported for {{RDBMS-Name}}. 0 N/A {{RDBMS-Name}}.minVersion The earliest version supported for {{RDBMS-Name}}. 0 N/A {{RDBMS-Name}}.tableCheckQuery The template query for the 'check table' operation in {{RDBMS-Name}}. H2 : CREATE TABLE {{TABLE_NAME}} ({{COLUMNS, PRIMARY_KEYS}}) MySQL : CREATE TABLE {{TABLE_NAME}} ({{COLUMNS, PRIMARY_KEYS}}) Oracle : CREATE TABLE {{TABLE_NAME}} ({{COLUMNS, PRIMARY_KEYS}}) Microsoft SQL Server : CREATE TABLE {{TABLE_NAME}} ({{COLUMNS, PRIMARY_KEYS}}) PostgreSQL : CREATE TABLE {{TABLE_NAME}} ({{COLUMNS, PRIMARY_KEYS}}) DB2. : CREATE TABLE {{TABLE_NAME}} ({{COLUMNS, PRIMARY_KEYS}}) N/A {{RDBMS-Name}}.tableCreateQuery The template query for the 'create table' operation in {{RDBMS-Name}}. H2 : SELECT 1 FROM {{TABLE_NAME}} LIMIT 1 MySQL : SELECT 1 FROM {{TABLE_NAME}} LIMIT 1 Oracle : SELECT 1 FROM {{TABLE_NAME}} WHERE rownum=1 Microsoft SQL Server : SELECT TOP 1 1 from {{TABLE_NAME}} PostgreSQL : SELECT 1 FROM {{TABLE_NAME}} LIMIT 1 DB2. : SELECT 1 FROM {{TABLE_NAME}} FETCH FIRST 1 ROWS ONLY N/A {{RDBMS-Name}}.indexCreateQuery The template query for the 'create index' operation in {{RDBMS-Name}}. H2 : CREATE INDEX {{TABLE_NAME}}_INDEX ON {{TABLE_NAME}} ({{INDEX_COLUMNS}}) MySQL : CREATE INDEX {{TABLE_NAME}}_INDEX ON {{TABLE_NAME}} ({{INDEX_COLUMNS}}) Oracle : CREATE INDEX {{TABLE_NAME}}_INDEX ON {{TABLE_NAME}} ({{INDEX_COLUMNS}}) Microsoft SQL Server : CREATE INDEX {{TABLE_NAME}}_INDEX ON {{TABLE_NAME}} ({{INDEX_COLUMNS}}) {{TABLE_NAME}} ({{INDEX_COLUMNS}}) PostgreSQL : CREATE INDEX {{TABLE_NAME}}_INDEX ON {{TABLE_NAME}} ({{INDEX_COLUMNS}}) DB2. : CREATE INDEX {{TABLE_NAME}}_INDEX ON {{TABLE_NAME}} ({{INDEX_COLUMNS}}) N/A {{RDBMS-Name}}.recordInsertQuery The template query for the 'insert record' operation in {{RDBMS-Name}}. H2 : INSERT INTO {{TABLE_NAME}} ({{COLUMNS}}) VALUES ({{Q}}) MySQL : INSERT INTO {{TABLE_NAME}} ({{COLUMNS}}) VALUES ({{Q}}) Oracle : INSERT INTO {{TABLE_NAME}} ({{COLUMNS}}) VALUES ({{Q}}) Microsoft SQL Server : INSERT INTO {{TABLE_NAME}} ({{COLUMNS}}) VALUES ({{Q}}) PostgreSQL : INSERT INTO {{TABLE_NAME}} ({{COLUMNS}}) VALUES ({{Q}}) DB2. : INSERT INTO {{TABLE_NAME}} ({{COLUMNS}}) VALUES ({{Q}}) N/A {{RDBMS-Name}}.recordUpdateQuery The template query for the 'update record' operation in {{RDBMS-Name}}. H2 : UPDATE {{TABLE_NAME}} SET {{COLUMNS_AND_VALUES}} {{CONDITION}} MySQL : UPDATE {{TABLE_NAME}} SET {{COLUMNS_AND_VALUES}} {{CONDITION}} Oracle : UPDATE {{TABLE_NAME}} SET {{COLUMNS_AND_VALUES}} {{CONDITION}} Microsoft SQL Server : UPDATE {{TABLE_NAME}} SET {{COLUMNS_AND_VALUES}} {{CONDITION}} PostgreSQL : UPDATE {{TABLE_NAME}} SET {{COLUMNS_AND_VALUES}} {{CONDITION}} DB2. : UPDATE {{TABLE_NAME}} SET {{COLUMNS_AND_VALUES}} {{CONDITION}} N/A {{RDBMS-Name}}.recordSelectQuery The template query for the 'select record' operation in {{RDBMS-Name}}. H2 : SELECT * FROM {{TABLE_NAME}} {{CONDITION}} MySQL : SELECT * FROM {{TABLE_NAME}} {{CONDITION}} Oracle : SELECT * FROM {{TABLE_NAME}} {{CONDITION}} Microsoft SQL Server : SELECT * FROM {{TABLE_NAME}} {{CONDITION}} PostgreSQL : SELECT * FROM {{TABLE_NAME}} {{CONDITION}} DB2. : SELECT * FROM {{TABLE_NAME}} {{CONDITION}} N/A {{RDBMS-Name}}.recordExistsQuery The template query for the 'check record existence' operation in {{RDBMS-Name}}. H2 : SELECT TOP 1 1 FROM {{TABLE_NAME}} {{CONDITION}} MySQL : SELECT 1 FROM {{TABLE_NAME}} {{CONDITION}} Oracle : SELECT COUNT(1) INTO existence FROM {{TABLE_NAME}} {{CONDITION}} Microsoft SQL Server : SELECT TOP 1 FROM {{TABLE_NAME}} {{CONDITION}} PostgreSQL : SELECT 1 FROM {{TABLE_NAME}} {{CONDITION}} LIMIT 1 DB2. : SELECT 1 FROM {{TABLE_NAME}} {{CONDITION}} FETCH FIRST 1 ROWS ONLY N/A {{RDBMS-Name}}.recordDeleteQuery The query for the 'delete record' operation in {{RDBMS-Name}}. H2 : DELETE FROM {{TABLE_NAME}} {{CONDITION}} MySQL : DELETE FROM {{TABLE_NAME}} {{CONDITION}} Oracle : DELETE FROM {{TABLE_NAME}} {{CONDITION}} Microsoft SQL Server : DELETE FROM {{TABLE_NAME}} {{CONDITION}} PostgreSQL : DELETE FROM {{TABLE_NAME}} {{CONDITION}} DB2. : DELETE FROM {{TABLE_NAME}} {{CONDITION}} N/A {{RDBMS-Name}}.stringSize This defines the length for the string fields in {{RDBMS-Name}}. H2 : 254 MySQL : 254 Oracle : 254 Microsoft SQL Server : 254 PostgreSQL : 254 DB2. : 254 N/A {{RDBMS-Name}}.fieldSizeLimit This defines the field size limit for select/switch to big string type from the default string type if the 'bigStringType' is available in field type list. H2 : N/A MySQL : N/A Oracle : 2000 Microsoft SQL Server : N/A PostgreSQL : N/A DB2. : N/A 0 = n = INT_MAX {{RDBMS-Name}}.batchSize This defines the batch size when operations are performed for batches of events. H2 : 1000 MySQL : 1000 Oracle : 1000 Microsoft SQL Server : 1000 PostgreSQL : 1000 DB2. : 1000 N/A {{RDBMS-Name}}.batchEnable This specifies whether 'Update' and 'Insert' operations can be performed for batches of events or not. H2 : true MySQL : true Oracle (versions 12.0 and less) : false Oracle (versions 12.1 and above) : true Microsoft SQL Server : true PostgreSQL : true DB2. : true N/A {{RDBMS-Name}}.transactionSupported This is used to specify whether the JDBC connection that is used supports JDBC transactions or not. H2 : true MySQL : true Oracle : true Microsoft SQL Server : true PostgreSQL : true DB2. : true N/A {{RDBMS-Name}}.typeMapping.binaryType This is used to specify the binary data type. An attribute defines as 'object' type in Siddhi stream will be stored into RDBMS with this type. H2 : BLOB MySQL : BLOB Oracle : BLOB Microsoft SQL Server : VARBINARY(max) PostgreSQL : BYTEA DB2. : BLOB(64000) N/A {{RDBMS-Name}}.typeMapping.booleanType This is used to specify the boolean data type. An attribute defines as 'bool' type in Siddhi stream will be stored into RDBMS with this type. H2 : TINYINT(1) MySQL : TINYINT(1) Oracle : NUMBER(1) Microsoft SQL Server : BIT PostgreSQL : BOOLEAN DB2. : SMALLINT N/A {{RDBMS-Name}}.typeMapping.doubleType This is used to specify the double data type. An attribute defines as 'double' type in Siddhi stream will be stored into RDBMS with this type. H2 : DOUBLE MySQL : DOUBLE Oracle : NUMBER(19,4) Microsoft SQL Server : FLOAT(32) PostgreSQL : DOUBLE PRECISION DB2. : DOUBLE N/A {{RDBMS-Name}}.typeMapping.floatType This is used to specify the float data type. An attribute defines as 'float' type in Siddhi stream will be stored into RDBMS with this type. H2 : FLOAT MySQL : FLOAT Oracle : NUMBER(19,4) Microsoft SQL Server : REAL PostgreSQL : REAL DB2. : REAL N/A {{RDBMS-Name}}.typeMapping.integerType This is used to specify the integer data type. An attribute defines as 'int' type in Siddhi stream will be stored into RDBMS with this type. H2 : INTEGER MySQL : INTEGER Oracle : NUMBER(10) Microsoft SQL Server : INTEGER PostgreSQL : INTEGER DB2. : INTEGER N/A {{RDBMS-Name}}.typeMapping.longType This is used to specify the long data type. An attribute defines as 'long' type in Siddhi stream will be stored into RDBMS with this type. H2 : BIGINT MySQL : BIGINT Oracle : NUMBER(19) Microsoft SQL Server : BIGINT PostgreSQL : BIGINT DB2. : BIGINT N/A {{RDBMS-Name}}.typeMapping.stringType This is used to specify the string data type. An attribute defines as 'string' type in Siddhi stream will be stored into RDBMS with this type. H2 : VARCHAR(stringSize) MySQL : VARCHAR(stringSize) Oracle : VARCHAR(stringSize) Microsoft SQL Server : VARCHAR(stringSize) PostgreSQL : VARCHAR(stringSize) DB2. : VARCHAR(stringSize) N/A {{RDBMS-Name}}.typeMapping.bigStringType This is used to specify the big string data type. An attribute defines as 'string' type in Siddhi stream and field.length define in the annotation is greater than the fieldSizeLimit, will be stored into RDBMS with this type. H2 : N/A MySQL : N/A Oracle : CLOB Microsoft SQL Server : N/A PostgreSQL : N/A DB2.* : N/A N/A Examples EXAMPLE 1 @Store(type=\"rdbms\", jdbc.url=\"jdbc:mysql://localhost:3306/stocks\", username=\"root\", password=\"root\", jdbc.driver.name=\"com.mysql.jdbc.Driver\",field.length=\"symbol:100\") @PrimaryKey(\"id\", \"symbol\") @Index(\"volume\") define table StockTable (id string, symbol string, price float, volume long); The above example creates an event table named 'StockTable' in the database if it does not already exist (with four attributes named id , symbol , price , and volume of the types 'string', 'string', 'float', and 'long' respectively). The connection is made as specified by the parameters configured for the '@Store' annotation. The @PrimaryKey() and @Index() annotations can be used to define primary keys or indexes for the table and they follow Siddhi query syntax. RDBMS store supports having more than one attributes in the @PrimaryKey or @Index annotations. In this example a composite Primary key of both attributes id and symbol will be created. EXAMPLE 2 @Store(type=\"rdbms\", jdbc.url=\"jdbc:mysql://localhost:3306/das\", username=\"root\", password=\"root\" , jdbc.driver.name=\"org.h2.Driver\",field.length=\"symbol:100\") @PrimaryKey(\"symbol\") @Index(\"symbol\") define table StockTable (symbol string, price float, volume long); define stream InputStream (symbol string, volume long); from InputStream as a join StockTable as b on str:contains(b.symbol, a.symbol) select a.symbol as symbol, b.volume as volume insert into FooStream; The above example creates an event table named 'StockTable' in the database if it does not already exist (with three attributes named 'symbol', 'price', and 'volume' of the types 'string', 'float' and 'long' respectively). Then the table is joined with a stream named 'InputStream' based on a condition. The following operations are included in the condition: [ AND, OR, Comparisons( = = == !=), IS NULL, NOT, str:contains(Table Column , Stream Attribute or Search.String)] EXAMPLE 3 @Store(type=\"rdbms\", jdbc.url=\"jdbc:mysql://localhost:3306/das\", table.name=\"StockTable\", username=\"root\", password=\"root\" , jdbc.driver.name=\"org.h2.Driver\", field.length=\"symbol:100\", table.check.query=\"SELECT 1 FROM StockTable LIMIT 1\") @PrimaryKey(\"symbol\") @Index(\"symbol\") define table StockTable (symbol string, price float, volume long); define stream InputStream (symbol string, volume long); from InputStream as a join StockTable as b on str:contains(b.symbol, a.symbol) select a.symbol as symbol, b.volume as volume insert into FooStream; The above example creates an event table named 'StockTable' in the database if it does not already exist (with three attributes named 'symbol', 'price', and 'volume' of the types 'string', 'float' and 'long' respectively). Then the table is joined with a stream named 'InputStream' based on a condition. The following operations are included in the condition: [ AND, OR, Comparisons( = = == !=), IS NULL, NOT, str:contains(Table Column , Stream Attribute or Search.String)]","title":"rdbms (Store)"},{"location":"docs/api/5.1.0/#redis-store","text":"This extension assigns data source and connection instructions to event tables. It also implements read write operations on connected datasource. This extension only can be used to read the data which persisted using the same extension since unique implementation has been used to map the relational data in to redis's key and value representation Origin: siddhi-store-redis:3.1.1 Syntax @Store(type=\"redis\", table.name=\" STRING \", cluster.mode=\" BOOL \", nodes=\" STRING \", ttl.seconds=\" LONG \", ttl.on.update=\" BOOL \", ttl.on.read=\" BOOL \") @PrimaryKey(\"PRIMARY_KEY\") @Index(\"INDEX\") QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic table.name The name with which the event table should be persisted in the store. If noname is specified via this parameter, the event table is persisted with the same name as the Siddhi table. The tale name defined in the siddhi app STRING Yes No cluster.mode This will decide the redis mode. if this is false, client will connect to a single redis node. false BOOL No No nodes host, port and the password of the node(s).In single node mode node details can be provided as follows- \"node='hosts:port@password'\" In clustered mode host and port of all the master nodes should be provided separated by a comma(,). As an example \"nodes = 'localhost:30001,localhost:30002'\". localhost:6379@root STRING Yes No ttl.seconds Time to live in seconds for each record -1 LONG Yes No ttl.on.update Set ttl on row update false BOOL Yes No ttl.on.read Set ttl on read rows false BOOL Yes No Examples EXAMPLE 1 @store(type='redis',nodes='localhost:6379@root',table.name='fooTable',cluster.mode=false)define table fooTable(time long, date String) Above example will create a redis table with the name fooTable and work on asingle redis node. EXAMPLE 2 @Store(type='redis', table.name='SweetProductionTable', nodes='localhost:30001,localhost:30002,localhost:30003', cluster.mode='true') @primaryKey('symbol') @index('price') define table SweetProductionTable (symbol string, price float, volume long); Above example demonstrate how to use the redis extension to connect in to redis cluster. Please note that, as nodes all the master node's host and port should be provided in order to work correctly. In clustered node password will not besupported EXAMPLE 3 @store(type='redis',nodes='localhost:6379@root',table.name='fooTable', ttl.seconds='30', ttl.onUpdate='true', ttl.onRead='true')define table fooTable(time long, date String) Above example will create a redis table with the name fooTable and work on asingle redis node. All rows inserted, updated or read will have its ttl set to 30 seconds","title":"redis (Store)"},{"location":"docs/api/5.1.0/#str","text":"","title":"Str"},{"location":"docs/api/5.1.0/#groupconcat-aggregate-function","text":"This function aggregates the received events by concatenating the keys in those events using a separator, e.g.,a comma (,) or a hyphen (-), and returns the concatenated key string. Origin: siddhi-execution-string:5.0.7 Syntax STRING str:groupConcat( STRING key) STRING str:groupConcat( STRING key, STRING ...) STRING str:groupConcat( STRING key, STRING separator, BOOL distinct) STRING str:groupConcat( STRING key, STRING separator, BOOL distinct, STRING order) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic key The string that needs to be aggregated. STRING No Yes separator The separator that separates each string key after concatenating the keys. , STRING Yes Yes distinct This is used to only have distinct values in the concatenated string that is returned. false BOOL Yes Yes order This parameter accepts 'ASC' or 'DESC' strings to sort the string keys in either ascending or descending order respectively. No order STRING Yes Yes Examples EXAMPLE 1 from InputStream#window.time(5 min) select str:groupConcat(\"key\") as groupedKeys input OutputStream; When we input events having values for the key as 'A' , 'B' , 'S' , 'C' , 'A' , it returns \"A,B,S,C,A\" to the 'OutputStream'. EXAMPLE 2 from InputStream#window.time(5 min) select groupConcat(\"key\",\"-\",true,\"ASC\") as groupedKeys input OutputStream; When we input events having values for the key as 'A' , 'B' , 'S' , 'C' , 'A' , specify the seperator as hyphen and choose the order to be ascending, the function returns \"A-B-C-S\" to the 'OutputStream'.","title":"groupConcat (Aggregate Function)"},{"location":"docs/api/5.1.0/#charat-function","text":"This function returns the 'char' value that is present at the given index position. of the input string. Origin: siddhi-execution-string:5.0.7 Syntax STRING str:charAt( STRING input.value, INT index) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.value The input string of which the char value at the given position needs to be returned. STRING No Yes index The variable that specifies the index of the char value that needs to be returned. INT No Yes Examples EXAMPLE 1 charAt(\"WSO2\", 1) In this case, the functiion returns the character that exists at index 1. Hence, it returns 'S'.","title":"charAt (Function)"},{"location":"docs/api/5.1.0/#coalesce-function_1","text":"This returns the first input parameter value of the given argument, that is not null. Origin: siddhi-execution-string:5.0.7 Syntax INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT str:coalesce( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg, INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT ...) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg It can have one or more input parameters in any data type. However, all the specified parameters are required to be of the same type. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No Yes Examples EXAMPLE 1 coalesce(null, \"BBB\", \"CCC\") This returns the first input parameter that is not null. In this example, it returns \"BBB\".","title":"coalesce (Function)"},{"location":"docs/api/5.1.0/#concat-function","text":"This function returns a string value that is obtained as a result of concatenating two or more input string values. Origin: siddhi-execution-string:5.0.7 Syntax STRING str:concat( STRING arg, STRING ...) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg This can have two or more string type input parameters. STRING No Yes Examples EXAMPLE 1 concat(\"D533\", \"8JU^\", \"XYZ\") This returns a string value by concatenating two or more given arguments. In the example shown above, it returns \"D5338JU^XYZ\".","title":"concat (Function)"},{"location":"docs/api/5.1.0/#contains-function_1","text":"This function returns true if the input.string contains the specified sequence of char values in the search.string . Origin: siddhi-execution-string:5.0.7 Syntax BOOL str:contains( STRING input.string, STRING search.string) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string Input string value. STRING No Yes search.string The string value to be searched for in the input.string . STRING No Yes Examples EXAMPLE 1 contains(\"21 products are produced by WSO2 currently\", \"WSO2\") This returns a boolean value as the output. In this case, it returns true .","title":"contains (Function)"},{"location":"docs/api/5.1.0/#equalsignorecase-function","text":"This returns a boolean value by comparing two strings lexicographically without considering the letter case. Origin: siddhi-execution-string:5.0.7 Syntax BOOL str:equalsIgnoreCase( STRING arg1, STRING arg2) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg1 The first input string argument. STRING No Yes arg2 The second input string argument. This is compared with the first argument. STRING No Yes Examples EXAMPLE 1 equalsIgnoreCase(\"WSO2\", \"wso2\") This returns a boolean value as the output. In this scenario, it returns \"true\".","title":"equalsIgnoreCase (Function)"},{"location":"docs/api/5.1.0/#filltemplate-function","text":"fillTemplate(string, map) will replace all the keys in the string using values in the map. fillTemplate(string, r1, r2 ..) replace all the entries {{1}}, {{2}}, {{3}} with r1 , r2, r3. Origin: siddhi-execution-string:5.0.7 Syntax STRING str:fillTemplate( STRING template, STRING|INT|LONG|DOUBLE|FLOAT|BOOL replacement.type, STRING|INT|LONG|DOUBLE|FLOAT|BOOL ...) STRING str:fillTemplate( STRING template, OBJECT map) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic template The string with templated fields that needs to be filled with the given strings. The format of the templated fields should be as follows: {{KEY}} where 'KEY' is a STRING if you are using fillTemplate(string, map) {{KEY}} where 'KEY' is an INT if you are using fillTemplate(string, r1, r2 ..) This KEY is used to map the values STRING No Yes replacement.type A set of arguments with any type string|int|long|double|float|bool. - STRING INT LONG DOUBLE FLOAT BOOL Yes Yes map A map with key-value pairs to be replaced. - OBJECT Yes Yes Examples EXAMPLE 1 str:fillTemplate(\"{{prize}} 100 {{salary}} 10000\", map:create('prize', 300, 'salary', 10000)) In this example, the template is '{{prize}} 100 {{salary}} 10000'.Here, the templated string {{prize}} is replaced with the value corresponding to the 'prize' key in the given map. Likewise salary replace with the salary value of the map EXAMPLE 2 str:fillTemplate(\"{{1}} 100 {{2}} 10000\", 200, 300) In this example, the template is '{{1}} 100 {{2}} 10000'.Here, the templated string {{1}} is replaced with the corresponding 1 st value 200. Likewise {{2}} replace with the 300","title":"fillTemplate (Function)"},{"location":"docs/api/5.1.0/#hex-function_1","text":"This function returns a hexadecimal string by converting each byte of each character in the input string to two hexadecimal digits. Origin: siddhi-execution-string:5.0.7 Syntax STRING str:hex( STRING input.string) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string The input string to derive the hexadecimal value. STRING No Yes Examples EXAMPLE 1 hex(\"MySQL\") This returns the hexadecimal value of the input.string. In this scenario, the output is \"4d7953514c\".","title":"hex (Function)"},{"location":"docs/api/5.1.0/#length-function","text":"Returns the length of the input string. Origin: siddhi-execution-string:5.0.7 Syntax INT str:length( STRING input.string) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string The input string to derive the length. STRING No Yes Examples EXAMPLE 1 length(\"Hello World\") This outputs the length of the provided string. In this scenario, the, output is 11 .","title":"length (Function)"},{"location":"docs/api/5.1.0/#lower-function","text":"Converts the capital letters in the input string to the equivalent simple letters. Origin: siddhi-execution-string:5.0.7 Syntax STRING str:lower( STRING input.string) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string The input string to convert to the lower case (i.e., equivalent simple letters). STRING No Yes Examples EXAMPLE 1 lower(\"WSO2 cep \") This converts the capital letters in the input.string to the equivalent simple letters. In this scenario, the output is \"wso2 cep \".","title":"lower (Function)"},{"location":"docs/api/5.1.0/#regexp-function","text":"Returns a boolean value based on the matchability of the input string and the given regular expression. Origin: siddhi-execution-string:5.0.7 Syntax BOOL str:regexp( STRING input.string, STRING regex) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string The input string to match with the given regular expression. STRING No Yes regex The regular expression to be matched with the input string. STRING No Yes Examples EXAMPLE 1 regexp(\"WSO2 abcdh\", \"WSO(.*h)\") This returns a boolean value after matching regular expression with the given string. In this scenario, it returns \"true\" as the output.","title":"regexp (Function)"},{"location":"docs/api/5.1.0/#repeat-function","text":"Repeats the input string for a specified number of times. Origin: siddhi-execution-string:5.0.7 Syntax STRING str:repeat( STRING input.string, INT times) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string The input string that is repeated the number of times as defined by the user. STRING No Yes times The number of times the input.string needs to be repeated . INT No Yes Examples EXAMPLE 1 repeat(\"StRing 1\", 3) This returns a string value by repeating the string for a specified number of times. In this scenario, the output is \"StRing 1StRing 1StRing 1\".","title":"repeat (Function)"},{"location":"docs/api/5.1.0/#replaceall-function","text":"Finds all the substrings of the input string that matches with the given expression, and replaces them with the given replacement string. Origin: siddhi-execution-string:5.0.7 Syntax STRING str:replaceAll( STRING input.string, STRING regex, STRING replacement.string) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string The input string to be replaced. STRING No Yes regex The regular expression to be matched with the input string. STRING No Yes replacement.string The string with which each substring that matches the given expression should be replaced. STRING No Yes Examples EXAMPLE 1 replaceAll(\"hello hi hello\", 'hello', 'test') This returns a string after replacing the substrings of the input string with the replacement string. In this scenario, the output is \"test hi test\" .","title":"replaceAll (Function)"},{"location":"docs/api/5.1.0/#replacefirst-function","text":"Finds the first substring of the input string that matches with the given regular expression, and replaces itwith the given replacement string. Origin: siddhi-execution-string:5.0.7 Syntax STRING str:replaceFirst( STRING input.string, STRING regex, STRING replacement.string) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string The input string that should be replaced. STRING No Yes regex The regular expression with which the input string should be matched. STRING No Yes replacement.string The string with which the first substring of input string that matches the regular expression should be replaced. STRING No Yes Examples EXAMPLE 1 replaceFirst(\"hello WSO2 A hello\", 'WSO2(.*)A', 'XXXX') This returns a string after replacing the first substring with the given replacement string. In this scenario, the output is \"hello XXXX hello\".","title":"replaceFirst (Function)"},{"location":"docs/api/5.1.0/#reverse-function","text":"Returns the input string in the reverse order character-wise and string-wise. Origin: siddhi-execution-string:5.0.7 Syntax STRING str:reverse( STRING input.string) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string The input string to be reversed. STRING No Yes Examples EXAMPLE 1 reverse(\"Hello World\") This outputs a string value by reversing the incoming input.string . In this scenario, the output is \"dlroW olleH\".","title":"reverse (Function)"},{"location":"docs/api/5.1.0/#split-function","text":"Splits the input.string into substrings using the value parsed in the split.string and returns the substring at the position specified in the group.number . Origin: siddhi-execution-string:5.0.7 Syntax STRING str:split( STRING input.string, STRING split.string, INT group.number) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string The input string to be replaced. STRING No Yes split.string The string value to be used to split the input.string . STRING No Yes group.number The index of the split group INT No Yes Examples EXAMPLE 1 split(\"WSO2,ABM,NSFT\", \",\", 0) This splits the given input.string by given split.string and returns the string in the index given by group.number. In this scenario, the output will is \"WSO2\".","title":"split (Function)"},{"location":"docs/api/5.1.0/#strcmp-function","text":"Compares two strings lexicographically and returns an integer value. If both strings are equal, 0 is returned. If the first string is lexicographically greater than the second string, a positive value is returned. If the first string is lexicographically greater than the second string, a negative value is returned. Origin: siddhi-execution-string:5.0.7 Syntax INT str:strcmp( STRING arg1, STRING arg2) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg1 The first input string argument. STRING No Yes arg2 The second input string argument that should be compared with the first argument lexicographically. STRING No Yes Examples EXAMPLE 1 strcmp(\"AbCDefghiJ KLMN\", 'Hello') This compares two strings lexicographically and outputs an integer value.","title":"strcmp (Function)"},{"location":"docs/api/5.1.0/#substr-function","text":"Returns a substring of the input string by considering a subset or all of the following factors: starting index, length, regular expression, and regex group number. Origin: siddhi-execution-string:5.0.7 Syntax STRING str:substr( STRING input.string, INT begin.index) STRING str:substr( STRING input.string, INT begin.index, INT length) STRING str:substr( STRING input.string, STRING regex) STRING str:substr( STRING input.string, STRING regex, INT group.number) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string The input string to be processed. STRING No Yes begin.index Starting index to consider for the substring. - INT Yes Yes length The length of the substring. input.string .length - begin.index INT Yes Yes regex The regular expression that should be matched with the input string. - STRING Yes Yes group.number The regex group number 0 INT Yes Yes Examples EXAMPLE 1 substr(\"AbCDefghiJ KLMN\", 4) This outputs the substring based on the given begin.index . In this scenario, the output is \"efghiJ KLMN\". EXAMPLE 2 substr(\"AbCDefghiJ KLMN\", 2, 4) This outputs the substring based on the given begin.index and length. In this scenario, the output is \"CDef\". EXAMPLE 3 substr(\"WSO2D efghiJ KLMN\", '^WSO2(.*)') This outputs the substring by applying the regex. In this scenario, the output is \"WSO2D efghiJ KLMN\". EXAMPLE 4 substr(\"WSO2 cep WSO2 XX E hi hA WSO2 heAllo\", 'WSO2(.*)A(.*)', 2) This outputs the substring by applying the regex and considering the group.number . In this scenario, the output is \" ello\".","title":"substr (Function)"},{"location":"docs/api/5.1.0/#trim-function","text":"Returns a copy of the input string without the leading and trailing whitespace (if any). Origin: siddhi-execution-string:5.0.7 Syntax STRING str:trim( STRING input.string) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string The input string that needs to be trimmed. STRING No Yes Examples EXAMPLE 1 trim(\" AbCDefghiJ KLMN \") This returns a copy of the input.string with the leading and/or trailing white-spaces omitted. In this scenario, the output is \"AbCDefghiJ KLMN\".","title":"trim (Function)"},{"location":"docs/api/5.1.0/#unhex-function","text":"Returns a string by converting the hexadecimal characters in the input string. Origin: siddhi-execution-string:5.0.7 Syntax STRING str:unhex( STRING input.string) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string The hexadecimal input string that needs to be converted to string. STRING No Yes Examples EXAMPLE 1 unhex(\"4d7953514c\") This converts the hexadecimal value to string.","title":"unhex (Function)"},{"location":"docs/api/5.1.0/#upper-function","text":"Converts the simple letters in the input string to the equivalent capital/block letters. Origin: siddhi-execution-string:5.0.7 Syntax STRING str:upper( STRING input.string) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string The input string that should be converted to the upper case (equivalent capital/block letters). STRING No Yes Examples EXAMPLE 1 upper(\"Hello World\") This converts the simple letters in the input.string to theequivalent capital letters. In this scenario, the output is \"HELLO WORLD\".","title":"upper (Function)"},{"location":"docs/api/5.1.0/#tokenize-stream-processor_2","text":"This function splits the input string into tokens using a given regular expression and returns the split tokens. Origin: siddhi-execution-string:5.0.7 Syntax str:tokenize( STRING input.string, STRING regex) str:tokenize( STRING input.string, STRING regex, BOOL distinct) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string The input string which needs to be split. STRING No Yes regex The string value which is used to tokenize the 'input.string'. STRING No Yes distinct This flag is used to return only distinct values. false BOOL Yes Yes Extra Return Attributes Name Description Possible Types token The attribute which contains a single token. STRING Examples EXAMPLE 1 define stream inputStream (str string); @info(name = 'query1') from inputStream#str:tokenize(str , ',') select token insert into outputStream; This query performs tokenization on the given string. If the str is \"Android,Windows8,iOS\", then the string is split into 3 events containing the token attribute values, i.e., Android , Windows8 and iOS .","title":"tokenize (Stream Processor)"},{"location":"docs/api/5.1.0/#time","text":"","title":"Time"},{"location":"docs/api/5.1.0/#currentdate-function","text":"Function returns the system time in yyyy-MM-dd format. Origin: siddhi-execution-time:5.0.4 Syntax STRING time:currentDate() Examples EXAMPLE 1 time:currentDate() Returns the current date in the yyyy-MM-dd format, such as 2019-06-21 .","title":"currentDate (Function)"},{"location":"docs/api/5.1.0/#currenttime-function","text":"Function returns system time in the HH ss format. Origin: siddhi-execution-time:5.0.4 Syntax STRING time:currentTime() Examples EXAMPLE 1 time:currentTime() Returns the current date in the HH ss format, such as 15:23:24 .","title":"currentTime (Function)"},{"location":"docs/api/5.1.0/#currenttimestamp-function","text":"When no argument is provided, function returns the system current timestamp in yyyy-MM-dd HH ss format, and when a timezone is provided as an argument, it converts and return the current system time to the given timezone format. Origin: siddhi-execution-time:5.0.4 Syntax STRING time:currentTimestamp() STRING time:currentTimestamp( STRING timezone) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic timezone The timezone to which the current time need to be converted. For example, Asia/Kolkata , PST . Get the supported timezone IDs from here System timezone STRING Yes No Examples EXAMPLE 1 time:currentTimestamp() Returns current system time in yyyy-MM-dd HH ss format, such as 2019-03-31 14:07:00 . EXAMPLE 2 time:currentTimestamp('Asia/Kolkata') Returns current system time converted to 'Asia/Kolkata' timezone yyyy-MM-dd HH ss format, such as 2019-03-31 19:07:00 . Get the supported timezone IDs from here EXAMPLE 3 time:currentTimestamp('CST') Returns current system time converted to 'CST' timezone yyyy-MM-dd HH ss format, such as 2019-03-31 02:07:00 . Get the supported timezone IDs from here","title":"currentTimestamp (Function)"},{"location":"docs/api/5.1.0/#date-function","text":"Extracts the date part of a date or date-time and return it in yyyy-MM-dd format. Origin: siddhi-execution-time:5.0.4 Syntax STRING time:date( STRING date.value, STRING date.format) STRING time:date( STRING date.value) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic date.value The value of the date. For example, 2014-11-11 13:23:44.657 , 2014-11-11 , 13:23:44.657 . STRING No Yes date.format The format of the date value provided. For example, yyyy/MM/dd HH ss.SSS . yyyy-MM-dd HH:mm:ss.SSS STRING Yes Yes Examples EXAMPLE 1 time:date('2014/11/11 13:23:44', 'yyyy/MM/dd HH:mm:ss') Extracts the date and returns 2014-11-11 . EXAMPLE 2 time:date('2014-11-23 13:23:44.345') Extracts the date and returns 2014-11-13 . EXAMPLE 3 time:date('13:23:44', 'HH:mm:ss') Extracts the date and returns 1970-01-01 .","title":"date (Function)"},{"location":"docs/api/5.1.0/#dateadd-function","text":"Adds the specified time interval to a date. Origin: siddhi-execution-time:5.0.4 Syntax STRING time:dateAdd( STRING date.value, INT expr, STRING unit) STRING time:dateAdd( LONG timestamp.in.milliseconds, INT expr, STRING unit) STRING time:dateAdd( STRING date.value, INT expr, STRING unit, STRING date.format) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic date.value The value of the date. For example, 2014-11-11 13:23:44.657 , 2014-11-11 , 13:23:44.657 . - STRING Yes Yes expr The amount by which the selected part of the date should be incremented. For example 2 , 5 , 10 , etc. INT No Yes unit This is the part of the date that needs to be modified. For example, MINUTE , HOUR , MONTH , YEAR , QUARTER , WEEK , DAY , SECOND . STRING No No date.format The format of the date value provided. For example, yyyy-MM-dd HH ss.SSS . yyyy-MM-dd HH:mm:ss.SSS STRING Yes Yes timestamp.in.milliseconds The date value in milliseconds. For example, 1415712224000L . - LONG Yes Yes Examples EXAMPLE 1 time:dateAdd('2014-11-11 13:23:44.657', 5, 'YEAR', 'yyyy-MM-dd HH:mm:ss.SSS') Adds five years to the given date value and returns 2019-11-11 13:23:44.657 . EXAMPLE 2 time:dateAdd('2014-11-11 13:23:44.657', 5, 'YEAR') Adds five years to the given date value and returns 2019-11-11 13:23:44.657 using the default date.format yyyy-MM-dd HH ss.SSS . EXAMPLE 3 time:dateAdd( 1415712224000L, 1, 'HOUR') Adds one hour and 1415715824000 as a string .","title":"dateAdd (Function)"},{"location":"docs/api/5.1.0/#datediff-function","text":"Returns difference between two dates in days. Origin: siddhi-execution-time:5.0.4 Syntax INT time:dateDiff( STRING date.value1, STRING date.value2, STRING date.format1, STRING date.format2) INT time:dateDiff( STRING date.value1, STRING date.value2) INT time:dateDiff( LONG timestamp.in.milliseconds1, LONG timestamp.in.milliseconds2) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic date.value1 The value of the first date parameter. For example, 2014-11-11 13:23:44.657 , 2014-11-11 , 13:23:44.657 . - STRING Yes Yes date.value2 The value of the second date parameter. For example, 2014-11-11 13:23:44.657 , 2014-11-11 , 13:23:44.657 . - STRING Yes Yes date.format1 The format of the first date value provided. For example, yyyy-MM-dd HH ss.SSS . yyyy-MM-dd HH:mm:ss.SSS STRING Yes Yes date.format2 The format of the second date value provided. For example, yyyy-MM-dd HH ss.SSS . yyyy-MM-dd HH:mm:ss.SSS STRING Yes Yes timestamp.in.milliseconds1 The first date value in milliseconds from the epoch. For example, 1415712224000L . - LONG Yes Yes timestamp.in.milliseconds2 The second date value in milliseconds from the epoch. For example, 1415712224000L . - LONG Yes Yes Examples EXAMPLE 1 time:dateDiff('2014-11-11 13:23:44', '2014-11-9 13:23:44', 'yyyy-MM-dd HH:mm:ss', 'yyyy-MM-dd HH:mm:ss') Returns the date difference between the two given dates as 2 . EXAMPLE 2 time:dateDiff('2014-11-13 13:23:44', '2014-11-9 13:23:44') Returns the date difference between the two given dates as 4 . EXAMPLE 3 time:dateDiff(1415692424000L, 1412841224000L) Returns the date difference between the two given dates as 33 .","title":"dateDiff (Function)"},{"location":"docs/api/5.1.0/#dateformat-function","text":"Formats the data in string or milliseconds format to the given date format. Origin: siddhi-execution-time:5.0.4 Syntax STRING time:dateFormat( STRING date.value, STRING date.target.format, STRING date.source.format) STRING time:dateFormat( STRING date.value, STRING date.target.format) STRING time:dateFormat( LONG timestamp.in.milliseconds, STRING date.target.format) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic date.value The value of the date. For example, 2014-11-11 13:23:44.657 , 2014-11-11 , 13:23:44.657 . - STRING Yes Yes date.target.format The format of the date into which the date value needs to be converted. For example, yyyy/MM/dd HH ss . STRING No Yes date.source.format The format input date.value.For example, yyyy-MM-dd HH ss.SSS . yyyy-MM-dd HH:mm:ss.SSS STRING Yes Yes timestamp.in.milliseconds The date value in milliseconds from the epoch. For example, 1415712224000L . - LONG Yes Yes Examples EXAMPLE 1 time:dateFormat('2014/11/11 13:23:44', 'mm:ss', 'yyyy/MM/dd HH:mm:ss') Converts date based on the target date format mm:ss and returns 23:44 . EXAMPLE 2 time:dateFormat('2014-11-11 13:23:44', 'HH:mm:ss') Converts date based on the target date format HH ss and returns 13:23:44 . EXAMPLE 3 time:dateFormat(1415692424000L, 'yyyy-MM-dd') Converts date in millisecond based on the target date format yyyy-MM-dd and returns 2014-11-11 .","title":"dateFormat (Function)"},{"location":"docs/api/5.1.0/#datesub-function","text":"Subtracts the specified time interval from the given date. Origin: siddhi-execution-time:5.0.4 Syntax STRING time:dateSub( STRING date.value, INT expr, STRING unit) STRING time:dateSub( STRING date.value, INT expr, STRING unit, STRING date.format) STRING time:dateSub( LONG timestamp.in.milliseconds, INT expr, STRING unit) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic date.value The value of the date. For example, 2014-11-11 13:23:44.657 , 2014-11-11 , 13:23:44.657 . - STRING Yes Yes expr The amount by which the selected part of the date should be decremented. For example 2 , 5 , 10 , etc. INT No Yes unit This is the part of the date that needs to be modified. For example, MINUTE , HOUR , MONTH , YEAR , QUARTER , WEEK , DAY , SECOND . STRING No No date.format The format of the date value provided. For example, yyyy-MM-dd HH ss.SSS . yyyy-MM-dd HH:mm:ss.SSS STRING Yes Yes timestamp.in.milliseconds The date value in milliseconds. For example, 1415712224000L . - LONG Yes Yes Examples EXAMPLE 1 time:dateSub('2019-11-11 13:23:44.657', 5, 'YEAR', 'yyyy-MM-dd HH:mm:ss.SSS') Subtracts five years to the given date value and returns 2014-11-11 13:23:44.657 . EXAMPLE 2 time:dateSub('2019-11-11 13:23:44.657', 5, 'YEAR') Subtracts five years to the given date value and returns 2014-11-11 13:23:44.657 using the default date.format yyyy-MM-dd HH ss.SSS . EXAMPLE 3 time:dateSub( 1415715824000L, 1, 'HOUR') Subtracts one hour and 1415712224000 as a string .","title":"dateSub (Function)"},{"location":"docs/api/5.1.0/#dayofweek-function","text":"Extracts the day on which a given date falls. Origin: siddhi-execution-time:5.0.4 Syntax STRING time:dayOfWeek( STRING date.value, STRING date.format) STRING time:dayOfWeek( STRING date.value) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic date.value The value of the date. For example, 2014-11-11 13:23:44.657 , 2014-11-11 , 13:23:44.657 . STRING No Yes date.format The format of the date value provided. For example, yyyy/MM/dd HH ss.SSS . yyyy-MM-dd HH:mm:ss.SSS STRING Yes Yes Examples EXAMPLE 1 time:date('2014/12/11 13:23:44', 'yyyy/MM/dd HH:mm:ss') Extracts the date and returns Thursday . EXAMPLE 2 time:date('2014-11-11 13:23:44.345') Extracts the date and returns Tuesday .","title":"dayOfWeek (Function)"},{"location":"docs/api/5.1.0/#extract-function","text":"Function extracts a date unit from the date. Origin: siddhi-execution-time:5.0.4 Syntax INT time:extract( STRING unit, STRING date.value) INT time:extract( STRING unit, STRING date.value, STRING date.format) INT time:extract( STRING unit, STRING date.value, STRING date.format, STRING locale) INT time:extract( LONG timestamp.in.milliseconds, STRING unit) INT time:extract( LONG timestamp.in.milliseconds, STRING unit, STRING locale) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic unit This is the part of the date that needs to be modified. For example, MINUTE , HOUR , MONTH , YEAR , QUARTER , WEEK , DAY , SECOND . STRING No No date.value The value of the date. For example, 2014-11-11 13:23:44.657 , 2014-11-11 , 13:23:44.657 . - STRING Yes Yes date.format The format of the date value provided. For example, yyyy-MM-dd HH ss.SSS . yyyy-MM-dd HH:mm:ss.SSS STRING Yes Yes timestamp.in.milliseconds The date value in milliseconds. For example, 1415712224000L . - LONG Yes Yes locale Represents a specific geographical, political or cultural region. For example en_US and fr_FR Current default locale set in the Java Virtual Machine. STRING Yes No Examples EXAMPLE 1 time:extract('YEAR', '2019/11/11 13:23:44.657', 'yyyy/MM/dd HH:mm:ss.SSS') Extracts the year amount and returns 2019 . EXAMPLE 2 time:extract('DAY', '2019-11-12 13:23:44.657') Extracts the day amount and returns 12 . EXAMPLE 3 time:extract(1394556804000L, 'HOUR') Extracts the hour amount and returns 22 .","title":"extract (Function)"},{"location":"docs/api/5.1.0/#timestampinmilliseconds-function","text":"Returns the system time or the given time in milliseconds. Origin: siddhi-execution-time:5.0.4 Syntax LONG time:timestampInMilliseconds() LONG time:timestampInMilliseconds( STRING date.value, STRING date.format) LONG time:timestampInMilliseconds( STRING date.value) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic date.value The value of the date. For example, 2014-11-11 13:23:44.657 , 2014-11-11 , 13:23:44.657 . Current system time STRING Yes Yes date.format The format of the date value provided. For example, yyyy/MM/dd HH ss.SSS . yyyy-MM-dd HH:mm:ss.SSS STRING Yes Yes Examples EXAMPLE 1 time:timestampInMilliseconds() Returns the system current time in milliseconds. EXAMPLE 2 time:timestampInMilliseconds('2007-11-30 10:30:19', 'yyyy-MM-DD HH:MM:SS') Converts 2007-11-30 10:30:19 in yyyy-MM-DD HH:MM:SS format to milliseconds as 1170131400019 . EXAMPLE 3 time:timestampInMilliseconds('2007-11-30 10:30:19.000') Converts 2007-11-30 10:30:19 in yyyy-MM-DD HH:MM:ss.SSS format to milliseconds as 1196398819000 .","title":"timestampInMilliseconds (Function)"},{"location":"docs/api/5.1.0/#utctimestamp-function","text":"Function returns the system current time in UTC timezone with yyyy-MM-dd HH ss format. Origin: siddhi-execution-time:5.0.4 Syntax STRING time:utcTimestamp() Examples EXAMPLE 1 time:utcTimestamp() Returns the system current time in UTC timezone with yyyy-MM-dd HH ss format, and a sample output will be like 2019-07-03 09:58:34 .","title":"utcTimestamp (Function)"},{"location":"docs/api/5.1.0/#unique","text":"","title":"Unique"},{"location":"docs/api/5.1.0/#deduplicate-stream-processor","text":"Removes duplicate events based on the unique.key parameter that arrive within the time.interval gap from one another. Origin: siddhi-execution-unique:5.0.4 Syntax unique:deduplicate( INT|LONG|FLOAT|BOOL|DOUBLE|STRING unique.key, INT|LONG time.interval) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic unique.key Parameter to uniquely identify events. INT LONG FLOAT BOOL DOUBLE STRING No Yes time.interval The sliding time period within which the duplicate events are dropped. INT LONG No No Examples EXAMPLE 1 define stream TemperatureStream (sensorId string, temperature double) from TemperatureStream#unique:deduplicate(sensorId, 30 sec) select * insert into UniqueTemperatureStream; Query that removes duplicate events of TemperatureStream stream based on sensorId attribute when they arrive within 30 seconds.","title":"deduplicate (Stream Processor)"},{"location":"docs/api/5.1.0/#ever-window","text":"Window that retains the latest events based on a given unique keys. When a new event arrives with the same key it replaces the one that exist in the window. b This function is not recommended to be used when the maximum number of unique attributes are undefined, as there is a risk of system going out to memory /b . Origin: siddhi-execution-unique:5.0.4 Syntax unique:ever( INT|LONG|FLOAT|BOOL|DOUBLE|STRING unique.key) unique:ever( INT|LONG|FLOAT|BOOL|DOUBLE|STRING unique.key, INT|LONG|FLOAT|BOOL|DOUBLE|STRING ...) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic unique.key The attribute used to checked for uniqueness. INT LONG FLOAT BOOL DOUBLE STRING No Yes Examples EXAMPLE 1 define stream LoginEvents (timestamp long, ip string); from LoginEvents#window.unique:ever(ip) select count(ip) as ipCount insert events into UniqueIps; Query collects all unique events based on the ip attribute by retaining the latest unique events from the LoginEvents stream. Then the query counts the unique ip s arrived so far and outputs the ipCount via the UniqueIps stream. EXAMPLE 2 define stream DriverChangeStream (trainID string, driver string); from DriverChangeStream#window.unique:ever(trainID) select trainID, driver insert expired events into PreviousDriverChangeStream; Query collects all unique events based on the trainID attribute by retaining the latest unique events from the DriverChangeStream stream. The query outputs the previous unique event stored in the window as the expired events are emitted via PreviousDriverChangeStream stream. EXAMPLE 3 define stream StockStream (symbol string, price float); define stream PriceRequestStream(symbol string); from StockStream#window.unique:ever(symbol) as s join PriceRequestStream as p on s.symbol == p.symbol select s.symbol as symbol, s.price as price insert events into PriceResponseStream; Query stores the last unique event for each symbol attribute of StockStream stream, and joins them with events arriving on the PriceRequestStream for equal symbol attributes to fetch the latest price for each requested symbol and output via PriceResponseStream stream.","title":"ever (Window)"},{"location":"docs/api/5.1.0/#externaltimebatch-window_1","text":"This is a batch (tumbling) time window that is determined based on an external time, i.e., time stamps that are specified via an attribute in the events. It holds the latest unique events that arrived during the last window time period. The unique events are determined based on the value for a specified unique key parameter. When a new event arrives within the time window with a value for the unique key parameter that is the same as that of an existing event in the window, the existing event expires and it is replaced by the new event. Origin: siddhi-execution-unique:5.0.4 Syntax unique:externalTimeBatch( INT|LONG|FLOAT|BOOL|DOUBLE|STRING unique.key, LONG time.stamp, INT|LONG window.time) unique:externalTimeBatch( INT|LONG|FLOAT|BOOL|DOUBLE|STRING unique.key, LONG time.stamp, INT|LONG window.time, INT start.time) unique:externalTimeBatch( INT|LONG|FLOAT|BOOL|DOUBLE|STRING unique.key, LONG time.stamp, INT|LONG window.time, INT start.time, INT|LONG time.out) unique:externalTimeBatch( INT|LONG|FLOAT|BOOL|DOUBLE|STRING unique.key, LONG time.stamp, INT|LONG window.time, INT start.time, INT|LONG time.out, INT|LONG replace.time.stamp.with.batch.end.time) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic unique.key The attribute that should be checked for uniqueness. INT LONG FLOAT BOOL DOUBLE STRING No Yes time.stamp The time which the window determines as the current time and acts upon. The value of this parameter should be monotonically increasing. LONG No No window.time The sliding time period for which the window should hold events. INT LONG No No start.time This specifies an offset in milliseconds in order to start the window at a time different to the standard time. Timestamp of first event INT Yes No time.out Time to wait for arrival of a new event, before flushing and returning the output for events belonging to a specific batch. The system waits till an event from the next batch arrives to flush the current batch INT LONG Yes No replace.time.stamp.with.batch.end.time Replaces the 'timestamp' value with the corresponding batch end time stamp. false INT LONG Yes No Examples EXAMPLE 1 define stream LoginEvents (timestamp long, ip string); from LoginEvents#window.unique:externalTimeBatch(ip, timestamp, 1 sec, 0, 2 sec) select timestamp, ip, count() as total insert into UniqueIps ; In this query, the window holds the latest unique events that arrive from the 'LoginEvent' stream during each second. The latest events are determined based on the external time stamp. At a given time, all the events held in the window have unique values for the 'ip' and monotonically increasing values for 'timestamp' attributes. The events in the window are inserted into the 'UniqueIps' output stream. The system waits for 2 seconds for the arrival of a new event before flushing the current batch.","title":"externalTimeBatch (Window)"},{"location":"docs/api/5.1.0/#first-window","text":"This is a window that holds only the first set of unique events according to the unique key parameter. When a new event arrives with a key that is already in the window, that event is not processed by the window. Origin: siddhi-execution-unique:5.0.4 Syntax unique:first( INT|LONG|FLOAT|BOOL|DOUBLE|STRING unique.key) unique:first( INT|LONG|FLOAT|BOOL|DOUBLE|STRING unique.key, INT|LONG|FLOAT|BOOL|DOUBLE|STRING ...) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic unique.key The attribute that should be checked for uniqueness. If there is more than one parameter to check for uniqueness, it can be specified as an array separated by commas. INT LONG FLOAT BOOL DOUBLE STRING No Yes Examples EXAMPLE 1 define stream LoginEvents (timeStamp long, ip string); from LoginEvents#window.unique:first(ip) insert into UniqueIps ; This returns the first set of unique items that arrive from the 'LoginEvents' stream, and returns them to the 'UniqueIps' stream. The unique events are only those with a unique value for the 'ip' attribute.","title":"first (Window)"},{"location":"docs/api/5.1.0/#firstlengthbatch-window","text":"This is a batch (tumbling) window that holds a specific number of unique events (depending on which events arrive first). The unique events are selected based on a specific parameter that is considered as the unique key. When a new event arrives with a value for the unique key parameter that matches the same of an existing event in the window, that event is not processed by the window. Origin: siddhi-execution-unique:5.0.4 Syntax unique:firstLengthBatch( INT|LONG|FLOAT|BOOL|DOUBLE|STRING unique.key, INT window.length) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic unique.key The attribute that should be checked for uniqueness. INT LONG FLOAT BOOL DOUBLE STRING No Yes window.length The number of events the window should tumble. INT No Yes Examples EXAMPLE 1 define window CseEventWindow (symbol string, price float, volume int) from CseEventStream#window.unique:firstLengthBatch(symbol, 10) select symbol, price, volume insert all events into OutputStream ; The window in this configuration holds the first unique events from the 'CseEventStream' stream every second, and outputs them all into the the 'OutputStream' stream. All the events in a window during a given second should have a unique value for the 'symbol' attribute.","title":"firstLengthBatch (Window)"},{"location":"docs/api/5.1.0/#firsttimebatch-window","text":"A batch-time or tumbling window that holds the unique events according to the unique key parameters that have arrived within the time period of that window and gets updated for each such time window. When a new event arrives with a key which is already in the window, that event is not processed by the window. Origin: siddhi-execution-unique:5.0.4 Syntax unique:firstTimeBatch( INT|LONG|FLOAT|BOOL|DOUBLE|STRING unique.key, INT|LONG window.time) unique:firstTimeBatch( INT|LONG|FLOAT|BOOL|DOUBLE|STRING unique.key, INT|LONG window.time, INT|LONG start.time) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic unique.key The attribute that should be checked for uniqueness. INT LONG FLOAT BOOL DOUBLE STRING No Yes window.time The sliding time period for which the window should hold events. INT LONG No Yes start.time This specifies an offset in milliseconds in order to start the window at a time different to the standard time. Timestamp of the first event. INT LONG Yes Yes Examples EXAMPLE 1 define stream CseEventStream (symbol string, price float, volume int) from CseEventStream#window.unique:firstTimeBatch(symbol,1 sec) select symbol, price, volume insert all events into OutputStream ; This holds the first unique events that arrive from the 'cseEventStream' input stream during each second, based on the symbol,as a batch, and returns all the events to the 'OutputStream'.","title":"firstTimeBatch (Window)"},{"location":"docs/api/5.1.0/#length-window_1","text":"This is a sliding length window that holds the events of the latest window length with the unique key and gets updated for the expiry and arrival of each event. When a new event arrives with the key that is already there in the window, then the previous event expires and new event is kept within the window. Origin: siddhi-execution-unique:5.0.4 Syntax unique:length( INT|LONG|FLOAT|BOOL|DOUBLE|STRING unique.key, INT window.length) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic unique.key The attribute that should be checked for uniqueness. INT LONG FLOAT BOOL DOUBLE STRING No Yes window.length The number of events that should be included in a sliding length window. INT No Yes Examples EXAMPLE 1 define stream CseEventStream (symbol string, price float, volume int) from CseEventStream#window.unique:length(symbol,10) select symbol, price, volume insert all events into OutputStream; In this configuration, the window holds the latest 10 unique events. The latest events are selected based on the symbol attribute. If the 'CseEventStream' receives an event for which the value for the symbol attribute is the same as that of an existing event in the window, the existing event is replaced by the new event. All the events are returned to the 'OutputStream' event stream once an event expires or is added to the window.","title":"length (Window)"},{"location":"docs/api/5.1.0/#lengthbatch-window_1","text":"This is a batch (tumbling) window that holds a specified number of latest unique events. The unique events are determined based on the value for a specified unique key parameter. The window is updated for every window length, i.e., for the last set of events of the specified number in a tumbling manner. When a new event arrives within the window length having the same value for the unique key parameter as an existing event in the window, the previous event is replaced by the new event. Origin: siddhi-execution-unique:5.0.4 Syntax unique:lengthBatch( INT|LONG|FLOAT|BOOL|DOUBLE|STRING unique.key, INT window.length) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic unique.key The attribute that should be checked for uniqueness. INT LONG FLOAT BOOL DOUBLE STRING No Yes window.length The number of events the window should tumble. INT No Yes Examples EXAMPLE 1 define window CseEventWindow (symbol string, price float, volume int) from CseEventStream#window.unique:lengthBatch(symbol, 10) select symbol, price, volume insert expired events into OutputStream ; In this query, the window at any give time holds the last 10 unique events from the 'CseEventStream' stream. Each of the 10 events within the window at a given time has a unique value for the symbol attribute. If a new event has the same value for the symbol attribute as an existing event within the window length, the existing event expires and it is replaced by the new event. The query returns expired individual events as well as expired batches of events to the 'OutputStream' stream.","title":"lengthBatch (Window)"},{"location":"docs/api/5.1.0/#time-window_1","text":"This is a sliding time window that holds the latest unique events that arrived during the previous time window. The unique events are determined based on the value for a specified unique key parameter. The window is updated with the arrival and expiry of each event. When a new event that arrives within a window time period has the same value for the unique key parameter as an existing event in the window, the previous event is replaced by the new event. Origin: siddhi-execution-unique:5.0.4 Syntax unique:time( INT|LONG|FLOAT|BOOL|DOUBLE|STRING unique.key, INT|LONG window.time) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic unique.key The attribute that should be checked for uniqueness. INT LONG FLOAT BOOL DOUBLE STRING No No window.time The sliding time period for which the window should hold events. INT LONG No No Examples EXAMPLE 1 define stream CseEventStream (symbol string, price float, volume int) from CseEventStream#window.unique:time(symbol, 1 sec) select symbol, price, volume insert expired events into OutputStream ; In this query, the window holds the latest unique events that arrived within the last second from the 'CseEventStream', and returns the expired events to the 'OutputStream' stream. During any given second, each event in the window should have a unique value for the 'symbol' attribute. If a new event that arrives within the same second has the same value for the symbol attribute as an existing event in the window, the existing event expires.","title":"time (Window)"},{"location":"docs/api/5.1.0/#timebatch-window_1","text":"This is a batch (tumbling) time window that is updated with the latest events based on a unique key parameter. If a new event that arrives within the time period of a windowhas a value for the key parameter which matches that of an existing event, the existing event expires and it is replaced by the latest event. Origin: siddhi-execution-unique:5.0.4 Syntax unique:timeBatch( INT|LONG|FLOAT|BOOL|DOUBLE|STRING unique.key, INT|LONG window.time) unique:timeBatch( INT|LONG|FLOAT|BOOL|DOUBLE|STRING unique.key, INT|LONG window.time, INT|LONG start.time) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic unique.key The attribute that should be checked for uniqueness. INT LONG FLOAT BOOL DOUBLE STRING No Yes window.time The tumbling time period for which the window should hold events. INT LONG No Yes start.time This specifies an offset in milliseconds in order to start the window at a time different to the standard time. Timestamp of first event INT LONG Yes Yes Examples EXAMPLE 1 define stream CseEventStream (symbol string, price float, volume int) from CseEventStream#window.unique:timeBatch(symbol, 1 sec) select symbol, price, volume insert all events into OutputStream ; This window holds the latest unique events that arrive from the 'CseEventStream' at a given time, and returns all the events to the 'OutputStream' stream. It is updated every second based on the latest values for the 'symbol' attribute.","title":"timeBatch (Window)"},{"location":"docs/api/5.1.0/#timelengthbatch-window","text":"This is a batch or tumbling time length window that is updated with the latest events based on a unique key parameter. The window tumbles upon the elapse of the time window, or when a number of unique events have arrived. If a new event that arrives within the period of the window has a value for the key parameter which matches the value of an existing event, the existing event expires and it is replaced by the new event. Origin: siddhi-execution-unique:5.0.4 Syntax unique:timeLengthBatch( INT|LONG|FLOAT|BOOL|DOUBLE|STRING unique.key, INT|LONG window.time, INT window.length) unique:timeLengthBatch( INT|LONG|FLOAT|BOOL|DOUBLE|STRING unique.key, INT|LONG window.time, INT|LONG start.time, INT window.length) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic unique.key The attribute that should be checked for uniqueness. INT LONG FLOAT BOOL DOUBLE STRING No Yes window.time The sliding time period for which the window should hold the events. INT LONG No Yes start.time This specifies an offset in milliseconds in order to start the window at a time different to the standard time. Timestamp of first event INT LONG Yes Yes window.length The number of events the window should tumble. INT No Yes Examples EXAMPLE 1 define stream CseEventStream (symbol string, price float, volume int) from CseEventStream#window.unique:timeLengthBatch(symbol, 1 sec, 20) select symbol, price, volume insert all events into OutputStream; This window holds the latest unique events that arrive from the 'CseEventStream' at a given time, and returns all the events to the 'OutputStream' stream. It is updated every second based on the latest values for the 'symbol' attribute.","title":"timeLengthBatch (Window)"},{"location":"docs/api/5.1.0/#unitconversion","text":"","title":"Unitconversion"},{"location":"docs/api/5.1.0/#mmtokm-function","text":"This converts the input given in megameters into kilometers. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:MmTokm( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from megameters into kilometers. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:MmTokm(1) The megameter value '1' is converted into kilometers as '1000.0' .","title":"MmTokm (Function)"},{"location":"docs/api/5.1.0/#cmtoft-function","text":"This converts the input given in centimeters into feet. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:cmToft( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from centimeters into feet. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:cmToft(100) The centimeters value '100' is converted into feet as '3.280' .","title":"cmToft (Function)"},{"location":"docs/api/5.1.0/#cmtoin-function","text":"This converts the input given in centimeters into inches. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:cmToin( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from centimeters into inches. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:cmToin(100) Input centimeters value '100' is converted into inches as '39.37'.","title":"cmToin (Function)"},{"location":"docs/api/5.1.0/#cmtokm-function","text":"This converts the input value given in centimeters into kilometers. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:cmTokm( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from centimeters into kilometers. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:cmTokm(100) The centimeters value '100' is converted into kilometers as '0.001'.","title":"cmTokm (Function)"},{"location":"docs/api/5.1.0/#cmtom-function","text":"This converts the input given in centimeters into meters. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:cmTom( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from centimeters into meters. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:cmTom(100) The centimeters value '100' is converted into meters as '1.0' .","title":"cmTom (Function)"},{"location":"docs/api/5.1.0/#cmtomi-function","text":"This converts the input given in centimeters into miles. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:cmTomi( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from centimeters into miles. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:cmTomi(10000) The centimeters value '10000' is converted into miles as '0.062' .","title":"cmTomi (Function)"},{"location":"docs/api/5.1.0/#cmtomm-function","text":"This converts the input given in centimeters into millimeters. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:cmTomm( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from centimeters into millimeters. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:cmTomm(1) The centimeter value '1' is converted into millimeters as '10.0' .","title":"cmTomm (Function)"},{"location":"docs/api/5.1.0/#cmtonm-function","text":"This converts the input given in centimeters into nanometers. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:cmTonm( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from centimeters into nanometers. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:cmTonm(1) The centimeter value '1' is converted into nanometers as '10000000' .","title":"cmTonm (Function)"},{"location":"docs/api/5.1.0/#cmtoum-function","text":"This converts the input in centimeters into micrometers. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:cmToum( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from centimeters into micrometers. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:cmToum(100) The centimeters value '100' is converted into micrometers as '1000000.0' .","title":"cmToum (Function)"},{"location":"docs/api/5.1.0/#cmtoyd-function","text":"This converts the input given in centimeters into yards. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:cmToyd( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from centimeters into yards. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:cmToyd(1) The centimeter value '1' is converted into yards as '0.01' .","title":"cmToyd (Function)"},{"location":"docs/api/5.1.0/#dtoh-function","text":"This converts the input given in days into hours. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:dToh( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from days into hours. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:dToh(1) The day value '1' is converted into hours as '24.0'.","title":"dToh (Function)"},{"location":"docs/api/5.1.0/#gtokg-function","text":"This converts the input given in grams into kilograms. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:gTokg( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from grams into kilograms. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:gTokg(1000) The grams value '1000' is converted into kilogram as '1.0' .","title":"gTokg (Function)"},{"location":"docs/api/5.1.0/#gtomg-function","text":"This converts the input given in grams into milligrams. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:gTomg( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from grams into milligrams. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:gTomg(1) The gram value '1' is converted into milligrams as '1000.0' .","title":"gTomg (Function)"},{"location":"docs/api/5.1.0/#gtoug-function","text":"This converts the input given in grams into micrograms. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:gToug( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from grams into micrograms. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:gToug(1) The gram value '1' is converted into micrograms as '1000000.0' .","title":"gToug (Function)"},{"location":"docs/api/5.1.0/#htom-function","text":"This converts the input given in hours into minutes. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:hTom( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from hours into minutes. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:hTom(1) The hour value '1' is converted into minutes as '60.0' .","title":"hTom (Function)"},{"location":"docs/api/5.1.0/#htos-function","text":"This converts the input given in hours into seconds. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:hTos( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from hours into seconds. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:hTos(1) The hour value '1' is converted into seconds as '3600.0'.","title":"hTos (Function)"},{"location":"docs/api/5.1.0/#kgtolt-function","text":"This converts the input given in kilograms into imperial tons. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:kgToLT( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from kilograms into imperial tons. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:kgToLT(1000) The kilograms value '1000' is converted into imperial tons as '0.9842' .","title":"kgToLT (Function)"},{"location":"docs/api/5.1.0/#kgtost-function","text":"This converts the input given in kilograms into US tons. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:kgToST( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from kilograms into US tons. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:kgToST(1000) The kilograms value '1000 is converted into US tons as '1.10' .","title":"kgToST (Function)"},{"location":"docs/api/5.1.0/#kgtog-function","text":"This converts the input given in kilograms into grams. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:kgTog( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from kilograms into grams. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:kgTog(1) The kilogram value '1' is converted into grams as '1000'.","title":"kgTog (Function)"},{"location":"docs/api/5.1.0/#kgtolb-function","text":"This converts the input given in kilograms into pounds. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:kgTolb( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from kilograms into pounds. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:kgTolb(1) The kilogram value '1' is converted into pounds as '2.2' .","title":"kgTolb (Function)"},{"location":"docs/api/5.1.0/#kgtooz-function","text":"This converts the input given in kilograms into ounces. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:kgTooz( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from kilograms into ounces. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:kgTooz(1) The kilogram value '1' is converted into ounces as ' 35.274' .","title":"kgTooz (Function)"},{"location":"docs/api/5.1.0/#kgtost-function_1","text":"This converts the input given in kilograms into imperial stones. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:kgTost( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from kilograms into imperial stones. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:kgTost(1) The kilogram value '1' is converted into imperial stones as '0.157' .","title":"kgTost (Function)"},{"location":"docs/api/5.1.0/#kgtot-function","text":"This converts the input given in kilograms into tonnes. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:kgTot( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from kilograms into tonnes. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:kgTot(1) The kilogram value '1' is converted into tonnes as '0.001' .","title":"kgTot (Function)"},{"location":"docs/api/5.1.0/#kmtocm-function","text":"This converts the input given in kilometers into centimeters. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:kmTocm( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from kilometers into centimeters. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:kmTocm(1) The kilometer value '1' is converted into centimeters as '100000.0' .","title":"kmTocm (Function)"},{"location":"docs/api/5.1.0/#kmtoft-function","text":"This converts the input given in kilometers into feet. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:kmToft( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from kilometers into feet. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:kmToft(1) The kilometer value '1' is converted into feet as '3280.8' .","title":"kmToft (Function)"},{"location":"docs/api/5.1.0/#kmtoin-function","text":"This converts the input given in kilometers into inches. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:kmToin( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from kilometers into inches. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:kmToin(1) The kilometer value '1' is converted into inches as '39370.08' .","title":"kmToin (Function)"},{"location":"docs/api/5.1.0/#kmtom-function","text":"This converts the input given in kilometers into meters. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:kmTom( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from kilometers into meters. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:kmTom(1) The kilometer value '1' is converted into meters as '1000.0' .","title":"kmTom (Function)"},{"location":"docs/api/5.1.0/#kmtomi-function","text":"This converts the input given in kilometers into miles. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:kmTomi( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from kilometers into miles. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:kmTomi(1) The kilometer value '1' is converted into miles as '0.621' .","title":"kmTomi (Function)"},{"location":"docs/api/5.1.0/#kmtomm-function","text":"This converts the input given in kilometers into millimeters. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:kmTomm( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from kilometers into millimeters. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:kmTomm(1) The kilometer value '1' is converted into millimeters as '1000000.0' .","title":"kmTomm (Function)"},{"location":"docs/api/5.1.0/#kmtonm-function","text":"This converts the input given in kilometers into nanometers. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:kmTonm( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from kilometers into nanometers. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:kmTonm(1) The kilometer value '1' is converted into nanometers as '1000000000000.0' .","title":"kmTonm (Function)"},{"location":"docs/api/5.1.0/#kmtoum-function","text":"This converts the input given in kilometers into micrometers. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:kmToum( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from kilometers into micrometers. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:kmToum(1) The kilometer value '1' is converted into micrometers as '1000000000.0' .","title":"kmToum (Function)"},{"location":"docs/api/5.1.0/#kmtoyd-function","text":"This converts the input given in kilometers into yards. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:kmToyd( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from kilometers into yards. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:kmToyd(1) The kilometer value '1' is converted into yards as '1093.6' .","title":"kmToyd (Function)"},{"location":"docs/api/5.1.0/#ltom3-function","text":"This converts the input given in liters into cubic meters. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:lTom3( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from liters into cubic meters. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:lTom3(1000) The liters value '1000' is converted into cubic meters as '1' .","title":"lTom3 (Function)"},{"location":"docs/api/5.1.0/#ltoml-function","text":"This converts the input given in liters into milliliters. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:lToml( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from liters into milliliters. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:lToml(1) The liter value '1' is converted into milliliters as '1000.0' .","title":"lToml (Function)"},{"location":"docs/api/5.1.0/#m3tol-function","text":"This converts the input given in cubic meters into liters. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:m3Tol( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from meters into liters. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:m3Tol(1) The cubic meter value '1' is converted into liters as '1000.0' .","title":"m3Tol (Function)"},{"location":"docs/api/5.1.0/#mtocm-function","text":"This converts the input given in meters into centimeters. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:mTocm( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from meters into centimeters. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:mTocm(1) The meter value '1' is converted to centimeters as '100.0' .","title":"mTocm (Function)"},{"location":"docs/api/5.1.0/#mtoft-function","text":"This converts the input given in meters into feet. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:mToft( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from meters into feet. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:mToft(1) The meter value '1' is converted into feet as '3.280' .","title":"mToft (Function)"},{"location":"docs/api/5.1.0/#mtomm-function","text":"This converts the input given in meters into millimeters. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:mTomm( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from meters into millimeters. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:mTomm(1) The meter value '1' is converted into millimeters as '1000.0' .","title":"mTomm (Function)"},{"location":"docs/api/5.1.0/#mtos-function","text":"This converts the input given in minutes into seconds. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:mTos( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from minutes into seconds. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:mTos(1) The minute value '1' is converted into seconds as '60.0' .","title":"mTos (Function)"},{"location":"docs/api/5.1.0/#mtoyd-function","text":"This converts the input given in meters into yards. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:mToyd( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from meters into yards. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:mToyd(1) The meter value '1' is converted into yards as '1.093' .","title":"mToyd (Function)"},{"location":"docs/api/5.1.0/#mitokm-function","text":"This converts the input given in miles into kilometers. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:miTokm( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from miles into kilometers. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:miTokm(1) The mile value '1' is converted into kilometers as '1.6' .","title":"miTokm (Function)"},{"location":"docs/api/5.1.0/#mltol-function","text":"This converts the input given in milliliters into liters. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:mlTol( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from milliliters into liters. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:mlTol(1000) The milliliters value '1000' is converted into liters as '1'.","title":"mlTol (Function)"},{"location":"docs/api/5.1.0/#stoms-function","text":"This converts the input given in seconds into milliseconds. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:sToms( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from seconds into milliseconds. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:sToms(1) The second value '1' is converted into milliseconds as '1000.0' .","title":"sToms (Function)"},{"location":"docs/api/5.1.0/#stons-function","text":"This converts the input given in seconds into nanoseconds. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:sTons( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from seconds into nanoseconds. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:sTons(1) The second value '1' is converted into nanoseconds as '1000000000.0' .","title":"sTons (Function)"},{"location":"docs/api/5.1.0/#stous-function","text":"This converts the input given in seconds into microseconds. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:sTous( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from seconds into microseconds. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:sTous(1) The second value '1' is converted into microseconds as '1000000.0' .","title":"sTous (Function)"},{"location":"docs/api/5.1.0/#ttog-function","text":"This converts the input given in tonnes into grams. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:tTog( INT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from Tonnes into grams. INT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:tTog(1) The tonne value '1' is converted into grams as '1000000.0' .","title":"tTog (Function)"},{"location":"docs/api/5.1.0/#ttokg-function","text":"This converts the input given in tonnes into kilograms. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:tTokg( INT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from tonnes into kilograms. INT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:tTokg(inValue) The tonne value is converted into kilograms as '1000.0' .","title":"tTokg (Function)"},{"location":"docs/api/5.1.0/#ytod-function","text":"This converts the given input in years into days. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:yTod( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from years into days. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:yTod(1) The year value '1' is converted into days as '365.2525' .","title":"yTod (Function)"},{"location":"docs/api/5.1.1/","text":"API Docs - v5.1.1 Core and (Aggregate Function) Returns the results of AND operation for all the events. Origin: siddhi-core:5.1.8 Syntax BOOL and( BOOL arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be AND operation. BOOL No Yes Examples EXAMPLE 1 from cscStream#window.lengthBatch(10) select and(isFraud) as isFraudTransaction insert into alertStream; This will returns the result for AND operation of isFraud values as a boolean value for event chunk expiry by window length batch. avg (Aggregate Function) Calculates the average for all the events. Origin: siddhi-core:5.1.8 Syntax DOUBLE avg( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that need to be averaged. INT LONG DOUBLE FLOAT No Yes Examples EXAMPLE 1 from fooStream#window.timeBatch select avg(temp) as avgTemp insert into barStream; avg(temp) returns the average temp value for all the events based on their arrival and expiry. count (Aggregate Function) Returns the count of all the events. Origin: siddhi-core:5.1.8 Syntax LONG count() LONG count( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg This function accepts one parameter. It can belong to any one of the available types. INT LONG DOUBLE FLOAT STRING BOOL OBJECT Yes Yes Examples EXAMPLE 1 from fooStream#window.timeBatch(10 sec) select count() as count insert into barStream; This will return the count of all the events for time batch in 10 seconds. distinctCount (Aggregate Function) This returns the count of distinct occurrences for a given arg. Origin: siddhi-core:5.1.8 Syntax LONG distinctCount( INT|LONG|DOUBLE|FLOAT|STRING arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The object for which the number of distinct occurences needs to be counted. INT LONG DOUBLE FLOAT STRING No Yes Examples EXAMPLE 1 from fooStream select distinctcount(pageID) as count insert into barStream; distinctcount(pageID) for the following output returns '3' when the available values are as follows. \"WEB_PAGE_1\" \"WEB_PAGE_1\" \"WEB_PAGE_2\" \"WEB_PAGE_3\" \"WEB_PAGE_1\" \"WEB_PAGE_2\" The three distinct occurences identified are 'WEB_PAGE_1', 'WEB_PAGE_2', and 'WEB_PAGE_3'. max (Aggregate Function) Returns the maximum value for all the events. Origin: siddhi-core:5.1.8 Syntax INT|LONG|DOUBLE|FLOAT max( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be compared to find the maximum value. INT LONG DOUBLE FLOAT No Yes Examples EXAMPLE 1 from fooStream#window.timeBatch(10 sec) select max(temp) as maxTemp insert into barStream; max(temp) returns the maximum temp value recorded for all the events based on their arrival and expiry. maxForever (Aggregate Function) This is the attribute aggregator to store the maximum value for a given attribute throughout the lifetime of the query regardless of any windows in-front. Origin: siddhi-core:5.1.8 Syntax INT|LONG|DOUBLE|FLOAT maxForever( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be compared to find the maximum value. INT LONG DOUBLE FLOAT No Yes Examples EXAMPLE 1 from inputStream select maxForever(temp) as max insert into outputStream; maxForever(temp) returns the maximum temp value recorded for all the events throughout the lifetime of the query. min (Aggregate Function) Returns the minimum value for all the events. Origin: siddhi-core:5.1.8 Syntax INT|LONG|DOUBLE|FLOAT min( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be compared to find the minimum value. INT LONG DOUBLE FLOAT No Yes Examples EXAMPLE 1 from inputStream select min(temp) as minTemp insert into outputStream; min(temp) returns the minimum temp value recorded for all the events based on their arrival and expiry. minForever (Aggregate Function) This is the attribute aggregator to store the minimum value for a given attribute throughout the lifetime of the query regardless of any windows in-front. Origin: siddhi-core:5.1.8 Syntax INT|LONG|DOUBLE|FLOAT minForever( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be compared to find the minimum value. INT LONG DOUBLE FLOAT No Yes Examples EXAMPLE 1 from inputStream select minForever(temp) as max insert into outputStream; minForever(temp) returns the minimum temp value recorded for all the events throughoutthe lifetime of the query. or (Aggregate Function) Returns the results of OR operation for all the events. Origin: siddhi-core:5.1.8 Syntax BOOL or( BOOL arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be OR operation. BOOL No Yes Examples EXAMPLE 1 from cscStream#window.lengthBatch(10) select or(isFraud) as isFraudTransaction insert into alertStream; This will returns the result for OR operation of isFraud values as a boolean value for event chunk expiry by window length batch. stdDev (Aggregate Function) Returns the calculated standard deviation for all the events. Origin: siddhi-core:5.1.8 Syntax DOUBLE stdDev( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that should be used to calculate the standard deviation. INT LONG DOUBLE FLOAT No Yes Examples EXAMPLE 1 from inputStream select stddev(temp) as stdTemp insert into outputStream; stddev(temp) returns the calculated standard deviation of temp for all the events based on their arrival and expiry. sum (Aggregate Function) Returns the sum for all the events. Origin: siddhi-core:5.1.8 Syntax LONG|DOUBLE sum( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be summed. INT LONG DOUBLE FLOAT No Yes Examples EXAMPLE 1 from inputStream select sum(volume) as sumOfVolume insert into outputStream; This will returns the sum of volume values as a long value for each event arrival and expiry. unionSet (Aggregate Function) Union multiple sets. This attribute aggregator maintains a union of sets. The given input set is put into the union set and the union set is returned. Origin: siddhi-core:5.1.8 Syntax OBJECT unionSet( OBJECT set) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic set The java.util.Set object that needs to be added into the union set. OBJECT No Yes Examples EXAMPLE 1 from stockStream select createSet(symbol) as initialSet insert into initStream from initStream#window.timeBatch(10 sec) select unionSet(initialSet) as distinctSymbols insert into distinctStockStream; distinctStockStream will return the set object which contains the distinct set of stock symbols received during a sliding window of 10 seconds. UUID (Function) Generates a UUID (Universally Unique Identifier). Origin: siddhi-core:5.1.8 Syntax STRING UUID() Examples EXAMPLE 1 from TempStream select convert(roomNo, 'string') as roomNo, temp, UUID() as messageID insert into RoomTempStream; This will converts a room number to string, introducing a message ID to each event asUUID() returns a34eec40-32c2-44fe-8075-7f4fde2e2dd8 from TempStream select convert(roomNo, 'string') as roomNo, temp, UUID() as messageID insert into RoomTempStream; cast (Function) Converts the first parameter according to the cast.to parameter. Incompatible arguments cause Class Cast exceptions if further processed. This function is used with map extension that returns attributes of the object type. You can use this function to cast the object to an accurate and concrete type. Origin: siddhi-core:5.1.8 Syntax INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT cast( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT to.be.caster, STRING cast.to) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic to.be.caster This specifies the attribute to be casted. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No Yes cast.to A string constant parameter expressing the cast to type using one of the following strings values: int, long, float, double, string, bool. STRING No Yes Examples EXAMPLE 1 from fooStream select symbol as name, cast(temp, 'double') as temp insert into barStream; This will cast the fooStream temp field value into 'double' format. coalesce (Function) Returns the value of the first input parameter that is not null, and all input parameters have to be on the same type. Origin: siddhi-core:5.1.8 Syntax INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT coalesce( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg, INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT ...) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg This function accepts one or more parameters. They can belong to any one of the available types. All the specified parameters should be of the same type. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No Yes Examples EXAMPLE 1 from fooStream select coalesce('123', null, '789') as value insert into barStream; This will returns first null value 123. EXAMPLE 2 from fooStream select coalesce(null, 76, 567) as value insert into barStream; This will returns first null value 76. EXAMPLE 3 from fooStream select coalesce(null, null, null) as value insert into barStream; This will returns null as there are no notnull values. convert (Function) Converts the first input parameter according to the convertedTo parameter. Origin: siddhi-core:5.1.8 Syntax INT|LONG|DOUBLE|FLOAT|STRING|BOOL convert( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT to.be.converted, STRING converted.to) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic to.be.converted This specifies the value to be converted. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No Yes converted.to A string constant parameter to which type the attribute need to be converted using one of the following strings values: 'int', 'long', 'float', 'double', 'string', 'bool'. STRING No Yes Examples EXAMPLE 1 from fooStream select convert(temp, 'double') as temp insert into barStream; This will convert fooStream temp value into 'double'. EXAMPLE 2 from fooStream select convert(temp, 'int') as temp insert into barStream; This will convert fooStream temp value into 'int' (value = \"convert(45.9, 'int') returns 46\"). createSet (Function) Includes the given input parameter in a java.util.HashSet and returns the set. Origin: siddhi-core:5.1.8 Syntax OBJECT createSet( INT|LONG|DOUBLE|FLOAT|STRING|BOOL input) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input The input that needs to be added into the set. INT LONG DOUBLE FLOAT STRING BOOL No Yes Examples EXAMPLE 1 from stockStream select createSet(symbol) as initialSet insert into initStream; For every incoming stockStream event, the initStream stream will produce a set object having only one element: the symbol in the incoming stockStream. currentTimeMillis (Function) Returns the current timestamp of siddhi application in milliseconds. Origin: siddhi-core:5.1.8 Syntax LONG currentTimeMillis() Examples EXAMPLE 1 from fooStream select symbol as name, currentTimeMillis() as eventTimestamp insert into barStream; This will extract current siddhi application timestamp. default (Function) Checks if the 'attribute' parameter is null and if so returns the value of the 'default' parameter Origin: siddhi-core:5.1.8 Syntax INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT default( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT attribute, INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT default) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic attribute The attribute that could be null. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No Yes default The default value that will be used when 'attribute' parameter is null INT LONG DOUBLE FLOAT STRING BOOL OBJECT No Yes Examples EXAMPLE 1 from TempStream select default(temp, 0.0) as temp, roomNum insert into StandardTempStream; This will replace TempStream's temp attribute with default value if the temp is null. eventTimestamp (Function) Returns the timestamp of the processed event. Origin: siddhi-core:5.1.8 Syntax LONG eventTimestamp() Examples EXAMPLE 1 from fooStream select symbol as name, eventTimestamp() as eventTimestamp insert into barStream; This will extract current events timestamp. ifThenElse (Function) Evaluates the 'condition' parameter and returns value of the 'if.expression' parameter if the condition is true, or returns value of the 'else.expression' parameter if the condition is false. Here both 'if.expression' and 'else.expression' should be of the same type. Origin: siddhi-core:5.1.8 Syntax INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT ifThenElse( BOOL condition, INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT if.expression, INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT else.expression) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic condition This specifies the if then else condition value. BOOL No Yes if.expression This specifies the value to be returned if the value of the condition parameter is true. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No Yes else.expression This specifies the value to be returned if the value of the condition parameter is false. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No Yes Examples EXAMPLE 1 @info(name = 'query1') from sensorEventStream select sensorValue, ifThenElse(sensorValue 35,'High','Low') as status insert into outputStream; This will returns High if sensorValue = 50. EXAMPLE 2 @info(name = 'query1') from sensorEventStream select sensorValue, ifThenElse(voltage 5, 0, 1) as status insert into outputStream; This will returns 1 if voltage= 12. EXAMPLE 3 @info(name = 'query1') from userEventStream select userName, ifThenElse(password == 'admin', true, false) as passwordState insert into outputStream; This will returns passwordState as true if password = admin. instanceOfBoolean (Function) Checks whether the parameter is an instance of Boolean or not. Origin: siddhi-core:5.1.8 Syntax BOOL instanceOfBoolean( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No Yes Examples EXAMPLE 1 from fooStream select instanceOfBoolean(switchState) as state insert into barStream; This will return true if the value of switchState is true. EXAMPLE 2 from fooStream select instanceOfBoolean(value) as state insert into barStream; if the value = 32 then this will returns false as the value is not an instance of the boolean. instanceOfDouble (Function) Checks whether the parameter is an instance of Double or not. Origin: siddhi-core:5.1.8 Syntax BOOL instanceOfDouble( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No Yes Examples EXAMPLE 1 from fooStream select instanceOfDouble(value) as state insert into barStream; This will return true if the value field format is double ex : 56.45. EXAMPLE 2 from fooStream select instanceOfDouble(switchState) as state insert into barStream; if the switchState = true then this will returns false as the value is not an instance of the double. instanceOfFloat (Function) Checks whether the parameter is an instance of Float or not. Origin: siddhi-core:5.1.8 Syntax BOOL instanceOfFloat( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No Yes Examples EXAMPLE 1 from fooStream select instanceOfFloat(value) as state insert into barStream; This will return true if the value field format is float ex : 56.45f. EXAMPLE 2 from fooStream select instanceOfFloat(switchState) as state insert into barStream; if the switchState = true then this will returns false as the value is an instance of the boolean not a float. instanceOfInteger (Function) Checks whether the parameter is an instance of Integer or not. Origin: siddhi-core:5.1.8 Syntax BOOL instanceOfInteger( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No Yes Examples EXAMPLE 1 from fooStream select instanceOfInteger(value) as state insert into barStream; This will return true if the value field format is integer. EXAMPLE 2 from fooStream select instanceOfInteger(switchState) as state insert into barStream; if the switchState = true then this will returns false as the value is an instance of the boolean not a long. instanceOfLong (Function) Checks whether the parameter is an instance of Long or not. Origin: siddhi-core:5.1.8 Syntax BOOL instanceOfLong( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No Yes Examples EXAMPLE 1 from fooStream select instanceOfLong(value) as state insert into barStream; This will return true if the value field format is long ex : 56456l. EXAMPLE 2 from fooStream select instanceOfLong(switchState) as state insert into barStream; if the switchState = true then this will returns false as the value is an instance of the boolean not a long. instanceOfString (Function) Checks whether the parameter is an instance of String or not. Origin: siddhi-core:5.1.8 Syntax BOOL instanceOfString( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No Yes Examples EXAMPLE 1 from fooStream select instanceOfString(value) as state insert into barStream; This will return true if the value field format is string ex : 'test'. EXAMPLE 2 from fooStream select instanceOfString(switchState) as state insert into barStream; if the switchState = true then this will returns false as the value is an instance of the boolean not a string. maximum (Function) Returns the maximum value of the input parameters. Origin: siddhi-core:5.1.8 Syntax INT|LONG|DOUBLE|FLOAT maximum( INT|LONG|DOUBLE|FLOAT arg, INT|LONG|DOUBLE|FLOAT ...) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg This function accepts one or more parameters. They can belong to any one of the available types. All the specified parameters should be of the same type. INT LONG DOUBLE FLOAT No Yes Examples EXAMPLE 1 @info(name = 'query1') from inputStream select maximum(price1, price2, price3) as max insert into outputStream; This will returns the maximum value of the input parameters price1, price2, price3. minimum (Function) Returns the minimum value of the input parameters. Origin: siddhi-core:5.1.8 Syntax INT|LONG|DOUBLE|FLOAT minimum( INT|LONG|DOUBLE|FLOAT arg, INT|LONG|DOUBLE|FLOAT ...) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg This function accepts one or more parameters. They can belong to any one of the available types. All the specified parameters should be of the same type. INT LONG DOUBLE FLOAT No Yes Examples EXAMPLE 1 @info(name = 'query1') from inputStream select maximum(price1, price2, price3) as max insert into outputStream; This will returns the minimum value of the input parameters price1, price2, price3. sizeOfSet (Function) Returns the size of an object of type java.util.Set. Origin: siddhi-core:5.1.8 Syntax INT sizeOfSet( OBJECT set) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic set The set object. This parameter should be of type java.util.Set. A set object may be created by the 'set' attribute aggregator in Siddhi. OBJECT No Yes Examples EXAMPLE 1 from stockStream select initSet(symbol) as initialSet insert into initStream; ;from initStream#window.timeBatch(10 sec) select union(initialSet) as distinctSymbols insert into distinctStockStream; from distinctStockStream select sizeOfSet(distinctSymbols) sizeOfSymbolSet insert into sizeStream; The sizeStream stream will output the number of distinct stock symbols received during a sliding window of 10 seconds. pol2Cart (Stream Function) The pol2Cart function calculating the cartesian coordinates x y for the given theta, rho coordinates and adding them as new attributes to the existing events. Origin: siddhi-core:5.1.8 Syntax pol2Cart( DOUBLE theta, DOUBLE rho) pol2Cart( DOUBLE theta, DOUBLE rho, DOUBLE z) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic theta The theta value of the coordinates. DOUBLE No Yes rho The rho value of the coordinates. DOUBLE No Yes z z value of the cartesian coordinates. If z value is not given, drop the third parameter of the output. DOUBLE Yes Yes Examples EXAMPLE 1 from PolarStream#pol2Cart(theta, rho) select x, y insert into outputStream ; This will return cartesian coordinates (4.99953024681082, 0.06853693328228748) for theta: 0.7854 and rho: 5. EXAMPLE 2 from PolarStream#pol2Cart(theta, rho, 3.4) select x, y, z insert into outputStream ; This will return cartesian coordinates (4.99953024681082, 0.06853693328228748, 3.4)for theta: 0.7854 and rho: 5 and z: 3.4. log (Stream Processor) Logs the message on the given priority with or without the processed event. Origin: siddhi-core:5.1.8 Syntax log() log( STRING log.message) log( BOOL is.event.logged) log( STRING log.message, BOOL is.event.logged) log( STRING priority, STRING log.message) log( STRING priority, STRING log.message, BOOL is.event.logged) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic priority The priority/type of this log message (INFO, DEBUG, WARN, FATAL, ERROR, OFF, TRACE). INFO STRING Yes No log.message This message will be logged. : STRING Yes Yes is.event.logged To log the processed event. true BOOL Yes No Examples EXAMPLE 1 from FooStream#log() select * insert into BarStream; Logs events with SiddhiApp name message prefix on default log level INFO. EXAMPLE 2 from FooStream#log(\"Sample Event :\") select * insert into BarStream; Logs events with the message prefix \"Sample Event :\" on default log level INFO. EXAMPLE 3 from FooStream#log(\"DEBUG\", \"Sample Event :\", true) select * insert into BarStream; Logs events with the message prefix \"Sample Event :\" on log level DEBUG. EXAMPLE 4 from FooStream#log(\"Event Arrived\", false) select * insert into BarStream; For each event logs a message \"Event Arrived\" on default log level INFO. EXAMPLE 5 from FooStream#log(\"Sample Event :\", true) select * insert into BarStream; Logs events with the message prefix \"Sample Event :\" on default log level INFO. EXAMPLE 6 from FooStream#log(true) select * insert into BarStream; Logs events with on default log level INFO. batch (Window) A window that holds an incoming events batch. When a new set of events arrives, the previously arrived old events will be expired. Batch window can be used to aggregate events that comes in batches. If it has the parameter length specified, then batch window process the batch as several chunks. Origin: siddhi-core:5.1.8 Syntax batch() batch( INT window.length) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.length The length of a chunk If length value was not given it assign 0 as length and process the whole batch as once INT Yes No Examples EXAMPLE 1 define stream consumerItemStream (itemId string, price float) from consumerItemStream#window.batch() select price, str:groupConcat(itemId) as itemIds group by price insert into outputStream; This will output comma separated items IDs that have the same price for each incoming batch of events. cron (Window) This window outputs the arriving events as and when they arrive, and resets (expires) the window periodically based on the given cron expression. Origin: siddhi-core:5.1.8 Syntax cron( STRING cron.expression) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic cron.expression The cron expression that resets the window. STRING No No Examples EXAMPLE 1 define stream InputEventStream (symbol string, price float, volume int); @info(name = 'query1') from InputEventStream#cron('*/5 * * * * ?') select symbol, sum(price) as totalPrice insert into OutputStream; This let the totalPrice to gradually increase and resets to zero as a batch every 5 seconds. EXAMPLE 2 define stream StockEventStream (symbol string, price float, volume int) define window StockEventWindow (symbol string, price float, volume int) cron('*/5 * * * * ?'); @info(name = 'query0') from StockEventStream insert into StockEventWindow; @info(name = 'query1') from StockEventWindow select symbol, sum(price) as totalPrice insert into OutputStream ; The defined window will let the totalPrice to gradually increase and resets to zero as a batch every 5 seconds. delay (Window) A delay window holds events for a specific time period that is regarded as a delay period before processing them. Origin: siddhi-core:5.1.8 Syntax delay( INT|LONG|TIME window.delay) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.delay The time period (specified in sec, min, ms) for which the window should delay the events. INT LONG TIME No No Examples EXAMPLE 1 define window delayWindow(symbol string, volume int) delay(1 hour); define stream PurchaseStream(symbol string, volume int); define stream DeliveryStream(symbol string); define stream OutputStream(symbol string); @info(name='query1') from PurchaseStream select symbol, volume insert into delayWindow; @info(name='query2') from delayWindow join DeliveryStream on delayWindow.symbol == DeliveryStream.symbol select delayWindow.symbol insert into OutputStream; In this example, purchase events that arrive in the 'PurchaseStream' stream are directed to a delay window. At any given time, this delay window holds purchase events that have arrived within the last hour. These purchase events in the window are matched by the 'symbol' attribute, with delivery events that arrive in the 'DeliveryStream' stream. This monitors whether the delivery of products is done with a minimum delay of one hour after the purchase. externalTime (Window) A sliding time window based on external time. It holds events that arrived during the last windowTime period from the external timestamp, and gets updated on every monotonically increasing timestamp. Origin: siddhi-core:5.1.8 Syntax externalTime( LONG timestamp, INT|LONG|TIME window.time) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic timestamp The time which the window determines as current time and will act upon. The value of this parameter should be monotonically increasing. LONG No Yes window.time The sliding time period for which the window should hold events. INT LONG TIME No No Examples EXAMPLE 1 define window cseEventWindow (symbol string, price float, volume int) externalTime(eventTime, 20 sec) output expired events; @info(name = 'query0') from cseEventStream insert into cseEventWindow; @info(name = 'query1') from cseEventWindow select symbol, sum(price) as price insert expired events into outputStream ; processing events arrived within the last 20 seconds from the eventTime and output expired events. externalTimeBatch (Window) A batch (tumbling) time window based on external time, that holds events arrived during windowTime periods, and gets updated for every windowTime. Origin: siddhi-core:5.1.8 Syntax externalTimeBatch( LONG timestamp, INT|LONG|TIME window.time) externalTimeBatch( LONG timestamp, INT|LONG|TIME window.time, INT|LONG|TIME start.time) externalTimeBatch( LONG timestamp, INT|LONG|TIME window.time, INT|LONG|TIME start.time, INT|LONG|TIME timeout) externalTimeBatch( LONG timestamp, INT|LONG|TIME window.time, INT|LONG|TIME start.time, INT|LONG|TIME timeout, BOOL replace.with.batchtime) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic timestamp The time which the window determines as current time and will act upon. The value of this parameter should be monotonically increasing. LONG No Yes window.time The batch time period for which the window should hold events. INT LONG TIME No No start.time User defined start time. This could either be a constant (of type int, long or time) or an attribute of the corresponding stream (of type long). If an attribute is provided, initial value of attribute would be considered as startTime. Timestamp of first event INT LONG TIME Yes Yes timeout Time to wait for arrival of new event, before flushing and giving output for events belonging to a specific batch. System waits till an event from next batch arrives to flush current batch INT LONG TIME Yes No replace.with.batchtime This indicates to replace the expired event timeStamp as the batch end timeStamp System waits till an event from next batch arrives to flush current batch BOOL Yes No Examples EXAMPLE 1 define window cseEventWindow (symbol string, price float, volume int) externalTimeBatch(eventTime, 1 sec) output expired events; @info(name = 'query0') from cseEventStream insert into cseEventWindow; @info(name = 'query1') from cseEventWindow select symbol, sum(price) as price insert expired events into outputStream ; This will processing events that arrive every 1 seconds from the eventTime. EXAMPLE 2 define window cseEventWindow (symbol string, price float, volume int) externalTimeBatch(eventTime, 20 sec, 0) output expired events; This will processing events that arrive every 1 seconds from the eventTime. Starts on 0 th millisecond of an hour. EXAMPLE 3 define window cseEventWindow (symbol string, price float, volume int) externalTimeBatch(eventTime, 2 sec, eventTimestamp, 100) output expired events; This will processing events that arrive every 2 seconds from the eventTim. Considers the first event's eventTimestamp value as startTime. Waits 100 milliseconds for the arrival of a new event before flushing current batch. frequent (Window) Deprecated This window returns the latest events with the most frequently occurred value for a given attribute(s). Frequency calculation for this window processor is based on Misra-Gries counting algorithm. Origin: siddhi-core:5.1.8 Syntax frequent( INT event.count) frequent( INT event.count, STRING attribute) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic event.count The number of most frequent events to be emitted to the stream. INT No No attribute The attributes to group the events. If no attributes are given, the concatenation of all the attributes of the event is considered. The concatenation of all the attributes of the event is considered. STRING Yes Yes Examples EXAMPLE 1 @info(name = 'query1') from purchase[price = 30]#window.frequent(2) select cardNo, price insert all events into PotentialFraud; This will returns the 2 most frequent events. EXAMPLE 2 @info(name = 'query1') from purchase[price = 30]#window.frequent(2, cardNo) select cardNo, price insert all events into PotentialFraud; This will returns the 2 latest events with the most frequently appeared card numbers. length (Window) A sliding length window that holds the last 'window.length' events at a given time, and gets updated for each arrival and expiry. Origin: siddhi-core:5.1.8 Syntax length( INT window.length) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.length The number of events that should be included in a sliding length window. INT No No Examples EXAMPLE 1 define window StockEventWindow (symbol string, price float, volume int) length(10) output all events; @info(name = 'query0') from StockEventStream insert into StockEventWindow; @info(name = 'query1') from StockEventWindow select symbol, sum(price) as price insert all events into outputStream ; This will process last 10 events in a sliding manner. lengthBatch (Window) A batch (tumbling) length window that holds and process a number of events as specified in the window.length. Origin: siddhi-core:5.1.8 Syntax lengthBatch( INT window.length) lengthBatch( INT window.length, BOOL stream.current.event) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.length The number of events the window should tumble. INT No No stream.current.event Let the window stream the current events out as and when they arrive to the window while expiring them in batches. false BOOL Yes No Examples EXAMPLE 1 define stream InputEventStream (symbol string, price float, volume int); @info(name = 'query1') from InputEventStream#lengthBatch(10) select symbol, sum(price) as price insert into OutputStream; This collect and process 10 events as a batch and output them. EXAMPLE 2 define stream InputEventStream (symbol string, price float, volume int); @info(name = 'query1') from InputEventStream#lengthBatch(10, true) select symbol, sum(price) as sumPrice insert into OutputStream; This window sends the arriving events directly to the output letting the sumPrice to increase gradually, after every 10 events it clears the window as a batch and resets the sumPrice to zero. EXAMPLE 3 define stream InputEventStream (symbol string, price float, volume int); define window StockEventWindow (symbol string, price float, volume int) lengthBatch(10) output all events; @info(name = 'query0') from InputEventStream insert into StockEventWindow; @info(name = 'query1') from StockEventWindow select symbol, sum(price) as price insert all events into OutputStream ; This uses an defined window to process 10 events as a batch and output all events. lossyFrequent (Window) Deprecated This window identifies and returns all the events of which the current frequency exceeds the value specified for the supportThreshold parameter. Origin: siddhi-core:5.1.8 Syntax lossyFrequent( DOUBLE support.threshold) lossyFrequent( DOUBLE support.threshold, DOUBLE error.bound) lossyFrequent( DOUBLE support.threshold, DOUBLE error.bound, STRING attribute) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic support.threshold The support threshold value. DOUBLE No No error.bound The error bound value. support.threshold /10 DOUBLE Yes No attribute The attributes to group the events. If no attributes are given, the concatenation of all the attributes of the event is considered. The concatenation of all the attributes of the event is considered. STRING Yes Yes Examples EXAMPLE 1 define stream purchase (cardNo string, price float); define window purchaseWindow (cardNo string, price float) lossyFrequent(0.1, 0.01); @info(name = 'query0') from purchase[price = 30] insert into purchaseWindow; @info(name = 'query1') from purchaseWindow select cardNo, price insert all events into PotentialFraud; lossyFrequent(0.1, 0.01) returns all the events of which the current frequency exceeds 0.1, with an error bound of 0.01. EXAMPLE 2 define stream purchase (cardNo string, price float); define window purchaseWindow (cardNo string, price float) lossyFrequent(0.3, 0.05, cardNo); @info(name = 'query0') from purchase[price = 30] insert into purchaseWindow; @info(name = 'query1') from purchaseWindow select cardNo, price insert all events into PotentialFraud; lossyFrequent(0.3, 0.05, cardNo) returns all the events of which the cardNo attributes frequency exceeds 0.3, with an error bound of 0.05. session (Window) This is a session window that holds events that belong to a specific session. The events that belong to a specific session are identified by a grouping attribute (i.e., a session key). A session gap period is specified to determine the time period after which the session is considered to be expired. A new event that arrives with a specific value for the session key is matched with the session window with the same session key. There can be out of order and late arrival of events, these events can arrive after the session is expired, to include those events to the matching session key specify a latency time period that is less than the session gap period.To have aggregate functions with session windows, the events need to be grouped by the session key via a 'group by' clause. Origin: siddhi-core:5.1.8 Syntax session( INT|LONG|TIME window.session) session( INT|LONG|TIME window.session, STRING window.key) session( INT|LONG|TIME window.session, STRING window.key, INT|LONG|TIME window.allowed.latency) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.session The time period for which the session considered is valid. This is specified in seconds, minutes, or milliseconds (i.e., 'min', 'sec', or 'ms'. INT LONG TIME No No window.key The grouping attribute for events. default-key STRING Yes Yes window.allowed.latency This specifies the time period for which the session window is valid after the expiration of the session. The time period specified here should be less than the session time gap (which is specified via the 'window.session' parameter). 0 INT LONG TIME Yes No Examples EXAMPLE 1 define stream PurchaseEventStream (user string, item_number int, price float, quantity int); @info(name='query0) from PurchaseEventStream#window.session(5 sec, user, 2 sec) select * insert all events into OutputStream; This query processes events that arrive at the PurchaseEvent input stream. The 'user' attribute is the session key, and the session gap is 5 seconds. '2 sec' is specified as the allowed latency. Therefore, events with the matching user name that arrive 2 seconds after the expiration of the session are also considered when performing aggregations for the session identified by the given user name. sort (Window) This window holds a batch of events that equal the number specified as the windowLength and sorts them in the given order. Origin: siddhi-core:5.1.8 Syntax sort( INT window.length, STRING|DOUBLE|INT|LONG|FLOAT|LONG attribute) sort( INT window.length, STRING|DOUBLE|INT|LONG|FLOAT|LONG attribute, STRING order, STRING ...) sort( INT window.length, STRING|DOUBLE|INT|LONG|FLOAT|LONG attribute, STRING order, STRING|DOUBLE|INT|LONG|FLOAT|LONG attribute, STRING|DOUBLE|INT|LONG|FLOAT|LONG ...) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.length The size of the window length. INT No No attribute The attribute that should be checked for the order. The concatenation of all the attributes of the event is considered. STRING DOUBLE INT LONG FLOAT LONG No Yes order The order define as \"asc\" or \"desc\". asc STRING Yes No Examples EXAMPLE 1 define stream cseEventStream (symbol string, price float, volume long); define window cseEventWindow (symbol string, price float, volume long) sort(2,volume, 'asc'); @info(name = 'query0') from cseEventStream insert into cseEventWindow; @info(name = 'query1') from cseEventWindow select volume insert all events into outputStream ; sort(5, price, 'asc') keeps the events sorted by price in the ascending order. Therefore, at any given time, the window contains the 5 lowest prices. time (Window) A sliding time window that holds events that arrived during the last windowTime period at a given time, and gets updated for each event arrival and expiry. Origin: siddhi-core:5.1.8 Syntax time( INT|LONG|TIME window.time) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.time The sliding time period for which the window should hold events. INT LONG TIME No No Examples EXAMPLE 1 define window cseEventWindow (symbol string, price float, volume int) time(20) output all events; @info(name = 'query0') from cseEventStream insert into cseEventWindow; @info(name = 'query1') from cseEventWindow select symbol, sum(price) as price insert all events into outputStream ; This will processing events that arrived within the last 20 milliseconds. timeBatch (Window) A batch (tumbling) time window that holds and process events that arrive during 'window.time' period as a batch. Origin: siddhi-core:5.1.8 Syntax timeBatch( INT|LONG|TIME window.time) timeBatch( INT|LONG|TIME window.time, INT|LONG start.time) timeBatch( INT|LONG|TIME window.time, BOOL stream.current.event) timeBatch( INT|LONG|TIME window.time, INT|LONG start.time, BOOL stream.current.event) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.time The batch time period in which the window process the events. INT LONG TIME No No start.time This specifies an offset in milliseconds in order to start the window at a time different to the standard time. Timestamp of first event INT LONG Yes No stream.current.event Let the window stream the current events out as and when they arrive to the window while expiring them in batches. false BOOL Yes No Examples EXAMPLE 1 define stream InputEventStream (symbol string, price float, volume int); @info(name = 'query1') from InputEventStream#timeBatch(20 sec) select symbol, sum(price) as price insert into OutputStream; This collect and process incoming events as a batch every 20 seconds and output them. EXAMPLE 2 define stream InputEventStream (symbol string, price float, volume int); @info(name = 'query1') from InputEventStream#timeBatch(20 sec, true) select symbol, sum(price) as sumPrice insert into OutputStream; This window sends the arriving events directly to the output letting the sumPrice to increase gradually and on every 20 second interval it clears the window as a batch resetting the sumPrice to zero. EXAMPLE 3 define stream InputEventStream (symbol string, price float, volume int); define window StockEventWindow (symbol string, price float, volume int) timeBatch(20 sec) output all events; @info(name = 'query0') from InputEventStream insert into StockEventWindow; @info(name = 'query1') from StockEventWindow select symbol, sum(price) as price insert all events into OutputStream ; This uses an defined window to process events arrived every 20 seconds as a batch and output all events. timeLength (Window) A sliding time window that, at a given time holds the last window.length events that arrived during last window.time period, and gets updated for every event arrival and expiry. Origin: siddhi-core:5.1.8 Syntax timeLength( INT|LONG|TIME window.time, INT window.length) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.time The sliding time period for which the window should hold events. INT LONG TIME No No window.length The number of events that should be be included in a sliding length window.. INT No No Examples EXAMPLE 1 define stream cseEventStream (symbol string, price float, volume int); define window cseEventWindow (symbol string, price float, volume int) timeLength(2 sec, 10); @info(name = 'query0') from cseEventStream insert into cseEventWindow; @info(name = 'query1') from cseEventWindow select symbol, price, volume insert all events into outputStream; window.timeLength(2 sec, 10) holds the last 10 events that arrived during last 2 seconds and gets updated for every event arrival and expiry. Js eval (Function) This extension evaluates a given string and return the output according to the user specified data type. Origin: siddhi-script-js:5.0.2 Syntax INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT js:eval( STRING expression, STRING return.type) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic expression Any single line js expression or function. STRING No Yes return.type The return type of the evaluated expression. Supported types are int|long|float|double|bool|string. STRING No No Examples EXAMPLE 1 js:eval(\"700 800\", 'bool') In this example, the expression 700 800 will be evaluated and return result as false because user specified return type as bool. Json group (Aggregate Function) This function aggregates the JSON elements and returns a JSON object by adding enclosing.element if it is provided. If enclosing.element is not provided it aggregate the JSON elements returns a JSON array. Origin: siddhi-execution-json:2.0.4 Syntax OBJECT json:group( STRING|OBJECT json) OBJECT json:group( STRING|OBJECT json, BOOL distinct) OBJECT json:group( STRING|OBJECT json, STRING enclosing.element) OBJECT json:group( STRING|OBJECT json, STRING enclosing.element, BOOL distinct) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json The JSON element that needs to be aggregated. STRING OBJECT No Yes enclosing.element The JSON element used to enclose the aggregated JSON elements. EMPTY_STRING STRING Yes Yes distinct This is used to only have distinct JSON elements in the concatenated JSON object/array that is returned. false BOOL Yes Yes Examples EXAMPLE 1 from InputStream#window.length(5) select json:group(\"json\") as groupedJSONArray input OutputStream; When we input events having values for the json as {\"date\":\"2013-11-19\",\"time\":\"10:30\"} and {\"date\":\"2013-11-19\",\"time\":\"12:20\"} , it returns [{\"date\":\"2013-11-19\",\"time\":\"10:30\"}{\"date\":\"2013-11-19\",\"time\":\"12:20\"}] to the 'OutputStream'. EXAMPLE 2 from InputStream#window.length(5) select json:group(\"json\", true) as groupedJSONArray input OutputStream; When we input events having values for the json as {\"date\":\"2013-11-19\",\"time\":\"10:30\"} and {\"date\":\"2013-11-19\",\"time\":\"10:30\"} , it returns [{\"date\":\"2013-11-19\",\"time\":\"10:30\"}] to the 'OutputStream'. EXAMPLE 3 from InputStream#window.length(5) select json:group(\"json\", \"result\") as groupedJSONArray input OutputStream; When we input events having values for the json as {\"date\":\"2013-11-19\",\"time\":\"10:30\"} and {\"date\":\"2013-11-19\",\"time\":\"12:20\"} , it returns {\"result\":[{\"date\":\"2013-11-19\",\"time\":\"10:30\"},{\"date\":\"2013-11-19\",\"time\":\"12:20\"}} to the 'OutputStream'. EXAMPLE 4 from InputStream#window.length(5) select json:group(\"json\", \"result\", true) as groupedJSONArray input OutputStream; When we input events having values for the json as {\"date\":\"2013-11-19\",\"time\":\"10:30\"} and {\"date\":\"2013-11-19\",\"time\":\"10:30\"} , it returns {\"result\":[{\"date\":\"2013-11-19\",\"time\":\"10:30\"}]} to the 'OutputStream'. groupAsObject (Aggregate Function) This function aggregates the JSON elements and returns a JSON object by adding enclosing.element if it is provided. If enclosing.element is not provided it aggregate the JSON elements returns a JSON array. Origin: siddhi-execution-json:2.0.4 Syntax OBJECT json:groupAsObject( STRING|OBJECT json) OBJECT json:groupAsObject( STRING|OBJECT json, BOOL distinct) OBJECT json:groupAsObject( STRING|OBJECT json, STRING enclosing.element) OBJECT json:groupAsObject( STRING|OBJECT json, STRING enclosing.element, BOOL distinct) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json The JSON element that needs to be aggregated. STRING OBJECT No Yes enclosing.element The JSON element used to enclose the aggregated JSON elements. EMPTY_STRING STRING Yes Yes distinct This is used to only have distinct JSON elements in the concatenated JSON object/array that is returned. false BOOL Yes Yes Examples EXAMPLE 1 from InputStream#window.length(5) select json:groupAsObject(\"json\") as groupedJSONArray input OutputStream; When we input events having values for the json as {\"date\":\"2013-11-19\",\"time\":\"10:30\"} and {\"date\":\"2013-11-19\",\"time\":\"12:20\"} , it returns [{\"date\":\"2013-11-19\",\"time\":\"10:30\"}{\"date\":\"2013-11-19\",\"time\":\"12:20\"}] to the 'OutputStream'. EXAMPLE 2 from InputStream#window.length(5) select json:groupAsObject(\"json\", true) as groupedJSONArray input OutputStream; When we input events having values for the json as {\"date\":\"2013-11-19\",\"time\":\"10:30\"} and {\"date\":\"2013-11-19\",\"time\":\"10:30\"} , it returns [{\"date\":\"2013-11-19\",\"time\":\"10:30\"}] to the 'OutputStream'. EXAMPLE 3 from InputStream#window.length(5) select json:groupAsObject(\"json\", \"result\") as groupedJSONArray input OutputStream; When we input events having values for the json as {\"date\":\"2013-11-19\",\"time\":\"10:30\"} and {\"date\":\"2013-11-19\",\"time\":\"12:20\"} , it returns {\"result\":[{\"date\":\"2013-11-19\",\"time\":\"10:30\"},{\"date\":\"2013-11-19\",\"time\":\"12:20\"}} to the 'OutputStream'. EXAMPLE 4 from InputStream#window.length(5) select json:groupAsObject(\"json\", \"result\", true) as groupedJSONArray input OutputStream; When we input events having values for the json as {\"date\":\"2013-11-19\",\"time\":\"10:30\"} and {\"date\":\"2013-11-19\",\"time\":\"10:30\"} , it returns {\"result\":[{\"date\":\"2013-11-19\",\"time\":\"10:30\"}]} to the 'OutputStream'. getBool (Function) Function retrieves the 'boolean' value specified in the given path of the JSON element. Origin: siddhi-execution-json:2.0.4 Syntax BOOL json:getBool( STRING|OBJECT json, STRING path) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json The JSON input containing boolean value. STRING OBJECT No Yes path The JSON path to fetch the boolean value. STRING No Yes Examples EXAMPLE 1 json:getBool(json,'$.married') If the json is the format {'name' : 'John', 'married' : true} , the function returns true as there is a matching boolean at .married /code . /p p /p span id=\"example-2\" class=\"md-typeset\" style=\"display: block; color: rgba(0, 0, 0, 0.54); font-size: 12.8px; font-weight: bold;\" EXAMPLE 2 /span json:getBool(json,'$.name') p /p p style=\"word-wrap: break-word;margin: 0;\" If the code json /code is the format code {'name' : 'John', 'married' : true} /code , the function returns code null /code as there is no matching boolean at code .married</code>.</p> <p></p> <span id=\"example-2\" class=\"md-typeset\" style=\"display: block; color: rgba(0, 0, 0, 0.54); font-size: 12.8px; font-weight: bold;\">EXAMPLE 2</span> <pre class=\"codehilite\"><code>json:getBool(json,'$.name')</code></pre> <p></p> <p style=\"word-wrap: break-word;margin: 0;\">If the <code>json</code> is the format <code>{'name' : 'John', 'married' : true}</code>, the function returns <code>null</code> as there is no matching boolean at <code> .name . EXAMPLE 3 json:getBool(json,'$.foo') If the json is the format {'name' : 'John', 'married' : true} , the function returns null as there is no matching element at $.foo . getDouble (Function) Function retrieves the 'double' value specified in the given path of the JSON element. Origin: siddhi-execution-json:2.0.4 Syntax DOUBLE json:getDouble( STRING|OBJECT json, STRING path) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json The JSON input containing double value. STRING OBJECT No Yes path The JSON path to fetch the double value. STRING No Yes Examples EXAMPLE 1 json:getDouble(json,'$.salary') If the json is the format {'name' : 'John', 'salary' : 12000.0} , the function returns 12000.0 as there is a matching double at .salary /code . /p p /p span id=\"example-2\" class=\"md-typeset\" style=\"display: block; color: rgba(0, 0, 0, 0.54); font-size: 12.8px; font-weight: bold;\" EXAMPLE 2 /span json:getDouble(json,'$.salary') p /p p style=\"word-wrap: break-word;margin: 0;\" If the code json /code is the format code {'name' : 'John', 'age' : 23} /code , the function returns code null /code as there are no matching element at code .salary</code>.</p> <p></p> <span id=\"example-2\" class=\"md-typeset\" style=\"display: block; color: rgba(0, 0, 0, 0.54); font-size: 12.8px; font-weight: bold;\">EXAMPLE 2</span> <pre class=\"codehilite\"><code>json:getDouble(json,'$.salary')</code></pre> <p></p> <p style=\"word-wrap: break-word;margin: 0;\">If the <code>json</code> is the format <code>{'name' : 'John', 'age' : 23}</code>, the function returns <code>null</code> as there are no matching element at <code> .salary . EXAMPLE 3 json:getDouble(json,'$.name') If the json is the format {'name' : 'John', 'age' : 23} , the function returns null as there are no matching double at $.name . getFloat (Function) Function retrieves the 'float' value specified in the given path of the JSON element. Origin: siddhi-execution-json:2.0.4 Syntax FLOAT json:getFloat( STRING|OBJECT json, STRING path) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json The JSON input containing float value. STRING OBJECT No Yes path The JSON path to fetch the float value. STRING No Yes Examples EXAMPLE 1 json:getFloat(json,'$.salary') If the json is the format {'name' : 'John', 'salary' : 12000.0} , the function returns 12000 as there is a matching float at .salary /code . /p p /p span id=\"example-2\" class=\"md-typeset\" style=\"display: block; color: rgba(0, 0, 0, 0.54); font-size: 12.8px; font-weight: bold;\" EXAMPLE 2 /span json:getFloat(json,'$.salary') p /p p style=\"word-wrap: break-word;margin: 0;\" If the code json /code is the format code {'name' : 'John', 'age' : 23} /code , the function returns code null /code as there are no matching element at code .salary</code>.</p> <p></p> <span id=\"example-2\" class=\"md-typeset\" style=\"display: block; color: rgba(0, 0, 0, 0.54); font-size: 12.8px; font-weight: bold;\">EXAMPLE 2</span> <pre class=\"codehilite\"><code>json:getFloat(json,'$.salary')</code></pre> <p></p> <p style=\"word-wrap: break-word;margin: 0;\">If the <code>json</code> is the format <code>{'name' : 'John', 'age' : 23}</code>, the function returns <code>null</code> as there are no matching element at <code> .salary . EXAMPLE 3 json:getFloat(json,'$.name') If the json is the format {'name' : 'John', 'age' : 23} , the function returns null as there are no matching float at $.name . getInt (Function) Function retrieves the 'int' value specified in the given path of the JSON element. Origin: siddhi-execution-json:2.0.4 Syntax INT json:getInt( STRING|OBJECT json, STRING path) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json The JSON input containing int value. STRING OBJECT No Yes path The JSON path to fetch the int value. STRING No Yes Examples EXAMPLE 1 json:getInt(json,'$.age') If the json is the format {'name' : 'John', 'age' : 23} , the function returns 23 as there is a matching int at .age /code . /p p /p span id=\"example-2\" class=\"md-typeset\" style=\"display: block; color: rgba(0, 0, 0, 0.54); font-size: 12.8px; font-weight: bold;\" EXAMPLE 2 /span json:getInt(json,'$.salary') p /p p style=\"word-wrap: break-word;margin: 0;\" If the code json /code is the format code {'name' : 'John', 'age' : 23} /code , the function returns code null /code as there are no matching element at code .age</code>.</p> <p></p> <span id=\"example-2\" class=\"md-typeset\" style=\"display: block; color: rgba(0, 0, 0, 0.54); font-size: 12.8px; font-weight: bold;\">EXAMPLE 2</span> <pre class=\"codehilite\"><code>json:getInt(json,'$.salary')</code></pre> <p></p> <p style=\"word-wrap: break-word;margin: 0;\">If the <code>json</code> is the format <code>{'name' : 'John', 'age' : 23}</code>, the function returns <code>null</code> as there are no matching element at <code> .salary . EXAMPLE 3 json:getInt(json,'$.name') If the json is the format {'name' : 'John', 'age' : 23} , the function returns null as there are no matching int at $.name . getLong (Function) Function retrieves the 'long' value specified in the given path of the JSON element. Origin: siddhi-execution-json:2.0.4 Syntax LONG json:getLong( STRING|OBJECT json, STRING path) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json The JSON input containing long value. STRING OBJECT No Yes path The JSON path to fetch the long value. STRING No Yes Examples EXAMPLE 1 json:getLong(json,'$.age') If the json is the format {'name' : 'John', 'age' : 23} , the function returns 23 as there is a matching long at .age /code . /p p /p span id=\"example-2\" class=\"md-typeset\" style=\"display: block; color: rgba(0, 0, 0, 0.54); font-size: 12.8px; font-weight: bold;\" EXAMPLE 2 /span json:getLong(json,'$.salary') p /p p style=\"word-wrap: break-word;margin: 0;\" If the code json /code is the format code {'name' : 'John', 'age' : 23} /code , the function returns code null /code as there are no matching element at code .age</code>.</p> <p></p> <span id=\"example-2\" class=\"md-typeset\" style=\"display: block; color: rgba(0, 0, 0, 0.54); font-size: 12.8px; font-weight: bold;\">EXAMPLE 2</span> <pre class=\"codehilite\"><code>json:getLong(json,'$.salary')</code></pre> <p></p> <p style=\"word-wrap: break-word;margin: 0;\">If the <code>json</code> is the format <code>{'name' : 'John', 'age' : 23}</code>, the function returns <code>null</code> as there are no matching element at <code> .salary . EXAMPLE 3 json:getLong(json,'$.name') If the json is the format {'name' : 'John', 'age' : 23} , the function returns null as there are no matching long at $.name . getObject (Function) Function retrieves the object specified in the given path of the JSON element. Origin: siddhi-execution-json:2.0.4 Syntax OBJECT json:getObject( STRING|OBJECT json, STRING path) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json The JSON input containing the object. STRING OBJECT No Yes path The JSON path to fetch the object. STRING No Yes Examples EXAMPLE 1 json:getObject(json,'$.address') If the json is the format {'name' : 'John', 'address' : {'city' : 'NY', 'country' : 'USA'}} , the function returns {'city' : 'NY', 'country' : 'USA'} as there is a matching object at .address /code . /p p /p span id=\"example-2\" class=\"md-typeset\" style=\"display: block; color: rgba(0, 0, 0, 0.54); font-size: 12.8px; font-weight: bold;\" EXAMPLE 2 /span json:getObject(json,'$.age') p /p p style=\"word-wrap: break-word;margin: 0;\" If the code json /code is the format code {'name' : 'John', 'age' : 23} /code , the function returns code 23 /code as there is a matching object at code .address</code>.</p> <p></p> <span id=\"example-2\" class=\"md-typeset\" style=\"display: block; color: rgba(0, 0, 0, 0.54); font-size: 12.8px; font-weight: bold;\">EXAMPLE 2</span> <pre class=\"codehilite\"><code>json:getObject(json,'$.age')</code></pre> <p></p> <p style=\"word-wrap: break-word;margin: 0;\">If the <code>json</code> is the format <code>{'name' : 'John', 'age' : 23}</code>, the function returns <code>23</code> as there is a matching object at <code> .age . EXAMPLE 3 json:getObject(json,'$.salary') If the json is the format {'name' : 'John', 'age' : 23} , the function returns null as there are no matching element at $.salary . getString (Function) Function retrieves value specified in the given path of the JSON element as a string. Origin: siddhi-execution-json:2.0.4 Syntax STRING json:getString( STRING|OBJECT json, STRING path) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json The JSON input containing value. STRING OBJECT No Yes path The JSON path to fetch the value. STRING No Yes Examples EXAMPLE 1 json:getString(json,'$.name') If the json is the format {'name' : 'John', 'age' : 23} , the function returns John as there is a matching string at .name /code . /p p /p span id=\"example-2\" class=\"md-typeset\" style=\"display: block; color: rgba(0, 0, 0, 0.54); font-size: 12.8px; font-weight: bold;\" EXAMPLE 2 /span json:getString(json,'$.salary') p /p p style=\"word-wrap: break-word;margin: 0;\" If the code json /code is the format code {'name' : 'John', 'age' : 23} /code , the function returns code null /code as there are no matching element at code .name</code>.</p> <p></p> <span id=\"example-2\" class=\"md-typeset\" style=\"display: block; color: rgba(0, 0, 0, 0.54); font-size: 12.8px; font-weight: bold;\">EXAMPLE 2</span> <pre class=\"codehilite\"><code>json:getString(json,'$.salary')</code></pre> <p></p> <p style=\"word-wrap: break-word;margin: 0;\">If the <code>json</code> is the format <code>{'name' : 'John', 'age' : 23}</code>, the function returns <code>null</code> as there are no matching element at <code> .salary . EXAMPLE 3 json:getString(json,'$.age') If the json is the format {'name' : 'John', 'age' : 23} , the function returns 23 as a string as there is a matching element at .age /code . /p p /p span id=\"example-4\" class=\"md-typeset\" style=\"display: block; color: rgba(0, 0, 0, 0.54); font-size: 12.8px; font-weight: bold;\" EXAMPLE 4 /span json:getString(json,'$.address') p /p p style=\"word-wrap: break-word;margin: 0;\" If the code json /code is the format code {'name' : 'John', 'address' : {'city' : 'NY', 'country' : 'USA'}} /code , the function returns code {'city' : 'NY', 'country' : 'USA'} /code as a string as there is a matching element at code .age</code>.</p> <p></p> <span id=\"example-4\" class=\"md-typeset\" style=\"display: block; color: rgba(0, 0, 0, 0.54); font-size: 12.8px; font-weight: bold;\">EXAMPLE 4</span> <pre class=\"codehilite\"><code>json:getString(json,'$.address')</code></pre> <p></p> <p style=\"word-wrap: break-word;margin: 0;\">If the <code>json</code> is the format <code>{'name' : 'John', 'address' : {'city' : 'NY', 'country' : 'USA'}}</code>, the function returns <code>{'city' : 'NY', 'country' : 'USA'}</code> as a string as there is a matching element at <code> .address . isExists (Function) Function checks whether there is a JSON element present in the given path or not. Origin: siddhi-execution-json:2.0.4 Syntax BOOL json:isExists( STRING|OBJECT json, STRING path) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json The JSON input that needs to be searched for an elements. STRING OBJECT No Yes path The JSON path to check for the element. STRING No Yes Examples EXAMPLE 1 json:isExists(json, '$.name') If the json is the format {'name' : 'John', 'age' : 23} , the function returns true as there is an element in the given path. EXAMPLE 2 json:isExists(json, '$.salary') If the json is the format {'name' : 'John', 'age' : 23} , the function returns false as there is no element in the given path. setElement (Function) Function sets JSON element into a given JSON at the specific path. Origin: siddhi-execution-json:2.0.4 Syntax OBJECT json:setElement( STRING|OBJECT json, STRING path, STRING|BOOL|DOUBLE|FLOAT|INT|LONG|OBJECT json.element) OBJECT json:setElement( STRING|OBJECT json, STRING path, STRING|BOOL|DOUBLE|FLOAT|INT|LONG|OBJECT json.element, STRING key) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json The JSON to which a JSON element needs to be added/replaced. STRING OBJECT No Yes path The JSON path where the JSON element should be added/replaced. STRING No Yes json.element The JSON element being added. STRING BOOL DOUBLE FLOAT INT LONG OBJECT No Yes key The key to be used to refer the newly added element in the input JSON. Assumes the element is added to a JSON array, or the element selected by the JSON path will be updated. STRING Yes Yes Examples EXAMPLE 1 json:setElement(json, '$', \"{'country' : 'USA'}\", 'address') If the json is the format {'name' : 'John', 'married' : true} ,the function updates the json as {'name' : 'John', 'married' : true, 'address' : {'country' : 'USA'}} by adding 'address' element and returns the updated JSON. EXAMPLE 2 json:setElement(json, '$', 40, 'age') If the json is the format {'name' : 'John', 'married' : true} ,the function updates the json as {'name' : 'John', 'married' : true, 'age' : 40} by adding 'age' element and returns the updated JSON. EXAMPLE 3 json:setElement(json, '$', 45, 'age') If the json is the format {'name' : 'John', 'married' : true, 'age' : 40} , the function updates the json as {'name' : 'John', 'married' : true, 'age' : 45} by replacing 'age' element and returns the updated JSON. EXAMPLE 4 json:setElement(json, '$.items', 'book') If the json is the format {'name' : 'Stationary', 'items' : ['pen', 'pencil']} , the function updates the json as {'name' : 'John', 'items' : ['pen', 'pencil', 'book']} by adding 'book' in the items array and returns the updated JSON. EXAMPLE 5 json:setElement(json, '$.item', 'book') If the json is the format {'name' : 'Stationary', 'item' : 'pen'} , the function updates the json as {'name' : 'John', 'item' : 'book'} by replacing 'item' element and returns the updated JSON. EXAMPLE 6 json:setElement(json, '$.address', 'city', 'SF') If the json is the format {'name' : 'John', 'married' : true} ,the function will not update, but returns the original JSON as there are no valid path for $.address . toObject (Function) Function generate JSON object from the given JSON string. Origin: siddhi-execution-json:2.0.4 Syntax OBJECT json:toObject( STRING json) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json A valid JSON string that needs to be converted to a JSON object. STRING No Yes Examples EXAMPLE 1 json:toJson(json) This returns the JSON object corresponding to the given JSON string. toString (Function) Function generates a JSON string corresponding to a given JSON object. Origin: siddhi-execution-json:2.0.4 Syntax STRING json:toString( OBJECT json) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json A valid JSON object to generates a JSON string. OBJECT No Yes Examples EXAMPLE 1 json:toString(json) This returns the JSON string corresponding to a given JSON object. tokenize (Stream Processor) Stream processor tokenizes the given JSON into to multiple JSON string elements and sends them as separate events. Origin: siddhi-execution-json:2.0.4 Syntax json:tokenize( STRING|OBJECT json, STRING path) json:tokenize( STRING|OBJECT json, STRING path, BOOL fail.on.missing.attribute) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json The input JSON that needs to be tokenized. STRING OBJECT No Yes path The path of the set of elements that will be tokenized. STRING No Yes fail.on.missing.attribute If there are no element on the given path, when set to true the system will drop the event, and when set to false the system will pass 'null' value to the jsonElement output attribute. true BOOL Yes No Extra Return Attributes Name Description Possible Types jsonElement The JSON element retrieved based on the given path will be returned as a JSON string. If the 'path' selects a JSON array then the system returns each element in the array as a JSON string via a separate events. STRING Examples EXAMPLE 1 define stream InputStream (json string, path string); @info(name = 'query1') from InputStream#json:tokenizeAsObject(json, path) select path, jsonElement insert into OutputStream; If the input 'json' is {name:'John', enrolledSubjects:['Mathematics', 'Physics']} , and the 'path' is passed as .enrolledSubjects /code then for both the elements in the selected JSON array, it generates it generates events as code (' .enrolledSubjects</code> then for both the elements in the selected JSON array, it generates it generates events as <code>(' .enrolledSubjects', 'Mathematics') , and (' .enrolledSubjects', 'Physics') /code . br For the same input JSON, if the 'path' is passed as code .enrolledSubjects', 'Physics')</code>.<br>For the same input JSON, if the 'path' is passed as <code> .name then it will only produce one event (' .name', 'John') /code as the 'path' provided a single JSON element. /p p /p span id=\"example-2\" class=\"md-typeset\" style=\"display: block; color: rgba(0, 0, 0, 0.54); font-size: 12.8px; font-weight: bold;\" EXAMPLE 2 /span define stream InputStream (json string, path string); @info(name = 'query1') from InputStream#json:tokenizeAsObject(json, path, true) select path, jsonElement insert into OutputStream; p /p p style=\"word-wrap: break-word;margin: 0;\" If the input 'json' is code {name:'John', age:25} /code ,and the 'path' is passed as code .name', 'John')</code> as the 'path' provided a single JSON element.</p> <p></p> <span id=\"example-2\" class=\"md-typeset\" style=\"display: block; color: rgba(0, 0, 0, 0.54); font-size: 12.8px; font-weight: bold;\">EXAMPLE 2</span> <pre class=\"codehilite\"><code>define stream InputStream (json string, path string); @info(name = 'query1') from InputStream#json:tokenizeAsObject(json, path, true) select path, jsonElement insert into OutputStream;</code></pre> <p></p> <p style=\"word-wrap: break-word;margin: 0;\">If the input 'json' is <code>{name:'John', age:25}</code>,and the 'path' is passed as <code> .salary then the system will produce (' .salary', null) /code , as the 'fail.on.missing.attribute' is code true /code and there are no matching element for code .salary', null)</code>, as the 'fail.on.missing.attribute' is <code>true</code> and there are no matching element for <code> .salary . tokenizeAsObject (Stream Processor) Stream processor tokenizes the given JSON into to multiple JSON object elements and sends them as separate events. Origin: siddhi-execution-json:2.0.4 Syntax json:tokenizeAsObject( STRING|OBJECT json, STRING path) json:tokenizeAsObject( STRING|OBJECT json, STRING path, BOOL fail.on.missing.attribute) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json The input JSON that needs to be tokenized. STRING OBJECT No Yes path The path of the set of elements that will be tokenized. STRING No Yes fail.on.missing.attribute If there are no element on the given path, when set to true the system will drop the event, and when set to false the system will pass 'null' value to the jsonElement output attribute. true BOOL Yes No Extra Return Attributes Name Description Possible Types jsonElement The JSON element retrieved based on the given path will be returned as a JSON object. If the 'path' selects a JSON array then the system returns each element in the array as a JSON object via a separate events. OBJECT Examples EXAMPLE 1 define stream InputStream (json string, path string); @info(name = 'query1') from InputStream#json:tokenizeAsObject(json, path) select path, jsonElement insert into OutputStream; If the input 'json' is {name:'John', enrolledSubjects:['Mathematics', 'Physics']} , and the 'path' is passed as .enrolledSubjects /code then for both the elements in the selected JSON array, it generates it generates events as code (' .enrolledSubjects</code> then for both the elements in the selected JSON array, it generates it generates events as <code>(' .enrolledSubjects', 'Mathematics') , and (' .enrolledSubjects', 'Physics') /code . br For the same input JSON, if the 'path' is passed as code .enrolledSubjects', 'Physics')</code>.<br>For the same input JSON, if the 'path' is passed as <code> .name then it will only produce one event (' .name', 'John') /code as the 'path' provided a single JSON element. /p p /p span id=\"example-2\" class=\"md-typeset\" style=\"display: block; color: rgba(0, 0, 0, 0.54); font-size: 12.8px; font-weight: bold;\" EXAMPLE 2 /span define stream InputStream (json string, path string); @info(name = 'query1') from InputStream#json:tokenizeAsObject(json, path, true) select path, jsonElement insert into OutputStream; p /p p style=\"word-wrap: break-word;margin: 0;\" If the input 'json' is code {name:'John', age:25} /code ,and the 'path' is passed as code .name', 'John')</code> as the 'path' provided a single JSON element.</p> <p></p> <span id=\"example-2\" class=\"md-typeset\" style=\"display: block; color: rgba(0, 0, 0, 0.54); font-size: 12.8px; font-weight: bold;\">EXAMPLE 2</span> <pre class=\"codehilite\"><code>define stream InputStream (json string, path string); @info(name = 'query1') from InputStream#json:tokenizeAsObject(json, path, true) select path, jsonElement insert into OutputStream;</code></pre> <p></p> <p style=\"word-wrap: break-word;margin: 0;\">If the input 'json' is <code>{name:'John', age:25}</code>,and the 'path' is passed as <code> .salary then the system will produce (' .salary', null) /code , as the 'fail.on.missing.attribute' is code true /code and there are no matching element for code .salary', null)</code>, as the 'fail.on.missing.attribute' is <code>true</code> and there are no matching element for <code> .salary . List collect (Aggregate Function) Collects multiple values to construct a list. Origin: siddhi-execution-list:1.0.0 Syntax OBJECT list:collect( OBJECT|INT|LONG|FLOAT|DOUBLE|BOOL|STRING value) OBJECT list:collect( OBJECT|INT|LONG|FLOAT|DOUBLE|BOOL|STRING value, BOOL is.distinct) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic value Value of the list element OBJECT INT LONG FLOAT DOUBLE BOOL STRING No Yes is.distinct If true only distinct elements are collected false BOOL Yes Yes Examples EXAMPLE 1 from StockStream#window.lengthBatch(10) select list:collect(symbol) as stockSymbols insert into OutputStream; For the window expiry of 10 events, the collect() function will collect attributes of symbol to a single list and return as stockSymbols. merge (Aggregate Function) Collects multiple lists to merge as a single list. Origin: siddhi-execution-list:1.0.0 Syntax OBJECT list:merge( OBJECT list) OBJECT list:merge( OBJECT list, BOOL is.distinct) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic list List to be merged OBJECT No Yes is.distinct Whether to return list with distinct values false BOOL Yes Yes Examples EXAMPLE 1 from StockStream#window.lengthBatch(2) select list:merge(list) as stockSymbols insert into OutputStream; For the window expiry of 2 events, the merge() function will collect attributes of list and merge them to a single list, returned as stockSymbols. add (Function) Function returns the updated list after adding the given value. Origin: siddhi-execution-list:1.0.0 Syntax OBJECT list:add( OBJECT list, OBJECT|INT|LONG|FLOAT|DOUBLE|BOOL|STRING value) OBJECT list:add( OBJECT list, OBJECT|INT|LONG|FLOAT|DOUBLE|BOOL|STRING value, INT index) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic list The list to which the value should be added. OBJECT No Yes value The value to be added. OBJECT INT LONG FLOAT DOUBLE BOOL STRING No Yes index The index in which the value should to be added. last INT Yes Yes Examples EXAMPLE 1 list:add(stockSymbols, 'IBM') Function returns the updated list after adding the value IBM in the last index. EXAMPLE 2 list:add(stockSymbols, 'IBM', 0) Function returns the updated list after adding the value IBM in the 0 th index`. addAll (Function) Function returns the updated list after adding all the values from the given list. Origin: siddhi-execution-list:1.0.0 Syntax OBJECT list:addAll( OBJECT to.list, OBJECT from.list) OBJECT list:addAll( OBJECT to.list, OBJECT from.list, BOOL is.distinct) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic to.list The list into which the values need to copied. OBJECT No Yes from.list The list from which the values are copied. OBJECT No Yes is.distinct If true returns list with distinct values false BOOL Yes Yes Examples EXAMPLE 1 list:putAll(toList, fromList) If toList contains values ('IBM', 'WSO2), and if fromList contains values ('IBM', 'XYZ') then the function returns updated toList with values ('IBM', 'WSO2', 'IBM', 'XYZ'). EXAMPLE 2 list:putAll(toList, fromList, true) If toList contains values ('IBM', 'WSO2), and if fromList contains values ('IBM', 'XYZ') then the function returns updated toList with values ('IBM', 'WSO2', 'XYZ'). clear (Function) Function returns the cleared list. Origin: siddhi-execution-list:1.0.0 Syntax OBJECT list:clear( OBJECT list) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic list The list which needs to be cleared OBJECT No Yes Examples EXAMPLE 1 list:clear(stockDetails) Returns an empty list. clone (Function) Function returns the cloned list. Origin: siddhi-execution-list:1.0.0 Syntax OBJECT list:clone( OBJECT list) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic list The list to which needs to be cloned. OBJECT No Yes Examples EXAMPLE 1 list:clone(stockSymbols) Function returns cloned list of stockSymbols. contains (Function) Function checks whether the list contains the specific value. Origin: siddhi-execution-list:1.0.0 Syntax BOOL list:contains( OBJECT list, OBJECT|INT|LONG|FLOAT|DOUBLE|BOOL|STRING value) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic list The list that needs to be checked on whether it contains the value or not. OBJECT No Yes value The value that needs to be checked. OBJECT INT LONG FLOAT DOUBLE BOOL STRING No Yes Examples EXAMPLE 1 list:contains(stockSymbols, 'IBM') Returns 'true' if the stockSymbols list contains value IBM else it returns false . containsAll (Function) Function checks whether the list contains all the values in the given list. Origin: siddhi-execution-list:1.0.0 Syntax BOOL list:containsAll( OBJECT list, OBJECT given.list) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic list The list that needs to be checked on whether it contains all the values or not. OBJECT No Yes given.list The list which contains all the values to be checked. OBJECT No Yes Examples EXAMPLE 1 list:containsAll(stockSymbols, latestStockSymbols) Returns 'true' if the stockSymbols list contains values in latestStockSymbols else it returns false . create (Function) Function creates a list containing all values provided. Origin: siddhi-execution-list:1.0.0 Syntax OBJECT list:create() OBJECT list:create( OBJECT|INT|LONG|FLOAT|DOUBLE|BOOL|STRING value1) OBJECT list:create( OBJECT|INT|LONG|FLOAT|DOUBLE|BOOL|STRING value1, OBJECT|INT|LONG|FLOAT|DOUBLE|BOOL|STRING ...) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic value1 Value 1 OBJECT INT LONG FLOAT DOUBLE BOOL STRING Yes Yes Examples EXAMPLE 1 list:create(1, 2, 3, 4, 5, 6) This returns a list with values 1 , 2 , 3 , 4 , 5 and 6 . EXAMPLE 2 list:create() This returns an empty list. get (Function) Function returns the value at the specific index, null if index is out of range. Origin: siddhi-execution-list:1.0.0 Syntax OBJECT|INT|LONG|FLOAT|DOUBLE|BOOL|STRING list:get( OBJECT list, INT index) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic list Attribute containing the list OBJECT No Yes index Index of the element INT No Yes Examples EXAMPLE 1 list:get(stockSymbols, 1) This returns the element in the 1 st index in the stockSymbols list. indexOf (Function) Function returns the last index of the given element. Origin: siddhi-execution-list:1.0.0 Syntax INT list:indexOf( OBJECT list, OBJECT|INT|LONG|FLOAT|DOUBLE|BOOL|STRING value) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic list The list to be checked to get index of an element. OBJECT No Yes value Value for which last index needs to be identified. OBJECT INT LONG FLOAT DOUBLE BOOL STRING No Yes Examples EXAMPLE 1 list:indexOf(stockSymbols. `IBM`) Returns the last index of the element IBM if present else it returns -1. isEmpty (Function) Function checks if the list is empty. Origin: siddhi-execution-list:1.0.0 Syntax BOOL list:isEmpty( OBJECT list) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic list The list that needs to be checked whether it's empty or not. OBJECT No Yes Examples EXAMPLE 1 list:isEmpty(stockSymbols) Returns 'true' if the stockSymbols list is empty else it returns false . isList (Function) Function checks if the object is type of a list. Origin: siddhi-execution-list:1.0.0 Syntax BOOL list:isList( OBJECT|INT|LONG|FLOAT|DOUBLE|BOOL|STRING arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The argument the need to be determined whether it's a list or not. OBJECT INT LONG FLOAT DOUBLE BOOL STRING No Yes Examples EXAMPLE 1 list:isList(stockSymbols) Returns 'true' if the stockSymbols is and an instance of java.util.List else it returns false . lastIndexOf (Function) Function returns the index of the given value. Origin: siddhi-execution-list:1.0.0 Syntax INT list:lastIndexOf( OBJECT list, OBJECT|INT|LONG|FLOAT|DOUBLE|BOOL|STRING value) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic list The list to be checked to get index of an element. OBJECT No Yes value Value for which last index needs to be identified. OBJECT INT LONG FLOAT DOUBLE BOOL STRING No Yes Examples EXAMPLE 1 list:lastIndexOf(stockSymbols. `IBM`) Returns the last index of the element IBM if present else it returns -1. remove (Function) Function returns the updated list after removing the element with the specified value. Origin: siddhi-execution-list:1.0.0 Syntax OBJECT list:remove( OBJECT list, OBJECT|INT|LONG|FLOAT|DOUBLE|BOOL|STRING value) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic list The list that needs to be updated. OBJECT No Yes value The value of the element that needs to removed. OBJECT INT LONG FLOAT DOUBLE BOOL STRING No Yes Examples EXAMPLE 1 list:remove(stockSymbols, 'IBM') This returns the updated list, stockSymbols after stockSymbols the value IBM . removeAll (Function) Function returns the updated list after removing all the element with the specified list. Origin: siddhi-execution-list:1.0.0 Syntax OBJECT list:removeAll( OBJECT list, OBJECT given.list) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic list The list that needs to be updated. OBJECT No Yes given.list The list with all the elements that needs to removed. OBJECT No Yes Examples EXAMPLE 1 list:removeAll(stockSymbols, latestStockSymbols) This returns the updated list, stockSymbols after removing all the values in latestStockSymbols. removeByIndex (Function) Function returns the updated list after removing the element with the specified index. Origin: siddhi-execution-list:1.0.0 Syntax OBJECT list:removeByIndex( OBJECT list, INT index) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic list The list that needs to be updated. OBJECT No Yes index The index of the element that needs to removed. INT No Yes Examples EXAMPLE 1 list:removeByIndex(stockSymbols, 0) This returns the updated list, stockSymbols after removing value at 0 th index. retainAll (Function) Function returns the updated list after retaining all the elements in the specified list. Origin: siddhi-execution-list:1.0.0 Syntax OBJECT list:retainAll( OBJECT list, OBJECT given.list) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic list The list that needs to be updated. OBJECT No Yes given.list The list with all the elements that needs to reatined. OBJECT No Yes Examples EXAMPLE 1 list:retainAll(stockSymbols, latestStockSymbols) This returns the updated list, stockSymbols after retaining all the values in latestStockSymbols. setValue (Function) Function returns the updated list after replacing the element in the given index by the given value. Origin: siddhi-execution-list:1.0.0 Syntax OBJECT list:setValue( OBJECT list, INT index, OBJECT|INT|LONG|FLOAT|DOUBLE|BOOL|STRING value) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic list The list to which the value should be updated. OBJECT No Yes index The index in which the value should to be updated. INT No Yes value The value to be updated with. OBJECT INT LONG FLOAT DOUBLE BOOL STRING No Yes Examples EXAMPLE 1 list:set(stockSymbols, 0, 'IBM') Function returns the updated list after replacing the value at 0 th index with the value IBM size (Function) Function to return the size of the list. Origin: siddhi-execution-list:1.0.0 Syntax INT list:size( OBJECT list) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic list The list for which size should be returned. OBJECT No Yes Examples EXAMPLE 1 list:size(stockSymbols) Returns size of the stockSymbols list. sort (Function) Function returns lists sorted in ascending or descending order. Origin: siddhi-execution-list:1.0.0 Syntax OBJECT list:sort( OBJECT list) OBJECT list:sort( OBJECT list, STRING order) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic list The list which should be sorted. OBJECT No Yes order Order in which the list needs to be sorted (ASC/DESC/REV). REV STRING Yes No Examples EXAMPLE 1 list:sort(stockSymbols) Function returns the sorted list in ascending order. EXAMPLE 2 list:sort(stockSymbols, 'DESC') Function returns the sorted list in descending order. tokenize (Stream Processor) Tokenize the list and return each key, value as new attributes in events Origin: siddhi-execution-list:1.0.0 Syntax list:tokenize( OBJECT list) list:tokenize( OBJECT list, OBJECT ...) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic list Array list which needs to be tokenized OBJECT No Yes Extra Return Attributes Name Description Possible Types index Index of an entry consisted in the list INT value Value of an entry consisted in the list OBJECT Examples EXAMPLE 1 list:tokenize(customList) If custom list contains ('WSO2', 'IBM', 'XYZ') elements, then tokenize function will return 3 events with value attributes WSO2, IBM and XYZ respectively. Map collect (Aggregate Function) Collect multiple key-value pairs to construct a map. Only distinct keys are collected, if a duplicate key arrives, it overrides the old value Origin: siddhi-execution-map:5.0.5 Syntax OBJECT map:collect( INT|LONG|FLOAT|DOUBLE|FLOAT|BOOL|STRING key, OBJECT|INT|LONG|FLOAT|DOUBLE|BOOL|STRING value) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic key Key of the map entry INT LONG FLOAT DOUBLE FLOAT BOOL STRING No Yes value Value of the map entry OBJECT INT LONG FLOAT DOUBLE BOOL STRING No Yes Examples EXAMPLE 1 from StockStream#window.lengthBatch(10) select map:collect(symbol, price) as stockDetails insert into OutputStream; For the window expiry of 10 events, the collect() function will collect attributes of key and value to a single map and return as stockDetails. merge (Aggregate Function) Collect multiple maps to merge as a single map. Only distinct keys are collected, if a duplicate key arrives, it overrides the old value. Origin: siddhi-execution-map:5.0.5 Syntax OBJECT map:merge( OBJECT map) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic map Maps to be collected OBJECT No Yes Examples EXAMPLE 1 from StockStream#window.lengthBatch(2) select map:merge(map) as stockDetails insert into OutputStream; For the window expiry of 2 events, the merge() function will collect attributes of map and merge them to a single map, returned as stockDetails. clear (Function) Function returns the cleared map. Origin: siddhi-execution-map:5.0.5 Syntax OBJECT map:clear( OBJECT map) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic map The map which needs to be cleared OBJECT No Yes Examples EXAMPLE 1 map:clear(stockDetails) Returns an empty map. clone (Function) Function returns the cloned map. Origin: siddhi-execution-map:5.0.5 Syntax OBJECT map:clone( OBJECT map) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic map The map to which needs to be cloned. OBJECT No Yes Examples EXAMPLE 1 map:clone(stockDetails) Function returns cloned map of stockDetails. combineByKey (Function) Function returns the map after combining all the maps given as parameters, such that the keys, of all the maps will be matched with an Array list of values from each map respectively. Origin: siddhi-execution-map:5.0.5 Syntax OBJECT map:combineByKey( OBJECT map, OBJECT map) OBJECT map:combineByKey( OBJECT map, OBJECT map, OBJECT ...) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic map The map into which the key-values need to copied. OBJECT No Yes Examples EXAMPLE 1 map:combineByKey(map1, map2) If map2 contains key-value pairs ('symbol': 'wso2'), ('volume' : 100), and if map2 contains key-value pairs ('symbol': 'IBM'), ('price' : 12), then the function returns the map with key value pairs as follows, (symbol: ArrayList('wso2, 'IBM')), (volume: ArrayList(100, null)) and (price: ArrayList(null, 12)) containsKey (Function) Function checks if the map contains the key. Origin: siddhi-execution-map:5.0.5 Syntax BOOL map:containsKey( OBJECT map, INT|LONG|FLOAT|DOUBLE|BOOL|STRING key) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic map The map the needs to be checked on containing the key or not. OBJECT No Yes key The key to be checked. INT LONG FLOAT DOUBLE BOOL STRING No Yes Examples EXAMPLE 1 map:containsKey(stockDetails, '1234') Returns 'true' if the stockDetails map contains key 1234 else it returns false . containsValue (Function) Function checks if the map contains the value. Origin: siddhi-execution-map:5.0.5 Syntax BOOL map:containsValue( OBJECT map, INT|LONG|FLOAT|DOUBLE|BOOL|STRING value) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic map The map the needs to be checked on containing the value or not. OBJECT No Yes value The value to be checked. INT LONG FLOAT DOUBLE BOOL STRING No Yes Examples EXAMPLE 1 map:containsValue(stockDetails, 'IBM') Returns 'true' if the stockDetails map contains value IBM else it returns false . create (Function) Function creates a map pairing the keys and their corresponding values. Origin: siddhi-execution-map:5.0.5 Syntax OBJECT map:create() OBJECT map:create( OBJECT|INT|LONG|FLOAT|DOUBLE|FLOAT|BOOL|STRING key1, OBJECT|INT|LONG|FLOAT|DOUBLE|FLOAT|BOOL|STRING value1) OBJECT map:create( OBJECT|INT|LONG|FLOAT|DOUBLE|FLOAT|BOOL|STRING key1, OBJECT|INT|LONG|FLOAT|DOUBLE|FLOAT|BOOL|STRING value1, OBJECT|INT|LONG|FLOAT|DOUBLE|FLOAT|BOOL|STRING ...) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic key1 Key 1 - OBJECT INT LONG FLOAT DOUBLE FLOAT BOOL STRING Yes Yes value1 Value 1 - OBJECT INT LONG FLOAT DOUBLE FLOAT BOOL STRING Yes Yes Examples EXAMPLE 1 map:create(1, 'one', 2, 'two', 3, 'three') This returns a map with keys 1 , 2 , 3 mapped with their corresponding values, one , two , three . EXAMPLE 2 map:create() This returns an empty map. createFromJSON (Function) Function returns the map created by pairing the keys with their corresponding values given in the JSON string. Origin: siddhi-execution-map:5.0.5 Syntax OBJECT map:createFromJSON( STRING json.string) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json.string JSON as a string, which is used to create the map. STRING No Yes Examples EXAMPLE 1 map:createFromJSON(\"{\u2018symbol' : 'IBM', 'price' : 200, 'volume' : 100}\") This returns a map with the keys symbol , price , and volume , and their values, IBM , 200 and 100 respectively. createFromXML (Function) Function returns the map created by pairing the keys with their corresponding values,given as an XML string. Origin: siddhi-execution-map:5.0.5 Syntax OBJECT map:createFromXML( STRING xml.string) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic xml.string The XML string, which is used to create the map. STRING No Yes Examples EXAMPLE 1 map:createFromXML(\" stock symbol IBM /symbol price 200 /price volume 100 /volume /stock \") This returns a map with the keys symbol , price , volume , and with their values IBM , 200 and 100 respectively. get (Function) Function returns the value corresponding to the given key from the map. Origin: siddhi-execution-map:5.0.5 Syntax OBJECT|INT|LONG|FLOAT|DOUBLE|FLOAT|BOOL|STRING map:get( OBJECT map, INT|LONG|FLOAT|DOUBLE|FLOAT|BOOL|STRING key) OBJECT|INT|LONG|FLOAT|DOUBLE|FLOAT|BOOL|STRING map:get( OBJECT map, INT|LONG|FLOAT|DOUBLE|FLOAT|BOOL|STRING key, OBJECT|INT|LONG|FLOAT|DOUBLE|FLOAT|BOOL|STRING default.value) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic map The map from where the value should be obtained. OBJECT No Yes key The key to fetch the value. INT LONG FLOAT DOUBLE FLOAT BOOL STRING No Yes default.value The value to be returned if the map does not have the key. OBJECT INT LONG FLOAT DOUBLE FLOAT BOOL STRING Yes Yes Examples EXAMPLE 1 map:get(companyMap, 1) If the companyMap has key 1 and value ABC in it's set of key value pairs. The function returns ABC . EXAMPLE 2 map:get(companyMap, 2) If the companyMap does not have any value for key 2 then the function returns null . EXAMPLE 3 map:get(companyMap, 2, 'two') If the companyMap does not have any value for key 2 then the function returns two . isEmpty (Function) Function checks if the map is empty. Origin: siddhi-execution-map:5.0.5 Syntax BOOL map:isEmpty( OBJECT map) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic map The map the need to be checked whether it's empty or not. OBJECT No Yes Examples EXAMPLE 1 map:isEmpty(stockDetails) Returns 'true' if the stockDetails map is empty else it returns false . isMap (Function) Function checks if the object is type of a map. Origin: siddhi-execution-map:5.0.5 Syntax BOOL map:isMap( OBJECT|INT|LONG|FLOAT|DOUBLE|FLOAT|BOOL|STRING arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The argument the need to be determined whether it's a map or not. OBJECT INT LONG FLOAT DOUBLE FLOAT BOOL STRING No Yes Examples EXAMPLE 1 map:isMap(stockDetails) Returns 'true' if the stockDetails is and an instance of java.util.Map else it returns false . keys (Function) Function to return the keys of the map as a list. Origin: siddhi-execution-map:5.0.5 Syntax OBJECT map:keys( OBJECT map) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic map The map from which list of keys to be returned. OBJECT No Yes Examples EXAMPLE 1 map:keys(stockDetails) Returns keys of the stockDetails map. put (Function) Function returns the updated map after adding the given key-value pair. If the key already exist in the map the key is updated with the new value. Origin: siddhi-execution-map:5.0.5 Syntax OBJECT map:put( OBJECT map, OBJECT|INT|LONG|FLOAT|DOUBLE|FLOAT|BOOL|STRING key, OBJECT|INT|LONG|FLOAT|DOUBLE|FLOAT|BOOL|STRING value) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic map The map to which the value should be added. OBJECT No Yes key The key to be added. OBJECT INT LONG FLOAT DOUBLE FLOAT BOOL STRING No Yes value The value to be added. OBJECT INT LONG FLOAT DOUBLE FLOAT BOOL STRING No Yes Examples EXAMPLE 1 map:put(stockDetails , 'IBM' , '200') Function returns the updated map named stockDetails after adding the value 200 with the key IBM . putAll (Function) Function returns the updated map after adding all the key-value pairs from another map. If there are duplicate keys, the key will be assigned new values from the map that's being copied. Origin: siddhi-execution-map:5.0.5 Syntax OBJECT map:putAll( OBJECT to.map, OBJECT from.map) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic to.map The map into which the key-values need to copied. OBJECT No Yes from.map The map from which the key-values are copied. OBJECT No Yes Examples EXAMPLE 1 map:putAll(toMap, fromMap) If toMap contains key-value pairs ('symbol': 'wso2'), ('volume' : 100), and if fromMap contains key-value pairs ('symbol': 'IBM'), ('price' : 12), then the function returns updated toMap with key-value pairs ('symbol': 'IBM'), ('price' : 12), ('volume' : 100). putIfAbsent (Function) Function returns the updated map after adding the given key-value pair if key is absent. Origin: siddhi-execution-map:5.0.5 Syntax OBJECT map:putIfAbsent( OBJECT map, INT|LONG|FLOAT|DOUBLE|BOOL|STRING key, INT|LONG|FLOAT|DOUBLE|BOOL|STRING value) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic map The map to which the value should be added. OBJECT No Yes key The key to be added. INT LONG FLOAT DOUBLE BOOL STRING No Yes value The value to be added. INT LONG FLOAT DOUBLE BOOL STRING No Yes Examples EXAMPLE 1 map:putIfAbsent(stockDetails , 1234 , 'IBM') Function returns the updated map named stockDetails after adding the value IBM with the key 1234 if key is absent from the original map. remove (Function) Function returns the updated map after removing the element with the specified key. Origin: siddhi-execution-map:5.0.5 Syntax OBJECT map:remove( OBJECT map, OBJECT|INT|LONG|FLOAT|DOUBLE|FLOAT|BOOL|STRING key) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic map The map that needs to be updated. OBJECT No Yes key The key of the element that needs to removed. OBJECT INT LONG FLOAT DOUBLE FLOAT BOOL STRING No Yes Examples EXAMPLE 1 map:remove(stockDetails, 1234) This returns the updated map, stockDetails after removing the key-value pair corresponding to the key 1234 . replace (Function) Function returns the updated map after replacing the given key-value pair only if key is present. Origin: siddhi-execution-map:5.0.5 Syntax OBJECT map:replace( OBJECT map, INT|LONG|FLOAT|DOUBLE|FLOAT|BOOL|STRING key, INT|LONG|FLOAT|DOUBLE|BOOL|STRING value) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic map The map to which the key-value should be replaced. OBJECT No Yes key The key to be replaced. INT LONG FLOAT DOUBLE FLOAT BOOL STRING No Yes value The value to be replaced. INT LONG FLOAT DOUBLE BOOL STRING No Yes Examples EXAMPLE 1 map:replace(stockDetails , 1234 , 'IBM') Function returns the updated map named stockDetails after replacing the value IBM with the key 1234 if present. replaceAll (Function) Function returns the updated map after replacing all the key-value pairs from another map, if keys are present. Origin: siddhi-execution-map:5.0.5 Syntax OBJECT map:replaceAll( OBJECT to.map, OBJECT from.map) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic to.map The map into which the key-values need to copied. OBJECT No Yes from.map The map from which the key-values are copied. OBJECT No Yes Examples EXAMPLE 1 map:replaceAll(toMap, fromMap) If toMap contains key-value pairs ('symbol': 'wso2'), ('volume' : 100), and if fromMap contains key-value pairs ('symbol': 'IBM'), ('price' : 12), then the function returns updated toMap with key-value pairs ('symbol': 'IBM'), ('volume' : 100). size (Function) Function to return the size of the map. Origin: siddhi-execution-map:5.0.5 Syntax INT map:size( OBJECT map) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic map The map for which size should be returned. OBJECT No Yes Examples EXAMPLE 1 map:size(stockDetails) Returns size of the stockDetails map. toJSON (Function) Function converts a map into a JSON object and returns the JSON as a string. Origin: siddhi-execution-map:5.0.5 Syntax STRING map:toJSON( OBJECT map) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic map The map that needs to be converted to JSON OBJECT No Yes Examples EXAMPLE 1 map:toJSON(company) If company is a map with key-value pairs, ('symbol': 'wso2'),('volume' : 100), and ('price', 200), it returns the JSON string {\"symbol\" : \"wso2\", \"volume\" : 100 , \"price\" : 200} . toXML (Function) Function returns the map as an XML string. Origin: siddhi-execution-map:5.0.5 Syntax STRING map:toXML( OBJECT map) STRING map:toXML( OBJECT map, OBJECT|STRING root.element.name) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic map The map that needs to be converted to XML. OBJECT No Yes root.element.name The root element of the map. The XML root element will be ignored OBJECT STRING Yes Yes Examples EXAMPLE 1 toXML(company, 'abcCompany') If company is a map with key-value pairs, ('symbol' : 'wso2'), ('volume' : 100), and ('price' : 200), this function returns XML as a string, abcCompany symbol wso2 /symbol volume 100 /volume price 200 /price /abcCompany . EXAMPLE 2 toXML(company) If company is a map with key-value pairs, ('symbol' : 'wso2'), ('volume' : 100), and ('price' : 200), this function returns XML without root element as a string, symbol wso2 /symbol volume 100 /volume price 200 /price . values (Function) Function to return the values of the map. Origin: siddhi-execution-map:5.0.5 Syntax OBJECT map:values( OBJECT map) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic map The map from which list if values to be returned. OBJECT No Yes Examples EXAMPLE 1 map:values(stockDetails) Returns values of the stockDetails map. tokenize (Stream Processor) Tokenize the map and return each key, value as new attributes in events Origin: siddhi-execution-map:5.0.5 Syntax map:tokenize( OBJECT map) map:tokenize( OBJECT map, OBJECT ...) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic map Hash map containing key value pairs OBJECT No Yes Extra Return Attributes Name Description Possible Types key Key of an entry consisted in the map OBJECT value Value of an entry consisted in the map. If more than one map is given, then an Array List of values from each map is returned for the value attribute. OBJECT Examples EXAMPLE 1 define stream StockStream(symbol string, price float); from StockStream#window.lengthBatch(2) select map:collect(symbol, price) as symbolPriceMap insert into TempStream; from TempStream#map:tokenize(customMap) select key, value insert into SymbolStream; Based on the length batch window, symbolPriceMap will collect two events, and the map will then again tokenized to give 2 events with key and values being symbol name and price respectively. Math percentile (Aggregate Function) This functions returns the pth percentile value of a given argument. Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:percentile( INT|LONG|FLOAT|DOUBLE arg, DOUBLE p) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value of the parameter whose percentile should be found. INT LONG FLOAT DOUBLE No Yes p Estimate of the percentile to be found (pth percentile) where p is any number greater than 0 or lesser than or equal to 100. DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (sensorId int, temperature double); from InValueStream select math:percentile(temperature, 97.0) as percentile insert into OutMediationStream; This function returns the percentile value based on the argument given. For example, math:percentile(temperature, 97.0) returns the 97 th percentile value of all the temperature events. abs (Function) This function returns the absolute value of the given parameter. It wraps the java.lang.Math.abs() function. Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:abs( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The parameter whose absolute value is found. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:abs(inValue) as absValue insert into OutMediationStream; Irrespective of whether the 'invalue' in the input stream holds a value of abs(3) or abs(-3),the function returns 3 since the absolute value of both 3 and -3 is 3. The result directed to OutMediationStream stream. acos (Function) If -1 = p1 = 1, this function returns the arc-cosine (inverse cosine) value of p1.If the domain is invalid, it returns NULL. The value returned is in radian scale. This function wraps the java.lang.Math.acos() function. Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:acos( FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose arc-cosine (inverse cosine) value is found. FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:acos(inValue) as acosValue insert into OutMediationStream; If the 'inValue' in the input stream is given, the function calculates the arc-cosine value of it and returns the arc-cosine value to the output stream, OutMediationStream. For example, acos(0.5) returns 1.0471975511965979. asin (Function) If -1 = p1 = 1, this function returns the arc-sin (inverse sine) value of p1. If the domain is invalid, it returns NULL. The value returned is in radian scale. This function wraps the java.lang.Math.asin() function. Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:asin( FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose arc-sin (inverse sine) value is found. FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:asin(inValue) as asinValue insert into OutMediationStream; If the 'inValue' in the input stream is given, the function calculates the arc-sin value of it and returns the arc-sin value to the output stream, OutMediationStream. For example, asin(0.5) returns 0.5235987755982989. atan (Function) 1. If a single p1 is received, this function returns the arc-tangent (inverse tangent) value of p1 . 2. If p1 is received along with an optional p1 , it considers them as x and y coordinates and returns the arc-tangent (inverse tangent) value. The returned value is in radian scale. This function wraps the java.lang.Math.atan() function. Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:atan( INT|LONG|FLOAT|DOUBLE p1) DOUBLE math:atan( INT|LONG|FLOAT|DOUBLE p1, INT|LONG|FLOAT|DOUBLE p2) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose arc-tangent (inverse tangent) is found. If the optional second parameter is given this represents the x coordinate of the (x,y) coordinate pair. INT LONG FLOAT DOUBLE No Yes p2 This optional parameter represents the y coordinate of the (x,y) coordinate pair. 0D INT LONG FLOAT DOUBLE Yes Yes Examples EXAMPLE 1 define stream InValueStream (inValue1 double, inValue2 double); from InValueStream select math:atan(inValue1, inValue2) as convertedValue insert into OutMediationStream; If the 'inValue1' in the input stream is given, the function calculates the arc-tangent value of it and returns the arc-tangent value to the output stream, OutMediationStream. If both the 'inValue1' and 'inValue2' are given, then the function considers them to be x and y coordinates respectively and returns the calculated arc-tangent value to the output stream, OutMediationStream. For example, atan(12d, 5d) returns 1.1760052070951352. bin (Function) This function returns a string representation of the p1 argument, that is of either 'integer' or 'long' data type, as an unsigned integer in base 2. It wraps the java.lang.Integer.toBinaryString and java.lang.Long.toBinaryString` methods. Origin: siddhi-execution-math:5.0.4 Syntax STRING math:bin( INT|LONG p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value in either 'integer' or 'long', that should be converted into an unsigned integer of base 2. INT LONG No Yes Examples EXAMPLE 1 define stream InValueStream (inValue long); from InValueStream select math:bin(inValue) as binValue insert into OutMediationStream; If the 'inValue' in the input stream is given, the function converts it into an unsigned integer in base 2 and directs the output to the output stream, OutMediationStream. For example, bin(9) returns '1001'. cbrt (Function) This function returns the cube-root of 'p1' which is in radians. It wraps the java.lang.Math.cbrt() function. Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:cbrt( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose cube-root should be found. Input is required to be in radians. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:cbrt(inValue) as cbrtValue insert into OutMediationStream; If the 'inValue' is given, the function calculates the cube-root value for the same and directs the output to the output stream, OutMediationStream. For example, cbrt(17d) returns 2.5712815906582356. ceil (Function) This function returns the smallest double value, i.e., the closest to the negative infinity, that is greater than or equal to the p1 argument, and is equal to a mathematical integer. It wraps the java.lang.Math.ceil() method. Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:ceil( FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose ceiling value is found. FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:ceil(inValue) as ceilingValue insert into OutMediationStream; This function calculates the ceiling value of the given 'inValue' and directs the result to 'OutMediationStream' output stream. For example, ceil(423.187d) returns 424.0. conv (Function) This function converts a from the fromBase base to the toBase base. Origin: siddhi-execution-math:5.0.4 Syntax STRING math:conv( STRING a, INT from.base, INT to.base) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic a The value whose base should be changed. Input should be given as a 'String'. STRING No Yes from.base The source base of the input parameter 'a'. INT No Yes to.base The target base that the input parameter 'a' should be converted into. INT No Yes Examples EXAMPLE 1 define stream InValueStream (inValue string,fromBase int,toBase int); from InValueStream select math:conv(inValue,fromBase,toBase) as convertedValue insert into OutMediationStream; If the 'inValue' in the input stream is given, and the base in which it currently resides in and the base to which it should be converted to is specified, the function converts it into a string in the target base and directs it to the output stream, OutMediationStream. For example, conv(\"7f\", 16, 10) returns \"127\". copySign (Function) This function returns a value of an input with the received magnitude and sign of another input. It wraps the java.lang.Math.copySign() function. Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:copySign( INT|LONG|FLOAT|DOUBLE magnitude, INT|LONG|FLOAT|DOUBLE sign) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic magnitude The magnitude of this parameter is used in the output attribute. INT LONG FLOAT DOUBLE No Yes sign The sign of this parameter is used in the output attribute. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue1 double, inValue2 double); from InValueStream select math:copySign(inValue1,inValue2) as copysignValue insert into OutMediationStream; If two values are provided as 'inValue1' and 'inValue2', the function copies the magnitude and sign of the second argument into the first one and directs the result to the output stream, OutMediatonStream. For example, copySign(5.6d, -3.0d) returns -5.6. cos (Function) This function returns the cosine of p1 which is in radians. It wraps the java.lang.Math.cos() function. Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:cos( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose cosine value should be found.The input is required to be in radians. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:cos(inValue) as cosValue insert into OutMediationStream; If the 'inValue' is given, the function calculates the cosine value for the same and directs the output to the output stream, OutMediationStream. For example, cos(6d) returns 0.9601702866503661. cosh (Function) This function returns the hyperbolic cosine of p1 which is in radians. It wraps the java.lang.Math.cosh() function. Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:cosh( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose hyperbolic cosine should be found. The input is required to be in radians. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:cosh(inValue) as cosValue insert into OutMediationStream; If the 'inValue' is given, the function calculates the hyperbolic cosine value for the same and directs the output to the output stream, OutMediationStream. For example, cosh (6d) returns 201.7156361224559. e (Function) This function returns the java.lang.Math.E constant, which is the closest double value to e, where e is the base of the natural logarithms. Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:e() Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:e() as eValue insert into OutMediationStream; This function returns the constant, 2.7182818284590452354 which is the closest double value to e and directs the output to 'OutMediationStream' output stream. exp (Function) This function returns the Euler's number e raised to the power of p1 . It wraps the java.lang.Math.exp() function. Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:exp( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The power that the Euler's number e is raised to. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:exp(inValue) as expValue insert into OutMediationStream; If the 'inValue' in the inputstream holds a value, this function calculates the corresponding Euler's number 'e' and directs it to the output stream, OutMediationStream. For example, exp(10.23) returns 27722.51006805505. floor (Function) This function wraps the java.lang.Math.floor() function and returns the largest value, i.e., closest to the positive infinity, that is less than or equal to p1 , and is equal to a mathematical integer. Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:floor( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose floor value should be found. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:floor(inValue) as floorValue insert into OutMediationStream; This function calculates the floor value of the given 'inValue' input and directs the output to the 'OutMediationStream' output stream. For example, (10.23) returns 10.0. getExponent (Function) This function returns the unbiased exponent that is used in the representation of p1 . This function wraps the java.lang.Math.getExponent() function. Origin: siddhi-execution-math:5.0.4 Syntax INT math:getExponent( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of whose unbiased exponent representation should be found. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:getExponent(inValue) as expValue insert into OutMediationStream; This function calculates the unbiased exponent of a given input, 'inValue' and directs the result to the 'OutMediationStream' output stream. For example, getExponent(60984.1) returns 15. hex (Function) This function wraps the java.lang.Double.toHexString() function. It returns a hexadecimal string representation of the input, p1`. Origin: siddhi-execution-math:5.0.4 Syntax STRING math:hex( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose hexadecimal value should be found. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue int); from InValueStream select math:hex(inValue) as hexString insert into OutMediationStream; If the 'inValue' in the input stream is provided, the function converts this into its corresponding hexadecimal format and directs the output to the output stream, OutMediationStream. For example, hex(200) returns \"c8\". isInfinite (Function) This function wraps the java.lang.Float.isInfinite() and java.lang.Double.isInfinite() and returns true if p1 is infinitely large in magnitude and false if otherwise. Origin: siddhi-execution-math:5.0.4 Syntax BOOL math:isInfinite( FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 This is the value of the parameter that the function determines to be either infinite or finite. FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue1 double,inValue2 int); from InValueStream select math:isInfinite(inValue1) as isInfinite insert into OutMediationStream; If the value given in the 'inValue' in the input stream is of infinitely large magnitude, the function returns the value, 'true' and directs the result to the output stream, OutMediationStream'. For example, isInfinite(java.lang.Double.POSITIVE_INFINITY) returns true. isNan (Function) This function wraps the java.lang.Float.isNaN() and java.lang.Double.isNaN() functions and returns true if p1 is NaN (Not-a-Number), and returns false if otherwise. Origin: siddhi-execution-math:5.0.4 Syntax BOOL math:isNan( FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter which the function determines to be either NaN or a number. FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue1 double,inValue2 int); from InValueStream select math:isNan(inValue1) as isNaN insert into OutMediationStream; If the 'inValue1' in the input stream has a value that is undefined, then the function considers it as an 'NaN' value and directs 'True' to the output stream, OutMediationStream. For example, isNan(java.lang.Math.log(-12d)) returns true. ln (Function) This function returns the natural logarithm (base e) of p1 . Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:ln( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose natural logarithm (base e) should be found. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:ln(inValue) as lnValue insert into OutMediationStream; If the 'inValue' in the input stream is given, the function calculates its natural logarithm (base e) and directs the results to the output stream, 'OutMeditionStream'. For example, ln(11.453) returns 2.438251704415579. log (Function) This function returns the logarithm of the received number as per the given base . Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:log( INT|LONG|FLOAT|DOUBLE number, INT|LONG|FLOAT|DOUBLE base) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic number The value of the parameter whose base should be changed. INT LONG FLOAT DOUBLE No Yes base The base value of the ouput. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (number double, base double); from InValueStream select math:log(number, base) as logValue insert into OutMediationStream; If the number and the base to which it has to be converted into is given in the input stream, the function calculates the number to the base specified and directs the result to the output stream, OutMediationStream. For example, log(34, 2f) returns 5.08746284125034. log10 (Function) This function returns the base 10 logarithm of p1 . Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:log10( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose base 10 logarithm should be found. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:log10(inValue) as lnValue insert into OutMediationStream; If the 'inValue' in the input stream is given, the function calculates the base 10 logarithm of the same and directs the result to the output stream, OutMediatioStream. For example, log10(19.234) returns 1.2840696117100832. log2 (Function) This function returns the base 2 logarithm of p1 . Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:log2( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose base 2 logarithm should be found. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:log2(inValue) as lnValue insert into OutMediationStream; If the 'inValue' in the input stream is given, the function calculates the base 2 logarithm of the same and returns the value to the output stream, OutMediationStream. For example log2(91d) returns 6.507794640198696. max (Function) This function returns the greater value of p1 and p2 . Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:max( INT|LONG|FLOAT|DOUBLE p1, INT|LONG|FLOAT|DOUBLE p2) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 One of the input values to be compared in order to find the larger value of the two INT LONG FLOAT DOUBLE No Yes p2 The input value to be compared with 'p1' in order to find the larger value of the two. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue1 double,inValue2 int); from InValueStream select math:max(inValue1,inValue2) as maxValue insert into OutMediationStream; If two input values 'inValue1, and 'inValue2' are given, the function compares them and directs the larger value to the output stream, OutMediationStream. For example, max(123.67d, 91) returns 123.67. min (Function) This function returns the smaller value of p1 and p2 . Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:min( INT|LONG|FLOAT|DOUBLE p1, INT|LONG|FLOAT|DOUBLE p2) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 One of the input values that are to be compared in order to find the smaller value. INT LONG FLOAT DOUBLE No Yes p2 The input value that is to be compared with 'p1' in order to find the smaller value. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue1 double,inValue2 int); from InValueStream select math:min(inValue1,inValue2) as minValue insert into OutMediationStream; If two input values, 'inValue1' and 'inValue2' are given, the function compares them and directs the smaller value of the two to the output stream, OutMediationStream. For example, min(123.67d, 91) returns 91. oct (Function) This function converts the input parameter p1 to octal. Origin: siddhi-execution-math:5.0.4 Syntax STRING math:oct( INT|LONG p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose octal representation should be found. INT LONG No Yes Examples EXAMPLE 1 define stream InValueStream (inValue long); from InValueStream select math:oct(inValue) as octValue insert into OutMediationStream; If the 'inValue' in the input stream is given, this function calculates the octal value corresponding to the same and directs it to the output stream, OutMediationStream. For example, oct(99l) returns \"143\". parseDouble (Function) This function returns the double value of the string received. Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:parseDouble( STRING p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that should be converted into a double value. STRING No Yes Examples EXAMPLE 1 define stream InValueStream (inValue string); from InValueStream select math:parseDouble(inValue) as output insert into OutMediationStream; If the 'inValue' in the input stream holds a value, this function converts it into the corresponding double value and directs it to the output stream, OutMediationStream. For example, parseDouble(\"123\") returns 123.0. parseFloat (Function) This function returns the float value of the received string. Origin: siddhi-execution-math:5.0.4 Syntax FLOAT math:parseFloat( STRING p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that should be converted into a float value. STRING No Yes Examples EXAMPLE 1 define stream InValueStream (inValue string); from InValueStream select math:parseFloat(inValue) as output insert into OutMediationStream; The function converts the input value given in 'inValue',into its corresponding float value and directs the result into the output stream, OutMediationStream. For example, parseFloat(\"123\") returns 123.0. parseInt (Function) This function returns the integer value of the received string. Origin: siddhi-execution-math:5.0.4 Syntax INT math:parseInt( STRING p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that should be converted to an integer. STRING No Yes Examples EXAMPLE 1 define stream InValueStream (inValue string); from InValueStream select math:parseInt(inValue) as output insert into OutMediationStream; The function converts the 'inValue' into its corresponding integer value and directs the output to the output stream, OutMediationStream. For example, parseInt(\"123\") returns 123. parseLong (Function) This function returns the long value of the string received. Origin: siddhi-execution-math:5.0.4 Syntax LONG math:parseLong( STRING p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that should be converted to a long value. STRING No Yes Examples EXAMPLE 1 define stream InValueStream (inValue string); from InValueStream select math:parseLong(inValue) as output insert into OutMediationStream; The function converts the 'inValue' to its corresponding long value and directs the result to the output stream, OutMediationStream. For example, parseLong(\"123\") returns 123. pi (Function) This function returns the java.lang.Math.PI constant, which is the closest value to pi, i.e., the ratio of the circumference of a circle to its diameter. Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:pi() Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:pi() as piValue insert into OutMediationStream; pi() always returns 3.141592653589793. power (Function) This function raises the given value to a given power. Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:power( INT|LONG|FLOAT|DOUBLE value, INT|LONG|FLOAT|DOUBLE to.power) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic value The value that should be raised to the power of 'to.power' input parameter. INT LONG FLOAT DOUBLE No Yes to.power The power to which the 'value' input parameter should be raised. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue1 double, inValue2 double); from InValueStream select math:power(inValue1,inValue2) as powerValue insert into OutMediationStream; This function raises the 'inValue1' to the power of 'inValue2' and directs the output to the output stream, 'OutMediationStream. For example, (5.6d, 3.0d) returns 175.61599999999996. rand (Function) This returns a stream of pseudo-random numbers when a sequence of calls are sent to the rand() . Optionally, it is possible to define a seed, i.e., rand(seed) using which the pseudo-random numbers are generated. These functions internally use the java.util.Random class. Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:rand() DOUBLE math:rand( INT|LONG seed) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic seed An optional seed value that will be used to generate the random number sequence. defaultSeed INT LONG Yes Yes Examples EXAMPLE 1 define stream InValueStream (symbol string, price long, volume long); from InValueStream select symbol, math:rand() as randNumber select math:oct(inValue) as octValue insert into OutMediationStream; In the example given above, a random double value between 0 and 1 will be generated using math:rand(). round (Function) This function returns the value of the input argument rounded off to the closest integer/long value. Origin: siddhi-execution-math:5.0.4 Syntax INT|LONG math:round( FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that should be rounded off to the closest integer/long value. FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:round(inValue) as roundValue insert into OutMediationStream; The function rounds off 'inValue1' to the closest int/long value and directs the output to the output stream, 'OutMediationStream'. For example, round(3252.353) returns 3252. signum (Function) This returns +1, 0, or -1 for the given positive, zero and negative values respectively. This function wraps the java.lang.Math.signum() function. Origin: siddhi-execution-math:5.0.4 Syntax INT math:signum( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that should be checked to be positive, negative or zero. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:signum(inValue) as sign insert into OutMediationStream; The function evaluates the 'inValue' given to be positive, negative or zero and directs the result to the output stream, 'OutMediationStream'. For example, signum(-6.32d) returns -1. sin (Function) This returns the sine of the value given in radians. This function wraps the java.lang.Math.sin() function. Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:sin( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose sine value should be found. Input is required to be in radians. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:sin(inValue) as sinValue insert into OutMediationStream; The function calculates the sine value of the given 'inValue' and directs the output to the output stream, 'OutMediationStream. For example, sin(6d) returns -0.27941549819892586. sinh (Function) This returns the hyperbolic sine of the value given in radians. This function wraps the java.lang.Math.sinh() function. Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:sinh( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose hyperbolic sine value should be found. Input is required to be in radians. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:sinh(inValue) as sinhValue insert into OutMediationStream; This function calculates the hyperbolic sine value of 'inValue' and directs the output to the output stream, 'OutMediationStream'. For example, sinh(6d) returns 201.71315737027922. sqrt (Function) This function returns the square-root of the given value. It wraps the java.lang.Math.sqrt() s function. Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:sqrt( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose square-root value should be found. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:sqrt(inValue) as sqrtValue insert into OutMediationStream; The function calculates the square-root value of the 'inValue' and directs the output to the output stream, 'OutMediationStream'. For example, sqrt(4d) returns 2. tan (Function) This function returns the tan of the given value in radians. It wraps the java.lang.Math.tan() function. Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:tan( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose tan value should be found. Input is required to be in radians. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:tan(inValue) as tanValue insert into OutMediationStream; This function calculates the tan value of the 'inValue' given and directs the output to the output stream, 'OutMediationStream'. For example, tan(6d) returns -0.29100619138474915. tanh (Function) This function returns the hyperbolic tangent of the value given in radians. It wraps the java.lang.Math.tanh() function. Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:tanh( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose hyperbolic tangent value should be found. Input is required to be in radians. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:tanh(inValue) as tanhValue insert into OutMediationStream; If the 'inVaue' in the input stream is given, this function calculates the hyperbolic tangent value of the same and directs the output to 'OutMediationStream' stream. For example, tanh(6d) returns 0.9999877116507956. toDegrees (Function) This function converts the value given in radians to degrees. It wraps the java.lang.Math.toDegrees() function. Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:toDegrees( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The input value in radians that should be converted to degrees. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:toDegrees(inValue) as degreesValue insert into OutMediationStream; The function converts the 'inValue' in the input stream from radians to degrees and directs the output to 'OutMediationStream' output stream. For example, toDegrees(6d) returns 343.77467707849394. toRadians (Function) This function converts the value given in degrees to radians. It wraps the java.lang.Math.toRadians() function. Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:toRadians( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The input value in degrees that should be converted to radians. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:toRadians(inValue) as radiansValue insert into OutMediationStream; This function converts the input, from degrees to radians and directs the result to 'OutMediationStream' output stream. For example, toRadians(6d) returns 0.10471975511965977. Rdbms cud (Stream Processor) This function performs SQL CUD (INSERT, UPDATE, DELETE) queries on data sources. Note: This function to work data sources should be set at the Siddhi Manager level. Origin: siddhi-store-rdbms:7.0.2 Syntax rdbms:cud( STRING datasource.name, STRING query) rdbms:cud( STRING datasource.name, STRING query, STRING|BOOL|INT|DOUBLE|FLOAT|LONG parameter) rdbms:cud( STRING datasource.name, STRING query, STRING|BOOL|INT|DOUBLE|FLOAT|LONG parameter, STRING|BOOL|INT|DOUBLE|FLOAT|LONG ...) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic datasource.name The name of the datasource for which the query should be performed. If Siddhi is used as a Java/Python library the datasource should be explicitly set in the siddhi manager in order for the function to work. STRING No No query The update, delete, or insert query(formatted according to the relevant database type) that needs to be performed. STRING No Yes parameter If the second parameter is a parametrised SQL query, then siddhi attributes can be passed to set the values of the parameters STRING BOOL INT DOUBLE FLOAT LONG Yes Yes System Parameters Name Description Default Value Possible Parameters perform.CUD.operations If this parameter is set to 'true', the RDBMS CUD function is enabled to perform CUD operations. false true false Extra Return Attributes Name Description Possible Types numRecords The number of records manipulated by the query. INT Examples EXAMPLE 1 from TriggerStream#rdbms:cud(\"SAMPLE_DB\", \"UPDATE Customers_Table SET customerName='abc' where customerName='xyz'\") select numRecords insert into RecordStream; This query updates the events from the input stream named 'TriggerStream' with an additional attribute named 'numRecords', of which the value indicates the number of records manipulated. The updated events are inserted into an output stream named 'RecordStream'. EXAMPLE 2 from TriggerStream#rdbms:cud(\"SAMPLE_DB\", \"UPDATE Customers_Table SET customerName=? where customerName=?\", changedName, previousName) select numRecords insert into RecordStream; This query updates the events from the input stream named 'TriggerStream' with an additional attribute named 'numRecords', of which the value indicates the number of records manipulated. The updated events are inserted into an output stream named 'RecordStream'. Here the values of attributes changedName and previousName in the event will be set to the query. query (Stream Processor) This function performs SQL retrieval queries on data sources. Note: This function to work data sources should be set at the Siddhi Manager level. Origin: siddhi-store-rdbms:7.0.2 Syntax rdbms:query( STRING datasource.name, STRING attribute.definition.list, STRING query) rdbms:query( STRING datasource.name, STRING attribute.definition.list, STRING query, STRING|BOOL|INT|DOUBLE|FLOAT|LONG parameter) rdbms:query( STRING datasource.name, STRING attribute.definition.list, STRING query, STRING|BOOL|INT|DOUBLE|FLOAT|LONG parameter, STRING|BOOL|INT|DOUBLE|FLOAT|LONG ...) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic datasource.name The name of the datasource for which the query should be performed. If Siddhi is used as a Java/Python library the datasource should be explicitly set in the siddhi manager in order for the function to work. STRING No No attribute.definition.list This is provided as a comma-separated list in the ' AttributeName AttributeType ' format. The SQL query is expected to return the attributes in the given order. e.g., If one attribute is defined here, the SQL query should return one column result set. If more than one column is returned, then the first column is processed. The Siddhi data types supported are 'STRING', 'INT', 'LONG', 'DOUBLE', 'FLOAT', and 'BOOL'. Mapping of the Siddhi data type to the database data type can be done as follows, Siddhi Datatype - Datasource Datatype STRING - CHAR , VARCHAR , LONGVARCHAR INT - INTEGER LONG - BIGINT DOUBLE - DOUBLE FLOAT - REAL BOOL - BIT STRING No No query The select query(formatted according to the relevant database type) that needs to be performed STRING No Yes parameter If the second parameter is a parametrised SQL query, then siddhi attributes can be passed to set the values of the parameters STRING BOOL INT DOUBLE FLOAT LONG Yes Yes Extra Return Attributes Name Description Possible Types attributeName The return attributes will be the ones defined in the parameter attribute.definition.list . STRING INT LONG DOUBLE FLOAT BOOL Examples EXAMPLE 1 from TriggerStream#rdbms:query('SAMPLE_DB', 'creditcardno string, country string, transaction string, amount int', 'select * from Transactions_Table') select creditcardno, country, transaction, amount insert into recordStream; Events inserted into recordStream includes all records matched for the query i.e an event will be generated for each record retrieved from the datasource. The event will include as additional attributes, the attributes defined in the attribute.definition.list (creditcardno, country, transaction, amount). EXAMPLE 2 from TriggerStream#rdbms:query('SAMPLE_DB', 'creditcardno string, country string,transaction string, amount int', 'select * from where country=?', countrySearchWord) select creditcardno, country, transaction, amount insert into recordStream; Events inserted into recordStream includes all records matched for the query i.e an event will be generated for each record retrieved from the datasource. The event will include as additional attributes, the attributes defined in the attribute.definition.list (creditcardno, country, transaction, amount). countrySearchWord value from the event will be set in the query when querying the datasource. Regex find (Function) Finds the subsequence that matches the given regex pattern. Origin: siddhi-execution-regex:5.0.5 Syntax BOOL regex:find( STRING regex, STRING input.sequence) BOOL regex:find( STRING regex, STRING input.sequence, INT starting.index) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic regex A regular expression that is matched to a sequence in order to find the subsequence of the same. For example, \\d\\d(.*)WSO2 . STRING No Yes input.sequence The input sequence to be matched with the regular expression. For example, 21 products are produced by WSO2 . STRING No Yes starting.index The starting index of the input sequence from where the input sequence ismatched with the given regex pattern.For example, 10 . 0 INT Yes Yes Examples EXAMPLE 1 regex:find('\\d\\d(.*)WSO2', '21 products are produced by WSO2 currently') This method attempts to find the subsequence of the input.sequence that matches the regex pattern, \\d\\d(. )WSO2 . It returns true as a subsequence exists. EXAMPLE 2 regex:find('\\d\\d(.*)WSO2', '21 products are produced by WSO2.', 4) This method attempts to find the subsequence of the input.sequence that matches the regex pattern, \\d\\d(. )WSO2 starting from index 4 . It returns 'false' as subsequence does not exists. group (Function) Returns the subsequence captured by the given group during the regex match operation. Origin: siddhi-execution-regex:5.0.5 Syntax STRING regex:group( STRING regex, STRING input.sequence, INT group.id) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic regex A regular expression. For example, \\d\\d(.*)WSO2. STRING No Yes input.sequence The input sequence to be matched with the regular expression. For example, 2 1 products are produced by WSO2 . STRING No Yes group.id The given group id of the regex expression. For example, 2 . INT No Yes Examples EXAMPLE 1 regex:group('\\d\\d(.*)(WSO2.*)(WSO2.*)', '21 products are produced within 10 years by WSO2 currently by WSO2 employees', 3) Function returns 'WSO2 employees', the subsequence captured by the groupID 3 according to the regex pattern, \\d\\d(. )(WSO2. )(WSO2.*) . lookingAt (Function) Matches the input.sequence from the beginning against the regex pattern, and unlike regex:matches() it does not require that the entire input.sequence be matched. Origin: siddhi-execution-regex:5.0.5 Syntax BOOL regex:lookingAt( STRING regex, STRING input.sequence) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic regex A regular expression. For example, \\d\\d(.*)WSO2 . STRING No Yes input.sequence The input sequence to be matched with the regular expression. For example, 21 products are produced by WSO2 . STRING No Yes Examples EXAMPLE 1 regex:lookingAt('\\d\\d(.*)(WSO2.*)', '21 products are produced by WSO2 currently in Sri Lanka') Function matches the input.sequence against the regex pattern, \\d\\d(. )(WSO2. ) from the beginning, and as it matches it returns true . EXAMPLE 2 regex:lookingAt('WSO2(.*)middleware(.*)', 'sample test string and WSO2 is situated in trace and it's a middleware company') Function matches the input.sequence against the regex pattern, WSO2(. )middleware(. ) from the beginning, and as it does not match it returns false . matches (Function) Matches the entire input.sequence against the regex pattern. Origin: siddhi-execution-regex:5.0.5 Syntax BOOL regex:matches( STRING regex, STRING input.sequence) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic regex A regular expression. For example, \\d\\d(.*)WSO2 . STRING No Yes input.sequence The input sequence to be matched with the regular expression. For example, 21 products are produced by WSO2 . STRING No Yes Examples EXAMPLE 1 regex:matches('WSO2(.*)middleware(.*)', 'WSO2 is situated in trace and its a middleware company') Function matches the entire input.sequence against WSO2(. )middleware(. ) regex pattern, and as it matches it returns true . EXAMPLE 2 regex:matches('WSO2(.*)middleware', 'WSO2 is situated in trace and its a middleware company') Function matches the entire input.sequence against WSO2(.*)middleware regex pattern. As it does not match it returns false . Reorder akslack (Stream Processor) Stream processor performs reordering of out-of-order events optimized for a givenparameter using AQ-K-Slack algorithm . This is best for reordering events on attributes those are used for aggregations.data . Origin: siddhi-execution-reorder:5.0.3 Syntax reorder:akslack( LONG timestamp, INT|FLOAT|LONG|DOUBLE correlation.field) reorder:akslack( LONG timestamp, INT|FLOAT|LONG|DOUBLE correlation.field, LONG batch.size) reorder:akslack( LONG timestamp, INT|FLOAT|LONG|DOUBLE correlation.field, LONG batch.size, LONG timeout) reorder:akslack( LONG timestamp, INT|FLOAT|LONG|DOUBLE correlation.field, LONG batch.size, LONG timeout, LONG max.k) reorder:akslack( LONG timestamp, INT|FLOAT|LONG|DOUBLE correlation.field, LONG batch.size, LONG timeout, LONG max.k, BOOL discard.late.arrival) reorder:akslack( LONG timestamp, INT|FLOAT|LONG|DOUBLE correlation.field, LONG batch.size, LONG timeout, LONG max.k, BOOL discard.late.arrival, DOUBLE error.threshold, DOUBLE confidence.level) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic timestamp The event timestamp on which the events should be ordered. LONG No Yes correlation.field By monitoring the changes in this field Alpha K-Slack dynamically optimises its behavior. This field is used to calculate the runtime window coverage threshold, which represents the upper limit set for unsuccessfully handled late arrivals. INT FLOAT LONG DOUBLE No Yes batch.size The parameter 'batch.size' denotes the number of events that should be considered in the calculation of an alpha value. This should be greater than or equal to 15. 10,000 LONG Yes No timeout A timeout value in milliseconds, where the buffered events who are older than the given timeout period get flushed every second. -1 (timeout is infinite) LONG Yes No max.k The maximum K-Slack window threshold ('K' parameter). 9,223,372,036,854,775,807 (The maximum Long value) LONG Yes No discard.late.arrival If set to true the processor would discarded the out-of-order events arriving later than the K-Slack window, and in otherwise it allows the late arrivals to proceed. false BOOL Yes No error.threshold The error threshold to be applied in Alpha K-Slack algorithm. 0.03 (3%) DOUBLE Yes No confidence.level The confidence level to be applied in Alpha K-Slack algorithm. 0.95 (95%) DOUBLE Yes No Examples EXAMPLE 1 define stream StockStream (eventTime long, symbol string, volume long); @info(name = 'query1') from StockStream#reorder:akslack(eventTime, volume, 20)#window.time(5 min) select eventTime, symbol, sum(volume) as total insert into OutputStream; The query reorders events based on the 'eventTime' attribute value and optimises for aggregating 'volume' attribute considering last 20 events. kslack (Stream Processor) Stream processor performs reordering of out-of-order events using K-Slack algorithm . Origin: siddhi-execution-reorder:5.0.3 Syntax reorder:kslack( LONG timestamp) reorder:kslack( LONG timestamp, LONG timeout) reorder:kslack( LONG timestamp, BOOL discard.late.arrival) reorder:kslack( LONG timestamp, LONG timeout, LONG max.k) reorder:kslack( LONG timestamp, LONG timeout, BOOL discard.late.arrival) reorder:kslack( LONG timestamp, LONG timeout, LONG max.k, BOOL discard.late.arrival) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic timestamp The event timestamp on which the events should be ordered. LONG No Yes timeout A timeout value in milliseconds, where the buffered events who are older than the given timeout period get flushed every second. -1 (timeout is infinite) LONG Yes No max.k The maximum K-Slack window threshold ('K' parameter). 9,223,372,036,854,775,807 (The maximum Long value) LONG Yes No discard.late.arrival If set to true the processor would discarded the out-of-order events arriving later than the K-Slack window, and in otherwise it allows the late arrivals to proceed. false BOOL Yes No Examples EXAMPLE 1 define stream StockStream (eventTime long, symbol string, volume long); @info(name = 'query1') from StockStream#reorder:kslack(eventTime, 5000) select eventTime, symbol, volume insert into OutputStream; The query reorders events based on the 'eventTime' attribute value, and it forcefully flushes all the events who have arrived older than the given 'timeout' value ( 5000 milliseconds) every second. Script javascript (Script) This extension allows you to include JavaScript functions within the Siddhi Query Language. Origin: siddhi-script-js:5.0.2 Syntax define function FunctionName [javascript] return type { // Script code }; Examples EXAMPLE 1 define function concatJ[JavaScript] return string {\" var str1 = data[0]; var str2 = data[1]; var str3 = data[2]; var res = str1.concat(str2,str3); return res; }; This JS function will consume 3 var variables, concatenate them and will return as a string Sink email (Sink) The email sink uses the 'smtp' server to publish events via emails. The events can be published in 'text', 'xml' or 'json' formats. The user can define email sink parameters in either the ' SP_HOME /conf/ PROFILE /deployment yaml' file or in the stream definition. The email sink first checks the stream definition for parameters, and if they are no configured there, it checks the 'deployment.yaml' file. If the parameters are not configured in either place, default values are considered for optional parameters. If you need to configure server system parameters that are not provided as options in the stream definition, then those parameters need to be defined them in the 'deployment.yaml' file under 'email sink properties'. For more information about the SMTP server parameters, see https://javaee.github.io/javamail/SMTP-Transport. Further, some email accounts are required to enable the 'access to less secure apps' option. For gmail accounts, you can enable this option via https://myaccount.google.com/lesssecureapps. Origin: siddhi-io-email:2.0.5 Syntax @sink(type=\"email\", username=\" STRING \", address=\" STRING \", password=\" STRING \", host=\" STRING \", port=\" INT \", ssl.enable=\" BOOL \", auth=\" BOOL \", content.type=\" STRING \", subject=\" STRING \", to=\" STRING \", cc=\" STRING \", bcc=\" STRING \", attachments=\" STRING \", connection.pool.size=\" INT \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic username The username of the email account that is used to send emails. e.g., 'abc' is the username of the 'abc@gmail.com' account. STRING No No address The address of the email account that is used to send emails. STRING No No password The password of the email account. STRING No No host The host name of the SMTP server. e.g., 'smtp.gmail.com' is a host name for a gmail account. The default value 'smtp.gmail.com' is only valid if the email account is a gmail account. smtp.gmail.com STRING Yes No port The port that is used to create the connection. '465' the default value is only valid is SSL is enabled. INT Yes No ssl.enable This parameter specifies whether the connection should be established via a secure connection or not. The value can be either 'true' or 'false'. If it is 'true', then the connection is establish via the 493 port which is a secure connection. true BOOL Yes No auth This parameter specifies whether to use the 'AUTH' command when authenticating or not. If the parameter is set to 'true', an attempt is made to authenticate the user using the 'AUTH' command. true BOOL Yes No content.type The content type can be either 'text/plain' or 'text/html'. text/plain STRING Yes No subject The subject of the mail to be send. STRING No Yes to The address of the 'to' recipient. If there are more than one 'to' recipients, then all the required addresses can be given as a comma-separated list. STRING No Yes cc The address of the 'cc' recipient. If there are more than one 'cc' recipients, then all the required addresses can be given as a comma-separated list. None STRING Yes No bcc The address of the 'bcc' recipient. If there are more than one 'bcc' recipients, then all the required addresses can be given as a comma-separated list. None STRING Yes No attachments File paths of the files that need to be attached to the email. These paths should be absolute paths. They can be either directories or files . If the path is to a directory, all the files located at the first level (i.e., not within another sub directory) are attached. None STRING Yes Yes connection.pool.size Number of concurrent Email client connections. 1 INT Yes No System Parameters Name Description Default Value Possible Parameters mail.smtp.ssl.trust If this parameter is se, and a socket factory has not been specified, it enables the use of a MailSSLSocketFactory. If this parameter is set to \" \", all the hosts are trusted. If it is set to a whitespace-separated list of hosts, only those specified hosts are trusted. If not, the hosts trusted depends on the certificate presented by the server. String mail.smtp.connectiontimeout The socket connection timeout value in milliseconds. infinite timeout Any Integer mail.smtp.timeout The socket I/O timeout value in milliseconds. infinite timeout Any Integer mail.smtp.from The email address to use for the SMTP MAIL command. This sets the envelope return address. Defaults to msg.getFrom() or InternetAddress.getLocalAddress(). Any valid email address mail.smtp.localport The local port number to bind to when creating the SMTP socket. Defaults to the port number picked by the Socket class. Any Integer mail.smtp.ehlo If this parameter is set to 'false', you must not attempt to sign in with the EHLO command. true true or false mail.smtp.auth.login.disable If this is set to 'true', it is not allowed to use the 'AUTH LOGIN' command. false true or false mail.smtp.auth.plain.disable If this parameter is set to 'true', it is not allowed to use the 'AUTH PLAIN' command. false true or false mail.smtp.auth.digest-md5.disable If this parameter is set to 'true', it is not allowed to use the 'AUTH DIGEST-MD5' command. false true or false mail.smtp.auth.ntlm.disable If this parameter is set to 'true', it is not allowed to use the 'AUTH NTLM' command false true or false mail.smtp.auth.ntlm.domain The NTLM authentication domain. None The valid NTLM authentication domain name. mail.smtp.auth.ntlm.flags NTLM protocol-specific flags. For more details, see http://curl.haxx.se/rfc/ntlm.html#theNtlmFlags. None Valid NTLM protocol-specific flags. mail.smtp.dsn.notify The NOTIFY option to the RCPT command. None Either 'NEVER', or a combination of 'SUCCESS', 'FAILURE', and 'DELAY' (separated by commas). mail.smtp.dsn.ret The 'RET' option to the 'MAIL' command. None Either 'FULL' or 'HDRS'. mail.smtp.sendpartial If this parameter is set to 'true' and a message is addressed to both valid and invalid addresses, the message is sent with a log that reports the partial failure with a 'SendFailedException' error. If this parameter is set to 'false' (which is default), the message is not sent to any of the recipients when the recipient lists contain one or more invalid addresses. false true or false mail.smtp.sasl.enable If this parameter is set to 'true', the system attempts to use the 'javax.security.sasl' package to choose an authentication mechanism for the login. false true or false mail.smtp.sasl.mechanisms Enter a space or a comma-separated list of SASL mechanism names that the system shouldt try to use. None mail.smtp.sasl.authorizationid The authorization ID to be used in the SASL authentication. If no value is specified, the authentication ID (i.e., username) is used. username Valid ID mail.smtp.sasl.realm The realm to be used with the 'DIGEST-MD5' authentication. None mail.smtp.quitwait If this parameter is set to 'false', the 'QUIT' command is issued and the connection is immediately closed. If this parameter is set to 'true' (which is default), the transport waits for the response to the QUIT command. false true or false mail.smtp.reportsuccess If this parameter is set to 'true', the transport to includes an 'SMTPAddressSucceededException' for each address to which the message is successfully delivered. false true or false mail.smtp.socketFactory If this parameter is set to a class that implements the 'javax.net.SocketFactory' interface, this class is used to create SMTP sockets. None Socket Factory mail.smtp.socketFactory.class If this parameter is set, it specifies the name of a class that implements the 'javax.net.SocketFactory interface'. This class is used to create SMTP sockets. None mail.smtp.socketFactory.fallback If this parameter is set to 'true', the failure to create a socket using the specified socket factory class causes the socket to be created using the 'java.net.Socket' class. true true or false mail.smtp.socketFactory.port This specifies the port to connect to when using the specified socket factory. 25 Valid port number mail.smtp.ssl.protocols This specifies the SSL protocols that need to be enabled for the SSL connections. None This parameter specifies a whitespace separated list of tokens that are acceptable to the 'javax.net.ssl.SSLSocket.setEnabledProtocols' method. mail.smtp.starttls.enable If this parameter is set to 'true', it is possible to issue the 'STARTTLS' command (if supported by the server) to switch the connection to a TLS-protected connection before issuing any login commands. false true or false mail.smtp.starttls.required If this parameter is set to 'true', it is required to use the 'STARTTLS' command. If the server does not support the 'STARTTLS' command, or if the command fails, the connection method will fail. false true or false mail.smtp.socks.host This specifies the host name of a SOCKS5 proxy server to be used for the connections to the mail server. None mail.smtp.socks.port This specifies the port number for the SOCKS5 proxy server. This needs to be used only if the proxy server is not using the standard port number 1080. 1080 valid port number mail.smtp.auth.ntlm.disable If this parameter is set to 'true', the AUTH NTLM command cannot be issued. false true or false mail.smtp.mailextension The extension string to be appended to the MAIL command. None mail.smtp.userset If this parameter is set to 'true', you should use the 'RSET' command instead of the 'NOOP' command in the 'isConnected' method. In some scenarios, 'sendmail' responds slowly after many 'NOOP' commands. This is avoided by using 'RSET' instead. false true or false Examples EXAMPLE 1 @sink(type='email', @map(type ='json'), username='sender.account', address='sender.account@gmail.com',password='account.password',subject='Alerts from Wso2 Stream Processor',to='{{email}}',)define stream FooStream (email string, loginId int, name string); This example illustrates how to publish events via an email sink based on the values provided for the mandatory parameters. As shown in the example, it publishes events from the 'FooStream' in 'json' format as emails to the specified 'to' recipients via the email sink. The email is sent from the 'sender.account@gmail.com' email address via a secure connection. EXAMPLE 2 @sink(type='email', @map(type ='json'), subject='Alerts from Wso2 Stream Processor',to='{{email}}',)define stream FooStream (email string, loginId int, name string); This example illustrates how to configure the query parameters and the system parameters in the 'deployment.yaml' file. Corresponding parameters need to be configured under 'email', and namespace:'sink' as follows: siddhi: extensions: - extension: name:'email' namespace:'sink' properties: username: sender's email username address: sender's email address password: sender's email password As shown in the example, events from the FooStream are published in 'json' format via the email sink as emails to the given 'to' recipients. The email is sent from the 'sender.account@gmail.com' address via a secure connection. EXAMPLE 3 @sink(type='email', @map(type ='json'), username='sender.account', address='sender.account@gmail.com',password='account.password',host='smtp.gmail.com',port='465',ssl.enable='true',auth='true',content.type='text/html',subject='Alerts from Wso2 Stream Processor-{{name}}',to='to1.account@gmail.com, to2.account@gmail.com',cc='cc1.account@gmail.com, cc2.account@gmail.com',bcc='bcc1.account@gmail.com)define stream FooStream (name string, age int, country string); This example illustrates how to publish events via the email sink. Events from the 'FooStream' stream are published in 'xml' format via the email sink as a text/html message and sent to the specified 'to', 'cc', and 'bcc' recipients via a secure connection. The 'name' namespace in the 'subject' attribute is the value of the 'name' parameter in the corresponding output event. EXAMPLE 4 @sink(type='email', @map(type ='json'), username='sender.account', address='sender.account@gmail.com',password='account.password',host='smtp.gmail.com',port='465',ssl.enable='true',auth='true',content.type='text/html',subject='Alerts from Wso2 Stream Processor-{{name}}',to='to1.account@gmail.com, to2.account@gmail.com',cc='cc1.account@gmail.com, cc2.account@gmail.com',bcc='bcc1.account@gmail.comattachments= '{{attachments}}')define stream FooStream (name string, age int, country string, attachments string); This example illustrates how to publish events via the email sink. Here, the email also contains attachments. Events from the FooStream are published in 'xml' format via the email sink as a 'text/html' message to the specified 'to','cc', and 'bcc' recipients via a secure connection. The 'name' namespace in the 'subject' attribute is the value for the 'name' parameter in the corresponding output event. The attachments included in the email message are the local files available in the path specified as the value for the 'attachments' attribute. file (Sink) File Sink can be used to publish (write) event data which is processed within siddhi to files. Siddhi-io-file sink provides support to write both textual and binary data into files Origin: siddhi-io-file:2.0.3 Syntax @sink(type=\"file\", file.uri=\" STRING \", append=\" BOOL \", add.line.separator=\" BOOL \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic file.uri Used to specify the file for data to be written. STRING No Yes append This parameter is used to specify whether the data should be append to the file or not. If append = 'true', data will be write at the end of the file without changing the existing content. If file does not exist, a new fill will be crated and then data will be written. If append append = 'false', If given file exists, existing content will be deleted and then data will be written back to the file. If given file does not exist, a new file will be created and then data will be written on it. true BOOL Yes No add.line.separator This parameter is used to specify whether events added to the file should be separated by a newline. If add.event.separator= 'true',then a newline will be added after data is added to the file. true. (However, if csv mapper is used, it is false) BOOL Yes No Examples EXAMPLE 1 @sink(type='file', @map(type='json'), append='false', file.uri='/abc/{{symbol}}.txt') define stream BarStream (symbol string, price float, volume long); Under above configuration, for each event, a file will be generated if there's no such a file,and then data will be written to that file as json messagesoutput will looks like below. { \"event\":{ \"symbol\":\"WSO2\", \"price\":55.6, \"volume\":100 } } grpc (Sink) This extension publishes event data encoded into GRPC Classes as defined in the user input jar. This extension has a default gRPC service classes added. The default service is called \"EventService\". Please find the protobuf definition here . If we want to use our custom gRPC services, we have to pack auto-generated gRPC service classes and protobuf classes into a jar file and add it into the project classpath (or to the jars folder in the siddhi-tooling folder if we use it with siddhi-tooling ). Please find the custom protobuf definition that uses in examples here . This grpc sink is used for scenarios where we send a request and don't expect a response back. I.e getting a google.protobuf.Empty response back. Origin: siddhi-io-grpc:1.0.5 Syntax @sink(type=\"grpc\", publisher.url=\" STRING \", headers=\" STRING \", idle.timeout=\" LONG \", keep.alive.time=\" LONG \", keep.alive.timeout=\" LONG \", keep.alive.without.calls=\" BOOL \", enable.retry=\" BOOL \", max.retry.attempts=\" INT \", retry.buffer.size=\" LONG \", per.rpc.buffer.size=\" LONG \", channel.termination.waiting.time=\" LONG \", truststore.file=\" STRING \", truststore.password=\" STRING \", truststore.algorithm=\" STRING \", tls.store.type=\" STRING \", keystore.file=\" STRING \", keystore.password=\" STRING \", keystore.algorithm=\" STRING \", enable.ssl=\" BOOL \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic publisher.url The url to which the outgoing events should be published via this extension. This url should consist the host hostPort, port, fully qualified service name, method name in the following format. grpc://0.0.0.0:9763/ serviceName / methodName For example: grpc://0.0.0.0:9763/org.wso2.grpc.EventService/consume STRING No No headers GRPC Request headers in format \"' key : value ',' key : value '\" . If header parameter is not provided just the payload is sent - STRING Yes No idle.timeout Set the duration in seconds without ongoing RPCs before going to idle mode. 1800 LONG Yes No keep.alive.time Sets the time in seconds without read activity before sending a keepalive ping. Keepalives can increase the load on services so must be used with caution. By default set to Long.MAX_VALUE which disables keep alive pinging. Long.MAX_VALUE LONG Yes No keep.alive.timeout Sets the time in seconds waiting for read activity after sending a keepalive ping. 20 LONG Yes No keep.alive.without.calls Sets whether keepalive will be performed when there are no outstanding RPC on a connection. false BOOL Yes No enable.retry Enables the retry mechanism provided by the gRPC library. false BOOL Yes No max.retry.attempts Sets max number of retry attempts. The total number of retry attempts for each RPC will not exceed this number even if service config may allow a higher number. 5 INT Yes No retry.buffer.size Sets the retry buffer size in bytes. If the buffer limit is exceeded, no RPC could retry at the moment, and in hedging case all hedges but one of the same RPC will cancel. 16777216 LONG Yes No per.rpc.buffer.size Sets the per RPC buffer limit in bytes used for retry. The RPC is not retriable if its buffer limit is exceeded. 1048576 LONG Yes No channel.termination.waiting.time The time in seconds to wait for the channel to become terminated, giving up if the timeout is reached. 5 LONG Yes No truststore.file the file path of truststore. If this is provided then server authentication is enabled - STRING Yes No truststore.password the password of truststore. If this is provided then the integrity of the keystore is checked - STRING Yes No truststore.algorithm the encryption algorithm to be used for server authentication - STRING Yes No tls.store.type TLS store type - STRING Yes No keystore.file the file path of keystore. If this is provided then client authentication is enabled - STRING Yes No keystore.password the password of keystore - STRING Yes No keystore.algorithm the encryption algorithm to be used for client authentication - STRING Yes No enable.ssl to enable ssl. If set to true and truststore.file is not given then it will be set to default carbon jks by default FALSE BOOL Yes No Examples EXAMPLE 1 @sink(type='grpc', publisher.url = 'grpc://134.23.43.35:8080/org.wso2.grpc.EventService/consume', @map(type='json')) define stream FooStream (message String); Here a stream named FooStream is defined with grpc sink. A grpc server should be running at 194.23.98.100 listening to port 8080. sink.id is set to 1 here. So we can write a source with sink.id 1 so that it will listen to responses for requests published from this stream. Note that since we are using EventService/consume the sink will be operating in default mode EXAMPLE 2 @sink(type='grpc', publisher.url = 'grpc://134.23.43.35:8080/org.wso2.grpc.EventService/consume', headers='{{headers}}', @map(type='json'), @payload('{{message}}')) define stream FooStream (message String, headers String); A similar example to above but with headers. Headers are also send into the stream as a data. In the sink headers dynamic property reads the value and sends it as MetaData with the request EXAMPLE 3 @sink(type='grpc', publisher.url = 'grpc://134.23.43.35:8080/org.wso2.grpc.MyService/send', @map(type='protobuf'), define stream FooStream (stringValue string, intValue int,longValue long,booleanValue bool,floatValue float,doubleValue double); Here a stream named FooStream is defined with grpc sink. A grpc server should be running at 134.23.43.35 listening to port 8080 since there is no mapper provided, attributes of stream definition should be as same as the attributes of protobuf message definition. EXAMPLE 4 @sink(type='grpc', publisher.url = 'grpc://134.23.43.35:8080/org.wso2.grpc.MyService/testMap', @map(type='protobuf'), define stream FooStream (stringValue string, intValue int,map object); Here a stream named FooStream is defined with grpc sink. A grpc server should be running at 134.23.43.35 listening to port 8080. The 'map object' in the stream definition defines that this stream is going to use Map object with grpc service. We can use any map object that extends 'java.util.AbstractMap' class. EXAMPLE 5 @sink(type='grpc', publisher.url = 'grpc://134.23.43.35:8080/org.wso2.grpc.MyService/testMap', @map(type='protobuf', @payload(stringValue='a',longValue='b',intValue='c',booleanValue='d',floatValue = 'e', doubleValue = 'f'))) define stream FooStream (a string, b long, c int,d bool,e float,f double); Here a stream named FooStream is defined with grpc sink. A grpc server should be running at 194.23.98.100 listening to port 8080. @payload is provided in this stream, therefore we can use any name for the attributes in the stream definition, but we should correctly map those names with protobuf message attributes. If we are planning to send metadata within a stream we should use @payload to map attributes to identify the metadata attribute and the protobuf attributes separately. EXAMPLE 6 @sink(type='grpc', publisher.url = 'grpc://194.23.98.100:8888/org.wso2.grpc.test.StreamService/clientStream', @map(type='protobuf')) define stream FooStream (stringValue string, intValue int,longValue long,booleanValue bool,floatValue float,doubleValue double); Here in the grpc sink, we are sending a stream of requests to the server that runs on 194.23.98.100 and port 8888. When we need to send a stream of requests from the grpc sink we have to define a client stream RPC method.Then the siddhi will identify whether it's a unary method or a stream method and send requests according to the method type. grpc-call (Sink) This extension publishes event data encoded into GRPC Classes as defined in the user input jar. This extension has a default gRPC service classes jar added. The default service is called \"EventService\". Please find the protobuf definition here . If we want to use our custom gRPC services, we have to pack auto-generated gRPC service classes and protobuf classes into a jar file and add it into the project classpath (or to the jars folder in the siddhi-tooling folder if we use it with siddhi-tooling ). Please find the custom protobuf definition that uses in examples here . This grpc-call sink is used for scenarios where we send a request out and expect a response back. In default mode this will use EventService process method. grpc-call-response source is used to receive the responses. A unique sink.id is used to correlate between the sink and its corresponding source. Origin: siddhi-io-grpc:1.0.5 Syntax @sink(type=\"grpc-call\", publisher.url=\" STRING \", sink.id=\" INT \", headers=\" STRING \", idle.timeout=\" LONG \", keep.alive.time=\" LONG \", keep.alive.timeout=\" LONG \", keep.alive.without.calls=\" BOOL \", enable.retry=\" BOOL \", max.retry.attempts=\" INT \", retry.buffer.size=\" LONG \", per.rpc.buffer.size=\" LONG \", channel.termination.waiting.time=\" LONG \", max.inbound.message.size=\" LONG \", max.inbound.metadata.size=\" LONG \", truststore.file=\" STRING \", truststore.password=\" STRING \", truststore.algorithm=\" STRING \", tls.store.type=\" STRING \", keystore.file=\" STRING \", keystore.password=\" STRING \", keystore.algorithm=\" STRING \", enable.ssl=\" BOOL \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic publisher.url The url to which the outgoing events should be published via this extension. This url should consist the host hostPort, port, fully qualified service name, method name in the following format. grpc://0.0.0.0:9763/ serviceName / methodName For example: grpc://0.0.0.0:9763/org.wso2.grpc.EventService/consume STRING No No sink.id a unique ID that should be set for each grpc-call-sink. There is a 1:1 mapping between grpc-call sinks and grpc-call-response sources. Each sink has one particular source listening to the responses to requests published from that sink. So the same sink.id should be given when writing the source also. INT No No headers GRPC Request headers in format \"' key : value ',' key : value '\" . If header parameter is not provided just the payload is sent - STRING Yes No idle.timeout Set the duration in seconds without ongoing RPCs before going to idle mode. 1800 LONG Yes No keep.alive.time Sets the time in seconds without read activity before sending a keepalive ping. Keepalives can increase the load on services so must be used with caution. By default set to Long.MAX_VALUE which disables keep alive pinging. Long.MAX_VALUE LONG Yes No keep.alive.timeout Sets the time in seconds waiting for read activity after sending a keepalive ping. 20 LONG Yes No keep.alive.without.calls Sets whether keepalive will be performed when there are no outstanding RPC on a connection. false BOOL Yes No enable.retry Enables the retry and hedging mechanism provided by the gRPC library. false BOOL Yes No max.retry.attempts Sets max number of retry attempts. The total number of retry attempts for each RPC will not exceed this number even if service config may allow a higher number. 5 INT Yes No retry.buffer.size Sets the retry buffer size in bytes. If the buffer limit is exceeded, no RPC could retry at the moment, and in hedging case all hedges but one of the same RPC will cancel. 16777216 LONG Yes No per.rpc.buffer.size Sets the per RPC buffer limit in bytes used for retry. The RPC is not retriable if its buffer limit is exceeded. 1048576 LONG Yes No channel.termination.waiting.time The time in seconds to wait for the channel to become terminated, giving up if the timeout is reached. 5 LONG Yes No max.inbound.message.size Sets the maximum message size allowed to be received on the channel in bytes 4194304 LONG Yes No max.inbound.metadata.size Sets the maximum size of metadata allowed to be received in bytes 8192 LONG Yes No truststore.file the file path of truststore. If this is provided then server authentication is enabled - STRING Yes No truststore.password the password of truststore. If this is provided then the integrity of the keystore is checked - STRING Yes No truststore.algorithm the encryption algorithm to be used for server authentication - STRING Yes No tls.store.type TLS store type - STRING Yes No keystore.file the file path of keystore. If this is provided then client authentication is enabled - STRING Yes No keystore.password the password of keystore - STRING Yes No keystore.algorithm the encryption algorithm to be used for client authentication - STRING Yes No enable.ssl to enable ssl. If set to true and truststore.file is not given then it will be set to default carbon jks by default FALSE BOOL Yes No Examples EXAMPLE 1 @sink(type='grpc-call', publisher.url = 'grpc://194.23.98.100:8080/EventService/process', sink.id= '1', @map(type='json')) define stream FooStream (message String); @source(type='grpc-call-response', sink.id= '1') define stream BarStream (message String); Here a stream named FooStream is defined with grpc sink. A grpc server should be running at 194.23.98.100 listening to port 8080. sink.id is set to 1 here. So we can write a source with sink.id 1 so that it will listen to responses for requests published from this stream. Note that since we are using EventService/process the sink will be operating in default mode EXAMPLE 2 @sink(type='grpc-call', publisher.url = 'grpc://194.23.98.100:8080/EventService/process', sink.id= '1', @map(type='json')) define stream FooStream (message String); @source(type='grpc-call-response', sink.id= '1') define stream BarStream (message String); Here with the same FooStream definition we have added a BarStream which has a grpc-call-response source with the same sink.id 1. So the responses for calls sent from the FooStream will be added to BarStream. EXAMPLE 3 @sink(type='grpc-call', publisher.url = 'grpc://194.23.98.100:8888/org.wso2.grpc.test.MyService/process', sink.id= '1', @map(type='protobuf')) define stream FooStream (stringValue string, intValue int,longValue long,booleanValue bool,floatValue float,doubleValue double); @source(type='grpc-call-response', receiver.url = 'grpc://localhost:8888/org.wso2.grpc.MyService/process', sink.id= '1', @map(type='protobuf'))define stream FooStream (stringValue string, intValue int,longValue long,booleanValue bool,floatValue float,doubleValue double); Here a stream named FooStream is defined with grpc sink. A grpc server should be running at 194.23.98.100 listening to port 8080. We have added another stream called BarStream which is a grpc-call-response source with the same sink.id 1 and as same as FooStream definition. So the responses for calls sent from the FooStream will be added to BarStream. Since there is no mapping available in the stream definition attributes names should be as same as the attributes of the protobuf message definition. (Here the only reason we provide receiver.url in the grpc-call-response source is for protobuf mapper to map Response into a siddhi event, we can give any address and any port number in the URL, but we should provide the service name and the method name correctly) EXAMPLE 4 @sink(type='grpc-call', publisher.url = 'grpc://194.23.98.100:8888/org.wso2.grpc.test.MyService/process', sink.id= '1', @map(type='protobuf', @payload(stringValue='a',longValue='c',intValue='b',booleanValue='d',floatValue = 'e', doubleValue = 'f')))define stream FooStream (a string, b int,c long,d bool,e float,f double); @source(type='grpc-call-response', receiver.url = 'grpc://localhost:8888/org.wso2.grpc.test.MyService/process', sink.id= '1', @map(type='protobuf',@attributes(a = 'stringValue', b = 'intValue', c = 'longValue',d = 'booleanValue', e ='floatValue', f ='doubleValue')))define stream FooStream (a string, b int,c long,d bool,e float,f double); Here with the same FooStream definition we have added a BarStream which has a grpc-call-response source with the same sink.id 1. So the responses for calls sent from the FooStream will be added to BarStream. In this stream we provided mapping for both the sink and the source. so we can use any name for the attributes in the stream definition, but we have to map those attributes with correct protobuf attributes. As same as the grpc-sink, if we are planning to use metadata we should map the attributes. grpc-service-response (Sink) This extension is used to send responses back to a gRPC client after receiving requests through grpc-service source. This correlates with the particular source using a unique source.id Origin: siddhi-io-grpc:1.0.5 Syntax @sink(type=\"grpc-service-response\", source.id=\" INT \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic source.id A unique id to identify the correct source to which this sink is mapped. There is a 1:1 mapping between source and sink INT No No Examples EXAMPLE 1 @sink(type='grpc-service-response', source.id='1', @map(type='json')) define stream BarStream (messageId String, message String); @source(type='grpc-service', url='grpc://134.23.43.35:8080/org.wso2.grpc.EventService/process', source.id='1', @map(type='json', @attributes(messageId='trp:messageId', message='message'))) define stream FooStream (messageId String, message String); from FooStream select * insert into BarStream; The grpc requests are received through the grpc-service sink. Each received event is sent back through grpc-service-source. This is just a passthrough through Siddhi as we are selecting everything from FooStream and inserting into BarStream. http (Sink) HTTP sink publishes messages via HTTP or HTTPS protocols using methods such as POST, GET, PUT, and DELETE on formats text , XML and JSON . It can also publish to endpoints protected by basic authentication or OAuth 2.0. Origin: siddhi-io-http:2.2.0 Syntax @sink(type=\"http\", publisher.url=\" STRING \", basic.auth.username=\" STRING \", basic.auth.password=\" STRING \", https.truststore.file=\" STRING \", https.truststore.password=\" STRING \", oauth.username=\" STRING \", oauth.password=\" STRING \", consumer.key=\" STRING \", consumer.secret=\" STRING \", token.url=\" STRING \", refresh.token=\" STRING \", headers=\" STRING \", method=\" STRING \", socket.idle.timeout=\" INT \", chunk.disabled=\" BOOL \", ssl.protocol=\" STRING \", ssl.verification.disabled=\" BOOL \", tls.store.type=\" STRING \", ssl.configurations=\" STRING \", proxy.host=\" STRING \", proxy.port=\" STRING \", proxy.username=\" STRING \", proxy.password=\" STRING \", client.bootstrap.configurations=\" STRING \", max.pool.active.connections=\" INT \", min.pool.idle.connections=\" INT \", max.pool.idle.connections=\" INT \", min.evictable.idle.time=\" STRING \", time.between.eviction.runs=\" STRING \", max.wait.time=\" STRING \", test.on.borrow=\" BOOL \", test.while.idle=\" BOOL \", exhausted.action=\" INT \", hostname.verification.enabled=\" BOOL \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic publisher.url The URL to which the outgoing events should be published. Examples: http://localhost:8080/endpoint , https://localhost:8080/endpoint STRING No No basic.auth.username The username to be included in the authentication header when calling endpoints protected by basic authentication. basic.auth.password property should be also set when using this property. - STRING Yes No basic.auth.password The password to be included in the authentication header when calling endpoints protected by basic authentication. basic.auth.username property should be also set when using this property. - STRING Yes No https.truststore.file The file path of the client truststore when sending messages through https protocol. ${carbon.home}/resources/security/client-truststore.jks STRING Yes No https.truststore.password The password for the client-truststore. wso2carbon STRING Yes No oauth.username The username to be included in the authentication header when calling endpoints protected by OAuth 2.0. oauth.password property should be also set when using this property. - STRING Yes No oauth.password The password to be included in the authentication header when calling endpoints protected by OAuth 2.0. oauth.username property should be also set when using this property. - STRING Yes No consumer.key Consumer key used for calling endpoints protected by OAuth 2.0 - STRING Yes No consumer.secret Consumer secret used for calling endpoints protected by OAuth 2.0 - STRING Yes No token.url Token URL to generate a new access tokens when calling endpoints protected by OAuth 2.0 - STRING Yes No refresh.token Refresh token used for generating new access tokens when calling endpoints protected by OAuth 2.0 - STRING Yes No headers HTTP request headers in format \"' key : value ',' key : value '\" . When Content-Type header is not provided the system derives the Content-Type based on the provided sink mapper as following: - @map(type='xml') : application/xml - @map(type='json') : application/json - @map(type='text') : plain/text - @map(type='keyvalue') : application/x-www-form-urlencoded - For all other cases system defaults to plain/text Also the Content-Length header need not to be provided, as the system automatically defines it by calculating the size of the payload. Content-Type and Content-Length headers STRING Yes No method The HTTP method used for calling the endpoint. POST STRING Yes No socket.idle.timeout Socket timeout in millis. 6000 INT Yes No chunk.disabled Disable chunked transfer encoding. false BOOL Yes No ssl.protocol SSL/TLS protocol. TLS STRING Yes No ssl.verification.disabled Disable SSL verification. false BOOL Yes No tls.store.type TLS store type. JKS STRING Yes No ssl.configurations SSL/TSL configurations in format \"' key : value ',' key : value '\" . Some supported parameters: - SSL/TLS protocols: 'sslEnabledProtocols:TLSv1.1,TLSv1.2' - List of ciphers: 'ciphers:TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256' - Enable session creation: 'client.enable.session.creation:true' - Supported server names: 'server.suported.server.names:server' - Add HTTP SNIMatcher: 'server.supported.snimatchers:SNIMatcher' - STRING Yes No proxy.host Proxy server host - STRING Yes No proxy.port Proxy server port - STRING Yes No proxy.username Proxy server username - STRING Yes No proxy.password Proxy server password - STRING Yes No client.bootstrap.configurations Client bootstrap configurations in format \"' key : value ',' key : value '\" . Some supported configurations : - Client connect timeout in millis: 'client.bootstrap.connect.timeout:15000' - Client socket timeout in seconds: 'client.bootstrap.socket.timeout:15' - Client socket reuse: 'client.bootstrap.socket.reuse:true' - Enable TCP no delay: 'client.bootstrap.nodelay:true' - Enable client keep alive: 'client.bootstrap.keepalive:true' - Send buffer size: 'client.bootstrap.sendbuffersize:1048576' - Receive buffer size: 'client.bootstrap.recievebuffersize:1048576' - STRING Yes No max.pool.active.connections Maximum possible number of active connection per client pool. -1 INT Yes No min.pool.idle.connections Minimum number of idle connections that can exist per client pool. 0 INT Yes No max.pool.idle.connections Maximum number of idle connections that can exist per client pool. 100 INT Yes No min.evictable.idle.time Minimum time (in millis) a connection may sit idle in the client pool before it become eligible for eviction. 300000 STRING Yes No time.between.eviction.runs Time between two eviction operations (in millis) on the client pool. 30000 STRING Yes No max.wait.time The maximum time (in millis) the pool will wait (when there are no available connections) for a connection to be returned to the pool. 60000 STRING Yes No test.on.borrow Enable connections to be validated before being borrowed from the client pool. true BOOL Yes No test.while.idle Enable connections to be validated during the eviction operation (if any). true BOOL Yes No exhausted.action Action that should be taken when the maximum number of active connections are being used. This action should be indicated as an int and possible action values are following. 0 - Fail the request. 1 - Block the request, until a connection returns to the pool. 2 - Grow the connection pool size. 1 (Block when exhausted) INT Yes No hostname.verification.enabled Enable hostname verification. true BOOL Yes No System Parameters Name Description Default Value Possible Parameters clientBootstrapClientGroupSize Number of client threads to perform non-blocking read and write to one or more channels. (Number of available processors) * 2 Any positive integer clientBootstrapBossGroupSize Number of boss threads to accept incoming connections. Number of available processors Any positive integer clientBootstrapWorkerGroupSize Number of worker threads to accept the connections from boss threads and perform non-blocking read and write from one or more channels. (Number of available processors) * 2 Any positive integer trustStoreLocation The default truststore file path. ${carbon.home}/resources/security/client-truststore.jks Path to client truststore .jks file trustStorePassword The default truststore password. wso2carbon Truststore password as string Examples EXAMPLE 1 @sink(type = 'http', publisher.url = 'http://stocks.com/stocks', @map(type = 'json')) define stream StockStream (symbol string, price float, volume long); Events arriving on the StockStream will be published to the HTTP endpoint http://stocks.com/stocks using POST method with Content-Type application/json by converting those events to the default JSON format as following: { \"event\": { \"symbol\": \"FB\", \"price\": 24.5, \"volume\": 5000 } } EXAMPLE 2 @sink(type='http', publisher.url = 'http://localhost:8009/foo', client.bootstrap.configurations = \"'client.bootstrap.socket.timeout:20'\", max.pool.active.connections = '1', headers = \"{{headers}}\", @map(type='xml', @payload(\"\"\" stock {{payloadBody}} /stock \"\"\"))) define stream FooStream (payloadBody String, headers string); Events arriving on FooStream will be published to the HTTP endpoint http://localhost:8009/foo using POST method with Content-Type application/xml and setting payloadBody and header attribute values. If the payloadBody contains symbol WSO2 /symbol price 55.6 /price volume 100 /volume and header contains 'topic:foobar' values, then the system will generate an output with the body: stock symbol WSO2 /symbol price 55.6 /price volume 100 /volume /stock and HTTP headers: Content-Length:xxx , Content-Location:'xxx' , Content-Type:'application/xml' , HTTP_METHOD:'POST' http-call (Sink) The http-call sink publishes messages to endpoints via HTTP or HTTPS protocols using methods such as POST, GET, PUT, and DELETE on formats text , XML or JSON and consume responses through its corresponding http-call-response source. It also supports calling endpoints protected with basic authentication or OAuth 2.0. Origin: siddhi-io-http:2.2.0 Syntax @sink(type=\"http-call\", publisher.url=\" STRING \", sink.id=\" STRING \", basic.auth.username=\" STRING \", basic.auth.password=\" STRING \", https.truststore.file=\" STRING \", https.truststore.password=\" STRING \", oauth.username=\" STRING \", oauth.password=\" STRING \", consumer.key=\" STRING \", consumer.secret=\" STRING \", token.url=\" STRING \", refresh.token=\" STRING \", headers=\" STRING \", method=\" STRING \", downloading.enabled=\" BOOL \", download.path=\" STRING \", blocking.io=\" BOOL \", socket.idle.timeout=\" INT \", chunk.disabled=\" BOOL \", ssl.protocol=\" STRING \", ssl.verification.disabled=\" BOOL \", ssl.configurations=\" STRING \", proxy.host=\" STRING \", proxy.port=\" STRING \", proxy.username=\" STRING \", proxy.password=\" STRING \", client.bootstrap.configurations=\" STRING \", max.pool.active.connections=\" INT \", min.pool.idle.connections=\" INT \", max.pool.idle.connections=\" INT \", min.evictable.idle.time=\" STRING \", time.between.eviction.runs=\" STRING \", max.wait.time=\" STRING \", test.on.borrow=\" BOOL \", test.while.idle=\" BOOL \", exhausted.action=\" INT \", hostname.verification.enabled=\" BOOL \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic publisher.url The URL which should be called. Examples: http://localhost:8080/endpoint , https://localhost:8080/endpoint STRING No No sink.id Identifier to correlate the http-call sink to its corresponding http-call-response sources to retrieved the responses. STRING No No basic.auth.username The username to be included in the authentication header when calling endpoints protected by basic authentication. basic.auth.password property should be also set when using this property. - STRING Yes No basic.auth.password The password to be included in the authentication header when calling endpoints protected by basic authentication. basic.auth.username property should be also set when using this property. - STRING Yes No https.truststore.file The file path of the client truststore when sending messages through https protocol. ${carbon.home}/resources/security/client-truststore.jks STRING Yes No https.truststore.password The password for the client-truststore. wso2carbon STRING Yes No oauth.username The username to be included in the authentication header when calling endpoints protected by OAuth 2.0. oauth.password property should be also set when using this property. - STRING Yes No oauth.password The password to be included in the authentication header when calling endpoints protected by OAuth 2.0. oauth.username property should be also set when using this property. - STRING Yes No consumer.key Consumer key used for calling endpoints protected by OAuth 2.0 - STRING Yes No consumer.secret Consumer secret used for calling endpoints protected by OAuth 2.0 - STRING Yes No token.url Token URL to generate a new access tokens when calling endpoints protected by OAuth 2.0 - STRING Yes No refresh.token Refresh token used for generating new access tokens when calling endpoints protected by OAuth 2.0 - STRING Yes No headers HTTP request headers in format \"' key : value ',' key : value '\" . When the Content-Type header is not provided the system decides the Content-Type based on the provided sink mapper as following: - @map(type='xml') : application/xml - @map(type='json') : application/json - @map(type='text') : plain/text - @map(type='keyvalue') : application/x-www-form-urlencoded - For all other cases system defaults to plain/text Also the Content-Length header need not to be provided, as the system automatically defines it by calculating the size of the payload. Content-Type and Content-Length headers STRING Yes No method The HTTP method used for calling the endpoint. POST STRING Yes No downloading.enabled Enable response received by the http-call-response source to be written to a file. When this is enabled the download.path property should be also set. false BOOL Yes No download.path The absolute file path along with the file name where the downloads should be saved. - STRING Yes Yes blocking.io Blocks the request thread until a response it received from HTTP call-response source before sending any other request. false BOOL Yes No socket.idle.timeout Socket timeout in millis. 6000 INT Yes No chunk.disabled Disable chunked transfer encoding. false BOOL Yes No ssl.protocol SSL/TLS protocol. TLS STRING Yes No ssl.verification.disabled Disable SSL verification. false BOOL Yes No ssl.configurations SSL/TSL configurations. Expected format \"' key : value ',' key : value '\" . Some supported parameters: - SSL/TLS protocols: 'sslEnabledProtocols:TLSv1.1,TLSv1.2' - List of ciphers: 'ciphers:TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256' - Enable session creation: 'client.enable.session.creation:true' - Supported server names: 'server.suported.server.names:server' - Add HTTP SNIMatcher: 'server.supported.snimatchers:SNIMatcher' - STRING Yes No proxy.host Proxy server host - STRING Yes No proxy.port Proxy server port - STRING Yes No proxy.username Proxy server username - STRING Yes No proxy.password Proxy server password - STRING Yes No client.bootstrap.configurations Client bootstrap configurations in format \"' key : value ',' key : value '\" . Some supported configurations : - Client connect timeout in millis: 'client.bootstrap.connect.timeout:15000' - Client socket timeout in seconds: 'client.bootstrap.socket.timeout:15' - Client socket reuse: 'client.bootstrap.socket.reuse:true' - Enable TCP no delay: 'client.bootstrap.nodelay:true' - Enable client keep alive: 'client.bootstrap.keepalive:true' - Send buffer size: 'client.bootstrap.sendbuffersize:1048576' - Receive buffer size: 'client.bootstrap.recievebuffersize:1048576' - STRING Yes No max.pool.active.connections Maximum possible number of active connection per client pool. -1 INT Yes No min.pool.idle.connections Minimum number of idle connections that can exist per client pool. 0 INT Yes No max.pool.idle.connections Maximum number of idle connections that can exist per client pool. 100 INT Yes No min.evictable.idle.time Minimum time (in millis) a connection may sit idle in the client pool before it become eligible for eviction. 300000 STRING Yes No time.between.eviction.runs Time between two eviction operations (in millis) on the client pool. 30000 STRING Yes No max.wait.time The maximum time (in millis) the pool will wait (when there are no available connections) for a connection to be returned to the pool. 60000 STRING Yes No test.on.borrow Enable connections to be validated before being borrowed from the client pool. true BOOL Yes No test.while.idle Enable connections to be validated during the eviction operation (if any). true BOOL Yes No exhausted.action Action that should be taken when the maximum number of active connections are being used. This action should be indicated as an int and possible action values are following. 0 - Fail the request. 1 - Block the request, until a connection returns to the pool. 2 - Grow the connection pool size. 1 (Block when exhausted) INT Yes No hostname.verification.enabled Enable hostname verification true BOOL Yes No System Parameters Name Description Default Value Possible Parameters clientBootstrapClientGroupSize Number of client threads to perform non-blocking read and write to one or more channels. (Number of available processors) * 2 Any positive integer clientBootstrapBossGroupSize Number of boss threads to accept incoming connections. Number of available processors Any positive integer clientBootstrapWorkerGroupSize Number of worker threads to accept the connections from boss threads and perform non-blocking read and write from one or more channels. (Number of available processors) * 2 Any positive integer trustStoreLocation The default truststore file path. ${carbon.home}/resources/security/client-truststore.jks Path to client truststore .jks file trustStorePassword The default truststore password. wso2carbon Truststore password as string Examples EXAMPLE 1 @sink(type='http-call', sink.id='foo', publisher.url='http://localhost:8009/foo', @map(type='xml', @payload('{{payloadBody}}'))) define stream FooStream (payloadBody string); @source(type='http-call-response', sink.id='foo', @map(type='text', regex.A='((.|\\n)*)', @attributes(headers='trp:headers', message='A[1]'))) define stream ResponseStream(message string, headers string); When events arrive in FooStream , http-call sink makes calls to endpoint on url http://localhost:8009/foo with POST method and Content-Type application/xml . If the event payloadBody attribute contains following XML: item name apple /name price 55 /price quantity 5 /quantity /item the http-call sink maps that and sends it to the endpoint. When endpoint sends a response it will be consumed by the corresponding http-call-response source correlated via the same sink.id foo and that will map the response message and send it via ResponseStream steam by assigning the message body as message attribute and response headers as headers attribute of the event. EXAMPLE 2 @sink(type='http-call', publisher.url='http://localhost:8005/files/{{name}}' downloading.enabled='true', download.path='{{downloadPath}}{{name}}', method='GET', sink.id='download', @map(type='json')) define stream DownloadRequestStream(name String, id int, downloadPath string); @source(type='http-call-response', sink.id='download', http.status.code='2\\\\d+', @map(type='text', regex.A='((.|\\n)*)', @attributes(name='trp:name', id='trp:id', file='A[1]'))) define stream ResponseStream2xx(name string, id string, file string); @source(type='http-call-response', sink.id='download', http.status.code='4\\\\d+', @map(type='text', regex.A='((.|\\n)*)', @attributes(errorMsg='A[1]'))) define stream ResponseStream4xx(errorMsg string); When events arrive in DownloadRequestStream with name : foo.txt , id : 75 and downloadPath : /user/download/ the http-call sink sends a GET request to the url http://localhost:8005/files/foo.txt to download the file to the given path /user/download/foo.txt and capture the response via its corresponding http-call-response source based on the response status code. If the response status code is in the range of 200 the message will be received by the http-call-response source associated with the ResponseStream2xx stream which expects http.status.code with regex 2\\d+ while downloading the file to the local file system on the path /user/download/foo.txt and mapping the response message having the absolute file path to event's file attribute. If the response status code is in the range of 400 then the message will be received by the http-call-response source associated with the ResponseStream4xx stream which expects http.status.code with regex 4\\d+ while mapping the error response to the errorMsg attribute of the event. http-request (Sink) Deprecated (Use http-call sink instead). The http-request sink publishes messages to endpoints via HTTP or HTTPS protocols using methods such as POST, GET, PUT, and DELETE on formats text , XML or JSON and consume responses through its corresponding http-response source. It also supports calling endpoints protected with basic authentication or OAuth 2.0. Origin: siddhi-io-http:2.2.0 Syntax @sink(type=\"http-request\", publisher.url=\" STRING \", sink.id=\" STRING \", basic.auth.username=\" STRING \", basic.auth.password=\" STRING \", https.truststore.file=\" STRING \", https.truststore.password=\" STRING \", oauth.username=\" STRING \", oauth.password=\" STRING \", consumer.key=\" STRING \", consumer.secret=\" STRING \", token.url=\" STRING \", refresh.token=\" STRING \", headers=\" STRING \", method=\" STRING \", downloading.enabled=\" BOOL \", download.path=\" STRING \", blocking.io=\" BOOL \", socket.idle.timeout=\" INT \", chunk.disabled=\" BOOL \", ssl.protocol=\" STRING \", ssl.verification.disabled=\" BOOL \", ssl.configurations=\" STRING \", proxy.host=\" STRING \", proxy.port=\" STRING \", proxy.username=\" STRING \", proxy.password=\" STRING \", client.bootstrap.configurations=\" STRING \", max.pool.active.connections=\" INT \", min.pool.idle.connections=\" INT \", max.pool.idle.connections=\" INT \", min.evictable.idle.time=\" STRING \", time.between.eviction.runs=\" STRING \", max.wait.time=\" STRING \", test.on.borrow=\" BOOL \", test.while.idle=\" BOOL \", exhausted.action=\" INT \", hostname.verification.enabled=\" BOOL \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic publisher.url The URL which should be called. Examples: http://localhost:8080/endpoint , https://localhost:8080/endpoint STRING No No sink.id Identifier to correlate the http-request sink to its corresponding http-response sources to retrieved the responses. STRING No No basic.auth.username The username to be included in the authentication header when calling endpoints protected by basic authentication. basic.auth.password property should be also set when using this property. - STRING Yes No basic.auth.password The password to be included in the authentication header when calling endpoints protected by basic authentication. basic.auth.username property should be also set when using this property. - STRING Yes No https.truststore.file The file path of the client truststore when sending messages through https protocol. ${carbon.home}/resources/security/client-truststore.jks STRING Yes No https.truststore.password The password for the client-truststore. wso2carbon STRING Yes No oauth.username The username to be included in the authentication header when calling endpoints protected by OAuth 2.0. oauth.password property should be also set when using this property. - STRING Yes No oauth.password The password to be included in the authentication header when calling endpoints protected by OAuth 2.0. oauth.username property should be also set when using this property. - STRING Yes No consumer.key Consumer key used for calling endpoints protected by OAuth 2.0 - STRING Yes No consumer.secret Consumer secret used for calling endpoints protected by OAuth 2.0 - STRING Yes No token.url Token URL to generate a new access tokens when calling endpoints protected by OAuth 2.0 - STRING Yes No refresh.token Refresh token used for generating new access tokens when calling endpoints protected by OAuth 2.0 - STRING Yes No headers HTTP request headers in format \"' key : value ',' key : value '\" . When the Content-Type header is not provided the system decides the Content-Type based on the provided sink mapper as following: - @map(type='xml') : application/xml - @map(type='json') : application/json - @map(type='text') : plain/text - @map(type='keyvalue') : application/x-www-form-urlencoded - For all other cases system defaults to plain/text Also the Content-Length header need not to be provided, as the system automatically defines it by calculating the size of the payload. Content-Type and Content-Length headers STRING Yes No method The HTTP method used for calling the endpoint. POST STRING Yes No downloading.enabled Enable response received by the http-response source to be written to a file. When this is enabled the download.path property should be also set. false BOOL Yes No download.path The absolute file path along with the file name where the downloads should be saved. - STRING Yes Yes blocking.io Blocks the request thread until a response it received from HTTP call-response source before sending any other request. false BOOL Yes No socket.idle.timeout Socket timeout in millis. 6000 INT Yes No chunk.disabled Disable chunked transfer encoding. false BOOL Yes No ssl.protocol SSL/TLS protocol. TLS STRING Yes No ssl.verification.disabled Disable SSL verification. false BOOL Yes No ssl.configurations SSL/TSL configurations in format \"' key : value ',' key : value '\" . Some supported parameters: - SSL/TLS protocols: 'sslEnabledProtocols:TLSv1.1,TLSv1.2' - List of ciphers: 'ciphers:TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256' - Enable session creation: 'client.enable.session.creation:true' - Supported server names: 'server.suported.server.names:server' - Add HTTP SNIMatcher: 'server.supported.snimatchers:SNIMatcher' - STRING Yes No proxy.host Proxy server host - STRING Yes No proxy.port Proxy server port - STRING Yes No proxy.username Proxy server username - STRING Yes No proxy.password Proxy server password - STRING Yes No client.bootstrap.configurations Client bootstrap configurations in format \"' key : value ',' key : value '\" . Some supported configurations : - Client connect timeout in millis: 'client.bootstrap.connect.timeout:15000' - Client socket timeout in seconds: 'client.bootstrap.socket.timeout:15' - Client socket reuse: 'client.bootstrap.socket.reuse:true' - Enable TCP no delay: 'client.bootstrap.nodelay:true' - Enable client keep alive: 'client.bootstrap.keepalive:true' - Send buffer size: 'client.bootstrap.sendbuffersize:1048576' - Receive buffer size: 'client.bootstrap.recievebuffersize:1048576' - STRING Yes No max.pool.active.connections Maximum possible number of active connection per client pool. -1 INT Yes No min.pool.idle.connections Minimum number of idle connections that can exist per client pool. 0 INT Yes No max.pool.idle.connections Maximum number of idle connections that can exist per client pool. 100 INT Yes No min.evictable.idle.time Minimum time (in millis) a connection may sit idle in the client pool before it become eligible for eviction. 300000 STRING Yes No time.between.eviction.runs Time between two eviction operations (in millis) on the client pool. 30000 STRING Yes No max.wait.time The maximum time (in millis) the pool will wait (when there are no available connections) for a connection to be returned to the pool. 60000 STRING Yes No test.on.borrow Enable connections to be validated before being borrowed from the client pool. true BOOL Yes No test.while.idle Enable connections to be validated during the eviction operation (if any). true BOOL Yes No exhausted.action Action that should be taken when the maximum number of active connections are being used. This action should be indicated as an int and possible action values are following. 0 - Fail the request. 1 - Block the request, until a connection returns to the pool. 2 - Grow the connection pool size. 1 (Block when exhausted) INT Yes No hostname.verification.enabled Enable hostname verification true BOOL Yes No System Parameters Name Description Default Value Possible Parameters clientBootstrapClientGroupSize Number of client threads to perform non-blocking read and write to one or more channels. (Number of available processors) * 2 Any positive integer clientBootstrapBossGroupSize Number of boss threads to accept incoming connections. Number of available processors Any positive integer clientBootstrapWorkerGroupSize Number of worker threads to accept the connections from boss threads and perform non-blocking read and write from one or more channels. (Number of available processors) * 2 Any positive integer trustStoreLocation The default truststore file path. ${carbon.home}/resources/security/client-truststore.jks Path to client truststore .jks file trustStorePassword The default truststore password. wso2carbon Truststore password as string Examples EXAMPLE 1 @sink(type='http-request', sink.id='foo', publisher.url='http://localhost:8009/foo', @map(type='xml', @payload('{{payloadBody}}'))) define stream FooStream (payloadBody string); @source(type='http-response', sink.id='foo', @map(type='text', regex.A='((.|\\n)*)', @attributes(headers='trp:headers', message='A[1]'))) define stream ResponseStream(message string, headers string); When events arrive in FooStream , http-request sink makes calls to endpoint on url http://localhost:8009/foo with POST method and Content-Type application/xml . If the event payloadBody attribute contains following XML: item name apple /name price 55 /price quantity 5 /quantity /item the http-request sink maps that and sends it to the endpoint. When endpoint sends a response it will be consumed by the corresponding http-response source correlated via the same sink.id foo and that will map the response message and send it via ResponseStream steam by assigning the message body as message attribute and response headers as headers attribute of the event. EXAMPLE 2 @sink(type='http-request', publisher.url='http://localhost:8005/files/{{name}}' downloading.enabled='true', download.path='{{downloadPath}}{{name}}', method='GET', sink.id='download', @map(type='json')) define stream DownloadRequestStream(name String, id int, downloadPath string); @source(type='http-response', sink.id='download', http.status.code='2\\\\d+', @map(type='text', regex.A='((.|\\n)*)', @attributes(name='trp:name', id='trp:id', file='A[1]'))) define stream ResponseStream2xx(name string, id string, file string); @source(type='http-response', sink.id='download', http.status.code='4\\\\d+', @map(type='text', regex.A='((.|\\n)*)', @attributes(errorMsg='A[1]'))) define stream ResponseStream4xx(errorMsg string); When events arrive in DownloadRequestStream with name : foo.txt , id : 75 and downloadPath : /user/download/ the http-request sink sends a GET request to the url http://localhost:8005/files/foo.txt to download the file to the given path /user/download/foo.txt and capture the response via its corresponding http-response source based on the response status code. If the response status code is in the range of 200 the message will be received by the http-response source associated with the ResponseStream2xx stream which expects http.status.code with regex 2\\d+ while downloading the file to the local file system on the path /user/download/foo.txt and mapping the response message having the absolute file path to event's file attribute. If the response status code is in the range of 400 then the message will be received by the http-response source associated with the ResponseStream4xx stream which expects http.status.code with regex 4\\d+ while mapping the error response to the errorMsg attribute of the event. http-response (Sink) Deprecated (Use http-service-response sink instead). The http-response sink send responses of the requests consumed by its corresponding http-request source, by mapping the response messages to formats such as text , XML and JSON . Origin: siddhi-io-http:2.2.0 Syntax @sink(type=\"http-response\", source.id=\" STRING \", message.id=\" STRING \", headers=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic source.id Identifier to correlate the http-response sink to its corresponding http-request source which consumed the request. STRING No No message.id Identifier to correlate the response with the request received by http-request source. STRING No Yes headers HTTP request headers in format \"' key : value ',' key : value '\" . When the Content-Type header is not provided the system decides the Content-Type based on the provided sink mapper as following: - @map(type='xml') : application/xml - @map(type='json') : application/json - @map(type='text') : plain/text - @map(type='keyvalue') : application/x-www-form-urlencoded - For all other cases system defaults to plain/text Also the Content-Length header need not to be provided, as the system automatically defines it by calculating the size of the payload. Content-Type and Content-Length headers STRING Yes No Examples EXAMPLE 1 @source(type='http-request', receiver.url='http://localhost:5005/add', source.id='adder', @map(type='json, @attributes(messageId='trp:messageId', value1='$.event.value1', value2='$.event.value2'))) define stream AddStream (messageId string, value1 long, value2 long); @sink(type='http-response', source.id='adder', message.id='{{messageId}}', @map(type = 'json')) define stream ResultStream (messageId string, results long); @info(name = 'query1') from AddStream select messageId, value1 + value2 as results insert into ResultStream; The http-request source on stream AddStream listens on url http://localhost:5005/stocks for JSON messages with format: { \"event\": { \"value1\": 3, \"value2\": 4 } } and when events arrive it maps to AddStream events and pass them to query query1 for processing. The query results produced on ResultStream are sent as a response via http-response sink with format: { \"event\": { \"results\": 7 } } Here the request and response are correlated by passing the messageId produced by the http-request to the respective http-response sink. http-service-response (Sink) The http-service-response sink send responses of the requests consumed by its corresponding http-service source, by mapping the response messages to formats such as text , XML and JSON . Origin: siddhi-io-http:2.2.0 Syntax @sink(type=\"http-service-response\", source.id=\" STRING \", message.id=\" STRING \", headers=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic source.id Identifier to correlate the http-service-response sink to its corresponding http-service source which consumed the request. STRING No No message.id Identifier to correlate the response with the request received by http-service source. STRING No Yes headers HTTP request headers in format \"' key : value ',' key : value '\" . When the Content-Type header is not provided the system decides the Content-Type based on the provided sink mapper as following: - @map(type='xml') : application/xml - @map(type='json') : application/json - @map(type='text') : plain/text - @map(type='keyvalue') : application/x-www-form-urlencoded - For all other cases system defaults to plain/text Also the Content-Length header need not to be provided, as the system automatically defines it by calculating the size of the payload. Content-Type and Content-Length headers STRING Yes No Examples EXAMPLE 1 @source(type='http-service', receiver.url='http://localhost:5005/add', source.id='adder', @map(type='json, @attributes(messageId='trp:messageId', value1='$.event.value1', value2='$.event.value2'))) define stream AddStream (messageId string, value1 long, value2 long); @sink(type='http-service-response', source.id='adder', message.id='{{messageId}}', @map(type = 'json')) define stream ResultStream (messageId string, results long); @info(name = 'query1') from AddStream select messageId, value1 + value2 as results insert into ResultStream; The http-service source on stream AddStream listens on url http://localhost:5005/stocks for JSON messages with format: { \"event\": { \"value1\": 3, \"value2\": 4 } } and when events arrive it maps to AddStream events and pass them to query query1 for processing. The query results produced on ResultStream are sent as a response via http-service-response sink with format: { \"event\": { \"results\": 7 } } Here the request and response are correlated by passing the messageId produced by the http-service to the respective http-service-response sink. inMemory (Sink) In-memory sink publishes events to In-memory sources that are subscribe to the same topic to which the sink publishes. This provides a way to connect multiple Siddhi Apps deployed under the same Siddhi Manager (JVM). Here both the publisher and subscriber should have the same event schema (stream definition) for successful data transfer. Origin: siddhi-core:5.1.8 Syntax @sink(type=\"inMemory\", topic=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic topic Event are delivered to allthe subscribers subscribed on this topic. STRING No No Examples EXAMPLE 1 @sink(type='inMemory', topic='Stocks', @map(type='passThrough')) define stream StocksStream (symbol string, price float, volume long); Here the StocksStream uses inMemory sink to emit the Siddhi events to all the inMemory sources deployed in the same JVM and subscribed to the topic Stocks . jms (Sink) JMS Sink allows users to subscribe to a JMS broker and publish JMS messages. Origin: siddhi-io-jms:2.0.3 Syntax @sink(type=\"jms\", destination=\" STRING \", connection.factory.jndi.name=\" STRING \", factory.initial=\" STRING \", provider.url=\" STRING \", connection.factory.type=\" STRING \", connection.username=\" STRING \", connection.password=\" STRING \", connection.factory.nature=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic destination Queue/Topic name which JMS Source should subscribe to STRING No Yes connection.factory.jndi.name JMS Connection Factory JNDI name. This value will be used for the JNDI lookup to find the JMS Connection Factory. QueueConnectionFactory STRING Yes No factory.initial Naming factory initial value STRING No No provider.url Java naming provider URL. Property for specifying configuration information for the service provider to use. The value of the property should contain a URL string (e.g. \"ldap://somehost:389\") STRING No No connection.factory.type Type of the connection connection factory. This can be either queue or topic. queue STRING Yes No connection.username username for the broker. None STRING Yes No connection.password Password for the broker None STRING Yes No connection.factory.nature Connection factory nature for the broker(cached/pooled). default STRING Yes No Examples EXAMPLE 1 @sink(type='jms', @map(type='xml'), factory.initial='org.apache.activemq.jndi.ActiveMQInitialContextFactory', provider.url='vm://localhost',destination='DAS_JMS_OUTPUT_TEST', connection.factory.type='topic',connection.factory.jndi.name='TopicConnectionFactory') define stream inputStream (name string, age int, country string); This example shows how to publish to an ActiveMQ topic. EXAMPLE 2 @sink(type='jms', @map(type='xml'), factory.initial='org.apache.activemq.jndi.ActiveMQInitialContextFactory', provider.url='vm://localhost',destination='DAS_JMS_OUTPUT_TEST') define stream inputStream (name string, age int, country string); This example shows how to publish to an ActiveMQ queue. Note that we are not providing properties like connection factory type kafka (Sink) A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to use the Kafka transport, the type parameter should have kafka as its value. Origin: siddhi-io-kafka:5.0.5 Syntax @sink(type=\"kafka\", bootstrap.servers=\" STRING \", topic=\" STRING \", partition.no=\" INT \", sequence.id=\" STRING \", key=\" STRING \", is.binary.message=\" BOOL \", optional.configuration=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No is.binary.message In order to send the binary events via kafka sink, this parameter is set to 'True'. null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='topic_with_partitions', partition.no='0', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka sink configuration publishes to 0 th partition of the topic named topic_with_partitions . EXAMPLE 2 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='{{symbol}}', partition.no='{{volume}}', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes dynamic topic and partitions that are taken from the Siddhi event. The value for partition.no is taken from the volume attribute, and the topic value is taken from the symbol attribute. kafkaMultiDC (Sink) A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to publish events via the Kafka transport, and using two Kafka brokers to publish events to the same topic, the type parameter must have kafkaMultiDC as its value. Origin: siddhi-io-kafka:5.0.5 Syntax @sink(type=\"kafkaMultiDC\", bootstrap.servers=\" STRING \", topic=\" STRING \", sequence.id=\" STRING \", key=\" STRING \", partition.no=\" INT \", is.binary.message=\" BOOL \", optional.configuration=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma -separated values. There must be at least two servers in this list. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No is.binary.message In order to send the binary events via kafkaMultiDCSink, it is required to set this parameter to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink(type='kafkaMultiDC', topic='myTopic', partition.no='0',bootstrap.servers='host1:9092, host2:9092', @map(type='xml'))Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes to the default (i.e., 0 th ) partition of the brokers in two data centers log (Sink) This is a sink that can be used as a logger. This will log the output events in the output stream with user specified priority and a prefix Origin: siddhi-core:5.1.8 Syntax @sink(type=\"log\", priority=\" STRING \", prefix=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic priority This will set the logger priority i.e log level. Accepted values are INFO, DEBUG, WARN, FATAL, ERROR, OFF, TRACE INFO STRING Yes No prefix This will be the prefix to the output message. If the output stream has event [2,4] and the prefix is given as \"Hello\" then the log will show \"Hello : [2,4]\" default prefix will be : STRING Yes No Examples EXAMPLE 1 @sink(type='log', prefix='My Log', priority='DEBUG') define stream BarStream (symbol string, price float, volume long) In this example BarStream uses log sink and the prefix is given as My Log. Also the priority is set to DEBUG. EXAMPLE 2 @sink(type='log', priority='DEBUG') define stream BarStream (symbol string, price float, volume long) In this example BarStream uses log sink and the priority is set to DEBUG. User has not specified prefix so the default prefix will be in the form Siddhi App Name : Stream Name EXAMPLE 3 @sink(type='log', prefix='My Log') define stream BarStream (symbol string, price float, volume long) In this example BarStream uses log sink and the prefix is given as My Log. User has not given a priority so it will be set to default INFO. EXAMPLE 4 @sink(type='log') define stream BarStream (symbol string, price float, volume long) In this example BarStream uses log sink. The user has not given prefix or priority so they will be set to their default values. nats (Sink) NATS Sink allows users to subscribe to a NATS broker and publish messages. Origin: siddhi-io-nats:2.0.8 Syntax @sink(type=\"nats\", destination=\" STRING \", bootstrap.servers=\" STRING \", client.id=\" STRING \", cluster.id=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic destination Subject name which NATS sink should publish to. STRING No Yes bootstrap.servers The NATS based url of the NATS server. nats://localhost:4222 STRING Yes No client.id The identifier of the client publishing/connecting to the NATS broker. Should be unique for each client connecting to the server/cluster. None STRING Yes No cluster.id The identifier of the NATS server/cluster. test-cluster STRING Yes No Examples EXAMPLE 1 @sink(type='nats', @map(type='xml'), destination='SP_NATS_OUTPUT_TEST', bootstrap.servers='nats://localhost:4222',client.id='nats_client',server.id='test-cluster') define stream outputStream (name string, age int, country string); This example shows how to publish to a NATS subject with all supporting configurations. With the following configuration the sink identified as 'nats-client' will publish to a subject named as 'SP_NATS_OUTPUT_TEST' which resides in a nats instance with a cluster id of 'test-cluster', running in localhost and listening to the port 4222 for client connection. EXAMPLE 2 @sink(type='nats', @map(type='xml'), destination='SP_NATS_OUTPUT_TEST') define stream outputStream (name string, age int, country string); This example shows how to publish to a NATS subject with mandatory configurations. With the following configuration the sink identified with an auto generated client id will publish to a subject named as 'SP_NATS_OUTPUT_TEST' which resides in a nats instance with a cluster id of 'test-cluster', running in localhost and listening to the port 4222 for client connection. prometheus (Sink) This sink publishes events processed by Siddhi into Prometheus metrics and exposes them to the Prometheus server at the specified URL. The created metrics can be published to Prometheus via 'server' or 'pushGateway', depending on your preference. The metric types that are supported by the Prometheus sink are 'counter', 'gauge', 'histogram', and 'summary'. The values and labels of the Prometheus metrics can be updated through the events. Origin: siddhi-io-prometheus:2.1.0 Syntax @sink(type=\"prometheus\", job=\" STRING \", publish.mode=\" STRING \", push.url=\" STRING \", server.url=\" STRING \", metric.type=\" STRING \", metric.help=\" STRING \", metric.name=\" STRING \", buckets=\" STRING \", quantiles=\" STRING \", quantile.error=\" DOUBLE \", value.attribute=\" STRING \", push.operation=\" STRING \", grouping.key=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic job This parameter specifies the job name of the metric. This must be the same job name that is defined in the Prometheus configuration file. siddhiJob STRING Yes No publish.mode The mode in which the metrics need to be exposed to the Prometheus server.The possible publishing modes are 'server' and 'pushgateway'.The server mode exposes the metrics through an HTTP server at the specified URL, and the 'pushGateway' mode pushes the metrics to the pushGateway that needs to be running at the specified URL. server STRING Yes No push.url This parameter specifies the target URL of the Prometheus pushGateway. This is the URL at which the pushGateway must be listening. This URL needs to be defined in the Prometheus configuration file as a target before it can be used here. http://localhost:9091 STRING Yes No server.url This parameter specifies the URL where the HTTP server is initiated to expose metrics in the 'server' publish mode. This URL needs to be defined in the Prometheus configuration file as a target before it can be used here. http://localhost:9080 STRING Yes No metric.type The type of Prometheus metric that needs to be created at the sink. The supported metric types are 'counter', 'gauge',c'histogram' and 'summary'. STRING No No metric.help A brief description of the metric and its purpose. STRING Yes No metric.name This parameter allows you to assign a preferred name for the metric. The metric name must match the regex format, i.e., [a-zA-Z_:][a-zA-Z0-9_:]*. STRING Yes No buckets The bucket values preferred by the user for histogram metrics. The bucket values must be in the 'string' format with each bucket value separated by a comma as shown in the example below. \"2,4,6,8\" null STRING Yes No quantiles This parameter allows you to specify quantile values for summary metrics as preferred. The quantile values must be in the 'string' format with each quantile value separated by a comma as shown in the example below. \"0.5,0.75,0.95\" null STRING Yes No quantile.error The error tolerance value for calculating quantiles in summary metrics. This must be a positive value, but less than 1. 0.001 DOUBLE Yes No value.attribute The name of the attribute in the stream definition that specifies the metric value. The defined 'value' attribute must be included in the stream definition. The system increases the metric value for the counter and gauge metric types by the value of the 'value attribute. The system observes the value of the 'value' attribute for the calculations of 'summary' and 'histogram' metric types. value STRING Yes No push.operation This parameter defines the mode for pushing metrics to the pushGateway. The available push operations are 'push' and 'pushadd'. The operations differ according to the existing metrics in pushGateway where 'push' operation replaces the existing metrics, and 'pushadd' operation only updates the newly created metrics. pushadd STRING Yes No grouping.key This parameter specifies the grouping key of created metrics in key-value pairs. The grouping key is used only in pushGateway mode in order to distinguish the metrics from already existing metrics. The expected format of the grouping key is as follows: \"'key1:value1','key2:value2'\" STRING Yes No System Parameters Name Description Default Value Possible Parameters jobName This property specifies the default job name for the metric. This job name must be the same as the job name defined in the Prometheus configuration file. siddhiJob Any string publishMode The default publish mode for the Prometheus sink for exposing metrics to the Prometheus server. The mode can be either 'server' or 'pushgateway'. server server or pushgateway serverURL This property configures the URL where the HTTP server is initiated to expose metrics. This URL needs to be defined in the Prometheus configuration file as a target to be identified by Prometheus before it can be used here. By default, the HTTP server is initiated at 'http://localhost:9080'. http://localhost:9080 Any valid URL pushURL This property configures the target URL of the Prometheus pushGateway (where the pushGateway needs to listen). This URL needs to be defined in the Prometheus configuration file as a target to be identified by Prometheus before it can be used here. http://localhost:9091 Any valid URL groupingKey This property configures the grouping key of created metrics in key-value pairs. Grouping key is used only in pushGateway mode in order to distinguish these metrics from already existing metrics under the same job. The expected format of the grouping key is as follows: \"'key1:value1','key2:value2'\" . null Any key value pairs in the supported format Examples EXAMPLE 1 @sink(type='prometheus',job='fooOrderCount', server.url ='http://localhost:9080', publish.mode='server', metric.type='counter', metric.help= 'Number of foo orders', @map(type='keyvalue')) define stream FooCountStream (Name String, quantity int, value int); In the above example, the Prometheus-sink creates a counter metric with the stream name and defined attributes as labels. The metric is exposed through an HTTP server at the target URL. EXAMPLE 2 @sink(type='prometheus',job='inventoryLevel', push.url='http://localhost:9080', publish.mode='pushGateway', metric.type='gauge', metric.help= 'Current level of inventory', @map(type='keyvalue')) define stream InventoryLevelStream (Name String, value int); In the above example, the Prometheus-sink creates a gauge metric with the stream name and defined attributes as labels.The metric is pushed to the Prometheus pushGateway at the target URL. rabbitmq (Sink) The rabbitmq sink pushes the events into a rabbitmq broker using the AMQP protocol Origin: siddhi-io-rabbitmq:3.0.2 Syntax @sink(type=\"rabbitmq\", uri=\" STRING \", heartbeat=\" INT \", exchange.name=\" STRING \", exchange.type=\" STRING \", exchange.durable.enabled=\" BOOL \", exchange.autodelete.enabled=\" BOOL \", delivery.mode=\" INT \", content.type=\" STRING \", content.encoding=\" STRING \", priority=\" INT \", correlation.id=\" STRING \", reply.to=\" STRING \", expiration=\" STRING \", message.id=\" STRING \", timestamp=\" STRING \", type=\" STRING \", user.id=\" STRING \", app.id=\" STRING \", routing.key=\" STRING \", headers=\" STRING \", tls.enabled=\" BOOL \", tls.truststore.path=\" STRING \", tls.truststore.password=\" STRING \", tls.truststore.type=\" STRING \", tls.version=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic uri The URI that used to connect to an AMQP server. If no URI is specified, an error is logged in the CLI.e.g., amqp://guest:guest , amqp://guest:guest@localhost:5672 STRING No No heartbeat The period of time (in seconds) after which the peer TCP connection should be considered unreachable (down) by RabbitMQ and client libraries. 60 INT Yes No exchange.name The name of the exchange that decides what to do with a message it sends.If the exchange.name already exists in the RabbitMQ server, then the system uses that exchange.name instead of redeclaring. STRING No Yes exchange.type The type of the exchange.name. The exchange types available are direct , fanout , topic and headers . For a detailed description of each type, see RabbitMQ - AMQP Concepts direct STRING Yes Yes exchange.durable.enabled If this is set to true , the exchange remains declared even if the broker restarts. false BOOL Yes Yes exchange.autodelete.enabled If this is set to true , the exchange is automatically deleted when it is not used anymore. false BOOL Yes Yes delivery.mode This determines whether the connection should be persistent or not. The value must be either 1 or 2 .If the delivery.mode = 1, then the connection is not persistent. If the delivery.mode = 2, then the connection is persistent. 1 INT Yes No content.type The message content type. This should be the MIME content type. null STRING Yes No content.encoding The message content encoding. The value should be MIME content encoding. null STRING Yes No priority Specify a value within the range 0 to 9 in this parameter to indicate the message priority. 0 INT Yes Yes correlation.id The message correlated to the current message. e.g., The request to which this message is a reply. When a request arrives, a message describing the task is pushed to the queue by the front end server. After that the frontend server blocks to wait for a response message with the same correlation ID. A pool of worker machines listen on queue, and one of them picks up the task, performs it, and returns the result as message. Once a message with right correlation ID arrives, thefront end server continues to return the response to the caller. null STRING Yes Yes reply.to This is an anonymous exclusive callback queue. When the RabbitMQ receives a message with the reply.to property, it sends the response to the mentioned queue. This is commonly used to name a reply queue (or any other identifier that helps a consumer application to direct its response). null STRING Yes No expiration The expiration time after which the message is deleted. The value of the expiration field describes the TTL (Time To Live) period in milliseconds. null STRING Yes No message.id The message identifier. If applications need to identify messages, it is recommended that they use this attribute instead of putting it into the message payload. null STRING Yes Yes timestamp Timestamp of the moment when the message was sent. If you do not specify a value for this parameter, the system automatically generates the current date and time as the timestamp value. The format of the timestamp value is dd/mm/yyyy . current timestamp STRING Yes No type The type of the message. e.g., The type of the event or the command represented by the message. null STRING Yes No user.id The user ID specified here is verified by RabbitMQ against theuser name of the actual connection. This is an optional parameter. null STRING Yes No app.id The identifier of the application that produced the message. null STRING Yes No routing.key The key based on which the excahnge determines how to route the message to the queue. The routing key is similar to an address for the message. empty STRING Yes Yes headers The headers of the message. The attributes used for routing are taken from the this paremeter. A message is considered matching if the value of the header equals the value specified upon binding. null STRING Yes Yes tls.enabled This parameter specifies whether an encrypted communication channel should be established or not. When this parameter is set to true , the tls.truststore.path and tls.truststore.password parameters are initialized. false BOOL Yes No tls.truststore.path The file path to the location of the truststore of the client that sends the RabbitMQ events via the AMQP protocol. A custom client-truststore can be specified if required. If a custom truststore is not specified, then the system uses the default client-trustore in the {carbon.home}/resources/security /code directory. /p /td td style=\"vertical-align: top\" {carbon.home}/resources/security</code> directory.</p></td> <td style=\"vertical-align: top\"> /resources/security/client-truststore.jks STRING Yes No tls.truststore.password The password for the client-truststore. A custom password can be specified if required. If no custom password is specified, then the system uses wso2carbon as the default password. wso2carbon STRING Yes No tls.truststore.type The type of the truststore. JKS STRING Yes No tls.version The version of the tls/ssl. SSL STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink(type ='rabbitmq', uri = 'amqp://guest:guest@localhost:5672', exchange.name = 'direct', routing.key= 'direct', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes events to the direct exchange with the direct exchange type and the directTest routing key. s3 (Sink) S3 sink publishes events as Amazon AWS S3 buckets. Origin: siddhi-io-s3:1.0.2 Syntax @sink(type=\"s3\", credential.provider.class=\" STRING \", aws.access.key=\" STRING \", aws.secret.key=\" STRING \", bucket.name=\" STRING \", aws.region=\" STRING \", versioning.enabled=\" BOOL \", object.path=\" STRING \", storage.class=\" STRING \", content.type=\" STRING \", bucket.acl=\" STRING \", node.id=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic credential.provider.class AWS credential provider class to be used. If blank along with the username and the password, default credential provider will be used. EMPTY_STRING STRING Yes No aws.access.key AWS access key. This cannot be used along with the credential.provider.class EMPTY_STRING STRING Yes No aws.secret.key AWS secret key. This cannot be used along with the credential.provider.class EMPTY_STRING STRING Yes No bucket.name Name of the S3 bucket STRING No No aws.region The region to be used to create the bucket EMPTY_STRING STRING Yes No versioning.enabled Flag to enable versioning support in the bucket false BOOL Yes No object.path Path for each S3 object STRING No Yes storage.class AWS storage class standard STRING Yes No content.type Content type of the event application/octet-stream STRING Yes Yes bucket.acl Access control list for the bucket EMPTY_STRING STRING Yes No node.id The node ID of the current publisher. This needs to be unique for each publisher instance as it may cause object overwrites while uploading the objects to same S3 bucket from different publishers. EMPTY_STRING STRING Yes No Examples EXAMPLE 1 @sink(type='s3', bucket.name='user-stream-bucket',object.path='bar/users', credential.provider='software.amazon.awssdk.auth.credentials.ProfileCredentialsProvider', flush.size='3', @map(type='json', enclosing.element='$.user', @payload(\"\"\"{\"name\": \"{{name}}\", \"age\": {{age}}}\"\"\"))) define stream UserStream(name string, age int); This creates a S3 bucket named 'user-stream-bucket'. Then this will collect 3 events together and create a JSON object and save that in S3. tcp (Sink) A Siddhi application can be configured to publish events via the TCP transport by adding the @Sink(type = 'tcp') annotation at the top of an event stream definition. Origin: siddhi-io-tcp:3.0.4 Syntax @sink(type=\"tcp\", url=\" STRING \", sync=\" STRING \", tcp.no.delay=\" BOOL \", keep.alive=\" BOOL \", worker.threads=\" INT|LONG \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic url The URL to which outgoing events should be published via TCP. The URL should adhere to tcp:// host : port / context format. STRING No No sync This parameter defines whether the events should be published in a synchronized manner or not. If sync = 'true', then the worker will wait for the ack after sending the message. Else it will not wait for an ack. false STRING Yes Yes tcp.no.delay This is to specify whether to disable Nagle algorithm during message passing. If tcp.no.delay = 'true', the execution of Nagle algorithm will be disabled in the underlying TCP logic. Hence there will be no delay between two successive writes to the TCP connection. Else there can be a constant ack delay. true BOOL Yes No keep.alive This property defines whether the server should be kept alive when there are no connections available. true BOOL Yes No worker.threads Number of threads to publish events. 10 INT LONG Yes No Examples EXAMPLE 1 @Sink(type = 'tcp', url='tcp://localhost:8080/abc', sync='true' @map(type='binary')) define stream Foo (attribute1 string, attribute2 int); A sink of type 'tcp' has been defined. All events arriving at Foo stream via TCP transport will be sent to the url tcp://localhost:8080/abc in a synchronous manner. Sinkmapper avro (Sink Mapper) This extension is a Siddhi Event to Avro Message output mapper.Transports that publish messages to Avro sink can utilize this extension to convert Siddhi events to Avro messages. You can either specify the Avro schema or provide the schema registry URL and the schema reference ID as parameters in the stream definition. If no Avro schema is specified, a flat Avro schema of the 'record' type is generated with the stream attributes as schema fields. Origin: siddhi-map-avro:2.0.6 Syntax @sink(..., @map(type=\"avro\", schema.def=\" STRING \", schema.registry=\" STRING \", schema.id=\" STRING \") QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic schema.def This specifies the required Avro schema to be used to convert Siddhi events to Avro messages. The schema needs to be specified as a quoted JSON string. STRING No No schema.registry This specifies the URL of the schema registry. STRING No No schema.id This specifies the ID of the avro schema. This ID is the global ID that is returned from the schema registry when posting the schema to the registry. The specified ID is used to retrieve the schema from the schema registry. STRING No No Examples EXAMPLE 1 @sink(type='inMemory', topic='stock', @map(type='avro',schema.def = \"\"\"{\"type\":\"record\",\"name\":\"stock\",\"namespace\":\"stock.example\",\"fields\":[{\"name\":\"symbol\",\"type\":\"string\"},{\"name\":\"price\",\"type\":\"float\"},{\"name\":\"volume\",\"type\":\"long\"}]}\"\"\")) define stream StockStream (symbol string, price float, volume long); The above configuration performs a default Avro mapping that generates an Avro message as an output ByteBuffer. EXAMPLE 2 @sink(type='inMemory', topic='stock', @map(type='avro',schema.registry = 'http://localhost:8081', schema.id ='22',@payload(\"\"\"{\"Symbol\":{{symbol}},\"Price\":{{price}},\"Volume\":{{volume}}}\"\"\" ))) define stream StockStream (symbol string, price float, volume long); The above configuration performs a custom Avro mapping that generates an Avro message as an output ByteBuffer. The Avro schema is retrieved from the given schema registry (localhost:8081) using the schema ID provided. binary (Sink Mapper) This section explains how to map events processed via Siddhi in order to publish them in the binary format. Origin: siddhi-map-binary:2.0.4 Syntax @sink(..., @map(type=\"binary\") Examples EXAMPLE 1 @sink(type='inMemory', topic='WSO2', @map(type='binary')) define stream FooStream (symbol string, price float, volume long); This will publish Siddhi event in binary format. csv (Sink Mapper) This output mapper extension allows you to convert Siddhi events processed by the WSO2 SP to CSV message before publishing them. You can either use custom placeholder to map a custom CSV message or use pre-defined CSV format where event conversion takes place without extra configurations. Origin: siddhi-map-csv:2.0.3 Syntax @sink(..., @map(type=\"csv\", delimiter=\" STRING \", header=\" BOOL \", event.grouping.enabled=\" BOOL \") QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic delimiter This parameter used to separate the output CSV data, when converting a Siddhi event to CSV format, , STRING Yes No header This parameter specifies whether the CSV messages will be generated with header or not. If this parameter is set to true, message will be generated with header false BOOL Yes No event.grouping.enabled If this parameter is set to true , events are grouped via a line.separator when multiple events are received. It is required to specify a value for the System.lineSeparator() when the value for this parameter is true . false BOOL Yes No Examples EXAMPLE 1 @sink(type='inMemory', topic='{{symbol}}', @map(type='csv')) define stream BarStream (symbol string, price float, volume long); Above configuration will perform a default CSV output mapping, which will generate output as follows: WSO2,55.6,100 OS supported line separator If header is true and delimiter is \"-\", then the output will be as follows: symbol-price-volume OS supported line separator WSO2-55.6-100 OS supported line separator EXAMPLE 2 @sink(type='inMemory', topic='{{symbol}}', @map(type='csv',header='true',delimiter='-',@payload(symbol='0',price='2',volume='1')))define stream BarStream (symbol string, price float,volume long); Above configuration will perform a custom CSV mapping. Here, user can add custom place order in the @payload. The place order indicates that where the attribute name's value will be appear in the output message, The output will be produced output as follows: WSO2,100,55.6 If header is true and delimiter is \"-\", then the output will be as follows: symbol-price-volume WSO2-55.6-100 OS supported line separator If event grouping is enabled, then the output is as follows: WSO2-55.6-100 OS supported line separator WSO2-55.6-100 OS supported line separator WSO2-55.6-100 OS supported line separator json (Sink Mapper) This extension is an Event to JSON output mapper. Transports that publish messages can utilize this extension to convert Siddhi events to JSON messages. You can either send a pre-defined JSON format or a custom JSON message. Origin: siddhi-map-json:5.0.5 Syntax @sink(..., @map(type=\"json\", validate.json=\" BOOL \", enclosing.element=\" STRING \") QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic validate.json If this property is set to true , it enables JSON validation for the JSON messages generated. When validation is carried out, messages that do not adhere to proper JSON standards are dropped. This property is set to 'false' by default. false BOOL Yes No enclosing.element This specifies the enclosing element to be used if multiple events are sent in the same JSON message. Siddhi treats the child elements of the given enclosing element as events and executes JSON expressions on them. If an enclosing.element is not provided, the multiple event scenario is disregarded and JSON path is evaluated based on the root element. $ STRING Yes No Examples EXAMPLE 1 @sink(type='inMemory', topic='stock', @map(type='json')) define stream FooStream (symbol string, price float, volume long); Above configuration does a default JSON input mapping that generates the output given below. { \"event\":{ \"symbol\":WSO2, \"price\":55.6, \"volume\":100 } } EXAMPLE 2 @sink(type='inMemory', topic='{{symbol}}', @map(type='json', enclosing.element='$.portfolio', validate.json='true', @payload( \"\"\"{\"StockData\":{\"Symbol\":\"{{symbol}}\",\"Price\":{{price}}}\"\"\"))) define stream BarStream (symbol string, price float, volume long); The above configuration performs a custom JSON mapping that generates the following JSON message as the output. {\"portfolio\":{ \"StockData\":{ \"Symbol\":WSO2, \"Price\":55.6 } } } keyvalue (Sink Mapper) The Event to Key-Value Map output mapper extension allows you to convert Siddhi events processed by WSO2 SP to key-value map events before publishing them. You can either use pre-defined keys where conversion takes place without extra configurations, or use custom keys with which the messages can be published. Origin: siddhi-map-keyvalue:2.0.5 Syntax @sink(..., @map(type=\"keyvalue\") Examples EXAMPLE 1 @sink(type='inMemory', topic='stock', @map(type='keyvalue')) define stream FooStream (symbol string, price float, volume long); This query performs a default Key-Value output mapping. The expected output is something similar to the following: symbol:'WSO2' price : 55.6f volume: 100L EXAMPLE 2 @sink(type='inMemory', topic='stock', @map(type='keyvalue', @payload(a='symbol',b='price',c='volume'))) define stream FooStream (symbol string, price float, volume long); This query performs a custom Key-Value output mapping where values are passed as objects. Values for symbol , price , and volume attributes are published with the keys a , b and c respectively. The expected output is a map similar to the following: a:'WSO2' b : 55.6f c: 100L EXAMPLE 3 @sink(type='inMemory', topic='stock', @map(type='keyvalue', @payload(a='{{symbol}} is here',b='`price`',c='volume'))) define stream FooStream (symbol string, price float, volume long); This query performs a custom Key-Value output mapping where the values of the a and b attributes are strings and c is object. The expected output should be a Map similar to the following: a:'WSO2 is here' b : 'price' c: 100L passThrough (Sink Mapper) Pass-through mapper passed events (Event[]) through without any mapping or modifications. Origin: siddhi-core:5.1.8 Syntax @sink(..., @map(type=\"passThrough\") Examples EXAMPLE 1 @sink(type='inMemory', @map(type='passThrough')) define stream BarStream (symbol string, price float, volume long); In the following example BarStream uses passThrough outputmapper which emit Siddhi event directly without any transformation into sink. protobuf (Sink Mapper) This output mapper allows you to convert Events to protobuf messages before publishing them. To work with this mapper you have to add auto-generated protobuf classes to the project classpath. When you use this output mapper, you can either define stream attributes as the same names as the protobuf message attributes or you can use custom mapping to map stream definition attributes with the protobuf attributes..Please find the sample proto definition here Origin: siddhi-map-protobuf:1.0.2 Syntax @sink(..., @map(type=\"protobuf\", class=\" STRING \") QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic class This specifies the class name of the protobuf message class, If sink type is grpc then it's not necessary to provide this parameter. - STRING Yes No Examples EXAMPLE 1 @sink(type='grpc', url = 'grpc://localhost:2000/org.wso2.grpc.test.MyService/process @map(type='protobuf')) define stream BarStream (stringValue string, intValue int,longValue long,booleanValue bool,floatValue float,doubleValue double) Above definition will map BarStream values into the protobuf message type of the 'process' method in 'MyService' service EXAMPLE 2 @sink(type='grpc', url = 'grpc://localhost:2000/org.wso2.grpc.test.MyService/process @map(type='protobuf'), @payload(stringValue='a',longValue='b',intValue='c',booleanValue='d',floatValue = 'e', doubleValue = 'f'))) define stream BarStream (a string, b long, c int,d bool,e float,f double); The above definition will map BarStream values to request message type of the 'process' method in 'MyService' service. and stream values will map like this, - value of 'a' will be assign 'stringValue' variable in the message class - value of 'b' will be assign 'longValue' variable in the message class - value of 'c' will be assign 'intValue' variable in the message class - value of 'd' will be assign 'booleanValue' variable in the message class - value of 'e' will be assign 'floatValue' variable in the message class - value of 'f' will be assign 'doubleValue' variable in the message class EXAMPLE 3 @sink(type='grpc', url = 'grpc://localhost:2000/org.wso2.grpc.test.MyService/testMap' @map(type='protobuf')) define stream BarStream (stringValue string,intValue int,map object); The above definition will map BarStream values to request message type of the 'testMap' method in 'MyService' service and since there is an object data type is inthe stream(map object) , mapper will assume that 'map' is an instance of 'java.util.Map' class, otherwise it will throws and error. EXAMPLE 4 @sink(type='inMemory', topic='test01', @map(type='protobuf', class='org.wso2.grpc.test.Request')) define stream BarStream (stringValue string, intValue int,longValue long,booleanValue bool,floatValue float,doubleValue double); The above definition will map BarStream values to 'org.wso2.grpc.test.Request'protobuf class type. If sink type is not a grpc, sink is expecting to get the mapping protobuf class from the 'class' parameter in the @map extension text (Sink Mapper) This extension is a Event to Text output mapper. Transports that publish text messages can utilize this extension to convert the Siddhi events to text messages. Users can use a pre-defined text format where event conversion is carried out without any additional configurations, or use custom placeholder(using {{ and }} ) to map custom text messages. Again, you can also enable mustache based custom mapping. In mustache based custom mapping you can use custom placeholder (using {{ and }} or {{{ and }}} ) to map custom text. In mustache based custom mapping, all variables are HTML escaped by default. For example: is replaced with amp; \" is replaced with quot; = is replaced with #61; If you want to return unescaped HTML, use the triple mustache {{{ instead of double {{ . Origin: siddhi-map-text:2.0.4 Syntax @sink(..., @map(type=\"text\", event.grouping.enabled=\" BOOL \", delimiter=\" STRING \", new.line.character=\" STRING \", mustache.enabled=\" BOOL \") QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic event.grouping.enabled If this parameter is set to true , events are grouped via a delimiter when multiple events are received. It is required to specify a value for the delimiter parameter when the value for this parameter is true . false BOOL Yes No delimiter This parameter specifies how events are separated when a grouped event is received. This must be a whole line and not a single character. ~ ~ ~ ~ STRING Yes No new.line.character This attribute indicates the new line character of the event that is expected to be received. This is used mostly when communication between 2 types of operating systems is expected. For example, Linux uses \\n whereas Windows uses \\r\\n as the end of line character. \\n STRING Yes No mustache.enabled If this parameter is set to true , then mustache mapping gets enabled forcustom text mapping. false BOOL Yes No Examples EXAMPLE 1 @sink(type='inMemory', topic='stock', @map(type='text')) define stream FooStream (symbol string, price float, volume long); This query performs a default text input mapping. The expected output is as follows: symbol:\"WSO2\", price:55.6, volume:100 EXAMPLE 2 @sink(type='inMemory', topic='stock', @map(type='text', event.grouping.enabled='true')) define stream FooStream (symbol string, price float, volume long); This query performs a default text input mapping with event grouping. The expected output is as follows: symbol:\"WSO2\", price:55.6, volume:100 ~ ~ ~ ~ symbol:\"WSO2\", price:55.6, volume:100 EXAMPLE 3 @sink(type='inMemory', topic='stock', @map(type='text', @payload(\"SensorID : {{symbol}}/{{volume}}, SensorPrice : Rs{{price}}/=, Value : {{volume}}ml\"))) define stream FooStream (symbol string, price float, volume long); This query performs a custom text mapping. The expected output is as follows: SensorID : wso2/100, SensorPrice : Rs1000/=, Value : 100ml for the following siddhi event. {wso2,1000,100} EXAMPLE 4 @sink(type='inMemory', topic='stock', @map(type='text', event.grouping.enabled='true', @payload(\"Stock price of {{symbol}} is {{price}}\"))) define stream FooStream (symbol string, price float, volume long); This query performs a custom text mapping with event grouping. The expected output is as follows: Stock price of WSO2 is 55.6 ~ ~ ~ ~ Stock price of WSO2 is 55.6 ~ ~ ~ ~ Stock price of WSO2 is 55.6 for the following siddhi event. {WSO2,55.6,10} EXAMPLE 5 @sink(type='inMemory', topic='stock', @map(type='text', mustache.enabled='true', @payload(\"SensorID : {{{symbol}}}/{{{volume}}}, SensorPrice : Rs{{{price}}}/=, Value : {{{volume}}}ml\"))) define stream FooStream (symbol string, price float, volume long); This query performs a custom text mapping to return unescaped HTML. The expected output is as follows: SensorID : a b/100, SensorPrice : Rs1000/=, Value : 100ml for the following siddhi event. {a b,1000,100} xml (Sink Mapper) This mapper converts Siddhi output events to XML before they are published via transports that publish in XML format. Users can either send a pre-defined XML format or a custom XML message containing event data. Origin: siddhi-map-xml:5.0.3 Syntax @sink(..., @map(type=\"xml\", validate.xml=\" BOOL \", enclosing.element=\" STRING \") QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic validate.xml This parameter specifies whether the XML messages generated should be validated or not. If this parameter is set to true, messages that do not adhere to proper XML standards are dropped. false BOOL Yes No enclosing.element When an enclosing element is specified, the child elements (e.g., the immediate child elements) of that element are considered as events. This is useful when you need to send multiple events in a single XML message. When an enclosing element is not specified, one XML message per every event will be emitted without enclosing. None in custom mapping and events in default mapping STRING Yes No Examples EXAMPLE 1 @sink(type='inMemory', topic='stock', @map(type='xml')) define stream FooStream (symbol string, price float, volume long); Above configuration will do a default XML input mapping which will generate below output events event symbol WSO2 /symbol price 55.6 /price volume 100 /volume /event /events EXAMPLE 2 @sink(type='inMemory', topic='{{symbol}}', @map(type='xml', enclosing.element=' portfolio ', validate.xml='true', @payload( \" StockData Symbol {{symbol}} /Symbol Price {{price}} /Price /StockData \"))) define stream BarStream (symbol string, price float, volume long); Above configuration will perform a custom XML mapping. Inside @payload you can specify the custom template that you want to send the messages out and addd placeholders to places where you need to add event attributes.Above config will produce below output XML message portfolio StockData Symbol WSO2 /Symbol Price 55.6 /Price /StockData /portfolio Source cdc (Source) The CDC source receives events when change events (i.e., INSERT, UPDATE, DELETE) are triggered for a database table. Events are received in the 'key-value' format. There are two modes you could perform CDC: Listening mode and Polling mode. In polling mode, the datasource is periodically polled for capturing the changes. The polling period can be configured. In polling mode, you can only capture INSERT and UPDATE changes. On listening mode, the Source will keep listening to the Change Log of the database and notify in case a change has taken place. Here, you are immediately notified about the change, compared to polling mode. The key values of the map of a CDC change event are as follows. For 'listening' mode: For insert: Keys are specified as columns of the table. For delete: Keys are followed by the specified table columns. This is achieved via 'before_'. e.g., specifying 'before_X' results in the key being added before the column named 'X'. For update: Keys are followed followed by the specified table columns. This is achieved via 'before_'. e.g., specifying 'before_X' results in the key being added before the column named 'X'. For 'polling' mode: Keys are specified as the columns of the table.#### Preparations required for working with Oracle Databases in listening mode Using the extension in Windows, Mac OSX and AIX are pretty straight forward inorder to achieve the required behaviour please follow the steps given below - Download the compatible version of oracle instantclient for the database version from here and extract - Extract and set the environment variable LD_LIBRARY_PATH to the location of instantclient which was exstracted as shown below export LD_LIBRARY_PATH= path to the instant client location - Inside the instantclient folder which was download there are two jars xstreams.jar and ojdbc version .jar convert them to OSGi bundles using the tools which were provided in the distribution /bin for converting the ojdbc.jar use the tool spi-provider.sh|bat and for the conversion of xstreams.jar use the jni-provider.sh as shown below(Note: this way of converting Xstreams jar is applicable only for Linux environments for other OSs this step is not required and converting it through the jartobundle.sh tool is enough) ./jni-provider.sh input-jar destination comma seperated native library names once ojdbc and xstreams jars are converted to OSGi copy the generated jars to the distribution /lib . Currently siddhi-io-cdc only supports the oracle database distributions 12 and above See parameter: mode for supported databases and change events. Origin: siddhi-io-cdc:2.0.4 Syntax @source(type=\"cdc\", url=\" STRING \", mode=\" STRING \", jdbc.driver.name=\" STRING \", username=\" STRING \", password=\" STRING \", pool.properties=\" STRING \", datasource.name=\" STRING \", table.name=\" STRING \", polling.column=\" STRING \", polling.interval=\" INT \", operation=\" STRING \", connector.properties=\" STRING \", database.server.id=\" STRING \", database.server.name=\" STRING \", wait.on.missed.record=\" BOOL \", missed.record.waiting.timeout=\" INT \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic url The connection URL to the database. F=The format used is: 'jdbc:mysql:// host : port / database_name ' STRING No No mode Mode to capture the change data. The type of events that can be received, and the required parameters differ based on the mode. The mode can be one of the following: 'polling': This mode uses a column named 'polling.column' to monitor the given table. It captures change events of the 'RDBMS', 'INSERT, and 'UPDATE' types. 'listening': This mode uses logs to monitor the given table. It currently supports change events only of the 'MySQL', 'INSERT', 'UPDATE', and 'DELETE' types. listening STRING Yes No jdbc.driver.name The driver class name for connecting the database. It is required to specify a value for this parameter when the mode is 'polling'. STRING Yes No username The username to be used for accessing the database. This user needs to have the 'SELECT', 'RELOAD', 'SHOW DATABASES', 'REPLICATION SLAVE', and 'REPLICATION CLIENT'privileges for the change data capturing table (specified via the 'table.name' parameter). To operate in the polling mode, the user needs 'SELECT' privileges. STRING No No password The password of the username you specified for accessing the database. STRING No No pool.properties The pool parameters for the database connection can be specified as key-value pairs. STRING Yes No datasource.name Name of the wso2 datasource to connect to the database. When datasource name is provided, the URL, username and password are not needed. A datasource based connection is given more priority over the URL based connection. This parameter is applicable only when the mode is set to 'polling', and it can be applied only when you use this extension with WSO2 Stream Processor. STRING Yes No table.name The name of the table that needs to be monitored for data changes. STRING No No polling.column The column name that is polled to capture the change data. It is recommended to have a TIMESTAMP field as the 'polling.column' in order to capture the inserts and updates. Numeric auto-incremental fields and char fields can also be used as 'polling.column'. However, note that fields of these types only support insert change capturing, and the possibility of using a char field also depends on how the data is input. It is required to enter a value for this parameter only when the mode is 'polling'. STRING Yes No polling.interval The time interval (specified in seconds) to poll the given table for changes. This parameter is applicable only when the mode is set to 'polling'. 1 INT Yes No operation The change event operation you want to carry out. Possible values are 'insert', 'update' or 'delete'. This parameter is not case sensitive. It is required to specify a value only when the mode is 'listening'. STRING No No connector.properties Here, you can specify Debezium connector properties as a comma-separated string. The properties specified here are given more priority over the parameters. This parameter is applicable only for the 'listening' mode. Empty_String STRING Yes No database.server.id An ID to be used when joining MySQL database cluster to read the bin log. This should be a unique integer between 1 to 2^32. This parameter is applicable only when the mode is 'listening'. Random integer between 5400 and 6400 STRING Yes No database.server.name A logical name that identifies and provides a namespace for the database server. This parameter is applicable only when the mode is 'listening'. {host}_{port} STRING Yes No wait.on.missed.record Indicates whether the process needs to wait on missing/out-of-order records. When this flag is set to 'true' the process will be held once it identifies a missing record. The missing recrod is identified by the sequence of the polling.column value. This can be used only with number fields and not recommended to use with time values as it will not be sequential. This should be enabled ONLY where the records can be written out-of-order, (eg. concurrent writers) as this degrades the performance. false BOOL Yes No missed.record.waiting.timeout The timeout (specified in seconds) to retry for missing/out-of-order record. This should be used along with the wait.on.missed.record parameter. If the parameter is not set, the process will indefinitely wait for the missing record. -1 INT Yes No Examples EXAMPLE 1 @source(type = 'cdc' , url = 'jdbc:mysql://localhost:3306/SimpleDB', username = 'cdcuser', password = 'pswd4cdc', table.name = 'students', operation = 'insert', @map(type='keyvalue', @attributes(id = 'id', name = 'name'))) define stream inputStream (id string, name string); In this example, the CDC source listens to the row insertions that are made in the 'students' table with the column name, and the ID. This table belongs to the 'SimpleDB' MySQL database that can be accessed via the given URL. EXAMPLE 2 @source(type = 'cdc' , url = 'jdbc:mysql://localhost:3306/SimpleDB', username = 'cdcuser', password = 'pswd4cdc', table.name = 'students', operation = 'update', @map(type='keyvalue', @attributes(id = 'id', name = 'name', before_id = 'before_id', before_name = 'before_name'))) define stream inputStream (before_id string, id string, before_name string , name string); In this example, the CDC source listens to the row updates that are made in the 'students' table. This table belongs to the 'SimpleDB' MySQL database that can be accessed via the given URL. EXAMPLE 3 @source(type = 'cdc' , url = 'jdbc:mysql://localhost:3306/SimpleDB', username = 'cdcuser', password = 'pswd4cdc', table.name = 'students', operation = 'delete', @map(type='keyvalue', @attributes(before_id = 'before_id', before_name = 'before_name'))) define stream inputStream (before_id string, before_name string); In this example, the CDC source listens to the row deletions made in the 'students' table. This table belongs to the 'SimpleDB' database that can be accessed via the given URL. EXAMPLE 4 @source(type = 'cdc', mode='polling', polling.column = 'id', jdbc.driver.name = 'com.mysql.jdbc.Driver', url = 'jdbc:mysql://localhost:3306/SimpleDB', username = 'cdcuser', password = 'pswd4cdc', table.name = 'students', @map(type='keyvalue'), @attributes(id = 'id', name = 'name')) define stream inputStream (id int, name string); In this example, the CDC source polls the 'students' table for inserts. 'id' that is specified as the polling colum' is an auto incremental field. The connection to the database is made via the URL, username, password, and the JDBC driver name. EXAMPLE 5 @source(type = 'cdc', mode='polling', polling.column = 'id', datasource.name = 'SimpleDB', table.name = 'students', @map(type='keyvalue'), @attributes(id = 'id', name = 'name')) define stream inputStream (id int, name string); In this example, the CDC source polls the 'students' table for inserts. The given polling column is a char column with the 'S001, S002, ... .' pattern. The connection to the database is made via a data source named 'SimpleDB'. Note that the 'datasource.name' parameter works only with the Stream Processor. EXAMPLE 6 @source(type = 'cdc', mode='polling', polling.column = 'last_updated', datasource.name = 'SimpleDB', table.name = 'students', @map(type='keyvalue')) define stream inputStream (name string); In this example, the CDC source polls the 'students' table for inserts and updates. The polling column is a timestamp field. EXAMPLE 7 @source(type='cdc', jdbc.driver.name='com.mysql.jdbc.Driver', url='jdbc:mysql://localhost:3306/SimpleDB', username='cdcuser', password='pswd4cdc', table.name='students', mode='polling', polling.column='id', operation='insert', wait.on.missed.record='true', missed.record.waiting.timeout='10', @map(type='keyvalue'), @attributes(batch_no='batch_no', item='item', qty='qty')) define stream inputStream (id int, name string); In this example, the CDC source polls the 'students' table for inserts. The polling column is a numeric field. This source expects the records in the database to be written concurrently/out-of-order so it waits if it encounters a missing record. If the record doesn't appear within 10 seconds it resumes the process. EXAMPLE 8 @source(type = 'cdc', url = 'jdbc:oracle:thin://localhost:1521/ORCLCDB', username='c##xstrm', password='xs', table.name='DEBEZIUM.sweetproductiontable', operation = 'insert', connector.properties='oracle.outserver.name=DBZXOUT,oracle.pdb=ORCLPDB1' @map(type = 'keyvalue')) define stream insertSweetProductionStream (ID int, NAME string, WEIGHT int); In this example, the CDC source connect to an Oracle database and listens for insert queries of sweetproduction table email (Source) The 'Email' source allows you to receive events via emails. An 'Email' source can be configured using the 'imap' or 'pop3' server to receive events. This allows you to filter the messages that satisfy the criteria specified under the 'search term' option. The email source parameters can be defined in either the ' SP_HOME /conf/ PROFILE /deployment yaml' file or the stream definition. If the parameter configurations are not available in either place, the default values are considered (i.e., if default values are available). If you need to configure server system parameters that are not provided as options in the stream definition, they need to be defined in the 'deployment yaml' file under 'email source properties'. For more information about 'imap' and 'pop3' server system parameters, see the following. JavaMail Reference Implementation - IMAP Store JavaMail Reference Implementation - POP3 Store Store Origin: siddhi-io-email:2.0.5 Syntax @source(type=\"email\", username=\" STRING \", password=\" STRING \", store=\" STRING \", host=\" STRING \", port=\" INT \", folder=\" STRING \", search.term=\" STRING \", polling.interval=\" LONG \", action.after.processed=\" STRING \", folder.to.move=\" STRING \", content.type=\" STRING \", ssl.enable=\" BOOL \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic username The user name of the email account. e.g., 'wso2mail' is the username of the 'wso2mail@gmail.com' mail account. STRING No No password The password of the email account STRING No No store The store type that used to receive emails. Possible values are 'imap' and 'pop3'. imap STRING Yes No host The host name of the server (e.g., 'imap.gmail.com' is the host name for a gmail account with an IMAP store.). The default value 'imap.gmail.com' is only valid if the email account is a gmail account with IMAP enabled. If store type is 'imap', then the default value is 'imap.gmail.com'. If the store type is 'pop3', then thedefault value is 'pop3.gmail.com'. STRING Yes No port The port that is used to create the connection. '993', the default value is valid only if the store is 'imap' and ssl-enabled. INT Yes No folder The name of the folder to which the emails should be fetched. INBOX STRING Yes No search.term The option that includes conditions such as key-value pairs to search for emails. In a string search term, the key and the value should be separated by a semicolon (';'). Each key-value pair must be within inverted commas (' '). The string search term can define multiple comma-separated key-value pairs. This string search term currently supports only the 'subject', 'from', 'to', 'bcc', and 'cc' keys. e.g., if you enter 'subject:DAS, from:carbon, bcc:wso2', the search term creates a search term instance that filters emails that contain 'DAS' in the subject, 'carbon' in the 'from' address, and 'wso2' in one of the 'bcc' addresses. The string search term carries out sub string matching that is case-sensitive. If '@' in included in the value for any key other than the 'subject' key, it checks for an address that is equal to the value given. e.g., If you search for 'abc@', the string search terms looks for an address that contains 'abc' before the '@' symbol. None STRING Yes No polling.interval This defines the time interval in seconds at which th email source should poll the account to check for new mail arrivals.in seconds. 600 LONG Yes No action.after.processed The action to be performed by the email source for the processed mail. Possible values are as follows: 'FLAGGED': Sets the flag as 'flagged'. 'SEEN': Sets the flag as 'read'. 'ANSWERED': Sets the flag as 'answered'. 'DELETE': Deletes tha mail after the polling cycle. 'MOVE': Moves the mail to the folder specified in the 'folder.to.move' parameter. If the folder specified is 'pop3', then the only option available is 'DELETE'. NONE STRING Yes No folder.to.move The name of the folder to which the mail must be moved once it is processed. If the action after processing is 'MOVE', it is required to specify a value for this parameter. STRING No No content.type The content type of the email. It can be either 'text/plain' or 'text/html.' text/plain STRING Yes No ssl.enable If this is set to 'true', a secure port is used to establish the connection. The possible values are 'true' and 'false'. true BOOL Yes No System Parameters Name Description Default Value Possible Parameters mail.imap.partialfetch This determines whether the IMAP partial-fetch capability should be used. true true or false mail.imap.fetchsize The partial fetch size in bytes. 16K value in bytes mail.imap.peek If this is set to 'true', the IMAP PEEK option should be used when fetching body parts to avoid setting the 'SEEN' flag on messages. The default value is 'false'. This can be overridden on a per-message basis by the 'setPeek method' in 'IMAPMessage'. false true or false mail.imap.connectiontimeout The socket connection timeout value in milliseconds. This timeout is implemented by 'java.net.Socket'. infinity timeout Any Integer value mail.imap.timeout The socket read timeout value in milliseconds. This timeout is implemented by 'java.net.Socket'. infinity timeout Any Integer value mail.imap.writetimeout The socket write timeout value in milliseconds. This timeout is implemented by using a 'java.util.concurrent.ScheduledExecutorService' per connection that schedules a thread to close the socket if the timeout period elapses. Therefore, the overhead of using this timeout is one thread per connection. infinity timeout Any Integer value mail.imap.statuscachetimeout The timeout value in milliseconds for the cache of 'STATUS' command response. 1000ms Time out in miliseconds mail.imap.appendbuffersize The maximum size of a message to buffer in memory when appending to an IMAP folder. None Any Integer value mail.imap.connectionpoolsize The maximum number of available connections in the connection pool. 1 Any Integer value mail.imap.connectionpooltimeout The timeout value in milliseconds for connection pool connections. 45000ms Any Integer mail.imap.separatestoreconnection If this parameter is set to 'true', it indicates that a dedicated store connection needs to be used for store commands. true true or false mail.imap.auth.login.disable If this is set to 'true', it is not possible to use the non-standard 'AUTHENTICATE LOGIN' command instead of the plain 'LOGIN' command. false true or false mail.imap.auth.plain.disable If this is set to 'true', the 'AUTHENTICATE PLAIN' command cannot be used. false true or false mail.imap.auth.ntlm.disable If true, prevents use of the AUTHENTICATE NTLM command. false true or false mail.imap.proxyauth.user If the server supports the PROXYAUTH extension, this property specifies the name of the user to act as. Authentication to log in to the server is carried out using the administrator's credentials. After authentication, the IMAP provider issues the 'PROXYAUTH' command with the user name specified in this property. None Valid string value mail.imap.localaddress The local address (host name) to bind to when creating the IMAP socket. Defaults to the address picked by the Socket class. Valid string value mail.imap.localport The local port number to bind to when creating the IMAP socket. Defaults to the port number picked by the Socket class. Valid String value mail.imap.sasl.enable If this parameter is set to 'true', the system attempts to use the 'javax.security.sasl' package to choose an authentication mechanism for the login. false true or false mail.imap.sasl.mechanisms A list of SASL mechanism names that the system should to try to use. The names can be separated by spaces or commas. None Valid string value mail.imap.sasl.authorizationid The authorization ID to use in the SASL authentication. If this parameter is not set, the authentication ID (username) is used. Valid string value mail.imap.sasl.realm The realm to use with SASL authentication mechanisms that require a realm, such as 'DIGEST-MD5'. None Valid string value mail.imap.auth.ntlm.domain The NTLM authentication domain. None Valid string value The NTLM authentication domain. NTLM protocol-specific flags. None Valid integer value mail.imap.socketFactory If this parameter is set to a class that implements the 'javax.net.SocketFactory' interface, this class is used to create IMAP sockets. None Valid SocketFactory mail.imap.socketFactory.class If this parameter is set, it specifies the name of a class that implements the 'javax.net.SocketFactory' interface. This class is used to create IMAP sockets. None Valid string mail.imap.socketFactory.fallback If this parameter is set to 'true', failure to create a socket using the specified socket factory class results in the socket being created using the 'java.net.Socket' class. true true or false mail.imap.socketFactory.port This specifies the port to connect to when using the specified socket factory. If this parameter is not set, the default port is used. 143 Valid Integer mail.imap.ssl.checkserveridentity If this parameter is set to 'true', the system checks the server identity as specified by RFC 2595. false true or false mail.imap.ssl.trust If this parameter is set and a socket factory has not been specified, it enables the use of a 'MailSSLSocketFactory'. If this parameter is set to ' ', all the hosts are trusted. If this parameter specifies list of hosts separated by white spaces, only those hosts are trusted. If the parameter is not set to any of the values mentioned above, trust depends on the certificate presented by the server. Valid String mail.imap.ssl.socketFactory If this parameter is set to a class that extends the 'javax.net.ssl.SSLSocketFactory' class this class is used to create IMAP SSL sockets. None SSL Socket Factory mail.imap.ssl.socketFactory.class If this parameter is set, it specifies the name of a class that extends the 'javax.net.ssl.SSLSocketFactory' class. This class is used to create IMAP SSL sockets. None Valid String mail.imap.ssl.socketFactory.port This specifies the port to connect to when using the specified socket factory. the default port 993 is used. valid port number mail.imap.ssl.protocols This specifies the SSL protocols that are enabled for SSL connections. The property value is a whitespace-separated list of tokens acceptable to the 'javax.net.ssl.SSLSocket.setEnabledProtocols' method. None Valid string mail.imap.starttls.enable If this parameter is set to 'true', it is possible to use the 'STARTTLS' command (if supported by the server) to switch the connection to a TLS-protected connection before issuing any login commands. false true or false mail.imap.socks.host This specifies the host name of a 'SOCKS5' proxy server that is used to connect to the mail server. None Valid String mail.imap.socks.port This specifies the port number for the 'SOCKS5' proxy server. This is needed if the proxy server is not using the standard port number 1080. 1080 Valid String mail.imap.minidletime This property sets the delay in milliseconds. 10 milliseconds time in seconds (Integer) mail.imap.enableimapevents If this property is set to 'true', it enables special IMAP-specific events to be delivered to the 'ConnectionListener' of the store. The unsolicited responses received during the idle method of the store are sent as connection events with 'IMAPStore.RESPONSE' as the type. The event's message is the raw IMAP response string. false true or false mail.imap.folder.class The class name of a subclass of 'com.sun.mail.imap.IMAPFolder'. The subclass can be used to provide support for additional IMAP commands. The subclass must have public constructors of the form 'public MyIMAPFolder'(String fullName, char separator, IMAPStore store, Boolean isNamespace) and public 'MyIMAPFolder'(ListInfo li, IMAPStore store) None Valid String mail.pop3.connectiontimeout The socket connection timeout value in milliseconds. Infinite timeout Integer value mail.pop3.timeout The socket I/O timeout value in milliseconds. Infinite timeout Integer value mail.pop3.message.class The class name of a subclass of 'com.sun.mail.pop3.POP3Message'. None Valid String mail.pop3.localaddress The local address (host name) to bind to when creating the POP3 socket. Defaults to the address picked by the Socket class. Valid String mail.pop3.localport The local port number to bind to when creating the POP3 socket. Defaults to the port number picked by the Socket class. Valid port number mail.pop3.apop.enable If this parameter is set to 'true', use 'APOP' instead of 'USER/PASS' to log in to the 'POP3' server (if the 'POP3' server supports 'APOP'). APOP sends a digest of the password instead of clearing the text password. false true or false mail.pop3.socketFactory If this parameter is set to a class that implements the 'javax.net.SocketFactory' interface, this class is used to create 'POP3' sockets. None Socket Factory mail.pop3.socketFactory.class If this parameter is set, it specifies the name of a class that implements the 'javax.net.SocketFactory' interface. This class is used to create 'POP3' sockets. None Valid String mail.pop3.socketFactory.fallback If this parameter is set to 'true', failure to create a socket using the specified socket factory class results in the socket being created using the 'java.net.Socket' class. false true or false mail.pop3.socketFactory.port This specifies the port to connect to when using the specified socket factory. Default port Valid port number mail.pop3.ssl.checkserveridentity If this parameter is set to 'true', check the server identity as specified by RFC 2595. false true or false mail.pop3.ssl.trust If this parameter is set and a socket factory has not been specified, it is possible to use a 'MailSSLSocketFactory'. If this parameter is set to ' ', all the hosts are trusted. If the parameter is set to a whitespace-separated list of hosts, only those hosts are trusted. If the parameter is not set to any of the values mentioned above, trust depends on the certificate presented by the server. Valid String mail.pop3.ssl.socketFactory If this parameter is set to a class that extends the 'javax.net.ssl.SSLSocketFactory' class, this class is used to create 'POP3' SSL sockets. None SSL Socket Factory mail.pop3.ssl.checkserveridentity If this parameter is set to 'true', the system checks the server identity as specified by 'RFC 2595'. false true or false mail.pop3.ssl.trust If this parameter is set and a socket factory has not been specified, it is possible to use a 'MailSSLSocketFactory'. If this parameter is set to '*', all the hosts are trusted. If the parameter is set to a whitespace-separated list of hosts, only those hosts are trusted. Trust depends on the certificate presented by the server. Valid String mail.pop3.ssl.socketFactory If this parameter is set to a class that extends the 'javax.net.ssl.SSLSocketFactory' class, this class is used to create 'POP3 SSL' sockets. None SSL Socket Factory mail.pop3.ssl.socketFactory.class If this parameter is set, it specifies the name of a class that extends the 'javax.net.ssl.SSLSocketFactory' class. This class is used to create 'POP3 SSL' sockets. None Valid String mail.pop3.ssl.socketFactory.p This parameter pecifies the port to connect to when using the specified socket factory. 995 Valid Integer mail.pop3.ssl.protocols This parameter specifies the SSL protocols that are enabled for SSL connections. The property value is a whitespace-separated list of tokens acceptable to the 'javax.net.ssl.SSLSocket.setEnabledProtocols' method. None Valid String mail.pop3.starttls.enable If this parameter is set to 'true', it is possible to use the 'STLS' command (if supported by the server) to switch the connection to a TLS-protected connection before issuing any login commands. false true or false mail.pop3.starttls.required If this parameter is set to 'true', it is required to use the 'STLS' command. The connect method fails if the server does not support the 'STLS' command or if the command fails. false true or false mail.pop3.socks.host This parameter specifies the host name of a 'SOCKS5' proxy server that can be used to connect to the mail server. None Valid String mail.pop3.socks.port This parameter specifies the port number for the 'SOCKS5' proxy server. None Valid String mail.pop3.disabletop If this parameter is set to 'true', the 'POP3 TOP' command is not used to fetch message headers. false true or false mail.pop3.forgettopheaders If this parameter is set to 'true', the headers that might have been retrieved using the 'POP3 TOP' command is forgotten and replaced by the headers retrieved when the 'POP3 RETR' command is executed. false true or false mail.pop3.filecache.enable If this parameter is set to 'true', the 'POP3' provider caches message data in a temporary file instead of caching them in memory. Messages are only added to the cache when accessing the message content. Message headers are always cached in memory (on demand). The file cache is removed when the folder is closed or the JVM terminates. false true or false mail.pop3.filecache.dir If the file cache is enabled, this property is used to override the default directory used by the JDK for temporary files. None Valid String mail.pop3.cachewriteto This parameter controls the behavior of the 'writeTo' method on a 'POP3' message object. If the parameter is set to 'true', the message content has not been cached yet, and the 'ignoreList' is null, the message is cached before being written. If not, the message is streamed directly to the output stream without being cached. false true or false mail.pop3.keepmessagecontent If this property is set to 'true', a hard reference to the cached content is retained, preventing the memory from being reused until the folder is closed, or until the cached content is explicitly invalidated (using the 'invalidate' method). false true or false Examples EXAMPLE 1 @source(type='email', @map(type='xml'), username='receiver.account', password='account.password',)define stream inputStream (name string, age int, country string); This example illustrates how to receive events in 'xml' format via the email source. In this example, only the required parameters are defined in the stream definition. The default values are taken for the other parameters. The search term is not defined, and therefore, all the new messages in the inbox folder are polled and taken. EXAMPLE 2 @source(type='email', @map(type='xml'), username='receiver.account', password='account.password',store = 'imap',host = 'imap.gmail.com',port = '993',searchTerm = 'subject:Stream Processor, from: from.account@ , cc: cc.account',polling.interval='500',action.after.processed='DELETE',content.type='text/html,)define stream inputStream (name string, age int, country string); This example illustrates how to receive events in 'xml' format via the email source. The email source polls the mail account every 500 seconds to check whether any new mails have arrived. It processes new mails only if they satisfy the conditions specified for the email search term (the value for 'from' of the email message should be 'from.account@. host name ', and the message should contain 'cc.account' in the cc receipient list and the word 'Stream Processor' in the mail subject). in this example, the action after processing is 'DELETE'. Therefore,after processing the event, corresponding mail is deleted from the mail folder. file (Source) File Source provides the functionality for user to feed data to siddhi from files. Both text and binary files are supported by file source. Origin: siddhi-io-file:2.0.3 Syntax @source(type=\"file\", dir.uri=\" STRING \", file.uri=\" STRING \", mode=\" STRING \", tailing=\" BOOL \", action.after.process=\" STRING \", action.after.failure=\" STRING \", move.after.process=\" STRING \", move.after.failure=\" STRING \", begin.regex=\" STRING \", end.regex=\" STRING \", file.polling.interval=\" STRING \", dir.polling.interval=\" STRING \", timeout=\" STRING \", file.read.wait.timeout=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic dir.uri Used to specify a directory to be processed. All the files inside this directory will be processed. Only one of 'dir.uri' and 'file.uri' should be provided. This uri MUST have the respective protocol specified. STRING No No file.uri Used to specify a file to be processed. Only one of 'dir.uri' and 'file.uri' should be provided. This uri MUST have the respective protocol specified. STRING No No mode This parameter is used to specify how files in given directory should.Possible values for this parameter are, 1. TEXT.FULL : to read a text file completely at once. 2. BINARY.FULL : to read a binary file completely at once. 3. LINE : to read a text file line by line. 4. REGEX : to read a text file and extract data using a regex. line STRING Yes No tailing This can either have value true or false. By default it will be true. This attribute allows user to specify whether the file should be tailed or not. If tailing is enabled, the first file of the directory will be tailed. Also tailing should not be enabled in 'binary.full' or 'text.full' modes. true BOOL Yes No action.after.process This parameter is used to specify the action which should be carried out after processing a file in the given directory. It can be either DELETE or MOVE and default value will be 'DELETE'. If the action.after.process is MOVE, user must specify the location to move consumed files using 'move.after.process' parameter. delete STRING Yes No action.after.failure This parameter is used to specify the action which should be carried out if a failure occurred during the process. It can be either DELETE or MOVE and default value will be 'DELETE'. If the action.after.failure is MOVE, user must specify the location to move consumed files using 'move.after.failure' parameter. delete STRING Yes No move.after.process If action.after.process is MOVE, user must specify the location to move consumed files using 'move.after.process' parameter. This should be the absolute path of the file that going to be created after moving is done. This uri MUST have the respective protocol specified. STRING No No move.after.failure If action.after.failure is MOVE, user must specify the location to move consumed files using 'move.after.failure' parameter. This should be the absolute path of the file that going to be created after moving is done. This uri MUST have the respective protocol specified. STRING No No begin.regex This will define the regex to be matched at the beginning of the retrieved content. None STRING Yes No end.regex This will define the regex to be matched at the end of the retrieved content. None STRING Yes No file.polling.interval This parameter is used to specify the time period (in milliseconds) of a polling cycle for a file. 1000 STRING Yes No dir.polling.interval This parameter is used to specify the time period (in milliseconds) of a polling cycle for a directory. 1000 STRING Yes No timeout This parameter is used to specify the maximum time period (in milliseconds) for waiting until a file is processed. 5000 STRING Yes No file.read.wait.timeout This parameter is used to specify the maximum time period (in milliseconds) till it waits before retrying to read the full file content. 1000 STRING Yes No Examples EXAMPLE 1 @source(type='file', mode='text.full', tailing='false' dir.uri='file://abc/xyz', action.after.process='delete', @map(type='json')) define stream FooStream (symbol string, price float, volume long); Under above configuration, all the files in directory will be picked and read one by one. In this case, it's assumed that all the files contains json valid json strings with keys 'symbol','price' and 'volume'. Once a file is read, its content will be converted to an event using siddhi-map-json extension and then, that event will be received to the FooStream. Finally, after reading is finished, the file will be deleted. EXAMPLE 2 @source(type='file', mode='files.repo.line', tailing='true', dir.uri='file://abc/xyz', @map(type='json')) define stream FooStream (symbol string, price float, volume long); Under above configuration, the first file in directory '/abc/xyz' will be picked and read line by line. In this case, it is assumed that the file contains lines json strings. For each line, line content will be converted to an event using siddhi-map-json extension and then, that event will be received to the FooStream. Once file content is completely read, it will keep checking whether a new entry is added to the file or not. If such entry is added, it will be immediately picked up and processed. grpc (Source) This extension starts a grpc server during initialization time. The server listens to requests from grpc stubs. This source has a default mode of operation and custom user defined grpc service mode. By default this uses EventService. Please find the proto definition here . In the default mode this source will use EventService consume method. If we want to use our custom gRPC services, we have to pack auto-generated gRPC service classes and protobuf classes into a jar file and add it into the project classpath (or to the jars folder in the siddhi-tooling folder if we use it with siddhi-tooling ). Please find the custom protobuf definition that uses in examples here . This method will receive requests and injects them into stream through a mapper. Origin: siddhi-io-grpc:1.0.5 Syntax @source(type=\"grpc\", receiver.url=\" STRING \", max.inbound.message.size=\" INT \", max.inbound.metadata.size=\" INT \", server.shutdown.waiting.time=\" LONG \", truststore.file=\" STRING \", truststore.password=\" STRING \", truststore.algorithm=\" STRING \", tls.store.type=\" STRING \", keystore.file=\" STRING \", keystore.password=\" STRING \", keystore.algorithm=\" STRING \", enable.ssl=\" BOOL \", threadpool.size=\" INT \", threadpool.buffer.size=\" INT \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic receiver.url The url which can be used by a client to access the grpc server in this extension. This url should consist the host hostPort, port, fully qualified service name, method name in the following format. grpc://0.0.0.0:9763/ serviceName / methodName For example: grpc://0.0.0.0:9763/org.wso2.grpc.EventService/consume STRING No No max.inbound.message.size Sets the maximum message size in bytes allowed to be received on the server. 4194304 INT Yes No max.inbound.metadata.size Sets the maximum size of metadata in bytes allowed to be received. 8192 INT Yes No server.shutdown.waiting.time The time in seconds to wait for the server to shutdown, giving up if the timeout is reached. 5 LONG Yes No truststore.file the file path of truststore. If this is provided then server authentication is enabled - STRING Yes No truststore.password the password of truststore. If this is provided then the integrity of the keystore is checked - STRING Yes No truststore.algorithm the encryption algorithm to be used for server authentication - STRING Yes No tls.store.type TLS store type - STRING Yes No keystore.file the file path of keystore. If this is provided then client authentication is enabled - STRING Yes No keystore.password the password of keystore - STRING Yes No keystore.algorithm the encryption algorithm to be used for client authentication - STRING Yes No enable.ssl to enable ssl. If set to true and truststore.file is not given then it will be set to default carbon jks by default FALSE BOOL Yes No threadpool.size Sets the maximum size of threadpool dedicated to serve requests at the gRPC server 100 INT Yes No threadpool.buffer.size Sets the maximum size of threadpool buffer server 100 INT Yes No System Parameters Name Description Default Value Possible Parameters keyStoreFile Path of the key store file src/main/resources/security/wso2carbon.jks valid path for a key store file keyStorePassword This is the password used with key store file wso2carbon valid password for the key store file keyStoreAlgorithm The encryption algorithm to be used for client authentication SunX509 - trustStoreFile This is the trust store file with the path src/main/resources/security/client-truststore.jks - trustStorePassword This is the password used with trust store file wso2carbon valid password for the trust store file trustStoreAlgorithm the encryption algorithm to be used for server authentication SunX509 - Examples EXAMPLE 1 @source(type='grpc', receiver.url='grpc://localhost:8888/org.wso2.grpc.EventService/consume', @map(type='json')) define stream BarStream (message String); Here the port is given as 8888. So a grpc server will be started on port 8888 and the server will expose EventService. This is the default service packed with the source. In EventService the consume method is EXAMPLE 2 @source(type='grpc', receiver.url='grpc://localhost:8888/org.wso2.grpc.EventService/consume', @map(type='json', @attributes(name='trp:name', age='trp:age', message='message'))) define stream BarStream (message String, name String, age int); Here we are getting headers sent with the request as transport properties and injecting them into the stream. With each request a header will be sent in MetaData in the following format: 'Name:John', 'Age:23' EXAMPLE 3 @source(type='grpc', receiver.url='grpc://localhost:8888/org.wso2.grpc.MyService/send', @map(type='protobuf')) define stream BarStream (stringValue string, intValue int,longValue long,booleanValue bool,floatValue float,doubleValue double); Here the port is given as 8888. So a grpc server will be started on port 8888 and sever will keep listening to the 'send' RPC method in the 'MyService' service. EXAMPLE 4 @source(type='grpc', receiver.url='grpc://localhost:8888/org.wso2.grpc.MyService/send', @map(type='protobuf', @attributes(a = 'stringValue', b = 'intValue', c = 'longValue',d = 'booleanValue', e ='floatValue', f ='doubleValue'))) define stream BarStream (a string ,c long,b int, d bool,e float,f double); Here the port is given as 8888. So a grpc server will be started on port 8888 and sever will keep listening to the 'send' method in the 'MyService' service. Since we provide mapping in the stream we can use any names for stream attributes, but we have to map those names with correct protobuf message attributes' names. If we want to send metadata, we should map the attributes. EXAMPLE 5 @source(type='grpc', receiver.url='grpc://localhost:8888/org.wso2.grpc.StreamService/clientStream', @map(type='protobuf')) define stream BarStream (stringValue string, intValue int,longValue long,booleanValue bool,floatValue float,doubleValue double); Here we receive a stream of requests to the grpc source. Whenever we want to use streaming with grpc source, we have to define the RPC method as client streaming method (look at the sample proto file provided in the resource folder here ), when we define a stream method siddhi will identify it as a stream RPC method and ready to accept stream of request from the client. grpc-call-response (Source) This grpc source receives responses received from gRPC server for requests sent from a grpc-call sink. The source will receive responses for sink with the same sink.id. For example if you have a gRPC sink with sink.id 15 then we need to set the sink.id as 15 in the source to receives responses. Sinks and sources have 1:1 mapping Origin: siddhi-io-grpc:1.0.5 Syntax @source(type=\"grpc-call-response\", sink.id=\" INT \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic sink.id a unique ID that should be set for each grpc-call source. There is a 1:1 mapping between grpc-call sinks and grpc-call-response sources. Each sink has one particular source listening to the responses to requests published from that sink. So the same sink.id should be given when writing the sink also. INT No No Examples EXAMPLE 1 @source(type='grpc-call-response', sink.id= '1') define stream BarStream (message String);@sink(type='grpc-call', publisher.url = 'grpc://194.23.98.100:8080/EventService/process', sink.id= '1', @map(type='json')) define stream FooStream (message String); Here we are listening to responses for requests sent from the sink with sink.id 1 will be received here. The results will be injected into BarStream grpc-service (Source) This extension implements a grpc server for receiving and responding to requests. During initialization time a grpc server is started on the user specified port exposing the required service as given in the url. This source also has a default mode and a user defined grpc service mode. By default this uses EventService. Please find the proto definition here In the default mode this will use the EventService process method. If we want to use our custom gRPC services, we have to pack auto-generated gRPC service classes and protobuf classes into a jar file and add it into the project classpath (or to the jars folder in the siddhi-tooling folder if we use it with siddhi-tooling ). Please find the custom protobuf definition that uses in examples here . This accepts grpc message class Event as defined in the EventService proto. This uses GrpcServiceResponse sink to send reponses back in the same Event message format. Origin: siddhi-io-grpc:1.0.5 Syntax @source(type=\"grpc-service\", receiver.url=\" STRING \", max.inbound.message.size=\" INT \", max.inbound.metadata.size=\" INT \", service.timeout=\" INT \", server.shutdown.waiting.time=\" LONG \", truststore.file=\" STRING \", truststore.password=\" STRING \", truststore.algorithm=\" STRING \", tls.store.type=\" STRING \", keystore.file=\" STRING \", keystore.password=\" STRING \", keystore.algorithm=\" STRING \", enable.ssl=\" BOOL \", threadpool.size=\" INT \", threadpool.buffer.size=\" INT \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic receiver.url The url which can be used by a client to access the grpc server in this extension. This url should consist the host hostPort, port, fully qualified service name, method name in the following format. grpc://0.0.0.0:9763/ serviceName / methodName For example: grpc://0.0.0.0:9763/org.wso2.grpc.EventService/consume STRING No No max.inbound.message.size Sets the maximum message size in bytes allowed to be received on the server. 4194304 INT Yes No max.inbound.metadata.size Sets the maximum size of metadata in bytes allowed to be received. 8192 INT Yes No service.timeout The period of time in milliseconds to wait for siddhi to respond to a request received. After this time period of receiving a request it will be closed with an error message. 10000 INT Yes No server.shutdown.waiting.time The time in seconds to wait for the server to shutdown, giving up if the timeout is reached. 5 LONG Yes No truststore.file the file path of truststore. If this is provided then server authentication is enabled - STRING Yes No truststore.password the password of truststore. If this is provided then the integrity of the keystore is checked - STRING Yes No truststore.algorithm the encryption algorithm to be used for server authentication - STRING Yes No tls.store.type TLS store type - STRING Yes No keystore.file the file path of keystore. If this is provided then client authentication is enabled - STRING Yes No keystore.password the password of keystore - STRING Yes No keystore.algorithm the encryption algorithm to be used for client authentication - STRING Yes No enable.ssl to enable ssl. If set to true and truststore.file is not given then it will be set to default carbon jks by default FALSE BOOL Yes No threadpool.size Sets the maximum size of threadpool dedicated to serve requests at the gRPC server 100 INT Yes No threadpool.buffer.size Sets the maximum size of threadpool buffer server 100 INT Yes No System Parameters Name Description Default Value Possible Parameters keyStoreFile This is the key store file with the path src/main/resources/security/wso2carbon.jks valid path for a key store file keyStorePassword This is the password used with key store file wso2carbon valid password for the key store file keyStoreAlgorithm The encryption algorithm to be used for client authentication SunX509 - trustStoreFile This is the trust store file with the path src/main/resources/security/client-truststore.jks - trustStorePassword This is the password used with trust store file wso2carbon valid password for the trust store file trustStoreAlgorithm the encryption algorithm to be used for server authentication SunX509 - Examples EXAMPLE 1 @source(type='grpc-service', receiver.url='grpc://localhost:8888/org.wso2.grpc.EventService/process', source.id='1', @map(type='json', @attributes(messageId='trp:messageId', message='message'))) define stream FooStream (messageId String, message String); Here a grpc server will be started at port 8888. The process method of EventService will be exposed for clients. source.id is set as 1. So a grpc-service-response sink with source.id = 1 will send responses back for requests received to this source. Note that it is required to specify the transport property messageId since we need to correlate the request message with the response. EXAMPLE 2 @sink(type='grpc-service-response', source.id='1', @map(type='json')) define stream BarStream (messageId String, message String); @source(type='grpc-service', receiver.url='grpc://134.23.43.35:8080/org.wso2.grpc.EventService/process', source.id='1', @map(type='json', @attributes(messageId='trp:messageId', message='message'))) define stream FooStream (messageId String, message String); from FooStream select * insert into BarStream; The grpc requests are received through the grpc-service sink. Each received event is sent back through grpc-service-source. This is just a passthrough through Siddhi as we are selecting everything from FooStream and inserting into BarStream. EXAMPLE 3 @source(type='grpc-service', source.id='1' receiver.url='grpc://locanhost:8888/org.wso2.grpc.EventService/consume', @map(type='json', @attributes(name='trp:name', age='trp:age', message='message'))) define stream BarStream (message String, name String, age int); Here we are getting headers sent with the request as transport properties and injecting them into the stream. With each request a header will be sent in MetaData in the following format: 'Name:John', 'Age:23' EXAMPLE 4 @sink(type='grpc-service-response', source.id='1', message.id='{{messageId}}', @map(type='protobuf', @payload(stringValue='a',intValue='b',longValue='c',booleanValue='d',floatValue = 'e', doubleValue ='f'))) define stream BarStream (a string,messageId string, b int,c long,d bool,e float,f double); @source(type='grpc-service', receiver.url='grpc://134.23.43.35:8888/org.wso2.grpc.test.MyService/process', source.id='1', @map(type='protobuf', @attributes(messageId='trp:message.id', a = 'stringValue', b = 'intValue', c = 'longValue',d = 'booleanValue', e = 'floatValue', f ='doubleValue'))) define stream FooStream (a string,messageId string, b int,c long,d bool,e float,f double); from FooStream select * insert into BarStream; Here a grpc server will be started at port 8888. The process method of the MyService will be exposed to the clients. 'source.id' is set as 1. So a grpc-service-response sink with source.id = 1 will send responses back for requests received to this source. Note that it is required to specify the transport property messageId since we need to correlate the request message with the response and also we should map stream attributes with correct protobuf message attributes even they define using the same name as protobuf message attributes. http (Source) HTTP source receives POST requests via HTTP and HTTPS protocols in format such as text , XML and JSON . It also supports basic authentication to ensure events are received from authorized users/systems. The request headers and properties can be accessed via transport properties in the format trp: header . Origin: siddhi-io-http:2.2.0 Syntax @source(type=\"http\", receiver.url=\" STRING \", basic.auth.enabled=\" STRING \", worker.count=\" INT \", socket.idle.timeout=\" INT \", ssl.verify.client=\" STRING \", ssl.protocol=\" STRING \", tls.store.type=\" STRING \", ssl.configurations=\" STRING \", request.size.validation.configurations=\" STRING \", header.validation.configurations=\" STRING \", server.bootstrap.configurations=\" STRING \", trace.log.enabled=\" BOOL \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic receiver.url The URL on which events should be received. To enable SSL use https protocol in the url. http://0.0.0.0:9763/ appNAme / streamName STRING Yes No basic.auth.enabled This only works in VM, Docker and Kubernetes. Where when enabled it authenticates each request using the Authorization:'Basic encodeBase64(username:Password)' header. false STRING Yes No worker.count The number of active worker threads to serve the incoming events. By default the value is set to 1 to ensure events are processed in the same order they arrived. By increasing this value, higher performance can be achieved in the expense of loosing event ordering. 1 INT Yes No socket.idle.timeout Idle timeout for HTTP connection in millis. 120000 INT Yes No ssl.verify.client The type of client certificate verification. Supported values are require , optional . - STRING Yes No ssl.protocol SSL/TLS protocol. TLS STRING Yes No tls.store.type TLS store type. JKS STRING Yes No ssl.configurations SSL/TSL configurations in format \"' key : value ',' key : value '\" . Some supported parameters: - SSL/TLS protocols: 'sslEnabledProtocols:TLSv1.1,TLSv1.2' - List of ciphers: 'ciphers:TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256' - Enable session creation: 'client.enable.session.creation:true' - Supported server names: 'server.suported.server.names:server' - Add HTTP SNIMatcher: 'server.supported.snimatchers:SNIMatcher' - STRING Yes No request.size.validation.configurations Configurations to validate the HTTP request size. Expected format \"' key : value ',' key : value '\" . Some supported configurations : - Enable request size validation: 'request.size.validation:true' If request size is validated - Maximum request size: 'request.size.validation.maximum.value:2048' - Response status code when request size validation fails: 'request.size.validation.reject.status.code:401' - Response message when request size validation fails: 'request.size.validation.reject.message:Message is bigger than the valid size' - Response Content-Type when request size validation fails: 'request.size.validation.reject.message.content.type:plain/text' - STRING Yes No header.validation.configurations Configurations to validate HTTP headers. Expected format \"' key : value ',' key : value '\" . Some supported configurations : - Enable header size validation: 'header.size.validation:true' If header size is validated - Maximum length of initial line: 'header.validation.maximum.request.line:4096' - Maximum length of all headers: 'header.validation.maximum.size:8192' - Maximum length of the content or each chunk: 'header.validation.maximum.chunk.size:8192' - Response status code when header validation fails: 'header.validation.reject.status.code:401' - Response message when header validation fails: 'header.validation.reject.message:Message header is bigger than the valid size' - Response Content-Type when header validation fails: 'header.validation.reject.message.content.type:plain/text' - STRING Yes No server.bootstrap.configurations Server bootstrap configurations in format \"' key : value ',' key : value '\" . Some supported configurations : - Server connect timeout in millis: 'server.bootstrap.connect.timeout:15000' - Server socket timeout in seconds: 'server.bootstrap.socket.timeout:15' - Enable TCP no delay: 'server.bootstrap.nodelay:true' - Enable server keep alive: 'server.bootstrap.keepalive:true' - Send buffer size: 'server.bootstrap.sendbuffersize:1048576' - Receive buffer size: 'server.bootstrap.recievebuffersize:1048576' - Number of connections queued: 'server.bootstrap.socket.backlog:100' - STRING Yes No trace.log.enabled Enable trace log for traffic monitoring. false BOOL Yes No System Parameters Name Description Default Value Possible Parameters serverBootstrapBossGroupSize Number of boss threads to accept incoming connections. Number of available processors Any positive integer serverBootstrapWorkerGroupSize Number of worker threads to accept the connections from boss threads and perform non-blocking read and write from one or more channels. (Number of available processors) * 2 Any positive integer serverBootstrapClientGroupSize Number of client threads to perform non-blocking read and write to one or more channels. (Number of available processors) * 2 Any positive integer defaultHost The default host of the transport. 0.0.0.0 Any valid host defaultScheme The default protocol. http http https defaultHttpPort The default HTTP port when default scheme is http . 8280 Any valid port defaultHttpsPort The default HTTPS port when default scheme is https . 8243 Any valid port keyStoreLocation The default keystore file path. ${carbon.home}/resources/security/wso2carbon.jks Path to .jks file keyStorePassword The default keystore password. wso2carbon Keystore password as string Examples EXAMPLE 1 @app.name('StockProcessor') @source(type='http', @map(type = 'json')) define stream StockStream (symbol string, price float, volume long); Above HTTP source listeners on url http://0.0.0.0:9763/StockProcessor/StockStream for JSON messages on the format: { \"event\": { \"symbol\": \"FB\", \"price\": 24.5, \"volume\": 5000 } } It maps the incoming messages and sends them to StockStream for processing. EXAMPLE 2 @source(type='http', receiver.url='http://localhost:5005/stocks', @map(type = 'xml')) define stream StockStream (symbol string, price float, volume long); Above HTTP source listeners on url http://localhost:5005/stocks for JSON messages on the format: events event symbol Fb /symbol price 55.6 /price volume 100 /volume /event /events It maps the incoming messages and sends them to StockStream for processing. http-call-response (Source) The http-call-response source receives the responses for the calls made by its corresponding http-call sink, and maps them from formats such as text , XML and JSON . To handle messages with different http status codes having different formats, multiple http-call-response sources are allowed to associate with a single http-call sink. It allows accessing the attributes of the event that initiated the call, and the response headers and properties via transport properties in the format trp: attribute name and trp: header/property respectively. Origin: siddhi-io-http:2.2.0 Syntax @source(type=\"http-call-response\", sink.id=\" STRING \", http.status.code=\" STRING \", allow.streaming.responses=\" BOOL \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic sink.id Identifier to correlate the http-call-response source with its corresponding http-call sink that published the messages. STRING No No http.status.code The matching http responses status code regex, that is used to filter the the messages which will be processed by the source.Eg: http.status.code = '200' , http.status.code = '4\\d+' 200 STRING Yes No allow.streaming.responses Enable consuming responses on a streaming manner. false BOOL Yes No Examples EXAMPLE 1 @sink(type='http-call', method='POST', publisher.url='http://localhost:8005/registry/employee', sink.id='employee-info', @map(type='json')) define stream EmployeeRequestStream (name string, id int); @source(type='http-call-response', sink.id='employee-info', http.status.code='2\\\\d+', @map(type='json', @attributes(name='trp:name', id='trp:id', location='$.town', age='$.age'))) define stream EmployeeResponseStream(name string, id int, location string, age int); @source(type='http-call-response', sink.id='employee-info', http.status.code='4\\\\d+', @map(type='text', regex.A='((.|\\n)*)', @attributes(error='A[1]'))) define stream EmployeeErrorStream(error string); When events arrive in EmployeeRequestStream , http-call sink makes calls to endpoint on url http://localhost:8005/registry/employee with POST method and Content-Type application/json . If the arriving event has attributes name : John and id : 1423 it will send a message with default JSON mapping as follows: { \"event\": { \"name\": \"John\", \"id\": 1423 } } When the endpoint responds with status code in the range of 200 the message will be received by the http-call-response source associated with the EmployeeResponseStream stream, because it is correlated with the sink by the same sink.id employee-info and as that expects messages with http.status.code in regex format 2\\d+ . If the response message is in the format { \"town\": \"NY\", \"age\": 24 } the source maps the location and age attributes by executing JSON path on the message and maps the name and id attributes by extracting them from the request event via as transport properties. If the response status code is in the range of 400 then the message will be received by the http-call-response source associated with the EmployeeErrorStream stream, because it is correlated with the sink by the same sink.id employee-info and it expects messages with http.status.code in regex format 4\\d+ , and maps the error response to the error attribute of the event. http-request (Source) Deprecated (Use http-service source instead). The http-request source receives POST requests via HTTP and HTTPS protocols in format such as text , XML and JSON and sends responses via its corresponding http-response sink correlated through a unique source.id . For request and response correlation, it generates a messageId upon each incoming request and expose it via transport properties in the format trp:messageId to correlate them with the responses at the http-response sink. The request headers and properties can be accessed via transport properties in the format trp: header . It also supports basic authentication to ensure events are received from authorized users/systems. Origin: siddhi-io-http:2.2.0 Syntax @source(type=\"http-request\", receiver.url=\" STRING \", source.id=\" STRING \", connection.timeout=\" INT \", basic.auth.enabled=\" STRING \", worker.count=\" INT \", socket.idle.timeout=\" INT \", ssl.verify.client=\" STRING \", ssl.protocol=\" STRING \", tls.store.type=\" STRING \", ssl.configurations=\" STRING \", request.size.validation.configurations=\" STRING \", header.validation.configurations=\" STRING \", server.bootstrap.configurations=\" STRING \", trace.log.enabled=\" BOOL \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic receiver.url The URL on which events should be received. To enable SSL use https protocol in the url. http://0.0.0.0:9763/ appNAme / streamName STRING Yes No source.id Identifier to correlate the http-request source to its corresponding http-response sinks to send responses. STRING No No connection.timeout Connection timeout in millis. The system will send a timeout, if a corresponding response is not sent by an associated http-response sink within the given time. 120000 INT Yes No basic.auth.enabled This only works in VM, Docker and Kubernetes. Where when enabled it authenticates each request using the Authorization:'Basic encodeBase64(username:Password)' header. false STRING Yes No worker.count The number of active worker threads to serve the incoming events. By default the value is set to 1 to ensure events are processed in the same order they arrived. By increasing this value, higher performance can be achieved in the expense of loosing event ordering. 1 INT Yes No socket.idle.timeout Idle timeout for HTTP connection in millis. 120000 INT Yes No ssl.verify.client The type of client certificate verification. Supported values are require , optional . - STRING Yes No ssl.protocol SSL/TLS protocol. TLS STRING Yes No tls.store.type TLS store type. JKS STRING Yes No ssl.configurations SSL/TSL configurations in format \"' key : value ',' key : value '\" . Some supported parameters: - SSL/TLS protocols: 'sslEnabledProtocols:TLSv1.1,TLSv1.2' - List of ciphers: 'ciphers:TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256' - Enable session creation: 'client.enable.session.creation:true' - Supported server names: 'server.suported.server.names:server' - Add HTTP SNIMatcher: 'server.supported.snimatchers:SNIMatcher' - STRING Yes No request.size.validation.configurations Configurations to validate the HTTP request size. Expected format \"' key : value ',' key : value '\" . Some supported configurations : - Enable request size validation: 'request.size.validation:true' If request size is validated - Maximum request size: 'request.size.validation.maximum.value:2048' - Response status code when request size validation fails: 'request.size.validation.reject.status.code:401' - Response message when request size validation fails: 'request.size.validation.reject.message:Message is bigger than the valid size' - Response Content-Type when request size validation fails: 'request.size.validation.reject.message.content.type:plain/text' - STRING Yes No header.validation.configurations Configurations to validate HTTP headers. Expected format \"' key : value ',' key : value '\" . Some supported configurations : - Enable header size validation: 'header.size.validation:true' If header size is validated - Maximum length of initial line: 'header.validation.maximum.request.line:4096' - Maximum length of all headers: 'header.validation.maximum.size:8192' - Maximum length of the content or each chunk: 'header.validation.maximum.chunk.size:8192' - Response status code when header validation fails: 'header.validation.reject.status.code:401' - Response message when header validation fails: 'header.validation.reject.message:Message header is bigger than the valid size' - Response Content-Type when header validation fails: 'header.validation.reject.message.content.type:plain/text' - STRING Yes No server.bootstrap.configurations Server bootstrap configurations in format \"' key : value ',' key : value '\" . Some supported configurations : - Server connect timeout in millis: 'server.bootstrap.connect.timeout:15000' - Server socket timeout in seconds: 'server.bootstrap.socket.timeout:15' - Enable TCP no delay: 'server.bootstrap.nodelay:true' - Enable server keep alive: 'server.bootstrap.keepalive:true' - Send buffer size: 'server.bootstrap.sendbuffersize:1048576' - Receive buffer size: 'server.bootstrap.recievebuffersize:1048576' - Number of connections queued: 'server.bootstrap.socket.backlog:100' - STRING Yes No trace.log.enabled Enable trace log for traffic monitoring. false BOOL Yes No System Parameters Name Description Default Value Possible Parameters serverBootstrapBossGroupSize Number of boss threads to accept incoming connections. Number of available processors Any positive integer serverBootstrapWorkerGroupSize Number of worker threads to accept the connections from boss threads and perform non-blocking read and write from one or more channels. (Number of available processors) * 2 Any positive integer serverBootstrapClientGroupSize Number of client threads to perform non-blocking read and write to one or more channels. (Number of available processors) * 2 Any positive integer defaultHost The default host of the transport. 0.0.0.0 Any valid host defaultScheme The default protocol. http http https defaultHttpPort The default HTTP port when default scheme is http . 8280 Any valid port defaultHttpsPort The default HTTPS port when default scheme is https . 8243 Any valid port keyStoreLocation The default keystore file path. ${carbon.home}/resources/security/wso2carbon.jks Path to .jks file keyStorePassword The default keystore password. wso2carbon Keystore password as string Examples EXAMPLE 1 @source(type='http-request', receiver.url='http://localhost:5005/add', source.id='adder', @map(type='json, @attributes(messageId='trp:messageId', value1='$.event.value1', value2='$.event.value2'))) define stream AddStream (messageId string, value1 long, value2 long); @sink(type='http-response', source.id='adder', message.id='{{messageId}}', @map(type = 'json')) define stream ResultStream (messageId string, results long); @info(name = 'query1') from AddStream select messageId, value1 + value2 as results insert into ResultStream; Above sample listens events on http://localhost:5005/stocks url for JSON messages on the format: { \"event\": { \"value1\": 3, \"value2\": 4 } } Map the vents into AddStream, process the events through query query1 , and sends the results produced on ResultStream via http-response sink on the message format: { \"event\": { \"results\": 7 } } http-response (Source) Deprecated (Use http-call-response source instead). The http-response source receives the responses for the calls made by its corresponding http-request sink, and maps them from formats such as text , XML and JSON . To handle messages with different http status codes having different formats, multiple http-response sources are allowed to associate with a single http-request sink. It allows accessing the attributes of the event that initiated the call, and the response headers and properties via transport properties in the format trp: attribute name and trp: header/property respectively. Origin: siddhi-io-http:2.2.0 Syntax @source(type=\"http-response\", sink.id=\" STRING \", http.status.code=\" STRING \", allow.streaming.responses=\" BOOL \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic sink.id Identifier to correlate the http-response source with its corresponding http-request sink that published the messages. STRING No No http.status.code The matching http responses status code regex, that is used to filter the the messages which will be processed by the source.Eg: http.status.code = '200' , http.status.code = '4\\d+' 200 STRING Yes No allow.streaming.responses Enable consuming responses on a streaming manner. false BOOL Yes No Examples EXAMPLE 1 @sink(type='http-request', method='POST', publisher.url='http://localhost:8005/registry/employee', sink.id='employee-info', @map(type='json')) define stream EmployeeRequestStream (name string, id int); @source(type='http-response', sink.id='employee-info', http.status.code='2\\\\d+', @map(type='json', @attributes(name='trp:name', id='trp:id', location='$.town', age='$.age'))) define stream EmployeeResponseStream(name string, id int, location string, age int); @source(type='http-response', sink.id='employee-info', http.status.code='4\\\\d+', @map(type='text', regex.A='((.|\\n)*)', @attributes(error='A[1]'))) define stream EmployeeErrorStream(error string); When events arrive in EmployeeRequestStream , http-request sink makes calls to endpoint on url http://localhost:8005/registry/employee with POST method and Content-Type application/json . If the arriving event has attributes name : John and id : 1423 it will send a message with default JSON mapping as follows: { \"event\": { \"name\": \"John\", \"id\": 1423 } } When the endpoint responds with status code in the range of 200 the message will be received by the http-response source associated with the EmployeeResponseStream stream, because it is correlated with the sink by the same sink.id employee-info and as that expects messages with http.status.code in regex format 2\\d+ . If the response message is in the format { \"town\": \"NY\", \"age\": 24 } the source maps the location and age attributes by executing JSON path on the message and maps the name and id attributes by extracting them from the request event via as transport properties. If the response status code is in the range of 400 then the message will be received by the http-response source associated with the EmployeeErrorStream stream, because it is correlated with the sink by the same sink.id employee-info and it expects messages with http.status.code in regex format 4\\d+ , and maps the error response to the error attribute of the event. http-service (Source) The http-service source receives POST requests via HTTP and HTTPS protocols in format such as text , XML and JSON and sends responses via its corresponding http-service-response sink correlated through a unique source.id . For request and response correlation, it generates a messageId upon each incoming request and expose it via transport properties in the format trp:messageId to correlate them with the responses at the http-service-response sink. The request headers and properties can be accessed via transport properties in the format trp: header . It also supports basic authentication to ensure events are received from authorized users/systems. Origin: siddhi-io-http:2.2.0 Syntax @source(type=\"http-service\", receiver.url=\" STRING \", source.id=\" STRING \", connection.timeout=\" INT \", basic.auth.enabled=\" STRING \", worker.count=\" INT \", socket.idle.timeout=\" INT \", ssl.verify.client=\" STRING \", ssl.protocol=\" STRING \", tls.store.type=\" STRING \", ssl.configurations=\" STRING \", request.size.validation.configurations=\" STRING \", header.validation.configurations=\" STRING \", server.bootstrap.configurations=\" STRING \", trace.log.enabled=\" BOOL \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic receiver.url The URL on which events should be received. To enable SSL use https protocol in the url. http://0.0.0.0:9763/ appNAme / streamName STRING Yes No source.id Identifier to correlate the http-service source to its corresponding http-service-response sinks to send responses. STRING No No connection.timeout Connection timeout in millis. The system will send a timeout, if a corresponding response is not sent by an associated http-service-response sink within the given time. 120000 INT Yes No basic.auth.enabled This only works in VM, Docker and Kubernetes. Where when enabled it authenticates each request using the Authorization:'Basic encodeBase64(username:Password)' header. false STRING Yes No worker.count The number of active worker threads to serve the incoming events. By default the value is set to 1 to ensure events are processed in the same order they arrived. By increasing this value, higher performance can be achieved in the expense of loosing event ordering. 1 INT Yes No socket.idle.timeout Idle timeout for HTTP connection in millis. 120000 INT Yes No ssl.verify.client The type of client certificate verification. Supported values are require , optional . - STRING Yes No ssl.protocol SSL/TLS protocol. TLS STRING Yes No tls.store.type TLS store type. JKS STRING Yes No ssl.configurations SSL/TSL configurations in format \"' key : value ',' key : value '\" . Some supported parameters: - SSL/TLS protocols: 'sslEnabledProtocols:TLSv1.1,TLSv1.2' - List of ciphers: 'ciphers:TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256' - Enable session creation: 'client.enable.session.creation:true' - Supported server names: 'server.suported.server.names:server' - Add HTTP SNIMatcher: 'server.supported.snimatchers:SNIMatcher' - STRING Yes No request.size.validation.configurations Configurations to validate the HTTP request size. Expected format \"' key : value ',' key : value '\" . Some supported configurations : - Enable request size validation: 'request.size.validation:true' If request size is validated - Maximum request size: 'request.size.validation.maximum.value:2048' - Response status code when request size validation fails: 'request.size.validation.reject.status.code:401' - Response message when request size validation fails: 'request.size.validation.reject.message:Message is bigger than the valid size' - Response Content-Type when request size validation fails: 'request.size.validation.reject.message.content.type:plain/text' - STRING Yes No header.validation.configurations Configurations to validate HTTP headers. Expected format \"' key : value ',' key : value '\" . Some supported configurations : - Enable header size validation: 'header.size.validation:true' If header size is validated - Maximum length of initial line: 'header.validation.maximum.request.line:4096' - Maximum length of all headers: 'header.validation.maximum.size:8192' - Maximum length of the content or each chunk: 'header.validation.maximum.chunk.size:8192' - Response status code when header validation fails: 'header.validation.reject.status.code:401' - Response message when header validation fails: 'header.validation.reject.message:Message header is bigger than the valid size' - Response Content-Type when header validation fails: 'header.validation.reject.message.content.type:plain/text' - STRING Yes No server.bootstrap.configurations Server bootstrap configurations in format \"' key : value ',' key : value '\" . Some supported configurations : - Server connect timeout in millis: 'server.bootstrap.connect.timeout:15000' - Server socket timeout in seconds: 'server.bootstrap.socket.timeout:15' - Enable TCP no delay: 'server.bootstrap.nodelay:true' - Enable server keep alive: 'server.bootstrap.keepalive:true' - Send buffer size: 'server.bootstrap.sendbuffersize:1048576' - Receive buffer size: 'server.bootstrap.recievebuffersize:1048576' - Number of connections queued: 'server.bootstrap.socket.backlog:100' - STRING Yes No trace.log.enabled Enable trace log for traffic monitoring. false BOOL Yes No System Parameters Name Description Default Value Possible Parameters serverBootstrapBossGroupSize Number of boss threads to accept incoming connections. Number of available processors Any positive integer serverBootstrapWorkerGroupSize Number of worker threads to accept the connections from boss threads and perform non-blocking read and write from one or more channels. (Number of available processors) * 2 Any positive integer serverBootstrapClientGroupSize Number of client threads to perform non-blocking read and write to one or more channels. (Number of available processors) * 2 Any positive integer defaultHost The default host of the transport. 0.0.0.0 Any valid host defaultScheme The default protocol. http http https defaultHttpPort The default HTTP port when default scheme is http . 8280 Any valid port defaultHttpsPort The default HTTPS port when default scheme is https . 8243 Any valid port keyStoreLocation The default keystore file path. ${carbon.home}/resources/security/wso2carbon.jks Path to .jks file keyStorePassword The default keystore password. wso2carbon Keystore password as string Examples EXAMPLE 1 @source(type='http-service', receiver.url='http://localhost:5005/add', source.id='adder', @map(type='json, @attributes(messageId='trp:messageId', value1='$.event.value1', value2='$.event.value2'))) define stream AddStream (messageId string, value1 long, value2 long); @sink(type='http-service-response', source.id='adder', message.id='{{messageId}}', @map(type = 'json')) define stream ResultStream (messageId string, results long); @info(name = 'query1') from AddStream select messageId, value1 + value2 as results insert into ResultStream; Above sample listens events on http://localhost:5005/stocks url for JSON messages on the format: { \"event\": { \"value1\": 3, \"value2\": 4 } } Map the vents into AddStream, process the events through query query1 , and sends the results produced on ResultStream via http-service-response sink on the message format: { \"event\": { \"results\": 7 } } inMemory (Source) In-memory source subscribes to a topic to consume events which are published on the same topic by In-memory sinks. This provides a way to connect multiple Siddhi Apps deployed under the same Siddhi Manager (JVM). Here both the publisher and subscriber should have the same event schema (stream definition) for successful data transfer. Origin: siddhi-core:5.1.8 Syntax @source(type=\"inMemory\", topic=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic topic Subscribes to the events sent on the given topic. STRING No No Examples EXAMPLE 1 @source(type='inMemory', topic='Stocks', @map(type='passThrough')) define stream StocksStream (symbol string, price float, volume long); Here the StocksStream uses inMemory source to consume events published on the topic Stocks by the inMemory sinks deployed in the same JVM. jms (Source) JMS Source allows users to subscribe to a JMS broker and receive JMS messages. It has the ability to receive Map messages and Text messages. Origin: siddhi-io-jms:2.0.3 Syntax @source(type=\"jms\", destination=\" STRING \", connection.factory.jndi.name=\" STRING \", factory.initial=\" STRING \", provider.url=\" STRING \", connection.factory.type=\" STRING \", worker.count=\" INT \", connection.username=\" STRING \", connection.password=\" STRING \", retry.interval=\" INT \", retry.count=\" INT \", use.receiver=\" BOOL \", subscription.durable=\" BOOL \", connection.factory.nature=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic destination Queue/Topic name which JMS Source should subscribe to STRING No No connection.factory.jndi.name JMS Connection Factory JNDI name. This value will be used for the JNDI lookup to find the JMS Connection Factory. QueueConnectionFactory STRING Yes No factory.initial Naming factory initial value STRING No No provider.url Java naming provider URL. Property for specifying configuration information for the service provider to use. The value of the property should contain a URL string (e.g. \"ldap://somehost:389\") STRING No No connection.factory.type Type of the connection connection factory. This can be either queue or topic. queue STRING Yes No worker.count Number of worker threads listening on the given queue/topic. 1 INT Yes No connection.username username for the broker. None STRING Yes No connection.password Password for the broker None STRING Yes No retry.interval Interval between each retry attempt in case of connection failure in milliseconds. 10000 INT Yes No retry.count Number of maximum reties that will be attempted in case of connection failure with broker. 5 INT Yes No use.receiver Implementation to be used when consuming JMS messages. By default transport will use MessageListener and tweaking this property will make make use of MessageReceiver false BOOL Yes No subscription.durable Property to enable durable subscription. false BOOL Yes No connection.factory.nature Connection factory nature for the broker. default STRING Yes No Examples EXAMPLE 1 @source(type='jms', @map(type='json'), factory.initial='org.apache.activemq.jndi.ActiveMQInitialContextFactory', provider.url='tcp://localhost:61616',destination='DAS_JMS_TEST', connection.factory.type='topic',connection.factory.jndi.name='TopicConnectionFactory') define stream inputStream (name string, age int, country string); This example shows how to connect to an ActiveMQ topic and receive messages. EXAMPLE 2 @source(type='jms', @map(type='json'), factory.initial='org.apache.activemq.jndi.ActiveMQInitialContextFactory', provider.url='tcp://localhost:61616',destination='DAS_JMS_TEST' ) define stream inputStream (name string, age int, country string); This example shows how to connect to an ActiveMQ queue and receive messages. Note that we are not providing properties like connection factory type kafka (Source) A Kafka source receives events to be processed by WSO2 SP from a topic with a partition for a Kafka cluster. The events received can be in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. Origin: siddhi-io-kafka:5.0.5 Syntax @source(type=\"kafka\", bootstrap.servers=\" STRING \", topic.list=\" STRING \", group.id=\" STRING \", threading.option=\" STRING \", partition.no.list=\" STRING \", seq.enabled=\" BOOL \", is.binary.message=\" BOOL \", topic.offsets.map=\" STRING \", enable.auto.commit=\" BOOL \", optional.configuration=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This specifies the list of Kafka servers to which the Kafka source must listen. This list can be provided as a set of comma-separated values. e.g., localhost:9092,localhost:9093 STRING No No topic.list This specifies the list of topics to which the source must listen. This list can be provided as a set of comma-separated values. e.g., topic_one,topic_two STRING No No group.id This is an ID to identify the Kafka source group. The group ID ensures that sources with the same topic and partition that are in the same group do not receive the same event. STRING No No threading.option This specifies whether the Kafka source is to be run on a single thread, or in multiple threads based on a condition. Possible values are as follows: single.thread : To run the Kafka source on a single thread. topic.wise : To use a separate thread per topic. partition.wise : To use a separate thread per partition. STRING No No partition.no.list The partition number list for the given topic. This is provided as a list of comma-separated values. e.g., 0,1,2, . 0 STRING Yes No seq.enabled If this parameter is set to true , the sequence of the events received via the source is taken into account. Therefore, each event should contain a sequence number as an attribute value to indicate the sequence. false BOOL Yes No is.binary.message In order to receive binary events via the Kafka source,it is required to setthis parameter to 'True'. false BOOL Yes No topic.offsets.map This parameter specifies reading offsets for each topic and partition. The value for this parameter is specified in the following format: topic = offset , topic = offset , When an offset is defined for a topic, the Kafka source skips reading the message with the number specified as the offset as well as all the messages sent previous to that message. If the offset is not defined for a specific topic it reads messages from the beginning. e.g., stocks=100,trades=50 reads from the 101th message of the stocks topic, and from the 51 st message of the trades topic. null STRING Yes No enable.auto.commit This parameter specifies whether to commit offsets automatically. By default, as the Siddhi Kafka source reads messages from Kafka, it will periodically(Default value is set to 1000ms. You can configure it with auto.commit.interval.ms property as an optional.configuration ) commit its current offset (defined as the offset of the next message to be read) for the partitions it is reading from back to Kafka. To guarantee at-least-once processing, we recommend you to enable Siddhi Periodic State Persistence when enable.auto.commit property is set to true . When you would like more control over exactly when offsets are committed, you can set enable.auto.commit to false and Siddhi will commit the offset once the records are successfully processed at the Source. When enable.auto.commit is set to false , manual committing would introduce a latency during consumption. true BOOL Yes No optional.configuration This parameter contains all the other possible configurations that the consumer is created with. e.g., ssl.keystore.type:JKS,batch.size:200 . null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic,kafka_topic2', group.id='test', threading.option='partition.wise', bootstrap.servers='localhost:9092', partition.no.list='0,1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This kafka source configuration listens to the kafka_topic and kafka_topic2 topics with 0 and 1 partitions. A thread is created for each topic and partition combination. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . EXAMPLE 2 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic', group.id='test', threading.option='single.thread', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka source configuration listens to the kafka_topic topic for the default partition because no partition.no.list is defined. Only one thread is created for the topic. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . kafkaMultiDC (Source) The Kafka Multi-Datacenter(DC) source receives records from the same topic in brokers deployed in two different kafka clusters. It filters out all the duplicate messages and ensuresthat the events are received in the correct order using sequential numbering. It receives events in formats such as TEXT , XML JSON and Binary`.The Kafka Source creates the default partition '0' for a given topic, if the topic has not yet been created in the Kafka cluster. Origin: siddhi-io-kafka:5.0.5 Syntax @source(type=\"kafkaMultiDC\", bootstrap.servers=\" STRING \", topic=\" STRING \", partition.no=\" INT \", is.binary.message=\" BOOL \", optional.configuration=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This contains the kafka server list which the kafka source listens to. This is given using comma-separated values. eg: 'localhost:9092,localhost:9093' STRING No No topic This is the topic that the source listens to. eg: 'topic_one' STRING No No partition.no This is the partition number of the given topic. 0 INT Yes No is.binary.message In order to receive the binary events via the Kafka Multi-DC source, the value of this parameter needs to be set to 'True'. false BOOL Yes No optional.configuration This contains all the other possible configurations with which the consumer can be created.eg: producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source(type='kafkaMultiDC', topic='kafka_topic', bootstrap.servers='host1:9092,host1:9093', partition.no='1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; The following query listens to 'kafka_topic' topic, deployed in the broker host1:9092 and host1:9093, with partition 1. A thread is created for each broker. The receiving xml events are mapped to a siddhi event and sent to the FooStream. nats (Source) NATS Source allows users to subscribe to a NATS broker and receive messages. It has the ability to receive all the message types supported by NATS. Origin: siddhi-io-nats:2.0.8 Syntax @source(type=\"nats\", destination=\" STRING \", bootstrap.servers=\" STRING \", client.id=\" STRING \", cluster.id=\" STRING \", queue.group.name=\" STRING \", durable.name=\" STRING \", subscription.sequence=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic destination Subject name which NATS Source should subscribe to. STRING No No bootstrap.servers The NATS based url of the NATS server. nats://localhost:4222 STRING Yes No client.id The identifier of the client subscribing/connecting to the NATS broker. None STRING Yes No cluster.id The identifier of the NATS server/cluster. test-cluster STRING Yes No queue.group.name This can be used when there is a requirement to share the load of a NATS subject. Clients belongs to the same queue group share the subscription load. None STRING Yes No durable.name This can be used to subscribe to a subject from the last acknowledged message when a client or connection failure happens. The client can be uniquely identified using the tuple (client.id, durable.name). None STRING Yes No subscription.sequence This can be used to subscribe to a subject from a given number of message sequence. All the messages from the given point of sequence number will be passed to the client. If not provided then the either the persisted value or 0 will be used. None STRING Yes No Examples EXAMPLE 1 @source(type='nats', @map(type='text'), destination='SP_NATS_INPUT_TEST', bootstrap.servers='nats://localhost:4222',client.id='nats_client',server.id='test-cluster',queue.group.name = 'group_nats',durable.name = 'nats-durable',subscription.sequence = '100') define stream inputStream (name string, age int, country string); This example shows how to subscribe to a NATS subject with all supporting configurations.With the following configuration the source identified as 'nats-client' will subscribes to a subject named as 'SP_NATS_INPUT_TEST' which resides in a nats instance with a cluster id of 'test-cluster', running in localhost and listening to the port 4222 for client connection. This subscription will receive all the messages from 100 th in the subject. EXAMPLE 2 @source(type='nats', @map(type='text'), destination='SP_NATS_INPUT_TEST', ) define stream inputStream (name string, age int, country string); This example shows how to subscribe to a NATS subject with mandatory configurations.With the following configuration the source identified with an auto generated client id will subscribes to a subject named as 'SP_NATS_INTPUT_TEST' which resides in a nats instance with a cluster id of 'test-cluster', running in localhost and listening to the port 4222 for client connection. This will receive all available messages in the subject EXAMPLE 3 @source(type='nats', @map(type='json', @attributes(name='$.name', age='$.age', country='$.country', sequenceNum='trp:sequenceNumber')), destination='SIDDHI_NATS_SOURCE_TEST_DEST', client.id='nats_client', bootstrap.servers='nats://localhost:4222', cluster.id='test-cluster') define stream inputStream (name string, age int, country string, sequenceNum string); This example shows how to pass NATS Streaming sequence number to the event. prometheus (Source) This source consumes Prometheus metrics that are exported from a specified URL as Siddhi events by sending HTTP requests to the URL. Based on the source configuration, it analyzes metrics from the text response and sends them as Siddhi events through key-value mapping.The user can retrieve metrics of the 'including', 'counter', 'gauge', 'histogram', and 'summary' types. The source retrieves the metrics from a text response of the target. Therefore, it is you need to use 'string' as the attribute type for the attributes that correspond with the Prometheus metric labels. Further, the Prometheus metric value is passed through the event as 'value'. This requires you to include an attribute named 'value' in the stream definition. The supported types for the 'value' attribute are 'INT', 'LONG', 'FLOAT', and 'DOUBLE'. Origin: siddhi-io-prometheus:2.1.0 Syntax @source(type=\"prometheus\", target.url=\" STRING \", scrape.interval=\" INT \", scrape.timeout=\" INT \", scheme=\" STRING \", metric.name=\" STRING \", metric.type=\" STRING \", username=\" STRING \", password=\" STRING \", client.truststore.file=\" STRING \", client.truststore.password=\" STRING \", headers=\" STRING \", job=\" STRING \", instance=\" STRING \", grouping.key=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic target.url This property specifies the target URL to which the Prometheus metrics are exported in the 'TEXT' format. STRING No No scrape.interval This property specifies the time interval in seconds within which the source should send an HTTP request to the specified target URL. 60 INT Yes No scrape.timeout This property is the time duration in seconds for a scrape request to get timed-out if the server at the URL does not respond. 10 INT Yes No scheme This property specifies the scheme of the target URL. The supported schemes are 'HTTP' and 'HTTPS'. HTTP STRING Yes No metric.name This property specifies the name of the metrics that are to be fetched. The metric name must match the regex format, i.e., '[a-zA-Z_:][a-zA-Z0-9_:]* '. Stream name STRING Yes No metric.type This property specifies the type of the Prometheus metric that is required to be fetched. The supported metric types are 'counter', 'gauge',\" 'histogram', and 'summary'. STRING No No username This property specifies the username that needs to be added in the authorization header of the HTTP request if basic authentication is enabled at the target. It is required to specify both the username and the password to enable basic authentication. If you do not provide a value for one or both of these parameters, an error is logged in the console. STRING Yes No password This property specifies the password that needs to be added in the authorization header of the HTTP request if basic authentication is enabled at the target. It is required to specify both the username and the password to enable basic authentication. If you do not provide a value for one or both of these parameters, an error is logged in the console. STRING Yes No client.truststore.file The file path to the location of the truststore to which the client needs to send HTTPS requests via the 'HTTPS' protocol. STRING Yes No client.truststore.password The password for the client-truststore. This is required to send HTTPS requests. A custom password can be specified if required. STRING Yes No headers Headers that need to be included as HTTP request headers in the request. The format of the supported input is as follows, \"'header1:value1','header2:value2'\" STRING Yes No job This property defines the job name of the exported Prometheus metrics that needs to be fetched. STRING Yes No instance This property defines the instance of the exported Prometheus metrics that needs to be fetched. STRING Yes No grouping.key This parameter specifies the grouping key of the required metrics in key-value pairs. The grouping key is used if the metrics are exported by Prometheus 'pushGateway' in order to distinguish those metrics from already existing metrics. The expected format of the grouping key is as follows: \"'key1:value1','key2:value2'\" STRING Yes No System Parameters Name Description Default Value Possible Parameters scrapeInterval The default time interval in seconds for the Prometheus source to send HTTP requests to the target URL. 60 Any integer value scrapeTimeout The default time duration (in seconds) for an HTTP request to time-out if the server at the URL does not respond. 10 Any integer value scheme The scheme of the target for the Prometheus source to send HTTP requests. The supported schemes are 'HTTP' and 'HTTPS'. HTTP HTTP or HTTPS username The username that needs to be added in the authorization header of the HTTP request if basic authentication is enabled at the target. It is required to specify both the username and password to enable basic authentication. If you do not specify a value for one or both of these parameters, an error is logged in the console. Any string password The password that needs to be added in the authorization header of the HTTP request if basic authentication is enabled at the target. It is required to specify both the username and password to enable basic authentication. If you do not specify a value for one or both of these parameters, an error is logged in the console. Any string trustStoreFile The default file path to the location of truststore that the client needs to access in order to send HTTPS requests through 'HTTPS' protocol. ${carbon.home}/resources/security/client-truststore.jks Any valid path for the truststore file trustStorePassword The default password for the client-truststore that the client needs to access in order to send HTTPS requests through 'HTTPS' protocol. wso2carbon Any string headers The headers that need to be included as HTTP request headers in the scrape request. The format of the supported input is as follows, \"'header1:value1','header2:value2'\" Any valid http headers job The default job name of the exported Prometheus metrics that needs to be fetched. Any valid job name instance The default instance of the exported Prometheus metrics that needs to be fetched. Any valid instance name groupingKey The default grouping key of the required Prometheus metrics in key-value pairs. The grouping key is used if the metrics are exported by the Prometheus pushGateway in order to distinguish these metrics from already existing metrics. The expected format of the grouping key is as follows: \"'key1:value1','key2:value2'\" Any valid grouping key pairs Examples EXAMPLE 1 @source(type= 'prometheus', target.url= 'http://localhost:9080/metrics', metric.type= 'counter', metric.name= 'sweet_production_counter', @map(type= 'keyvalue')) define stream FooStream1(metric_name string, metric_type string, help string, subtype string, name string, quantity string, value double); In this example, the Prometheus source sends an HTTP request to the 'target.url' and analyzes the response. From the analyzed response, the source retrieves the Prometheus counter metrics with the 'sweet_production_counter' nameand converts the filtered metrics into Siddhi events using the key-value mapper. The generated maps have keys and values as follows: metric_name - sweet_production_counter metric_type - counter help - help_string_of_metric subtype - null name - value_of_label_name quantity - value_of_label_quantity value - value_of_metric EXAMPLE 2 @source(type= 'prometheus', target.url= 'http://localhost:9080/metrics', metric.type= 'summary', metric.name= 'sweet_production_summary', @map(type= 'keyvalue')) define stream FooStream2(metric_name string, metric_type string, help string, subtype string, name string, quantity string, quantile string, value double); In this example, the Prometheus source sends an HTTP request to the 'target.url' and analyzes the response. From the analysed response, the source retrieves the Prometheus summary metrics with the 'sweet_production_summary' nameand converts the filtered metrics into Siddhi events using the key-value mapper. The generated maps have keys and values as follows: metric_name - sweet_production_summary metric_type - summary help - help_string_of_metric subtype - 'sum'/'count'/'null' name - value_of_label_name quantity - value_of_label_quantity quantile - value of the quantile value - value_of_metric EXAMPLE 3 @source(type= 'prometheus', target.url= 'http://localhost:9080/metrics', metric.type= 'histogram', metric.name= 'sweet_production_histogram', @map(type= 'keyvalue')) define stream FooStream3(metric_name string, metric_type string, help string, subtype string, name string, quantity string, le string, value double); In this example, the prometheus source sends an HTTP request to the 'target.url' and analyzes the response. From the analyzed response, the source retrieves the Prometheus histogram metrics with the 'sweet_production_histogram' name and converts the filtered metrics into Siddhi events using the key-value mapper. The generated maps have keys and values as follows, metric_name - sweet_production_histogram metric_type - histogram help - help_string_of_metric subtype - 'sum'/'count'/'bucket' name - value_of_label_name quantity - value_of_label_quantity le - value of the bucket value - value_of_metric rabbitmq (Source) The rabbitmq source receives the events from the rabbitmq broker via the AMQP protocol. Origin: siddhi-io-rabbitmq:3.0.2 Syntax @source(type=\"rabbitmq\", uri=\" STRING \", heartbeat=\" INT \", exchange.name=\" STRING \", exchange.type=\" STRING \", exchange.durable.enabled=\" BOOL \", exchange.autodelete.enabled=\" BOOL \", routing.key=\" STRING \", headers=\" STRING \", queue.name=\" STRING \", queue.durable.enabled=\" BOOL \", queue.exclusive.enabled=\" BOOL \", queue.autodelete.enabled=\" BOOL \", tls.enabled=\" BOOL \", tls.truststore.path=\" STRING \", tls.truststore.password=\" STRING \", tls.truststore.type=\" STRING \", tls.version=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic uri The URI that is used to connect to an AMQP server. If no URI is specified,an error is logged in the CLI.e.g., amqp://guest:guest , amqp://guest:guest@localhost:5672 STRING No No heartbeat The period of time (in seconds) after which the peer TCP connection should be considered unreachable (down) by RabbitMQ and client libraries. 60 INT Yes No exchange.name The name of the exchange that decides what to do with a message it receives.If the exchange.name already exists in the RabbitMQ server, then the system uses that exchange.name instead of redeclaring. STRING No No exchange.type The type of the exchange name. The exchange types available are direct , fanout , topic and headers . For a detailed description of each type, see RabbitMQ - AMQP Concepts . direct STRING Yes No exchange.durable.enabled If this is set to true , the exchange remains declared even if the broker restarts. false BOOL Yes No exchange.autodelete.enabled If this is set to true , the exchange is automatically deleted when it is not used anymore. false BOOL Yes No routing.key The key based on which the exchange determines how to route the message to queues. The routing key is like an address for the message. The routing.key must be initialized when the value for the exchange.type parameter is direct or topic . empty STRING Yes No headers The headers of the message. The attributes used for routing are taken from the this paremeter. A message is considered matching if the value of the header equals the value specified upon binding. null STRING Yes No queue.name A queue is a buffer that stores messages. If the queue name already exists in the RabbitMQ server, then the system usees that queue name instead of redeclaring it. If no value is specified for this parameter, the system uses the unique queue name that is automatically generated by the RabbitMQ server. system generated queue name STRING Yes No queue.durable.enabled If this parameter is set to true , the queue remains declared even if the broker restarts false BOOL Yes No queue.exclusive.enabled If this parameter is set to true , the queue is exclusive for the current connection. If it is set to false , it is also consumable by other connections. false BOOL Yes No queue.autodelete.enabled If this parameter is set to true , the queue is automatically deleted when it is not used anymore. false BOOL Yes No tls.enabled This parameter specifies whether an encrypted communication channel should be established or not. When this parameter is set to true , the tls.truststore.path and tls.truststore.password parameters are initialized. false BOOL Yes No tls.truststore.path The file path to the location of the truststore of the client that receives the RabbitMQ events via the AMQP protocol. A custom client-truststore can be specified if required. If a custom truststore is not specified, then the system uses the default client-trustore in the {carbon.home}/resources/security /code directory. /p /td td style=\"vertical-align: top\" {carbon.home}/resources/security</code> directory.</p></td> <td style=\"vertical-align: top\"> /resources/security/client-truststore.jks STRING Yes No tls.truststore.password The password for the client-truststore. A custom password can be specified if required. If no custom password is specified, then the system uses wso2carbon as the default password. wso2carbon STRING Yes No tls.truststore.type The type of the truststore. JKS STRING Yes No tls.version The version of the tls/ssl. SSL STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @source(type ='rabbitmq', uri = 'amqp://guest:guest@localhost:5672', exchange.name = 'direct', routing.key= 'direct', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query receives events from the direct exchange with the direct exchange type, and the directTest routing key. tcp (Source) A Siddhi application can be configured to receive events via the TCP transport by adding the @Source(type = 'tcp') annotation at the top of an event stream definition. When this is defined the associated stream will receive events from the TCP transport on the host and port defined in the system. Origin: siddhi-io-tcp:3.0.4 Syntax @source(type=\"tcp\", context=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic context The URL 'context' that should be used to receive the events. / STRING Yes No System Parameters Name Description Default Value Possible Parameters host Tcp server host. 0.0.0.0 Any valid host or IP port Tcp server port. 9892 Any integer representing valid port receiver.threads Number of threads to receive connections. 10 Any positive integer worker.threads Number of threads to serve events. 10 Any positive integer tcp.no.delay This is to specify whether to disable Nagle algorithm during message passing. If tcp.no.delay = 'true', the execution of Nagle algorithm will be disabled in the underlying TCP logic. Hence there will be no delay between two successive writes to the TCP connection. Else there can be a constant ack delay. true true false keep.alive This property defines whether the server should be kept alive when there are no connections available. true true false Examples EXAMPLE 1 @Source(type = 'tcp', context='abc', @map(type='binary')) define stream Foo (attribute1 string, attribute2 int ); Under this configuration, events are received via the TCP transport on default host,port, abc context, and they are passed to Foo stream for processing. Sourcemapper avro (Source Mapper) This extension is an Avro to Event input mapper. Transports that accept Avro messages can utilize this extension to convert the incoming Avro messages to Siddhi events. The Avro schema to be used for creating Avro messages can be specified as a parameter in the stream definition. If no Avro schema is specified, a flat avro schema of the 'record' type is generated with the stream attributes as schema fields. The generated/specified Avro schema is used to convert Avro messages to Siddhi events. Origin: siddhi-map-avro:2.0.6 Syntax @source(..., @map(type=\"avro\", schema.def=\" STRING \", schema.registry=\" STRING \", schema.id=\" STRING \", fail.on.missing.attribute=\" BOOL \") QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic schema.def This specifies the schema of the Avro message. The full schema used to create the Avro message needs to be specified as a quoted JSON string. STRING No No schema.registry This specifies the URL of the schema registry. STRING No No schema.id This specifies the ID of the Avro schema. This ID is the global ID that is returned from the schema registry when posting the schema to the registry. The schema is retrieved from the schema registry via the specified ID. STRING No No fail.on.missing.attribute If this parameter is set to 'true', a JSON execution failing or returning a null value results in that message being dropped by the system. If this parameter is set to 'false', a JSON execution failing or returning a null value results in the system being prompted to send the event with a null value to Siddhi so that the user can handle it as required (i.e., by assigning a default value. true BOOL Yes No Examples EXAMPLE 1 @source(type='inMemory', topic='user', @map(type='avro', schema .def = \"\"\"{\"type\":\"record\",\"name\":\"userInfo\",\"namespace\":\"user.example\",\"fields\":[{\"name\":\"name\",\"type\":\"string\"}, {\"name\":\"age\",\"type\":\"int\"}]}\"\"\")) define stream UserStream (name string, age int ); The above Siddhi query performs a default Avro input mapping. The input Avro message that contains user information is converted to a Siddhi event. The expected input is a byte array or ByteBuffer. EXAMPLE 2 @source(type='inMemory', topic='user', @map(type='avro', schema .def = \"\"\"{\"type\":\"record\",\"name\":\"userInfo\",\"namespace\":\"avro.userInfo\",\"fields\":[{\"name\":\"username\",\"type\":\"string\"}, {\"name\":\"age\",\"type\":\"int\"}]}\"\"\",@attributes(name=\"username\",age=\"age\"))) define stream userStream (name string, age int ); The above Siddhi query performs a custom Avro input mapping. The input Avro message that contains user information is converted to a Siddhi event. The expected input is a byte array or ByteBuffer. EXAMPLE 3 @source(type='inMemory', topic='user', @map(type='avro',schema.registry='http://192.168.2.5:9090', schema.id='1',@attributes(name=\"username\",age=\"age\"))) define stream UserStream (name string, age int ); The above Siddhi query performs a custom Avro input mapping. The input Avro message that contains user information is converted to a Siddhi event via the schema retrieved from the given schema registry(localhost:8081). The expected input is a byte array or ByteBuffer. binary (Source Mapper) This extension is a binary input mapper that converts events received in binary format to Siddhi events before they are processed. Origin: siddhi-map-binary:2.0.4 Syntax @source(..., @map(type=\"binary\") Examples EXAMPLE 1 @source(type='inMemory', topic='WSO2', @map(type='binary'))define stream FooStream (symbol string, price float, volume long); This query performs a mapping to convert an event of the binary format to a Siddhi event. csv (Source Mapper) This extension is used to convert CSV message to Siddhi event input mapper. You can either receive pre-defined CSV message where event conversion takes place without extra configurations,or receive custom CSV message where a custom place order to map from custom CSV message. Origin: siddhi-map-csv:2.0.3 Syntax @source(..., @map(type=\"csv\", delimiter=\" STRING \", header.present=\" BOOL \", fail.on.unknown.attribute=\" BOOL \", event.grouping.enabled=\" BOOL \") QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic delimiter When converting a CSV format message to Siddhi event, this parameter indicatesinput CSV message's data should be split by this parameter , STRING Yes No header.present When converting a CSV format message to Siddhi event, this parameter indicates whether CSV message has header or not. This can either have value true or false.If it's set to false then it indicates that CSV message has't header. false BOOL Yes No fail.on.unknown.attribute This parameter specifies how unknown attributes should be handled. If it's set to true and one or more attributes don't havevalues, then SP will drop that message. If this parameter is set to false , the Stream Processor adds the required attribute's values to such events with a null value and the event is converted to a Siddhi event. true BOOL Yes No event.grouping.enabled This parameter specifies whether event grouping is enabled or not. To receive a group of events together and generate multiple events, this parameter must be set to true . false BOOL Yes No Examples EXAMPLE 1 @source(type='inMemory', topic='stock', @map(type='csv')) define stream FooStream (symbol string, price float, volume int); Above configuration will do a default CSV input mapping. Expected input will look like below: WSO2 ,55.6 , 100OR \"WSO2,No10,Palam Groove Rd,Col-03\" ,55.6 , 100If header.present is true and delimiter is \"-\", then the input is as follows: symbol-price-volumeWSO2-55.6-100 EXAMPLE 2 @source(type='inMemory', topic='stock', @map(type='csv',header='true', @attributes(symbol = \"2\", price = \"0\", volume = \"1\"))) define stream FooStream (symbol string, price float, volume long); Above configuration will perform a custom CSV mapping. Here, user can add place order of each attribute in the @attribute. The place order indicates where the attribute name's value has appeared in the input.Expected input will look like below: 55.6,100,WSO2 OR55.6,100,\"WSO2,No10,Palm Groove Rd,Col-03\" If header is true and delimiter is \"-\", then the output is as follows: price-volume-symbol 55.6-100-WSO2 If group events is enabled then input should be as follows: price-volume-symbol 55.6-100-WSO2System.lineSeparator() 55.6-100-IBMSystem.lineSeparator() 55.6-100-IFSSystem.lineSeparator() json (Source Mapper) This extension is a JSON-to-Event input mapper. Transports that accept JSON messages can utilize this extension to convert an incoming JSON message into a Siddhi event. Users can either send a pre-defined JSON format, where event conversion happens without any configurations, or use the JSON path to map from a custom JSON message. In default mapping, the JSON string of the event can be enclosed by the element \"event\", though optional. Origin: siddhi-map-json:5.0.5 Syntax @source(..., @map(type=\"json\", enclosing.element=\" STRING \", fail.on.missing.attribute=\" BOOL \") QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic enclosing.element This is used to specify the enclosing element when sending multiple events in the same JSON message. Mapper treats the child elements of a given enclosing element as events and executes the JSON path expressions on these child elements. If the enclosing.element is not provided then the multiple-event scenario is disregarded and the JSON path is evaluated based on the root element. $ STRING Yes No fail.on.missing.attribute This parameter allows users to handle unknown attributes.The value of this can either be true or false. By default it is true. If a JSON execution fails or returns null, mapper drops that message. However, setting this property to false prompts mapper to send an event with a null value to Siddhi, where users can handle it as required, ie., assign a default value.) true BOOL Yes No Examples EXAMPLE 1 @source(type='inMemory', topic='stock', @map(type='json')) define stream FooStream (symbol string, price float, volume long); This configuration performs a default JSON input mapping. For a single event, the input is required to be in one of the following formats: { \"event\":{ \"symbol\":\"WSO2\", \"price\":55.6, \"volume\":100 } } or { \"symbol\":\"WSO2\", \"price\":55.6, \"volume\":100 } EXAMPLE 2 @source(type='inMemory', topic='stock', @map(type='json')) define stream FooStream (symbol string, price float, volume long); This configuration performs a default JSON input mapping. For multiple events, the input is required to be in one of the following formats: [ {\"event\":{\"symbol\":\"WSO2\",\"price\":55.6,\"volume\":100}}, {\"event\":{\"symbol\":\"WSO2\",\"price\":56.6,\"volume\":99}}, {\"event\":{\"symbol\":\"WSO2\",\"price\":57.6,\"volume\":80}} ] or [ {\"symbol\":\"WSO2\",\"price\":55.6,\"volume\":100}, {\"symbol\":\"WSO2\",\"price\":56.6,\"volume\":99}, {\"symbol\":\"WSO2\",\"price\":57.6,\"volume\":80} ] EXAMPLE 3 @source(type='inMemory', topic='stock', @map(type='json', enclosing.element=\"$.portfolio\", @attributes(symbol = \"company.symbol\", price = \"price\", volume = \"volume\"))) This configuration performs a custom JSON mapping. For a single event, the expected input is similar to the one shown below: { \"portfolio\":{ \"stock\":{ \"volume\":100, \"company\":{ \"symbol\":\"WSO2\" }, \"price\":55.6 } } } EXAMPLE 4 @source(type='inMemory', topic='stock', @map(type='json', enclosing.element=\"$.portfolio\", @attributes(symbol = \"stock.company.symbol\", price = \"stock.price\", volume = \"stock.volume\"))) define stream FooStream (symbol string, price float, volume long); The configuration performs a custom JSON mapping. For multiple events, expected input looks as follows. .{\"portfolio\": [ {\"stock\":{\"volume\":100,\"company\":{\"symbol\":\"wso2\"},\"price\":56.6}}, {\"stock\":{\"volume\":200,\"company\":{\"symbol\":\"wso2\"},\"price\":57.6}} ] } keyvalue (Source Mapper) Key-Value Map to Event input mapper extension allows transports that accept events as key value maps to convert those events to Siddhi events. You can either receive pre-defined keys where conversion takes place without extra configurations, or use custom keys to map from the message. Origin: siddhi-map-keyvalue:2.0.5 Syntax @source(..., @map(type=\"keyvalue\", fail.on.missing.attribute=\" BOOL \") QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic fail.on.missing.attribute If this parameter is set to true , if an event arrives without a matching key for a specific attribute in the connected stream, it is dropped and not processed by the Stream Processor. If this parameter is set to false the Stream Processor adds the required key to such events with a null value, and the event is converted to a Siddhi event so that you could handle them as required before they are further processed. true BOOL Yes No Examples EXAMPLE 1 @source(type='inMemory', topic='stock', @map(type='keyvalue')) define stream FooStream (symbol string, price float, volume long); This query performs a default key value input mapping. The expected input is a map similar to the following: symbol: 'WSO2' price: 55.6f volume: 100 EXAMPLE 2 @source(type='inMemory', topic='stock', @map(type='keyvalue', fail.on.missing.attribute='true', @attributes(symbol = 's', price = 'p', volume = 'v')))define stream FooStream (symbol string, price float, volume long); This query performs a custom key value input mapping. The matching keys for the symbol , price and volume attributes are be s , p , and v respectively. The expected input is a map similar to the following: s: 'WSO2' p: 55.6 v: 100 passThrough (Source Mapper) Pass-through mapper passed events (Event[]) through without any mapping or modifications. Origin: siddhi-core:5.1.8 Syntax @source(..., @map(type=\"passThrough\") Examples EXAMPLE 1 @source(type='tcp', @map(type='passThrough')) define stream BarStream (symbol string, price float, volume long); In this example BarStream uses passThrough inputmapper which passes the received Siddhi event directly without any transformation into source. protobuf (Source Mapper) This input mapper allows you to convert protobuf messages into Events. To work with this input mapper you have to add auto-generated protobuf classes to the project classpath. When you use this input mapper, you can either define stream attributes as the same names as the protobuf message attributes or you can use custom mapping to map stream definition attributes with the protobuf attributes..Please find the sample proto definition here Origin: siddhi-map-protobuf:1.0.2 Syntax @source(..., @map(type=\"protobuf\", class=\" STRING \") QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic class This specifies the class name of the protobuf message class, If sink type is grpc then it's not necessary to provide this field. - STRING Yes No Examples EXAMPLE 1 source(type='grpc', receiver.url = 'grpc://localhost: 2000/org.wso2.grpc.test.MyService/process', @map(type='protobuf')) define stream FooStream (stringValue string, intValue int,longValue long,booleanValue bool,floatValue float,doubleValue double); Above definition will convert the protobuf messages that are received to this source into siddhi events. EXAMPLE 2 source(type='grpc', receiver.url = 'grpc://localhost: 2000/org.wso2.grpc.test.MyService/process', @map(type='protobuf', @attributes(a = 'stringValue', b = 'intValue', c = 'longValue',d = 'booleanValue',' e = floatValue', f ='doubleValue'))) define stream FooStream (a string ,c long,b int, d bool,e float,f double); Above definition will convert the protobuf messages that are received to this source into siddhi events. since there's a mapping available for the stream, protobuf message object will be map like this, -'stringValue' of the protobuf message will be assign to the 'a' attribute of the stream - 'intValue' of the protobuf message will be assign to the 'b' attribute of the stream - 'longValue' of the protobuf message will be assign to the 'c' attribute of the stream - 'booleanValue' of the protobuf message will be assign to the 'd' attribute of the stream - 'floatValue' of the protobuf message will be assign to the 'e' attribute of the stream - 'doubleValue' of the protobuf message will be assign to the 'f' attribute of the stream EXAMPLE 3 source(type='grpc', receiver.url = 'grpc://localhost: 2000/org.wso2.grpc.test.MyService/testMap', @map(type='protobuf')) define stream FooStream (stringValue string ,intValue int,map object); Above definition will convert the protobuf messages that are received to this source into siddhi events. since there's an object type attribute available in the stream (map object), mapper will assume that object is an instance of 'java.util.Map' class. otherwise mapper will throws an exception EXAMPLE 4 @source(type='inMemory', topic='test01', @map(type='protobuf', class='org.wso2.grpc.test.Request')) define stream FooStream (stringValue string, intValue int,longValue long,booleanValue bool,floatValue float,doubleValue double); The above definition will convert the 'org.wso2.grpc.test.Request' type protobuf messages into siddhi events. If we did not provide the 'receiver.url' in the stream definition we have to provide the protobuf class name in the 'class' parameter inside @map. text (Source Mapper) This extension is a text to Siddhi event input mapper. Transports that accept text messages can utilize this extension to convert the incoming text message to Siddhi event. Users can either use a pre-defined text format where event conversion happens without any additional configurations, or specify a regex to map a text message using custom configurations. Origin: siddhi-map-text:2.0.4 Syntax @source(..., @map(type=\"text\", regex.groupid=\" STRING \", fail.on.missing.attribute=\" BOOL \", event.grouping.enabled=\" BOOL \", delimiter=\" STRING \", new.line.character=\" STRING \") QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic regex.groupid This parameter specifies a regular expression group. The groupid can be any capital letter (e.g., regex.A,regex.B .. etc). You can specify any number of regular expression groups. In the attribute annotation, you need to map all attributes to the regular expression group with the matching group index. If you need to to enable custom mapping, it is required to specifythe matching group for each and every attribute. STRING No No fail.on.missing.attribute This parameter specifies how unknown attributes should be handled. If it is set to true a message is dropped if its execution fails, or if one or more attributes do not have values. If this parameter is set to false , null values are assigned to attributes with missing values, and messages with such attributes are not dropped. true BOOL Yes No event.grouping.enabled This parameter specifies whether event grouping is enabled or not. To receive a group of events together and generate multiple events, this parameter must be set to true . false BOOL Yes No delimiter This parameter specifies how events must be separated when multiple events are received. This must be whole line and not a single character. ~ ~ ~ ~ STRING Yes No new.line.character This attribute indicates the new line character of the event that is expected to be received. This is used mostly when communication between 2 types of operating systems is expected. For example, Linux uses \\n as the end of line character whereas windows uses \\r\\n . \\n STRING Yes No Examples EXAMPLE 1 @source(type='inMemory', topic='stock', @map(type='text')) define stream FooStream (symbol string, price float, volume long); This query performs a default text input mapping. The expected input is as follows: symbol:\"WSO2\", price:55.6, volume:100 OR symbol:'WSO2', price:55.6, volume:100 If group events is enabled then input should be as follows: symbol:\"WSO2\", price:55.6, volume:100 ~ ~ ~ ~ symbol:\"WSO2\", price:55.6, volume:100 EXAMPLE 2 @source(type='inMemory', topic='stock', @map(type='text', fail.on.missing.attribute = 'true', regex.A='(\\w+)\\s([-0-9]+)',regex.B='volume\\s([-0-9]+)', @attributes(symbol = 'A[1]',price = 'A[2]',volume = 'B'))) define stream FooStream (symbol string, price float, volume long); This query performs a custom text mapping. The expected input is as follows: wos2 550 volume 100 If group events is enabled then input should be as follows: wos2 550 volume 100 ~ ~ ~ ~ wos2 550 volume 100 ~ ~ ~ ~ wos2 550 volume 100 xml (Source Mapper) This mapper converts XML input to Siddhi event. Transports which accepts XML messages can utilize this extension to convert the incoming XML message to Siddhi event. Users can either send a pre-defined XML format where event conversion will happen without any configs or can use xpath to map from a custom XML message. Origin: siddhi-map-xml:5.0.3 Syntax @source(..., @map(type=\"xml\", namespaces=\" STRING \", enclosing.element=\" STRING \", fail.on.missing.attribute=\" BOOL \") QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic namespaces Used to provide namespaces used in the incoming XML message beforehand to configure xpath expressions. User can provide a comma separated list. If these are not provided xpath evaluations will fail None STRING Yes No enclosing.element Used to specify the enclosing element in case of sending multiple events in same XML message. WSO2 DAS will treat the child element of given enclosing element as events and execute xpath expressions on child elements. If enclosing.element is not provided multiple event scenario is disregarded and xpaths will be evaluated with respect to root element. Root element STRING Yes No fail.on.missing.attribute This can either have value true or false. By default it will be true. This attribute allows user to handle unknown attributes. By default if an xpath execution fails or returns null DAS will drop that message. However setting this property to false will prompt DAS to send and event with null value to Siddhi where user can handle it accordingly(ie. Assign a default value) True BOOL Yes No Examples EXAMPLE 1 @source(type='inMemory', topic='stock', @map(type='xml')) define stream FooStream (symbol string, price float, volume long); Above configuration will do a default XML input mapping. Expected input will look like below. events event symbol WSO2 /symbol price 55.6 /price volume 100 /volume /event /events EXAMPLE 2 @source(type='inMemory', topic='stock', @map(type='xml', namespaces = \"dt=urn:schemas-microsoft-com:datatypes\", enclosing.element=\"//portfolio\", @attributes(symbol = \"company/symbol\", price = \"price\", volume = \"volume\"))) define stream FooStream (symbol string, price float, volume long); Above configuration will perform a custom XML mapping. In the custom mapping user can add xpath expressions representing each event attribute using @attribute annotation. Expected input will look like below. portfolio xmlns:dt=\"urn:schemas-microsoft-com:datatypes\" stock exchange=\"nasdaq\" volume 100 /volume company symbol WSO2 /symbol /company price dt:type=\"number\" 55.6 /price /stock /portfolio Store mongodb (Store) Using this extension a MongoDB Event Table can be configured to persist events in a MongoDB of user's choice. Origin: siddhi-store-mongodb:2.0.3 Syntax @Store(type=\"mongodb\", mongodb.uri=\" STRING \", collection.name=\" STRING \", secure.connection=\" STRING \", trust.store=\" STRING \", trust.store.password=\" STRING \", key.store=\" STRING \", key.store.password=\" STRING \") @PrimaryKey(\"PRIMARY_KEY\") @Index(\"INDEX\") QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic mongodb.uri The MongoDB URI for the MongoDB data store. The uri must be of the format mongodb://[username:password@]host1[:port1][,hostN[:portN]][/[database][?options]] The options specified in the uri will override any connection options specified in the deployment yaml file. Note: The user should have read permissions to the admindb as well as read/write permissions to the database accessed. STRING No No collection.name The name of the collection in the store this Event Table should be persisted as. Name of the siddhi event table. STRING Yes No secure.connection Describes enabling the SSL for the mongodb connection false STRING Yes No trust.store File path to the trust store. {carbon.home}/resources/security/client-truststore.jks /td td style=\"vertical-align: top\" STRING /td td style=\"vertical-align: top\" Yes /td td style=\"vertical-align: top\" No /td /tr tr td style=\"vertical-align: top\" trust.store.password /td td style=\"vertical-align: top; word-wrap: break-word\" p style=\"word-wrap: break-word;margin: 0;\" Password to access the trust store /p /td td style=\"vertical-align: top\" wso2carbon /td td style=\"vertical-align: top\" STRING /td td style=\"vertical-align: top\" Yes /td td style=\"vertical-align: top\" No /td /tr tr td style=\"vertical-align: top\" key.store /td td style=\"vertical-align: top; word-wrap: break-word\" p style=\"word-wrap: break-word;margin: 0;\" File path to the keystore. /p /td td style=\"vertical-align: top\" {carbon.home}/resources/security/client-truststore.jks</td> <td style=\"vertical-align: top\">STRING</td> <td style=\"vertical-align: top\">Yes</td> <td style=\"vertical-align: top\">No</td> </tr> <tr> <td style=\"vertical-align: top\">trust.store.password</td> <td style=\"vertical-align: top; word-wrap: break-word\"><p style=\"word-wrap: break-word;margin: 0;\">Password to access the trust store</p></td> <td style=\"vertical-align: top\">wso2carbon</td> <td style=\"vertical-align: top\">STRING</td> <td style=\"vertical-align: top\">Yes</td> <td style=\"vertical-align: top\">No</td> </tr> <tr> <td style=\"vertical-align: top\">key.store</td> <td style=\"vertical-align: top; word-wrap: break-word\"><p style=\"word-wrap: break-word;margin: 0;\">File path to the keystore.</p></td> <td style=\"vertical-align: top\"> /resources/security/client-truststore.jks STRING Yes No key.store.password Password to access the keystore wso2carbon STRING Yes No System Parameters Name Description Default Value Possible Parameters applicationName Sets the logical name of the application using this MongoClient. The application name may be used by the client to identify the application to the server, for use in server logs, slow query logs, and profile collection. null the logical name of the application using this MongoClient. The UTF-8 encoding may not exceed 128 bytes. cursorFinalizerEnabled Sets whether cursor finalizers are enabled. true true false requiredReplicaSetName The name of the replica set null the logical name of the replica set sslEnabled Sets whether to initiate connection with TSL/SSL enabled. true: Initiate the connection with TLS/SSL. false: Initiate the connection without TLS/SSL. false true false trustStore File path to the trust store. {carbon.home}/resources/security/client-truststore.jks /td td style=\"vertical-align: top\" Any valid file path. /td /tr tr td style=\"vertical-align: top\" trustStorePassword /td td style=\"vertical-align: top;\" p style=\"word-wrap: break-word;margin: 0;\" Password to access the trust store /p /td td style=\"vertical-align: top\" wso2carbon /td td style=\"vertical-align: top\" Any valid password. /td /tr tr td style=\"vertical-align: top\" keyStore /td td style=\"vertical-align: top;\" p style=\"word-wrap: break-word;margin: 0;\" File path to the keystore. /p /td td style=\"vertical-align: top\" {carbon.home}/resources/security/client-truststore.jks</td> <td style=\"vertical-align: top\">Any valid file path.</td> </tr> <tr> <td style=\"vertical-align: top\">trustStorePassword</td> <td style=\"vertical-align: top;\"><p style=\"word-wrap: break-word;margin: 0;\">Password to access the trust store</p></td> <td style=\"vertical-align: top\">wso2carbon</td> <td style=\"vertical-align: top\">Any valid password.</td> </tr> <tr> <td style=\"vertical-align: top\">keyStore</td> <td style=\"vertical-align: top;\"><p style=\"word-wrap: break-word;margin: 0;\">File path to the keystore.</p></td> <td style=\"vertical-align: top\"> /resources/security/client-truststore.jks Any valid file path. keyStorePassword Password to access the keystore wso2carbon Any valid password. connectTimeout The time in milliseconds to attempt a connection before timing out. 10000 Any positive integer connectionsPerHost The maximum number of connections in the connection pool. 100 Any positive integer minConnectionsPerHost The minimum number of connections in the connection pool. 0 Any natural number maxConnectionIdleTime The maximum number of milliseconds that a connection can remain idle in the pool before being removed and closed. A zero value indicates no limit to the idle time. A pooled connection that has exceeded its idle time will be closed and replaced when necessary by a new connection. 0 Any positive integer maxWaitTime The maximum wait time in milliseconds that a thread may wait for a connection to become available. A value of 0 means that it will not wait. A negative value means to wait indefinitely 120000 Any integer threadsAllowedToBlockForConnectionMultiplier The maximum number of connections allowed per host for this MongoClient instance. Those connections will be kept in a pool when idle. Once the pool is exhausted, any operation requiring a connection will block waiting for an available connection. 100 Any natural number maxConnectionLifeTime The maximum life time of a pooled connection. A zero value indicates no limit to the life time. A pooled connection that has exceeded its life time will be closed and replaced when necessary by a new connection. 0 Any positive integer socketKeepAlive Sets whether to keep a connection alive through firewalls false true false socketTimeout The time in milliseconds to attempt a send or receive on a socket before the attempt times out. Default 0 means never to timeout. 0 Any natural integer writeConcern The write concern to use. acknowledged acknowledged w1 w2 w3 unacknowledged fsynced journaled replica_acknowledged normal safe majority fsync_safe journal_safe replicas_safe readConcern The level of isolation for the reads from replica sets. default local majority linearizable readPreference Specifies the replica set read preference for the connection. primary primary secondary secondarypreferred primarypreferred nearest localThreshold The size (in milliseconds) of the latency window for selecting among multiple suitable MongoDB instances. 15 Any natural number serverSelectionTimeout Specifies how long (in milliseconds) to block for server selection before throwing an exception. A value of 0 means that it will timeout immediately if no server is available. A negative value means to wait indefinitely. 30000 Any integer heartbeatSocketTimeout The socket timeout for connections used for the cluster heartbeat. A value of 0 means that it will timeout immediately if no cluster member is available. A negative value means to wait indefinitely. 20000 Any integer heartbeatConnectTimeout The connect timeout for connections used for the cluster heartbeat. A value of 0 means that it will timeout immediately if no cluster member is available. A negative value means to wait indefinitely. 20000 Any integer heartbeatFrequency Specify the interval (in milliseconds) between checks, counted from the end of the previous check until the beginning of the next one. 10000 Any positive integer minHeartbeatFrequency Sets the minimum heartbeat frequency. In the event that the driver has to frequently re-check a server's availability, it will wait at least this long since the previous check to avoid wasted effort. 500 Any positive integer Examples EXAMPLE 1 @Store(type=\"mongodb\",mongodb.uri=\"mongodb://admin:admin@localhost/Foo\") @PrimaryKey(\"symbol\") @Index(\"volume:1\", {background:true,unique:true}\") define table FooTable (symbol string, price float, volume long); This will create a collection called FooTable for the events to be saved with symbol as Primary Key(unique index at mongoDB level) and index for the field volume will be created in ascending order with the index option to create the index in the background. Note: @PrimaryKey: This specifies a list of comma-separated values to be treated as unique fields in the table. Each record in the table must have a unique combination of values for the fields specified here. @Index: This specifies the fields that must be indexed at the database level. You can specify multiple values as a come-separated list. A single value to be in the format, FieldName : SortOrder . The last element is optional through which a valid index options can be passed. SortOrder : 1 for Ascending -1 for Descending. Optional, with default value as 1. IndexOptions : Index Options must be defined inside curly brackets. Options must follow the standard mongodb index options format. https://docs.mongodb.com/manual/reference/method/db.collection.createIndex/ Example 1: @Index( 'symbol:1' , '{\"unique\":true}' ) Example 2: @Index( 'symbol' , '{\"unique\":true}' ) Example 3: @Index( 'symbol:1' , 'volume:-1' , '{\"unique\":true}' ) rdbms (Store) This extension assigns data sources and connection instructions to event tables. It also implements read-write operations on connected data sources. Origin: siddhi-store-rdbms:7.0.2 Syntax @Store(type=\"rdbms\", jdbc.url=\" STRING \", username=\" STRING \", password=\" STRING \", jdbc.driver.name=\" STRING \", pool.properties=\" STRING \", jndi.resource=\" STRING \", datasource=\" STRING \", table.name=\" STRING \", field.length=\" STRING \", table.check.query=\" STRING \", use.collation=\" BOOL \") @PrimaryKey(\"PRIMARY_KEY\") @Index(\"INDEX\") QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic jdbc.url The JDBC URL via which the RDBMS data store is accessed. STRING No No username The username to be used to access the RDBMS data store. STRING No No password The password to be used to access the RDBMS data store. STRING No No jdbc.driver.name The driver class name for connecting the RDBMS data store. STRING No No pool.properties Any pool parameters for the database connection must be specified as key-value pairs. null STRING Yes No jndi.resource The name of the JNDI resource through which the connection is attempted. If this is found, the pool properties described above are not taken into account and the connection is attempted via JNDI lookup instead. null STRING Yes No datasource The name of the Carbon datasource that should be used for creating the connection with the database. If this is found, neither the pool properties nor the JNDI resource name described above are taken into account and the connection is attempted via Carbon datasources instead. Only works in Siddhi Distribution null STRING Yes No table.name The name with which the event table should be persisted in the store. If no name is specified via this parameter, the event table is persisted with the same name as the Siddhi table. The table name defined in the Siddhi App query. STRING Yes No field.length The number of characters that the values for fields of the 'STRING' type in the table definition must contain. Each required field must be provided as a comma-separated list of key-value pairs in the ' field.name : length ' format. If this is not specified, the default number of characters specific to the database type is considered. null STRING Yes No table.check.query This query will be used to check whether the table is exist in the given database. But the provided query should return an SQLException if the table does not exist in the database. Furthermore if the provided table is a database view, and it is not exists in the database a table from given name will be created in the database The tableCheckQuery which define in store rdbms configs STRING Yes No use.collation This property allows users to use collation for string attirbutes. By default it's false and binary collation is not used. Currently 'latin1_bin' and 'SQL_Latin1_General_CP1_CS_AS' are used as collations for MySQL and Microsoft SQL database types respectively. false BOOL Yes No System Parameters Name Description Default Value Possible Parameters {{RDBMS-Name}}.maxVersion The latest version supported for {{RDBMS-Name}}. 0 N/A {{RDBMS-Name}}.minVersion The earliest version supported for {{RDBMS-Name}}. 0 N/A {{RDBMS-Name}}.tableCheckQuery The template query for the 'check table' operation in {{RDBMS-Name}}. H2 : CREATE TABLE {{TABLE_NAME}} ({{COLUMNS, PRIMARY_KEYS}}) MySQL : CREATE TABLE {{TABLE_NAME}} ({{COLUMNS, PRIMARY_KEYS}}) Oracle : CREATE TABLE {{TABLE_NAME}} ({{COLUMNS, PRIMARY_KEYS}}) Microsoft SQL Server : CREATE TABLE {{TABLE_NAME}} ({{COLUMNS, PRIMARY_KEYS}}) PostgreSQL : CREATE TABLE {{TABLE_NAME}} ({{COLUMNS, PRIMARY_KEYS}}) DB2. : CREATE TABLE {{TABLE_NAME}} ({{COLUMNS, PRIMARY_KEYS}}) N/A {{RDBMS-Name}}.tableCreateQuery The template query for the 'create table' operation in {{RDBMS-Name}}. H2 : SELECT 1 FROM {{TABLE_NAME}} LIMIT 1 MySQL : SELECT 1 FROM {{TABLE_NAME}} LIMIT 1 Oracle : SELECT 1 FROM {{TABLE_NAME}} WHERE rownum=1 Microsoft SQL Server : SELECT TOP 1 1 from {{TABLE_NAME}} PostgreSQL : SELECT 1 FROM {{TABLE_NAME}} LIMIT 1 DB2. : SELECT 1 FROM {{TABLE_NAME}} FETCH FIRST 1 ROWS ONLY N/A {{RDBMS-Name}}.indexCreateQuery The template query for the 'create index' operation in {{RDBMS-Name}}. H2 : CREATE INDEX {{TABLE_NAME}}_INDEX ON {{TABLE_NAME}} ({{INDEX_COLUMNS}}) MySQL : CREATE INDEX {{TABLE_NAME}}_INDEX ON {{TABLE_NAME}} ({{INDEX_COLUMNS}}) Oracle : CREATE INDEX {{TABLE_NAME}}_INDEX ON {{TABLE_NAME}} ({{INDEX_COLUMNS}}) Microsoft SQL Server : CREATE INDEX {{TABLE_NAME}}_INDEX ON {{TABLE_NAME}} ({{INDEX_COLUMNS}}) {{TABLE_NAME}} ({{INDEX_COLUMNS}}) PostgreSQL : CREATE INDEX {{TABLE_NAME}}_INDEX ON {{TABLE_NAME}} ({{INDEX_COLUMNS}}) DB2. : CREATE INDEX {{TABLE_NAME}}_INDEX ON {{TABLE_NAME}} ({{INDEX_COLUMNS}}) N/A {{RDBMS-Name}}.recordInsertQuery The template query for the 'insert record' operation in {{RDBMS-Name}}. H2 : INSERT INTO {{TABLE_NAME}} ({{COLUMNS}}) VALUES ({{Q}}) MySQL : INSERT INTO {{TABLE_NAME}} ({{COLUMNS}}) VALUES ({{Q}}) Oracle : INSERT INTO {{TABLE_NAME}} ({{COLUMNS}}) VALUES ({{Q}}) Microsoft SQL Server : INSERT INTO {{TABLE_NAME}} ({{COLUMNS}}) VALUES ({{Q}}) PostgreSQL : INSERT INTO {{TABLE_NAME}} ({{COLUMNS}}) VALUES ({{Q}}) DB2. : INSERT INTO {{TABLE_NAME}} ({{COLUMNS}}) VALUES ({{Q}}) N/A {{RDBMS-Name}}.recordUpdateQuery The template query for the 'update record' operation in {{RDBMS-Name}}. H2 : UPDATE {{TABLE_NAME}} SET {{COLUMNS_AND_VALUES}} {{CONDITION}} MySQL : UPDATE {{TABLE_NAME}} SET {{COLUMNS_AND_VALUES}} {{CONDITION}} Oracle : UPDATE {{TABLE_NAME}} SET {{COLUMNS_AND_VALUES}} {{CONDITION}} Microsoft SQL Server : UPDATE {{TABLE_NAME}} SET {{COLUMNS_AND_VALUES}} {{CONDITION}} PostgreSQL : UPDATE {{TABLE_NAME}} SET {{COLUMNS_AND_VALUES}} {{CONDITION}} DB2. : UPDATE {{TABLE_NAME}} SET {{COLUMNS_AND_VALUES}} {{CONDITION}} N/A {{RDBMS-Name}}.recordSelectQuery The template query for the 'select record' operation in {{RDBMS-Name}}. H2 : SELECT * FROM {{TABLE_NAME}} {{CONDITION}} MySQL : SELECT * FROM {{TABLE_NAME}} {{CONDITION}} Oracle : SELECT * FROM {{TABLE_NAME}} {{CONDITION}} Microsoft SQL Server : SELECT * FROM {{TABLE_NAME}} {{CONDITION}} PostgreSQL : SELECT * FROM {{TABLE_NAME}} {{CONDITION}} DB2. : SELECT * FROM {{TABLE_NAME}} {{CONDITION}} N/A {{RDBMS-Name}}.recordExistsQuery The template query for the 'check record existence' operation in {{RDBMS-Name}}. H2 : SELECT TOP 1 1 FROM {{TABLE_NAME}} {{CONDITION}} MySQL : SELECT 1 FROM {{TABLE_NAME}} {{CONDITION}} Oracle : SELECT COUNT(1) INTO existence FROM {{TABLE_NAME}} {{CONDITION}} Microsoft SQL Server : SELECT TOP 1 FROM {{TABLE_NAME}} {{CONDITION}} PostgreSQL : SELECT 1 FROM {{TABLE_NAME}} {{CONDITION}} LIMIT 1 DB2. : SELECT 1 FROM {{TABLE_NAME}} {{CONDITION}} FETCH FIRST 1 ROWS ONLY N/A {{RDBMS-Name}}.recordDeleteQuery The query for the 'delete record' operation in {{RDBMS-Name}}. H2 : DELETE FROM {{TABLE_NAME}} {{CONDITION}} MySQL : DELETE FROM {{TABLE_NAME}} {{CONDITION}} Oracle : DELETE FROM {{TABLE_NAME}} {{CONDITION}} Microsoft SQL Server : DELETE FROM {{TABLE_NAME}} {{CONDITION}} PostgreSQL : DELETE FROM {{TABLE_NAME}} {{CONDITION}} DB2. : DELETE FROM {{TABLE_NAME}} {{CONDITION}} N/A {{RDBMS-Name}}.stringSize This defines the length for the string fields in {{RDBMS-Name}}. H2 : 254 MySQL : 254 Oracle : 254 Microsoft SQL Server : 254 PostgreSQL : 254 DB2. : 254 N/A {{RDBMS-Name}}.fieldSizeLimit This defines the field size limit for select/switch to big string type from the default string type if the 'bigStringType' is available in field type list. H2 : N/A MySQL : N/A Oracle : 2000 Microsoft SQL Server : N/A PostgreSQL : N/A DB2. : N/A 0 = n = INT_MAX {{RDBMS-Name}}.batchSize This defines the batch size when operations are performed for batches of events. H2 : 1000 MySQL : 1000 Oracle : 1000 Microsoft SQL Server : 1000 PostgreSQL : 1000 DB2. : 1000 N/A {{RDBMS-Name}}.batchEnable This specifies whether 'Update' and 'Insert' operations can be performed for batches of events or not. H2 : true MySQL : true Oracle (versions 12.0 and less) : false Oracle (versions 12.1 and above) : true Microsoft SQL Server : true PostgreSQL : true DB2. : true N/A {{RDBMS-Name}}.transactionSupported This is used to specify whether the JDBC connection that is used supports JDBC transactions or not. H2 : true MySQL : true Oracle : true Microsoft SQL Server : true PostgreSQL : true DB2. : true N/A {{RDBMS-Name}}.typeMapping.binaryType This is used to specify the binary data type. An attribute defines as 'object' type in Siddhi stream will be stored into RDBMS with this type. H2 : BLOB MySQL : BLOB Oracle : BLOB Microsoft SQL Server : VARBINARY(max) PostgreSQL : BYTEA DB2. : BLOB(64000) N/A {{RDBMS-Name}}.typeMapping.booleanType This is used to specify the boolean data type. An attribute defines as 'bool' type in Siddhi stream will be stored into RDBMS with this type. H2 : TINYINT(1) MySQL : TINYINT(1) Oracle : NUMBER(1) Microsoft SQL Server : BIT PostgreSQL : BOOLEAN DB2. : SMALLINT N/A {{RDBMS-Name}}.typeMapping.doubleType This is used to specify the double data type. An attribute defines as 'double' type in Siddhi stream will be stored into RDBMS with this type. H2 : DOUBLE MySQL : DOUBLE Oracle : NUMBER(19,4) Microsoft SQL Server : FLOAT(32) PostgreSQL : DOUBLE PRECISION DB2. : DOUBLE N/A {{RDBMS-Name}}.typeMapping.floatType This is used to specify the float data type. An attribute defines as 'float' type in Siddhi stream will be stored into RDBMS with this type. H2 : FLOAT MySQL : FLOAT Oracle : NUMBER(19,4) Microsoft SQL Server : REAL PostgreSQL : REAL DB2. : REAL N/A {{RDBMS-Name}}.typeMapping.integerType This is used to specify the integer data type. An attribute defines as 'int' type in Siddhi stream will be stored into RDBMS with this type. H2 : INTEGER MySQL : INTEGER Oracle : NUMBER(10) Microsoft SQL Server : INTEGER PostgreSQL : INTEGER DB2. : INTEGER N/A {{RDBMS-Name}}.typeMapping.longType This is used to specify the long data type. An attribute defines as 'long' type in Siddhi stream will be stored into RDBMS with this type. H2 : BIGINT MySQL : BIGINT Oracle : NUMBER(19) Microsoft SQL Server : BIGINT PostgreSQL : BIGINT DB2. : BIGINT N/A {{RDBMS-Name}}.typeMapping.stringType This is used to specify the string data type. An attribute defines as 'string' type in Siddhi stream will be stored into RDBMS with this type. H2 : VARCHAR(stringSize) MySQL : VARCHAR(stringSize) Oracle : VARCHAR(stringSize) Microsoft SQL Server : VARCHAR(stringSize) PostgreSQL : VARCHAR(stringSize) DB2. : VARCHAR(stringSize) N/A {{RDBMS-Name}}.typeMapping.bigStringType This is used to specify the big string data type. An attribute defines as 'string' type in Siddhi stream and field.length define in the annotation is greater than the fieldSizeLimit, will be stored into RDBMS with this type. H2 : N/A MySQL : N/A Oracle : CLOB Microsoft SQL Server : N/A PostgreSQL : N/A DB2.* : N/A N/A Examples EXAMPLE 1 @Store(type=\"rdbms\", jdbc.url=\"jdbc:mysql://localhost:3306/stocks\", username=\"root\", password=\"root\", jdbc.driver.name=\"com.mysql.jdbc.Driver\",field.length=\"symbol:100\") @PrimaryKey(\"id\", \"symbol\") @Index(\"volume\") define table StockTable (id string, symbol string, price float, volume long); The above example creates an event table named 'StockTable' in the database if it does not already exist (with four attributes named id , symbol , price , and volume of the types 'string', 'string', 'float', and 'long' respectively). The connection is made as specified by the parameters configured for the '@Store' annotation. The @PrimaryKey() and @Index() annotations can be used to define primary keys or indexes for the table and they follow Siddhi query syntax. RDBMS store supports having more than one attributes in the @PrimaryKey or @Index annotations. In this example a composite Primary key of both attributes id and symbol will be created. EXAMPLE 2 @Store(type=\"rdbms\", jdbc.url=\"jdbc:mysql://localhost:3306/das\", username=\"root\", password=\"root\" , jdbc.driver.name=\"org.h2.Driver\",field.length=\"symbol:100\") @PrimaryKey(\"symbol\") @Index(\"symbol\") define table StockTable (symbol string, price float, volume long); define stream InputStream (symbol string, volume long); from InputStream as a join StockTable as b on str:contains(b.symbol, a.symbol) select a.symbol as symbol, b.volume as volume insert into FooStream; The above example creates an event table named 'StockTable' in the database if it does not already exist (with three attributes named 'symbol', 'price', and 'volume' of the types 'string', 'float' and 'long' respectively). Then the table is joined with a stream named 'InputStream' based on a condition. The following operations are included in the condition: [ AND, OR, Comparisons( = = == !=), IS NULL, NOT, str:contains(Table Column , Stream Attribute or Search.String)] EXAMPLE 3 @Store(type=\"rdbms\", jdbc.url=\"jdbc:mysql://localhost:3306/das\", table.name=\"StockTable\", username=\"root\", password=\"root\" , jdbc.driver.name=\"org.h2.Driver\", field.length=\"symbol:100\", table.check.query=\"SELECT 1 FROM StockTable LIMIT 1\") @PrimaryKey(\"symbol\") @Index(\"symbol\") define table StockTable (symbol string, price float, volume long); define stream InputStream (symbol string, volume long); from InputStream as a join StockTable as b on str:contains(b.symbol, a.symbol) select a.symbol as symbol, b.volume as volume insert into FooStream; The above example creates an event table named 'StockTable' in the database if it does not already exist (with three attributes named 'symbol', 'price', and 'volume' of the types 'string', 'float' and 'long' respectively). Then the table is joined with a stream named 'InputStream' based on a condition. The following operations are included in the condition: [ AND, OR, Comparisons( = = == !=), IS NULL, NOT, str:contains(Table Column , Stream Attribute or Search.String)] redis (Store) This extension assigns data source and connection instructions to event tables. It also implements read write operations on connected datasource. This extension only can be used to read the data which persisted using the same extension since unique implementation has been used to map the relational data in to redis's key and value representation Origin: siddhi-store-redis:3.1.1 Syntax @Store(type=\"redis\", table.name=\" STRING \", cluster.mode=\" BOOL \", nodes=\" STRING \", ttl.seconds=\" LONG \", ttl.on.update=\" BOOL \", ttl.on.read=\" BOOL \") @PrimaryKey(\"PRIMARY_KEY\") @Index(\"INDEX\") QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic table.name The name with which the event table should be persisted in the store. If noname is specified via this parameter, the event table is persisted with the same name as the Siddhi table. The tale name defined in the siddhi app STRING Yes No cluster.mode This will decide the redis mode. if this is false, client will connect to a single redis node. false BOOL No No nodes host, port and the password of the node(s).In single node mode node details can be provided as follows- \"node='hosts:port@password'\" In clustered mode host and port of all the master nodes should be provided separated by a comma(,). As an example \"nodes = 'localhost:30001,localhost:30002'\". localhost:6379@root STRING Yes No ttl.seconds Time to live in seconds for each record -1 LONG Yes No ttl.on.update Set ttl on row update false BOOL Yes No ttl.on.read Set ttl on read rows false BOOL Yes No Examples EXAMPLE 1 @store(type='redis',nodes='localhost:6379@root',table.name='fooTable',cluster.mode=false)define table fooTable(time long, date String) Above example will create a redis table with the name fooTable and work on asingle redis node. EXAMPLE 2 @Store(type='redis', table.name='SweetProductionTable', nodes='localhost:30001,localhost:30002,localhost:30003', cluster.mode='true') @primaryKey('symbol') @index('price') define table SweetProductionTable (symbol string, price float, volume long); Above example demonstrate how to use the redis extension to connect in to redis cluster. Please note that, as nodes all the master node's host and port should be provided in order to work correctly. In clustered node password will not besupported EXAMPLE 3 @store(type='redis',nodes='localhost:6379@root',table.name='fooTable', ttl.seconds='30', ttl.onUpdate='true', ttl.onRead='true')define table fooTable(time long, date String) Above example will create a redis table with the name fooTable and work on asingle redis node. All rows inserted, updated or read will have its ttl set to 30 seconds Str groupConcat (Aggregate Function) This function aggregates the received events by concatenating the keys in those events using a separator, e.g.,a comma (,) or a hyphen (-), and returns the concatenated key string. Origin: siddhi-execution-string:5.0.7 Syntax STRING str:groupConcat( STRING key) STRING str:groupConcat( STRING key, STRING ...) STRING str:groupConcat( STRING key, STRING separator, BOOL distinct) STRING str:groupConcat( STRING key, STRING separator, BOOL distinct, STRING order) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic key The string that needs to be aggregated. STRING No Yes separator The separator that separates each string key after concatenating the keys. , STRING Yes Yes distinct This is used to only have distinct values in the concatenated string that is returned. false BOOL Yes Yes order This parameter accepts 'ASC' or 'DESC' strings to sort the string keys in either ascending or descending order respectively. No order STRING Yes Yes Examples EXAMPLE 1 from InputStream#window.time(5 min) select str:groupConcat(\"key\") as groupedKeys input OutputStream; When we input events having values for the key as 'A' , 'B' , 'S' , 'C' , 'A' , it returns \"A,B,S,C,A\" to the 'OutputStream'. EXAMPLE 2 from InputStream#window.time(5 min) select groupConcat(\"key\",\"-\",true,\"ASC\") as groupedKeys input OutputStream; When we input events having values for the key as 'A' , 'B' , 'S' , 'C' , 'A' , specify the seperator as hyphen and choose the order to be ascending, the function returns \"A-B-C-S\" to the 'OutputStream'. charAt (Function) This function returns the 'char' value that is present at the given index position. of the input string. Origin: siddhi-execution-string:5.0.7 Syntax STRING str:charAt( STRING input.value, INT index) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.value The input string of which the char value at the given position needs to be returned. STRING No Yes index The variable that specifies the index of the char value that needs to be returned. INT No Yes Examples EXAMPLE 1 charAt(\"WSO2\", 1) In this case, the functiion returns the character that exists at index 1. Hence, it returns 'S'. coalesce (Function) This returns the first input parameter value of the given argument, that is not null. Origin: siddhi-execution-string:5.0.7 Syntax INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT str:coalesce( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg, INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT ...) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg It can have one or more input parameters in any data type. However, all the specified parameters are required to be of the same type. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No Yes Examples EXAMPLE 1 coalesce(null, \"BBB\", \"CCC\") This returns the first input parameter that is not null. In this example, it returns \"BBB\". concat (Function) This function returns a string value that is obtained as a result of concatenating two or more input string values. Origin: siddhi-execution-string:5.0.7 Syntax STRING str:concat( STRING arg, STRING ...) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg This can have two or more string type input parameters. STRING No Yes Examples EXAMPLE 1 concat(\"D533\", \"8JU^\", \"XYZ\") This returns a string value by concatenating two or more given arguments. In the example shown above, it returns \"D5338JU^XYZ\". contains (Function) This function returns true if the input.string contains the specified sequence of char values in the search.string . Origin: siddhi-execution-string:5.0.7 Syntax BOOL str:contains( STRING input.string, STRING search.string) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string Input string value. STRING No Yes search.string The string value to be searched for in the input.string . STRING No Yes Examples EXAMPLE 1 contains(\"21 products are produced by WSO2 currently\", \"WSO2\") This returns a boolean value as the output. In this case, it returns true . equalsIgnoreCase (Function) This returns a boolean value by comparing two strings lexicographically without considering the letter case. Origin: siddhi-execution-string:5.0.7 Syntax BOOL str:equalsIgnoreCase( STRING arg1, STRING arg2) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg1 The first input string argument. STRING No Yes arg2 The second input string argument. This is compared with the first argument. STRING No Yes Examples EXAMPLE 1 equalsIgnoreCase(\"WSO2\", \"wso2\") This returns a boolean value as the output. In this scenario, it returns \"true\". fillTemplate (Function) fillTemplate(string, map) will replace all the keys in the string using values in the map. fillTemplate(string, r1, r2 ..) replace all the entries {{1}}, {{2}}, {{3}} with r1 , r2, r3. Origin: siddhi-execution-string:5.0.7 Syntax STRING str:fillTemplate( STRING template, STRING|INT|LONG|DOUBLE|FLOAT|BOOL replacement.type, STRING|INT|LONG|DOUBLE|FLOAT|BOOL ...) STRING str:fillTemplate( STRING template, OBJECT map) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic template The string with templated fields that needs to be filled with the given strings. The format of the templated fields should be as follows: {{KEY}} where 'KEY' is a STRING if you are using fillTemplate(string, map) {{KEY}} where 'KEY' is an INT if you are using fillTemplate(string, r1, r2 ..) This KEY is used to map the values STRING No Yes replacement.type A set of arguments with any type string|int|long|double|float|bool. - STRING INT LONG DOUBLE FLOAT BOOL Yes Yes map A map with key-value pairs to be replaced. - OBJECT Yes Yes Examples EXAMPLE 1 str:fillTemplate(\"{{prize}} 100 {{salary}} 10000\", map:create('prize', 300, 'salary', 10000)) In this example, the template is '{{prize}} 100 {{salary}} 10000'.Here, the templated string {{prize}} is replaced with the value corresponding to the 'prize' key in the given map. Likewise salary replace with the salary value of the map EXAMPLE 2 str:fillTemplate(\"{{1}} 100 {{2}} 10000\", 200, 300) In this example, the template is '{{1}} 100 {{2}} 10000'.Here, the templated string {{1}} is replaced with the corresponding 1 st value 200. Likewise {{2}} replace with the 300 hex (Function) This function returns a hexadecimal string by converting each byte of each character in the input string to two hexadecimal digits. Origin: siddhi-execution-string:5.0.7 Syntax STRING str:hex( STRING input.string) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string The input string to derive the hexadecimal value. STRING No Yes Examples EXAMPLE 1 hex(\"MySQL\") This returns the hexadecimal value of the input.string. In this scenario, the output is \"4d7953514c\". length (Function) Returns the length of the input string. Origin: siddhi-execution-string:5.0.7 Syntax INT str:length( STRING input.string) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string The input string to derive the length. STRING No Yes Examples EXAMPLE 1 length(\"Hello World\") This outputs the length of the provided string. In this scenario, the, output is 11 . lower (Function) Converts the capital letters in the input string to the equivalent simple letters. Origin: siddhi-execution-string:5.0.7 Syntax STRING str:lower( STRING input.string) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string The input string to convert to the lower case (i.e., equivalent simple letters). STRING No Yes Examples EXAMPLE 1 lower(\"WSO2 cep \") This converts the capital letters in the input.string to the equivalent simple letters. In this scenario, the output is \"wso2 cep \". regexp (Function) Returns a boolean value based on the matchability of the input string and the given regular expression. Origin: siddhi-execution-string:5.0.7 Syntax BOOL str:regexp( STRING input.string, STRING regex) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string The input string to match with the given regular expression. STRING No Yes regex The regular expression to be matched with the input string. STRING No Yes Examples EXAMPLE 1 regexp(\"WSO2 abcdh\", \"WSO(.*h)\") This returns a boolean value after matching regular expression with the given string. In this scenario, it returns \"true\" as the output. repeat (Function) Repeats the input string for a specified number of times. Origin: siddhi-execution-string:5.0.7 Syntax STRING str:repeat( STRING input.string, INT times) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string The input string that is repeated the number of times as defined by the user. STRING No Yes times The number of times the input.string needs to be repeated . INT No Yes Examples EXAMPLE 1 repeat(\"StRing 1\", 3) This returns a string value by repeating the string for a specified number of times. In this scenario, the output is \"StRing 1StRing 1StRing 1\". replaceAll (Function) Finds all the substrings of the input string that matches with the given expression, and replaces them with the given replacement string. Origin: siddhi-execution-string:5.0.7 Syntax STRING str:replaceAll( STRING input.string, STRING regex, STRING replacement.string) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string The input string to be replaced. STRING No Yes regex The regular expression to be matched with the input string. STRING No Yes replacement.string The string with which each substring that matches the given expression should be replaced. STRING No Yes Examples EXAMPLE 1 replaceAll(\"hello hi hello\", 'hello', 'test') This returns a string after replacing the substrings of the input string with the replacement string. In this scenario, the output is \"test hi test\" . replaceFirst (Function) Finds the first substring of the input string that matches with the given regular expression, and replaces itwith the given replacement string. Origin: siddhi-execution-string:5.0.7 Syntax STRING str:replaceFirst( STRING input.string, STRING regex, STRING replacement.string) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string The input string that should be replaced. STRING No Yes regex The regular expression with which the input string should be matched. STRING No Yes replacement.string The string with which the first substring of input string that matches the regular expression should be replaced. STRING No Yes Examples EXAMPLE 1 replaceFirst(\"hello WSO2 A hello\", 'WSO2(.*)A', 'XXXX') This returns a string after replacing the first substring with the given replacement string. In this scenario, the output is \"hello XXXX hello\". reverse (Function) Returns the input string in the reverse order character-wise and string-wise. Origin: siddhi-execution-string:5.0.7 Syntax STRING str:reverse( STRING input.string) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string The input string to be reversed. STRING No Yes Examples EXAMPLE 1 reverse(\"Hello World\") This outputs a string value by reversing the incoming input.string . In this scenario, the output is \"dlroW olleH\". split (Function) Splits the input.string into substrings using the value parsed in the split.string and returns the substring at the position specified in the group.number . Origin: siddhi-execution-string:5.0.7 Syntax STRING str:split( STRING input.string, STRING split.string, INT group.number) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string The input string to be replaced. STRING No Yes split.string The string value to be used to split the input.string . STRING No Yes group.number The index of the split group INT No Yes Examples EXAMPLE 1 split(\"WSO2,ABM,NSFT\", \",\", 0) This splits the given input.string by given split.string and returns the string in the index given by group.number. In this scenario, the output will is \"WSO2\". strcmp (Function) Compares two strings lexicographically and returns an integer value. If both strings are equal, 0 is returned. If the first string is lexicographically greater than the second string, a positive value is returned. If the first string is lexicographically greater than the second string, a negative value is returned. Origin: siddhi-execution-string:5.0.7 Syntax INT str:strcmp( STRING arg1, STRING arg2) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg1 The first input string argument. STRING No Yes arg2 The second input string argument that should be compared with the first argument lexicographically. STRING No Yes Examples EXAMPLE 1 strcmp(\"AbCDefghiJ KLMN\", 'Hello') This compares two strings lexicographically and outputs an integer value. substr (Function) Returns a substring of the input string by considering a subset or all of the following factors: starting index, length, regular expression, and regex group number. Origin: siddhi-execution-string:5.0.7 Syntax STRING str:substr( STRING input.string, INT begin.index) STRING str:substr( STRING input.string, INT begin.index, INT length) STRING str:substr( STRING input.string, STRING regex) STRING str:substr( STRING input.string, STRING regex, INT group.number) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string The input string to be processed. STRING No Yes begin.index Starting index to consider for the substring. - INT Yes Yes length The length of the substring. input.string .length - begin.index INT Yes Yes regex The regular expression that should be matched with the input string. - STRING Yes Yes group.number The regex group number 0 INT Yes Yes Examples EXAMPLE 1 substr(\"AbCDefghiJ KLMN\", 4) This outputs the substring based on the given begin.index . In this scenario, the output is \"efghiJ KLMN\". EXAMPLE 2 substr(\"AbCDefghiJ KLMN\", 2, 4) This outputs the substring based on the given begin.index and length. In this scenario, the output is \"CDef\". EXAMPLE 3 substr(\"WSO2D efghiJ KLMN\", '^WSO2(.*)') This outputs the substring by applying the regex. In this scenario, the output is \"WSO2D efghiJ KLMN\". EXAMPLE 4 substr(\"WSO2 cep WSO2 XX E hi hA WSO2 heAllo\", 'WSO2(.*)A(.*)', 2) This outputs the substring by applying the regex and considering the group.number . In this scenario, the output is \" ello\". trim (Function) Returns a copy of the input string without the leading and trailing whitespace (if any). Origin: siddhi-execution-string:5.0.7 Syntax STRING str:trim( STRING input.string) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string The input string that needs to be trimmed. STRING No Yes Examples EXAMPLE 1 trim(\" AbCDefghiJ KLMN \") This returns a copy of the input.string with the leading and/or trailing white-spaces omitted. In this scenario, the output is \"AbCDefghiJ KLMN\". unhex (Function) Returns a string by converting the hexadecimal characters in the input string. Origin: siddhi-execution-string:5.0.7 Syntax STRING str:unhex( STRING input.string) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string The hexadecimal input string that needs to be converted to string. STRING No Yes Examples EXAMPLE 1 unhex(\"4d7953514c\") This converts the hexadecimal value to string. upper (Function) Converts the simple letters in the input string to the equivalent capital/block letters. Origin: siddhi-execution-string:5.0.7 Syntax STRING str:upper( STRING input.string) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string The input string that should be converted to the upper case (equivalent capital/block letters). STRING No Yes Examples EXAMPLE 1 upper(\"Hello World\") This converts the simple letters in the input.string to theequivalent capital letters. In this scenario, the output is \"HELLO WORLD\". tokenize (Stream Processor) This function splits the input string into tokens using a given regular expression and returns the split tokens. Origin: siddhi-execution-string:5.0.7 Syntax str:tokenize( STRING input.string, STRING regex) str:tokenize( STRING input.string, STRING regex, BOOL distinct) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string The input string which needs to be split. STRING No Yes regex The string value which is used to tokenize the 'input.string'. STRING No Yes distinct This flag is used to return only distinct values. false BOOL Yes Yes Extra Return Attributes Name Description Possible Types token The attribute which contains a single token. STRING Examples EXAMPLE 1 define stream inputStream (str string); @info(name = 'query1') from inputStream#str:tokenize(str , ',') select token insert into outputStream; This query performs tokenization on the given string. If the str is \"Android,Windows8,iOS\", then the string is split into 3 events containing the token attribute values, i.e., Android , Windows8 and iOS . Time currentDate (Function) Function returns the system time in yyyy-MM-dd format. Origin: siddhi-execution-time:5.0.4 Syntax STRING time:currentDate() Examples EXAMPLE 1 time:currentDate() Returns the current date in the yyyy-MM-dd format, such as 2019-06-21 . currentTime (Function) Function returns system time in the HH ss format. Origin: siddhi-execution-time:5.0.4 Syntax STRING time:currentTime() Examples EXAMPLE 1 time:currentTime() Returns the current date in the HH ss format, such as 15:23:24 . currentTimestamp (Function) When no argument is provided, function returns the system current timestamp in yyyy-MM-dd HH ss format, and when a timezone is provided as an argument, it converts and return the current system time to the given timezone format. Origin: siddhi-execution-time:5.0.4 Syntax STRING time:currentTimestamp() STRING time:currentTimestamp( STRING timezone) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic timezone The timezone to which the current time need to be converted. For example, Asia/Kolkata , PST . Get the supported timezone IDs from here System timezone STRING Yes No Examples EXAMPLE 1 time:currentTimestamp() Returns current system time in yyyy-MM-dd HH ss format, such as 2019-03-31 14:07:00 . EXAMPLE 2 time:currentTimestamp('Asia/Kolkata') Returns current system time converted to 'Asia/Kolkata' timezone yyyy-MM-dd HH ss format, such as 2019-03-31 19:07:00 . Get the supported timezone IDs from here EXAMPLE 3 time:currentTimestamp('CST') Returns current system time converted to 'CST' timezone yyyy-MM-dd HH ss format, such as 2019-03-31 02:07:00 . Get the supported timezone IDs from here date (Function) Extracts the date part of a date or date-time and return it in yyyy-MM-dd format. Origin: siddhi-execution-time:5.0.4 Syntax STRING time:date( STRING date.value, STRING date.format) STRING time:date( STRING date.value) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic date.value The value of the date. For example, 2014-11-11 13:23:44.657 , 2014-11-11 , 13:23:44.657 . STRING No Yes date.format The format of the date value provided. For example, yyyy/MM/dd HH ss.SSS . yyyy-MM-dd HH:mm:ss.SSS STRING Yes Yes Examples EXAMPLE 1 time:date('2014/11/11 13:23:44', 'yyyy/MM/dd HH:mm:ss') Extracts the date and returns 2014-11-11 . EXAMPLE 2 time:date('2014-11-23 13:23:44.345') Extracts the date and returns 2014-11-13 . EXAMPLE 3 time:date('13:23:44', 'HH:mm:ss') Extracts the date and returns 1970-01-01 . dateAdd (Function) Adds the specified time interval to a date. Origin: siddhi-execution-time:5.0.4 Syntax STRING time:dateAdd( STRING date.value, INT expr, STRING unit) STRING time:dateAdd( LONG timestamp.in.milliseconds, INT expr, STRING unit) STRING time:dateAdd( STRING date.value, INT expr, STRING unit, STRING date.format) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic date.value The value of the date. For example, 2014-11-11 13:23:44.657 , 2014-11-11 , 13:23:44.657 . - STRING Yes Yes expr The amount by which the selected part of the date should be incremented. For example 2 , 5 , 10 , etc. INT No Yes unit This is the part of the date that needs to be modified. For example, MINUTE , HOUR , MONTH , YEAR , QUARTER , WEEK , DAY , SECOND . STRING No No date.format The format of the date value provided. For example, yyyy-MM-dd HH ss.SSS . yyyy-MM-dd HH:mm:ss.SSS STRING Yes Yes timestamp.in.milliseconds The date value in milliseconds. For example, 1415712224000L . - LONG Yes Yes Examples EXAMPLE 1 time:dateAdd('2014-11-11 13:23:44.657', 5, 'YEAR', 'yyyy-MM-dd HH:mm:ss.SSS') Adds five years to the given date value and returns 2019-11-11 13:23:44.657 . EXAMPLE 2 time:dateAdd('2014-11-11 13:23:44.657', 5, 'YEAR') Adds five years to the given date value and returns 2019-11-11 13:23:44.657 using the default date.format yyyy-MM-dd HH ss.SSS . EXAMPLE 3 time:dateAdd( 1415712224000L, 1, 'HOUR') Adds one hour and 1415715824000 as a string . dateDiff (Function) Returns difference between two dates in days. Origin: siddhi-execution-time:5.0.4 Syntax INT time:dateDiff( STRING date.value1, STRING date.value2, STRING date.format1, STRING date.format2) INT time:dateDiff( STRING date.value1, STRING date.value2) INT time:dateDiff( LONG timestamp.in.milliseconds1, LONG timestamp.in.milliseconds2) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic date.value1 The value of the first date parameter. For example, 2014-11-11 13:23:44.657 , 2014-11-11 , 13:23:44.657 . - STRING Yes Yes date.value2 The value of the second date parameter. For example, 2014-11-11 13:23:44.657 , 2014-11-11 , 13:23:44.657 . - STRING Yes Yes date.format1 The format of the first date value provided. For example, yyyy-MM-dd HH ss.SSS . yyyy-MM-dd HH:mm:ss.SSS STRING Yes Yes date.format2 The format of the second date value provided. For example, yyyy-MM-dd HH ss.SSS . yyyy-MM-dd HH:mm:ss.SSS STRING Yes Yes timestamp.in.milliseconds1 The first date value in milliseconds from the epoch. For example, 1415712224000L . - LONG Yes Yes timestamp.in.milliseconds2 The second date value in milliseconds from the epoch. For example, 1415712224000L . - LONG Yes Yes Examples EXAMPLE 1 time:dateDiff('2014-11-11 13:23:44', '2014-11-9 13:23:44', 'yyyy-MM-dd HH:mm:ss', 'yyyy-MM-dd HH:mm:ss') Returns the date difference between the two given dates as 2 . EXAMPLE 2 time:dateDiff('2014-11-13 13:23:44', '2014-11-9 13:23:44') Returns the date difference between the two given dates as 4 . EXAMPLE 3 time:dateDiff(1415692424000L, 1412841224000L) Returns the date difference between the two given dates as 33 . dateFormat (Function) Formats the data in string or milliseconds format to the given date format. Origin: siddhi-execution-time:5.0.4 Syntax STRING time:dateFormat( STRING date.value, STRING date.target.format, STRING date.source.format) STRING time:dateFormat( STRING date.value, STRING date.target.format) STRING time:dateFormat( LONG timestamp.in.milliseconds, STRING date.target.format) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic date.value The value of the date. For example, 2014-11-11 13:23:44.657 , 2014-11-11 , 13:23:44.657 . - STRING Yes Yes date.target.format The format of the date into which the date value needs to be converted. For example, yyyy/MM/dd HH ss . STRING No Yes date.source.format The format input date.value.For example, yyyy-MM-dd HH ss.SSS . yyyy-MM-dd HH:mm:ss.SSS STRING Yes Yes timestamp.in.milliseconds The date value in milliseconds from the epoch. For example, 1415712224000L . - LONG Yes Yes Examples EXAMPLE 1 time:dateFormat('2014/11/11 13:23:44', 'mm:ss', 'yyyy/MM/dd HH:mm:ss') Converts date based on the target date format mm:ss and returns 23:44 . EXAMPLE 2 time:dateFormat('2014-11-11 13:23:44', 'HH:mm:ss') Converts date based on the target date format HH ss and returns 13:23:44 . EXAMPLE 3 time:dateFormat(1415692424000L, 'yyyy-MM-dd') Converts date in millisecond based on the target date format yyyy-MM-dd and returns 2014-11-11 . dateSub (Function) Subtracts the specified time interval from the given date. Origin: siddhi-execution-time:5.0.4 Syntax STRING time:dateSub( STRING date.value, INT expr, STRING unit) STRING time:dateSub( STRING date.value, INT expr, STRING unit, STRING date.format) STRING time:dateSub( LONG timestamp.in.milliseconds, INT expr, STRING unit) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic date.value The value of the date. For example, 2014-11-11 13:23:44.657 , 2014-11-11 , 13:23:44.657 . - STRING Yes Yes expr The amount by which the selected part of the date should be decremented. For example 2 , 5 , 10 , etc. INT No Yes unit This is the part of the date that needs to be modified. For example, MINUTE , HOUR , MONTH , YEAR , QUARTER , WEEK , DAY , SECOND . STRING No No date.format The format of the date value provided. For example, yyyy-MM-dd HH ss.SSS . yyyy-MM-dd HH:mm:ss.SSS STRING Yes Yes timestamp.in.milliseconds The date value in milliseconds. For example, 1415712224000L . - LONG Yes Yes Examples EXAMPLE 1 time:dateSub('2019-11-11 13:23:44.657', 5, 'YEAR', 'yyyy-MM-dd HH:mm:ss.SSS') Subtracts five years to the given date value and returns 2014-11-11 13:23:44.657 . EXAMPLE 2 time:dateSub('2019-11-11 13:23:44.657', 5, 'YEAR') Subtracts five years to the given date value and returns 2014-11-11 13:23:44.657 using the default date.format yyyy-MM-dd HH ss.SSS . EXAMPLE 3 time:dateSub( 1415715824000L, 1, 'HOUR') Subtracts one hour and 1415712224000 as a string . dayOfWeek (Function) Extracts the day on which a given date falls. Origin: siddhi-execution-time:5.0.4 Syntax STRING time:dayOfWeek( STRING date.value, STRING date.format) STRING time:dayOfWeek( STRING date.value) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic date.value The value of the date. For example, 2014-11-11 13:23:44.657 , 2014-11-11 , 13:23:44.657 . STRING No Yes date.format The format of the date value provided. For example, yyyy/MM/dd HH ss.SSS . yyyy-MM-dd HH:mm:ss.SSS STRING Yes Yes Examples EXAMPLE 1 time:date('2014/12/11 13:23:44', 'yyyy/MM/dd HH:mm:ss') Extracts the date and returns Thursday . EXAMPLE 2 time:date('2014-11-11 13:23:44.345') Extracts the date and returns Tuesday . extract (Function) Function extracts a date unit from the date. Origin: siddhi-execution-time:5.0.4 Syntax INT time:extract( STRING unit, STRING date.value) INT time:extract( STRING unit, STRING date.value, STRING date.format) INT time:extract( STRING unit, STRING date.value, STRING date.format, STRING locale) INT time:extract( LONG timestamp.in.milliseconds, STRING unit) INT time:extract( LONG timestamp.in.milliseconds, STRING unit, STRING locale) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic unit This is the part of the date that needs to be modified. For example, MINUTE , HOUR , MONTH , YEAR , QUARTER , WEEK , DAY , SECOND . STRING No No date.value The value of the date. For example, 2014-11-11 13:23:44.657 , 2014-11-11 , 13:23:44.657 . - STRING Yes Yes date.format The format of the date value provided. For example, yyyy-MM-dd HH ss.SSS . yyyy-MM-dd HH:mm:ss.SSS STRING Yes Yes timestamp.in.milliseconds The date value in milliseconds. For example, 1415712224000L . - LONG Yes Yes locale Represents a specific geographical, political or cultural region. For example en_US and fr_FR Current default locale set in the Java Virtual Machine. STRING Yes No Examples EXAMPLE 1 time:extract('YEAR', '2019/11/11 13:23:44.657', 'yyyy/MM/dd HH:mm:ss.SSS') Extracts the year amount and returns 2019 . EXAMPLE 2 time:extract('DAY', '2019-11-12 13:23:44.657') Extracts the day amount and returns 12 . EXAMPLE 3 time:extract(1394556804000L, 'HOUR') Extracts the hour amount and returns 22 . timestampInMilliseconds (Function) Returns the system time or the given time in milliseconds. Origin: siddhi-execution-time:5.0.4 Syntax LONG time:timestampInMilliseconds() LONG time:timestampInMilliseconds( STRING date.value, STRING date.format) LONG time:timestampInMilliseconds( STRING date.value) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic date.value The value of the date. For example, 2014-11-11 13:23:44.657 , 2014-11-11 , 13:23:44.657 . Current system time STRING Yes Yes date.format The format of the date value provided. For example, yyyy/MM/dd HH ss.SSS . yyyy-MM-dd HH:mm:ss.SSS STRING Yes Yes Examples EXAMPLE 1 time:timestampInMilliseconds() Returns the system current time in milliseconds. EXAMPLE 2 time:timestampInMilliseconds('2007-11-30 10:30:19', 'yyyy-MM-DD HH:MM:SS') Converts 2007-11-30 10:30:19 in yyyy-MM-DD HH:MM:SS format to milliseconds as 1170131400019 . EXAMPLE 3 time:timestampInMilliseconds('2007-11-30 10:30:19.000') Converts 2007-11-30 10:30:19 in yyyy-MM-DD HH:MM:ss.SSS format to milliseconds as 1196398819000 . utcTimestamp (Function) Function returns the system current time in UTC timezone with yyyy-MM-dd HH ss format. Origin: siddhi-execution-time:5.0.4 Syntax STRING time:utcTimestamp() Examples EXAMPLE 1 time:utcTimestamp() Returns the system current time in UTC timezone with yyyy-MM-dd HH ss format, and a sample output will be like 2019-07-03 09:58:34 . Unique deduplicate (Stream Processor) Removes duplicate events based on the unique.key parameter that arrive within the time.interval gap from one another. Origin: siddhi-execution-unique:5.0.5 Syntax unique:deduplicate( INT|LONG|FLOAT|BOOL|DOUBLE|STRING unique.key, INT|LONG time.interval) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic unique.key Parameter to uniquely identify events. INT LONG FLOAT BOOL DOUBLE STRING No Yes time.interval The sliding time period within which the duplicate events are dropped. INT LONG No No Examples EXAMPLE 1 define stream TemperatureStream (sensorId string, temperature double) from TemperatureStream#unique:deduplicate(sensorId, 30 sec) select * insert into UniqueTemperatureStream; Query that removes duplicate events of TemperatureStream stream based on sensorId attribute when they arrive within 30 seconds. ever (Window) Window that retains the latest events based on a given unique keys. When a new event arrives with the same key it replaces the one that exist in the window. b This function is not recommended to be used when the maximum number of unique attributes are undefined, as there is a risk of system going out to memory /b . Origin: siddhi-execution-unique:5.0.5 Syntax unique:ever( INT|LONG|FLOAT|BOOL|DOUBLE|STRING unique.key) unique:ever( INT|LONG|FLOAT|BOOL|DOUBLE|STRING unique.key, INT|LONG|FLOAT|BOOL|DOUBLE|STRING ...) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic unique.key The attribute used to checked for uniqueness. INT LONG FLOAT BOOL DOUBLE STRING No Yes Examples EXAMPLE 1 define stream LoginEvents (timestamp long, ip string); from LoginEvents#window.unique:ever(ip) select count(ip) as ipCount insert events into UniqueIps; Query collects all unique events based on the ip attribute by retaining the latest unique events from the LoginEvents stream. Then the query counts the unique ip s arrived so far and outputs the ipCount via the UniqueIps stream. EXAMPLE 2 define stream DriverChangeStream (trainID string, driver string); from DriverChangeStream#window.unique:ever(trainID) select trainID, driver insert expired events into PreviousDriverChangeStream; Query collects all unique events based on the trainID attribute by retaining the latest unique events from the DriverChangeStream stream. The query outputs the previous unique event stored in the window as the expired events are emitted via PreviousDriverChangeStream stream. EXAMPLE 3 define stream StockStream (symbol string, price float); define stream PriceRequestStream(symbol string); from StockStream#window.unique:ever(symbol) as s join PriceRequestStream as p on s.symbol == p.symbol select s.symbol as symbol, s.price as price insert events into PriceResponseStream; Query stores the last unique event for each symbol attribute of StockStream stream, and joins them with events arriving on the PriceRequestStream for equal symbol attributes to fetch the latest price for each requested symbol and output via PriceResponseStream stream. externalTimeBatch (Window) This is a batch (tumbling) time window that is determined based on an external time, i.e., time stamps that are specified via an attribute in the events. It holds the latest unique events that arrived during the last window time period. The unique events are determined based on the value for a specified unique key parameter. When a new event arrives within the time window with a value for the unique key parameter that is the same as that of an existing event in the window, the existing event expires and it is replaced by the new event. Origin: siddhi-execution-unique:5.0.5 Syntax unique:externalTimeBatch( INT|LONG|FLOAT|BOOL|DOUBLE|STRING unique.key, LONG time.stamp, INT|LONG window.time) unique:externalTimeBatch( INT|LONG|FLOAT|BOOL|DOUBLE|STRING unique.key, LONG time.stamp, INT|LONG window.time, INT start.time) unique:externalTimeBatch( INT|LONG|FLOAT|BOOL|DOUBLE|STRING unique.key, LONG time.stamp, INT|LONG window.time, INT start.time, INT|LONG time.out) unique:externalTimeBatch( INT|LONG|FLOAT|BOOL|DOUBLE|STRING unique.key, LONG time.stamp, INT|LONG window.time, INT start.time, INT|LONG time.out, BOOL replace.time.stamp.with.batch.end.time) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic unique.key The attribute that should be checked for uniqueness. INT LONG FLOAT BOOL DOUBLE STRING No Yes time.stamp The time which the window determines as the current time and acts upon. The value of this parameter should be monotonically increasing. LONG No Yes window.time The sliding time period for which the window should hold events. INT LONG No No start.time This specifies an offset in milliseconds in order to start the window at a time different to the standard time. Timestamp of first event INT Yes No time.out Time to wait for arrival of a new event, before flushing and returning the output for events belonging to a specific batch. The system waits till an event from the next batch arrives to flush the current batch INT LONG Yes No replace.time.stamp.with.batch.end.time Replaces the 'timestamp' value with the corresponding batch end time stamp. false BOOL Yes No Examples EXAMPLE 1 define stream LoginEvents (timestamp long, ip string); from LoginEvents#window.unique:externalTimeBatch(ip, timestamp, 1 sec, 0, 2 sec) select timestamp, ip, count() as total insert into UniqueIps ; In this query, the window holds the latest unique events that arrive from the 'LoginEvent' stream during each second. The latest events are determined based on the external time stamp. At a given time, all the events held in the window have unique values for the 'ip' and monotonically increasing values for 'timestamp' attributes. The events in the window are inserted into the 'UniqueIps' output stream. The system waits for 2 seconds for the arrival of a new event before flushing the current batch. first (Window) This is a window that holds only the first set of unique events according to the unique key parameter. When a new event arrives with a key that is already in the window, that event is not processed by the window. Origin: siddhi-execution-unique:5.0.5 Syntax unique:first( INT|LONG|FLOAT|BOOL|DOUBLE|STRING unique.key) unique:first( INT|LONG|FLOAT|BOOL|DOUBLE|STRING unique.key, INT|LONG|FLOAT|BOOL|DOUBLE|STRING ...) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic unique.key The attribute that should be checked for uniqueness. If there is more than one parameter to check for uniqueness, it can be specified as an array separated by commas. INT LONG FLOAT BOOL DOUBLE STRING No Yes Examples EXAMPLE 1 define stream LoginEvents (timeStamp long, ip string); from LoginEvents#window.unique:first(ip) insert into UniqueIps ; This returns the first set of unique items that arrive from the 'LoginEvents' stream, and returns them to the 'UniqueIps' stream. The unique events are only those with a unique value for the 'ip' attribute. firstLengthBatch (Window) This is a batch (tumbling) window that holds a specific number of unique events (depending on which events arrive first). The unique events are selected based on a specific parameter that is considered as the unique key. When a new event arrives with a value for the unique key parameter that matches the same of an existing event in the window, that event is not processed by the window. Origin: siddhi-execution-unique:5.0.5 Syntax unique:firstLengthBatch( INT|LONG|FLOAT|BOOL|DOUBLE|STRING unique.key, INT window.length) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic unique.key The attribute that should be checked for uniqueness. INT LONG FLOAT BOOL DOUBLE STRING No Yes window.length The number of events the window should tumble. INT No No Examples EXAMPLE 1 define window CseEventWindow (symbol string, price float, volume int) from CseEventStream#window.unique:firstLengthBatch(symbol, 10) select symbol, price, volume insert all events into OutputStream ; The window in this configuration holds the first unique events from the 'CseEventStream' stream every second, and outputs them all into the the 'OutputStream' stream. All the events in a window during a given second should have a unique value for the 'symbol' attribute. firstTimeBatch (Window) A batch-time or tumbling window that holds the unique events according to the unique key parameters that have arrived within the time period of that window and gets updated for each such time window. When a new event arrives with a key which is already in the window, that event is not processed by the window. Origin: siddhi-execution-unique:5.0.5 Syntax unique:firstTimeBatch( INT|LONG|FLOAT|BOOL|DOUBLE|STRING unique.key, INT|LONG window.time) unique:firstTimeBatch( INT|LONG|FLOAT|BOOL|DOUBLE|STRING unique.key, INT|LONG window.time, INT|LONG start.time) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic unique.key The attribute that should be checked for uniqueness. INT LONG FLOAT BOOL DOUBLE STRING No Yes window.time The sliding time period for which the window should hold events. INT LONG No No start.time This specifies an offset in milliseconds in order to start the window at a time different to the standard time. Timestamp of the first event. INT LONG Yes No Examples EXAMPLE 1 define stream CseEventStream (symbol string, price float, volume int) from CseEventStream#window.unique:firstTimeBatch(symbol,1 sec) select symbol, price, volume insert all events into OutputStream ; This holds the first unique events that arrive from the 'cseEventStream' input stream during each second, based on the symbol,as a batch, and returns all the events to the 'OutputStream'. length (Window) This is a sliding length window that holds the events of the latest window length with the unique key and gets updated for the expiry and arrival of each event. When a new event arrives with the key that is already there in the window, then the previous event expires and new event is kept within the window. Origin: siddhi-execution-unique:5.0.5 Syntax unique:length( INT|LONG|FLOAT|BOOL|DOUBLE|STRING unique.key, INT window.length) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic unique.key The attribute that should be checked for uniqueness. INT LONG FLOAT BOOL DOUBLE STRING No Yes window.length The number of events that should be included in a sliding length window. INT No No Examples EXAMPLE 1 define stream CseEventStream (symbol string, price float, volume int) from CseEventStream#window.unique:length(symbol,10) select symbol, price, volume insert all events into OutputStream; In this configuration, the window holds the latest 10 unique events. The latest events are selected based on the symbol attribute. If the 'CseEventStream' receives an event for which the value for the symbol attribute is the same as that of an existing event in the window, the existing event is replaced by the new event. All the events are returned to the 'OutputStream' event stream once an event expires or is added to the window. lengthBatch (Window) This is a batch (tumbling) window that holds a specified number of latest unique events. The unique events are determined based on the value for a specified unique key parameter. The window is updated for every window length, i.e., for the last set of events of the specified number in a tumbling manner. When a new event arrives within the window length having the same value for the unique key parameter as an existing event in the window, the previous event is replaced by the new event. Origin: siddhi-execution-unique:5.0.5 Syntax unique:lengthBatch( INT|LONG|FLOAT|BOOL|DOUBLE|STRING unique.key, INT window.length) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic unique.key The attribute that should be checked for uniqueness. INT LONG FLOAT BOOL DOUBLE STRING No Yes window.length The number of events the window should tumble. INT No No Examples EXAMPLE 1 define window CseEventWindow (symbol string, price float, volume int) from CseEventStream#window.unique:lengthBatch(symbol, 10) select symbol, price, volume insert expired events into OutputStream ; In this query, the window at any give time holds the last 10 unique events from the 'CseEventStream' stream. Each of the 10 events within the window at a given time has a unique value for the symbol attribute. If a new event has the same value for the symbol attribute as an existing event within the window length, the existing event expires and it is replaced by the new event. The query returns expired individual events as well as expired batches of events to the 'OutputStream' stream. time (Window) This is a sliding time window that holds the latest unique events that arrived during the previous time window. The unique events are determined based on the value for a specified unique key parameter. The window is updated with the arrival and expiry of each event. When a new event that arrives within a window time period has the same value for the unique key parameter as an existing event in the window, the previous event is replaced by the new event. Origin: siddhi-execution-unique:5.0.5 Syntax unique:time( INT|LONG|FLOAT|BOOL|DOUBLE|STRING unique.key, INT|LONG window.time) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic unique.key The attribute that should be checked for uniqueness. INT LONG FLOAT BOOL DOUBLE STRING No Yes window.time The sliding time period for which the window should hold events. INT LONG No No Examples EXAMPLE 1 define stream CseEventStream (symbol string, price float, volume int) from CseEventStream#window.unique:time(symbol, 1 sec) select symbol, price, volume insert expired events into OutputStream ; In this query, the window holds the latest unique events that arrived within the last second from the 'CseEventStream', and returns the expired events to the 'OutputStream' stream. During any given second, each event in the window should have a unique value for the 'symbol' attribute. If a new event that arrives within the same second has the same value for the symbol attribute as an existing event in the window, the existing event expires. timeBatch (Window) This is a batch (tumbling) time window that is updated with the latest events based on a unique key parameter. If a new event that arrives within the time period of a windowhas a value for the key parameter which matches that of an existing event, the existing event expires and it is replaced by the latest event. Origin: siddhi-execution-unique:5.0.5 Syntax unique:timeBatch( INT|LONG|FLOAT|BOOL|DOUBLE|STRING unique.key, INT|LONG window.time) unique:timeBatch( INT|LONG|FLOAT|BOOL|DOUBLE|STRING unique.key, INT|LONG window.time, INT|LONG start.time) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic unique.key The attribute that should be checked for uniqueness. INT LONG FLOAT BOOL DOUBLE STRING No Yes window.time The tumbling time period for which the window should hold events. INT LONG No No start.time This specifies an offset in milliseconds in order to start the window at a time different to the standard time. Timestamp of first event INT LONG Yes No Examples EXAMPLE 1 define stream CseEventStream (symbol string, price float, volume int) from CseEventStream#window.unique:timeBatch(symbol, 1 sec) select symbol, price, volume insert all events into OutputStream ; This window holds the latest unique events that arrive from the 'CseEventStream' at a given time, and returns all the events to the 'OutputStream' stream. It is updated every second based on the latest values for the 'symbol' attribute. timeLengthBatch (Window) This is a batch or tumbling time length window that is updated with the latest events based on a unique key parameter. The window tumbles upon the elapse of the time window, or when a number of unique events have arrived. If a new event that arrives within the period of the window has a value for the key parameter which matches the value of an existing event, the existing event expires and it is replaced by the new event. Origin: siddhi-execution-unique:5.0.5 Syntax unique:timeLengthBatch( INT|LONG|FLOAT|BOOL|DOUBLE|STRING unique.key, INT|LONG window.time, INT window.length) unique:timeLengthBatch( INT|LONG|FLOAT|BOOL|DOUBLE|STRING unique.key, INT|LONG window.time, INT|LONG start.time, INT window.length) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic unique.key The attribute that should be checked for uniqueness. INT LONG FLOAT BOOL DOUBLE STRING No Yes window.time The sliding time period for which the window should hold the events. INT LONG No No start.time This specifies an offset in milliseconds in order to start the window at a time different to the standard time. Timestamp of first event INT LONG Yes No window.length The number of events the window should tumble. INT No No Examples EXAMPLE 1 define stream CseEventStream (symbol string, price float, volume int) from CseEventStream#window.unique:timeLengthBatch(symbol, 1 sec, 20) select symbol, price, volume insert all events into OutputStream; This window holds the latest unique events that arrive from the 'CseEventStream' at a given time, and returns all the events to the 'OutputStream' stream. It is updated every second based on the latest values for the 'symbol' attribute. Unitconversion MmTokm (Function) This converts the input given in megameters into kilometers. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:MmTokm( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from megameters into kilometers. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:MmTokm(1) The megameter value '1' is converted into kilometers as '1000.0' . cmToft (Function) This converts the input given in centimeters into feet. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:cmToft( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from centimeters into feet. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:cmToft(100) The centimeters value '100' is converted into feet as '3.280' . cmToin (Function) This converts the input given in centimeters into inches. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:cmToin( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from centimeters into inches. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:cmToin(100) Input centimeters value '100' is converted into inches as '39.37'. cmTokm (Function) This converts the input value given in centimeters into kilometers. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:cmTokm( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from centimeters into kilometers. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:cmTokm(100) The centimeters value '100' is converted into kilometers as '0.001'. cmTom (Function) This converts the input given in centimeters into meters. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:cmTom( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from centimeters into meters. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:cmTom(100) The centimeters value '100' is converted into meters as '1.0' . cmTomi (Function) This converts the input given in centimeters into miles. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:cmTomi( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from centimeters into miles. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:cmTomi(10000) The centimeters value '10000' is converted into miles as '0.062' . cmTomm (Function) This converts the input given in centimeters into millimeters. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:cmTomm( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from centimeters into millimeters. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:cmTomm(1) The centimeter value '1' is converted into millimeters as '10.0' . cmTonm (Function) This converts the input given in centimeters into nanometers. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:cmTonm( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from centimeters into nanometers. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:cmTonm(1) The centimeter value '1' is converted into nanometers as '10000000' . cmToum (Function) This converts the input in centimeters into micrometers. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:cmToum( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from centimeters into micrometers. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:cmToum(100) The centimeters value '100' is converted into micrometers as '1000000.0' . cmToyd (Function) This converts the input given in centimeters into yards. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:cmToyd( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from centimeters into yards. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:cmToyd(1) The centimeter value '1' is converted into yards as '0.01' . dToh (Function) This converts the input given in days into hours. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:dToh( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from days into hours. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:dToh(1) The day value '1' is converted into hours as '24.0'. gTokg (Function) This converts the input given in grams into kilograms. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:gTokg( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from grams into kilograms. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:gTokg(1000) The grams value '1000' is converted into kilogram as '1.0' . gTomg (Function) This converts the input given in grams into milligrams. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:gTomg( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from grams into milligrams. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:gTomg(1) The gram value '1' is converted into milligrams as '1000.0' . gToug (Function) This converts the input given in grams into micrograms. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:gToug( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from grams into micrograms. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:gToug(1) The gram value '1' is converted into micrograms as '1000000.0' . hTom (Function) This converts the input given in hours into minutes. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:hTom( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from hours into minutes. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:hTom(1) The hour value '1' is converted into minutes as '60.0' . hTos (Function) This converts the input given in hours into seconds. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:hTos( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from hours into seconds. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:hTos(1) The hour value '1' is converted into seconds as '3600.0'. kgToLT (Function) This converts the input given in kilograms into imperial tons. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:kgToLT( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from kilograms into imperial tons. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:kgToLT(1000) The kilograms value '1000' is converted into imperial tons as '0.9842' . kgToST (Function) This converts the input given in kilograms into US tons. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:kgToST( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from kilograms into US tons. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:kgToST(1000) The kilograms value '1000 is converted into US tons as '1.10' . kgTog (Function) This converts the input given in kilograms into grams. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:kgTog( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from kilograms into grams. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:kgTog(1) The kilogram value '1' is converted into grams as '1000'. kgTolb (Function) This converts the input given in kilograms into pounds. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:kgTolb( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from kilograms into pounds. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:kgTolb(1) The kilogram value '1' is converted into pounds as '2.2' . kgTooz (Function) This converts the input given in kilograms into ounces. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:kgTooz( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from kilograms into ounces. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:kgTooz(1) The kilogram value '1' is converted into ounces as ' 35.274' . kgTost (Function) This converts the input given in kilograms into imperial stones. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:kgTost( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from kilograms into imperial stones. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:kgTost(1) The kilogram value '1' is converted into imperial stones as '0.157' . kgTot (Function) This converts the input given in kilograms into tonnes. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:kgTot( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from kilograms into tonnes. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:kgTot(1) The kilogram value '1' is converted into tonnes as '0.001' . kmTocm (Function) This converts the input given in kilometers into centimeters. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:kmTocm( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from kilometers into centimeters. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:kmTocm(1) The kilometer value '1' is converted into centimeters as '100000.0' . kmToft (Function) This converts the input given in kilometers into feet. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:kmToft( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from kilometers into feet. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:kmToft(1) The kilometer value '1' is converted into feet as '3280.8' . kmToin (Function) This converts the input given in kilometers into inches. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:kmToin( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from kilometers into inches. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:kmToin(1) The kilometer value '1' is converted into inches as '39370.08' . kmTom (Function) This converts the input given in kilometers into meters. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:kmTom( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from kilometers into meters. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:kmTom(1) The kilometer value '1' is converted into meters as '1000.0' . kmTomi (Function) This converts the input given in kilometers into miles. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:kmTomi( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from kilometers into miles. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:kmTomi(1) The kilometer value '1' is converted into miles as '0.621' . kmTomm (Function) This converts the input given in kilometers into millimeters. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:kmTomm( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from kilometers into millimeters. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:kmTomm(1) The kilometer value '1' is converted into millimeters as '1000000.0' . kmTonm (Function) This converts the input given in kilometers into nanometers. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:kmTonm( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from kilometers into nanometers. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:kmTonm(1) The kilometer value '1' is converted into nanometers as '1000000000000.0' . kmToum (Function) This converts the input given in kilometers into micrometers. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:kmToum( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from kilometers into micrometers. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:kmToum(1) The kilometer value '1' is converted into micrometers as '1000000000.0' . kmToyd (Function) This converts the input given in kilometers into yards. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:kmToyd( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from kilometers into yards. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:kmToyd(1) The kilometer value '1' is converted into yards as '1093.6' . lTom3 (Function) This converts the input given in liters into cubic meters. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:lTom3( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from liters into cubic meters. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:lTom3(1000) The liters value '1000' is converted into cubic meters as '1' . lToml (Function) This converts the input given in liters into milliliters. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:lToml( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from liters into milliliters. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:lToml(1) The liter value '1' is converted into milliliters as '1000.0' . m3Tol (Function) This converts the input given in cubic meters into liters. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:m3Tol( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from meters into liters. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:m3Tol(1) The cubic meter value '1' is converted into liters as '1000.0' . mTocm (Function) This converts the input given in meters into centimeters. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:mTocm( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from meters into centimeters. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:mTocm(1) The meter value '1' is converted to centimeters as '100.0' . mToft (Function) This converts the input given in meters into feet. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:mToft( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from meters into feet. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:mToft(1) The meter value '1' is converted into feet as '3.280' . mTomm (Function) This converts the input given in meters into millimeters. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:mTomm( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from meters into millimeters. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:mTomm(1) The meter value '1' is converted into millimeters as '1000.0' . mTos (Function) This converts the input given in minutes into seconds. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:mTos( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from minutes into seconds. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:mTos(1) The minute value '1' is converted into seconds as '60.0' . mToyd (Function) This converts the input given in meters into yards. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:mToyd( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from meters into yards. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:mToyd(1) The meter value '1' is converted into yards as '1.093' . miTokm (Function) This converts the input given in miles into kilometers. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:miTokm( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from miles into kilometers. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:miTokm(1) The mile value '1' is converted into kilometers as '1.6' . mlTol (Function) This converts the input given in milliliters into liters. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:mlTol( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from milliliters into liters. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:mlTol(1000) The milliliters value '1000' is converted into liters as '1'. sToms (Function) This converts the input given in seconds into milliseconds. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:sToms( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from seconds into milliseconds. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:sToms(1) The second value '1' is converted into milliseconds as '1000.0' . sTons (Function) This converts the input given in seconds into nanoseconds. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:sTons( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from seconds into nanoseconds. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:sTons(1) The second value '1' is converted into nanoseconds as '1000000000.0' . sTous (Function) This converts the input given in seconds into microseconds. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:sTous( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from seconds into microseconds. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:sTous(1) The second value '1' is converted into microseconds as '1000000.0' . tTog (Function) This converts the input given in tonnes into grams. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:tTog( INT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from Tonnes into grams. INT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:tTog(1) The tonne value '1' is converted into grams as '1000000.0' . tTokg (Function) This converts the input given in tonnes into kilograms. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:tTokg( INT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from tonnes into kilograms. INT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:tTokg(inValue) The tonne value is converted into kilograms as '1000.0' . yTod (Function) This converts the given input in years into days. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:yTod( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from years into days. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:yTod(1) The year value '1' is converted into days as '365.2525' .","title":"5.1.1"},{"location":"docs/api/5.1.1/#api-docs-v511","text":"","title":"API Docs - v5.1.1"},{"location":"docs/api/5.1.1/#core","text":"","title":"Core"},{"location":"docs/api/5.1.1/#and-aggregate-function","text":"Returns the results of AND operation for all the events. Origin: siddhi-core:5.1.8 Syntax BOOL and( BOOL arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be AND operation. BOOL No Yes Examples EXAMPLE 1 from cscStream#window.lengthBatch(10) select and(isFraud) as isFraudTransaction insert into alertStream; This will returns the result for AND operation of isFraud values as a boolean value for event chunk expiry by window length batch.","title":"and (Aggregate Function)"},{"location":"docs/api/5.1.1/#avg-aggregate-function","text":"Calculates the average for all the events. Origin: siddhi-core:5.1.8 Syntax DOUBLE avg( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that need to be averaged. INT LONG DOUBLE FLOAT No Yes Examples EXAMPLE 1 from fooStream#window.timeBatch select avg(temp) as avgTemp insert into barStream; avg(temp) returns the average temp value for all the events based on their arrival and expiry.","title":"avg (Aggregate Function)"},{"location":"docs/api/5.1.1/#count-aggregate-function","text":"Returns the count of all the events. Origin: siddhi-core:5.1.8 Syntax LONG count() LONG count( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg This function accepts one parameter. It can belong to any one of the available types. INT LONG DOUBLE FLOAT STRING BOOL OBJECT Yes Yes Examples EXAMPLE 1 from fooStream#window.timeBatch(10 sec) select count() as count insert into barStream; This will return the count of all the events for time batch in 10 seconds.","title":"count (Aggregate Function)"},{"location":"docs/api/5.1.1/#distinctcount-aggregate-function","text":"This returns the count of distinct occurrences for a given arg. Origin: siddhi-core:5.1.8 Syntax LONG distinctCount( INT|LONG|DOUBLE|FLOAT|STRING arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The object for which the number of distinct occurences needs to be counted. INT LONG DOUBLE FLOAT STRING No Yes Examples EXAMPLE 1 from fooStream select distinctcount(pageID) as count insert into barStream; distinctcount(pageID) for the following output returns '3' when the available values are as follows. \"WEB_PAGE_1\" \"WEB_PAGE_1\" \"WEB_PAGE_2\" \"WEB_PAGE_3\" \"WEB_PAGE_1\" \"WEB_PAGE_2\" The three distinct occurences identified are 'WEB_PAGE_1', 'WEB_PAGE_2', and 'WEB_PAGE_3'.","title":"distinctCount (Aggregate Function)"},{"location":"docs/api/5.1.1/#max-aggregate-function","text":"Returns the maximum value for all the events. Origin: siddhi-core:5.1.8 Syntax INT|LONG|DOUBLE|FLOAT max( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be compared to find the maximum value. INT LONG DOUBLE FLOAT No Yes Examples EXAMPLE 1 from fooStream#window.timeBatch(10 sec) select max(temp) as maxTemp insert into barStream; max(temp) returns the maximum temp value recorded for all the events based on their arrival and expiry.","title":"max (Aggregate Function)"},{"location":"docs/api/5.1.1/#maxforever-aggregate-function","text":"This is the attribute aggregator to store the maximum value for a given attribute throughout the lifetime of the query regardless of any windows in-front. Origin: siddhi-core:5.1.8 Syntax INT|LONG|DOUBLE|FLOAT maxForever( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be compared to find the maximum value. INT LONG DOUBLE FLOAT No Yes Examples EXAMPLE 1 from inputStream select maxForever(temp) as max insert into outputStream; maxForever(temp) returns the maximum temp value recorded for all the events throughout the lifetime of the query.","title":"maxForever (Aggregate Function)"},{"location":"docs/api/5.1.1/#min-aggregate-function","text":"Returns the minimum value for all the events. Origin: siddhi-core:5.1.8 Syntax INT|LONG|DOUBLE|FLOAT min( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be compared to find the minimum value. INT LONG DOUBLE FLOAT No Yes Examples EXAMPLE 1 from inputStream select min(temp) as minTemp insert into outputStream; min(temp) returns the minimum temp value recorded for all the events based on their arrival and expiry.","title":"min (Aggregate Function)"},{"location":"docs/api/5.1.1/#minforever-aggregate-function","text":"This is the attribute aggregator to store the minimum value for a given attribute throughout the lifetime of the query regardless of any windows in-front. Origin: siddhi-core:5.1.8 Syntax INT|LONG|DOUBLE|FLOAT minForever( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be compared to find the minimum value. INT LONG DOUBLE FLOAT No Yes Examples EXAMPLE 1 from inputStream select minForever(temp) as max insert into outputStream; minForever(temp) returns the minimum temp value recorded for all the events throughoutthe lifetime of the query.","title":"minForever (Aggregate Function)"},{"location":"docs/api/5.1.1/#or-aggregate-function","text":"Returns the results of OR operation for all the events. Origin: siddhi-core:5.1.8 Syntax BOOL or( BOOL arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be OR operation. BOOL No Yes Examples EXAMPLE 1 from cscStream#window.lengthBatch(10) select or(isFraud) as isFraudTransaction insert into alertStream; This will returns the result for OR operation of isFraud values as a boolean value for event chunk expiry by window length batch.","title":"or (Aggregate Function)"},{"location":"docs/api/5.1.1/#stddev-aggregate-function","text":"Returns the calculated standard deviation for all the events. Origin: siddhi-core:5.1.8 Syntax DOUBLE stdDev( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that should be used to calculate the standard deviation. INT LONG DOUBLE FLOAT No Yes Examples EXAMPLE 1 from inputStream select stddev(temp) as stdTemp insert into outputStream; stddev(temp) returns the calculated standard deviation of temp for all the events based on their arrival and expiry.","title":"stdDev (Aggregate Function)"},{"location":"docs/api/5.1.1/#sum-aggregate-function","text":"Returns the sum for all the events. Origin: siddhi-core:5.1.8 Syntax LONG|DOUBLE sum( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be summed. INT LONG DOUBLE FLOAT No Yes Examples EXAMPLE 1 from inputStream select sum(volume) as sumOfVolume insert into outputStream; This will returns the sum of volume values as a long value for each event arrival and expiry.","title":"sum (Aggregate Function)"},{"location":"docs/api/5.1.1/#unionset-aggregate-function","text":"Union multiple sets. This attribute aggregator maintains a union of sets. The given input set is put into the union set and the union set is returned. Origin: siddhi-core:5.1.8 Syntax OBJECT unionSet( OBJECT set) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic set The java.util.Set object that needs to be added into the union set. OBJECT No Yes Examples EXAMPLE 1 from stockStream select createSet(symbol) as initialSet insert into initStream from initStream#window.timeBatch(10 sec) select unionSet(initialSet) as distinctSymbols insert into distinctStockStream; distinctStockStream will return the set object which contains the distinct set of stock symbols received during a sliding window of 10 seconds.","title":"unionSet (Aggregate Function)"},{"location":"docs/api/5.1.1/#uuid-function","text":"Generates a UUID (Universally Unique Identifier). Origin: siddhi-core:5.1.8 Syntax STRING UUID() Examples EXAMPLE 1 from TempStream select convert(roomNo, 'string') as roomNo, temp, UUID() as messageID insert into RoomTempStream; This will converts a room number to string, introducing a message ID to each event asUUID() returns a34eec40-32c2-44fe-8075-7f4fde2e2dd8 from TempStream select convert(roomNo, 'string') as roomNo, temp, UUID() as messageID insert into RoomTempStream;","title":"UUID (Function)"},{"location":"docs/api/5.1.1/#cast-function","text":"Converts the first parameter according to the cast.to parameter. Incompatible arguments cause Class Cast exceptions if further processed. This function is used with map extension that returns attributes of the object type. You can use this function to cast the object to an accurate and concrete type. Origin: siddhi-core:5.1.8 Syntax INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT cast( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT to.be.caster, STRING cast.to) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic to.be.caster This specifies the attribute to be casted. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No Yes cast.to A string constant parameter expressing the cast to type using one of the following strings values: int, long, float, double, string, bool. STRING No Yes Examples EXAMPLE 1 from fooStream select symbol as name, cast(temp, 'double') as temp insert into barStream; This will cast the fooStream temp field value into 'double' format.","title":"cast (Function)"},{"location":"docs/api/5.1.1/#coalesce-function","text":"Returns the value of the first input parameter that is not null, and all input parameters have to be on the same type. Origin: siddhi-core:5.1.8 Syntax INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT coalesce( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg, INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT ...) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg This function accepts one or more parameters. They can belong to any one of the available types. All the specified parameters should be of the same type. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No Yes Examples EXAMPLE 1 from fooStream select coalesce('123', null, '789') as value insert into barStream; This will returns first null value 123. EXAMPLE 2 from fooStream select coalesce(null, 76, 567) as value insert into barStream; This will returns first null value 76. EXAMPLE 3 from fooStream select coalesce(null, null, null) as value insert into barStream; This will returns null as there are no notnull values.","title":"coalesce (Function)"},{"location":"docs/api/5.1.1/#convert-function","text":"Converts the first input parameter according to the convertedTo parameter. Origin: siddhi-core:5.1.8 Syntax INT|LONG|DOUBLE|FLOAT|STRING|BOOL convert( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT to.be.converted, STRING converted.to) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic to.be.converted This specifies the value to be converted. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No Yes converted.to A string constant parameter to which type the attribute need to be converted using one of the following strings values: 'int', 'long', 'float', 'double', 'string', 'bool'. STRING No Yes Examples EXAMPLE 1 from fooStream select convert(temp, 'double') as temp insert into barStream; This will convert fooStream temp value into 'double'. EXAMPLE 2 from fooStream select convert(temp, 'int') as temp insert into barStream; This will convert fooStream temp value into 'int' (value = \"convert(45.9, 'int') returns 46\").","title":"convert (Function)"},{"location":"docs/api/5.1.1/#createset-function","text":"Includes the given input parameter in a java.util.HashSet and returns the set. Origin: siddhi-core:5.1.8 Syntax OBJECT createSet( INT|LONG|DOUBLE|FLOAT|STRING|BOOL input) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input The input that needs to be added into the set. INT LONG DOUBLE FLOAT STRING BOOL No Yes Examples EXAMPLE 1 from stockStream select createSet(symbol) as initialSet insert into initStream; For every incoming stockStream event, the initStream stream will produce a set object having only one element: the symbol in the incoming stockStream.","title":"createSet (Function)"},{"location":"docs/api/5.1.1/#currenttimemillis-function","text":"Returns the current timestamp of siddhi application in milliseconds. Origin: siddhi-core:5.1.8 Syntax LONG currentTimeMillis() Examples EXAMPLE 1 from fooStream select symbol as name, currentTimeMillis() as eventTimestamp insert into barStream; This will extract current siddhi application timestamp.","title":"currentTimeMillis (Function)"},{"location":"docs/api/5.1.1/#default-function","text":"Checks if the 'attribute' parameter is null and if so returns the value of the 'default' parameter Origin: siddhi-core:5.1.8 Syntax INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT default( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT attribute, INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT default) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic attribute The attribute that could be null. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No Yes default The default value that will be used when 'attribute' parameter is null INT LONG DOUBLE FLOAT STRING BOOL OBJECT No Yes Examples EXAMPLE 1 from TempStream select default(temp, 0.0) as temp, roomNum insert into StandardTempStream; This will replace TempStream's temp attribute with default value if the temp is null.","title":"default (Function)"},{"location":"docs/api/5.1.1/#eventtimestamp-function","text":"Returns the timestamp of the processed event. Origin: siddhi-core:5.1.8 Syntax LONG eventTimestamp() Examples EXAMPLE 1 from fooStream select symbol as name, eventTimestamp() as eventTimestamp insert into barStream; This will extract current events timestamp.","title":"eventTimestamp (Function)"},{"location":"docs/api/5.1.1/#ifthenelse-function","text":"Evaluates the 'condition' parameter and returns value of the 'if.expression' parameter if the condition is true, or returns value of the 'else.expression' parameter if the condition is false. Here both 'if.expression' and 'else.expression' should be of the same type. Origin: siddhi-core:5.1.8 Syntax INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT ifThenElse( BOOL condition, INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT if.expression, INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT else.expression) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic condition This specifies the if then else condition value. BOOL No Yes if.expression This specifies the value to be returned if the value of the condition parameter is true. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No Yes else.expression This specifies the value to be returned if the value of the condition parameter is false. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No Yes Examples EXAMPLE 1 @info(name = 'query1') from sensorEventStream select sensorValue, ifThenElse(sensorValue 35,'High','Low') as status insert into outputStream; This will returns High if sensorValue = 50. EXAMPLE 2 @info(name = 'query1') from sensorEventStream select sensorValue, ifThenElse(voltage 5, 0, 1) as status insert into outputStream; This will returns 1 if voltage= 12. EXAMPLE 3 @info(name = 'query1') from userEventStream select userName, ifThenElse(password == 'admin', true, false) as passwordState insert into outputStream; This will returns passwordState as true if password = admin.","title":"ifThenElse (Function)"},{"location":"docs/api/5.1.1/#instanceofboolean-function","text":"Checks whether the parameter is an instance of Boolean or not. Origin: siddhi-core:5.1.8 Syntax BOOL instanceOfBoolean( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No Yes Examples EXAMPLE 1 from fooStream select instanceOfBoolean(switchState) as state insert into barStream; This will return true if the value of switchState is true. EXAMPLE 2 from fooStream select instanceOfBoolean(value) as state insert into barStream; if the value = 32 then this will returns false as the value is not an instance of the boolean.","title":"instanceOfBoolean (Function)"},{"location":"docs/api/5.1.1/#instanceofdouble-function","text":"Checks whether the parameter is an instance of Double or not. Origin: siddhi-core:5.1.8 Syntax BOOL instanceOfDouble( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No Yes Examples EXAMPLE 1 from fooStream select instanceOfDouble(value) as state insert into barStream; This will return true if the value field format is double ex : 56.45. EXAMPLE 2 from fooStream select instanceOfDouble(switchState) as state insert into barStream; if the switchState = true then this will returns false as the value is not an instance of the double.","title":"instanceOfDouble (Function)"},{"location":"docs/api/5.1.1/#instanceoffloat-function","text":"Checks whether the parameter is an instance of Float or not. Origin: siddhi-core:5.1.8 Syntax BOOL instanceOfFloat( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No Yes Examples EXAMPLE 1 from fooStream select instanceOfFloat(value) as state insert into barStream; This will return true if the value field format is float ex : 56.45f. EXAMPLE 2 from fooStream select instanceOfFloat(switchState) as state insert into barStream; if the switchState = true then this will returns false as the value is an instance of the boolean not a float.","title":"instanceOfFloat (Function)"},{"location":"docs/api/5.1.1/#instanceofinteger-function","text":"Checks whether the parameter is an instance of Integer or not. Origin: siddhi-core:5.1.8 Syntax BOOL instanceOfInteger( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No Yes Examples EXAMPLE 1 from fooStream select instanceOfInteger(value) as state insert into barStream; This will return true if the value field format is integer. EXAMPLE 2 from fooStream select instanceOfInteger(switchState) as state insert into barStream; if the switchState = true then this will returns false as the value is an instance of the boolean not a long.","title":"instanceOfInteger (Function)"},{"location":"docs/api/5.1.1/#instanceoflong-function","text":"Checks whether the parameter is an instance of Long or not. Origin: siddhi-core:5.1.8 Syntax BOOL instanceOfLong( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No Yes Examples EXAMPLE 1 from fooStream select instanceOfLong(value) as state insert into barStream; This will return true if the value field format is long ex : 56456l. EXAMPLE 2 from fooStream select instanceOfLong(switchState) as state insert into barStream; if the switchState = true then this will returns false as the value is an instance of the boolean not a long.","title":"instanceOfLong (Function)"},{"location":"docs/api/5.1.1/#instanceofstring-function","text":"Checks whether the parameter is an instance of String or not. Origin: siddhi-core:5.1.8 Syntax BOOL instanceOfString( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No Yes Examples EXAMPLE 1 from fooStream select instanceOfString(value) as state insert into barStream; This will return true if the value field format is string ex : 'test'. EXAMPLE 2 from fooStream select instanceOfString(switchState) as state insert into barStream; if the switchState = true then this will returns false as the value is an instance of the boolean not a string.","title":"instanceOfString (Function)"},{"location":"docs/api/5.1.1/#maximum-function","text":"Returns the maximum value of the input parameters. Origin: siddhi-core:5.1.8 Syntax INT|LONG|DOUBLE|FLOAT maximum( INT|LONG|DOUBLE|FLOAT arg, INT|LONG|DOUBLE|FLOAT ...) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg This function accepts one or more parameters. They can belong to any one of the available types. All the specified parameters should be of the same type. INT LONG DOUBLE FLOAT No Yes Examples EXAMPLE 1 @info(name = 'query1') from inputStream select maximum(price1, price2, price3) as max insert into outputStream; This will returns the maximum value of the input parameters price1, price2, price3.","title":"maximum (Function)"},{"location":"docs/api/5.1.1/#minimum-function","text":"Returns the minimum value of the input parameters. Origin: siddhi-core:5.1.8 Syntax INT|LONG|DOUBLE|FLOAT minimum( INT|LONG|DOUBLE|FLOAT arg, INT|LONG|DOUBLE|FLOAT ...) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg This function accepts one or more parameters. They can belong to any one of the available types. All the specified parameters should be of the same type. INT LONG DOUBLE FLOAT No Yes Examples EXAMPLE 1 @info(name = 'query1') from inputStream select maximum(price1, price2, price3) as max insert into outputStream; This will returns the minimum value of the input parameters price1, price2, price3.","title":"minimum (Function)"},{"location":"docs/api/5.1.1/#sizeofset-function","text":"Returns the size of an object of type java.util.Set. Origin: siddhi-core:5.1.8 Syntax INT sizeOfSet( OBJECT set) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic set The set object. This parameter should be of type java.util.Set. A set object may be created by the 'set' attribute aggregator in Siddhi. OBJECT No Yes Examples EXAMPLE 1 from stockStream select initSet(symbol) as initialSet insert into initStream; ;from initStream#window.timeBatch(10 sec) select union(initialSet) as distinctSymbols insert into distinctStockStream; from distinctStockStream select sizeOfSet(distinctSymbols) sizeOfSymbolSet insert into sizeStream; The sizeStream stream will output the number of distinct stock symbols received during a sliding window of 10 seconds.","title":"sizeOfSet (Function)"},{"location":"docs/api/5.1.1/#pol2cart-stream-function","text":"The pol2Cart function calculating the cartesian coordinates x y for the given theta, rho coordinates and adding them as new attributes to the existing events. Origin: siddhi-core:5.1.8 Syntax pol2Cart( DOUBLE theta, DOUBLE rho) pol2Cart( DOUBLE theta, DOUBLE rho, DOUBLE z) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic theta The theta value of the coordinates. DOUBLE No Yes rho The rho value of the coordinates. DOUBLE No Yes z z value of the cartesian coordinates. If z value is not given, drop the third parameter of the output. DOUBLE Yes Yes Examples EXAMPLE 1 from PolarStream#pol2Cart(theta, rho) select x, y insert into outputStream ; This will return cartesian coordinates (4.99953024681082, 0.06853693328228748) for theta: 0.7854 and rho: 5. EXAMPLE 2 from PolarStream#pol2Cart(theta, rho, 3.4) select x, y, z insert into outputStream ; This will return cartesian coordinates (4.99953024681082, 0.06853693328228748, 3.4)for theta: 0.7854 and rho: 5 and z: 3.4.","title":"pol2Cart (Stream Function)"},{"location":"docs/api/5.1.1/#log-stream-processor","text":"Logs the message on the given priority with or without the processed event. Origin: siddhi-core:5.1.8 Syntax log() log( STRING log.message) log( BOOL is.event.logged) log( STRING log.message, BOOL is.event.logged) log( STRING priority, STRING log.message) log( STRING priority, STRING log.message, BOOL is.event.logged) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic priority The priority/type of this log message (INFO, DEBUG, WARN, FATAL, ERROR, OFF, TRACE). INFO STRING Yes No log.message This message will be logged. : STRING Yes Yes is.event.logged To log the processed event. true BOOL Yes No Examples EXAMPLE 1 from FooStream#log() select * insert into BarStream; Logs events with SiddhiApp name message prefix on default log level INFO. EXAMPLE 2 from FooStream#log(\"Sample Event :\") select * insert into BarStream; Logs events with the message prefix \"Sample Event :\" on default log level INFO. EXAMPLE 3 from FooStream#log(\"DEBUG\", \"Sample Event :\", true) select * insert into BarStream; Logs events with the message prefix \"Sample Event :\" on log level DEBUG. EXAMPLE 4 from FooStream#log(\"Event Arrived\", false) select * insert into BarStream; For each event logs a message \"Event Arrived\" on default log level INFO. EXAMPLE 5 from FooStream#log(\"Sample Event :\", true) select * insert into BarStream; Logs events with the message prefix \"Sample Event :\" on default log level INFO. EXAMPLE 6 from FooStream#log(true) select * insert into BarStream; Logs events with on default log level INFO.","title":"log (Stream Processor)"},{"location":"docs/api/5.1.1/#batch-window","text":"A window that holds an incoming events batch. When a new set of events arrives, the previously arrived old events will be expired. Batch window can be used to aggregate events that comes in batches. If it has the parameter length specified, then batch window process the batch as several chunks. Origin: siddhi-core:5.1.8 Syntax batch() batch( INT window.length) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.length The length of a chunk If length value was not given it assign 0 as length and process the whole batch as once INT Yes No Examples EXAMPLE 1 define stream consumerItemStream (itemId string, price float) from consumerItemStream#window.batch() select price, str:groupConcat(itemId) as itemIds group by price insert into outputStream; This will output comma separated items IDs that have the same price for each incoming batch of events.","title":"batch (Window)"},{"location":"docs/api/5.1.1/#cron-window","text":"This window outputs the arriving events as and when they arrive, and resets (expires) the window periodically based on the given cron expression. Origin: siddhi-core:5.1.8 Syntax cron( STRING cron.expression) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic cron.expression The cron expression that resets the window. STRING No No Examples EXAMPLE 1 define stream InputEventStream (symbol string, price float, volume int); @info(name = 'query1') from InputEventStream#cron('*/5 * * * * ?') select symbol, sum(price) as totalPrice insert into OutputStream; This let the totalPrice to gradually increase and resets to zero as a batch every 5 seconds. EXAMPLE 2 define stream StockEventStream (symbol string, price float, volume int) define window StockEventWindow (symbol string, price float, volume int) cron('*/5 * * * * ?'); @info(name = 'query0') from StockEventStream insert into StockEventWindow; @info(name = 'query1') from StockEventWindow select symbol, sum(price) as totalPrice insert into OutputStream ; The defined window will let the totalPrice to gradually increase and resets to zero as a batch every 5 seconds.","title":"cron (Window)"},{"location":"docs/api/5.1.1/#delay-window","text":"A delay window holds events for a specific time period that is regarded as a delay period before processing them. Origin: siddhi-core:5.1.8 Syntax delay( INT|LONG|TIME window.delay) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.delay The time period (specified in sec, min, ms) for which the window should delay the events. INT LONG TIME No No Examples EXAMPLE 1 define window delayWindow(symbol string, volume int) delay(1 hour); define stream PurchaseStream(symbol string, volume int); define stream DeliveryStream(symbol string); define stream OutputStream(symbol string); @info(name='query1') from PurchaseStream select symbol, volume insert into delayWindow; @info(name='query2') from delayWindow join DeliveryStream on delayWindow.symbol == DeliveryStream.symbol select delayWindow.symbol insert into OutputStream; In this example, purchase events that arrive in the 'PurchaseStream' stream are directed to a delay window. At any given time, this delay window holds purchase events that have arrived within the last hour. These purchase events in the window are matched by the 'symbol' attribute, with delivery events that arrive in the 'DeliveryStream' stream. This monitors whether the delivery of products is done with a minimum delay of one hour after the purchase.","title":"delay (Window)"},{"location":"docs/api/5.1.1/#externaltime-window","text":"A sliding time window based on external time. It holds events that arrived during the last windowTime period from the external timestamp, and gets updated on every monotonically increasing timestamp. Origin: siddhi-core:5.1.8 Syntax externalTime( LONG timestamp, INT|LONG|TIME window.time) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic timestamp The time which the window determines as current time and will act upon. The value of this parameter should be monotonically increasing. LONG No Yes window.time The sliding time period for which the window should hold events. INT LONG TIME No No Examples EXAMPLE 1 define window cseEventWindow (symbol string, price float, volume int) externalTime(eventTime, 20 sec) output expired events; @info(name = 'query0') from cseEventStream insert into cseEventWindow; @info(name = 'query1') from cseEventWindow select symbol, sum(price) as price insert expired events into outputStream ; processing events arrived within the last 20 seconds from the eventTime and output expired events.","title":"externalTime (Window)"},{"location":"docs/api/5.1.1/#externaltimebatch-window","text":"A batch (tumbling) time window based on external time, that holds events arrived during windowTime periods, and gets updated for every windowTime. Origin: siddhi-core:5.1.8 Syntax externalTimeBatch( LONG timestamp, INT|LONG|TIME window.time) externalTimeBatch( LONG timestamp, INT|LONG|TIME window.time, INT|LONG|TIME start.time) externalTimeBatch( LONG timestamp, INT|LONG|TIME window.time, INT|LONG|TIME start.time, INT|LONG|TIME timeout) externalTimeBatch( LONG timestamp, INT|LONG|TIME window.time, INT|LONG|TIME start.time, INT|LONG|TIME timeout, BOOL replace.with.batchtime) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic timestamp The time which the window determines as current time and will act upon. The value of this parameter should be monotonically increasing. LONG No Yes window.time The batch time period for which the window should hold events. INT LONG TIME No No start.time User defined start time. This could either be a constant (of type int, long or time) or an attribute of the corresponding stream (of type long). If an attribute is provided, initial value of attribute would be considered as startTime. Timestamp of first event INT LONG TIME Yes Yes timeout Time to wait for arrival of new event, before flushing and giving output for events belonging to a specific batch. System waits till an event from next batch arrives to flush current batch INT LONG TIME Yes No replace.with.batchtime This indicates to replace the expired event timeStamp as the batch end timeStamp System waits till an event from next batch arrives to flush current batch BOOL Yes No Examples EXAMPLE 1 define window cseEventWindow (symbol string, price float, volume int) externalTimeBatch(eventTime, 1 sec) output expired events; @info(name = 'query0') from cseEventStream insert into cseEventWindow; @info(name = 'query1') from cseEventWindow select symbol, sum(price) as price insert expired events into outputStream ; This will processing events that arrive every 1 seconds from the eventTime. EXAMPLE 2 define window cseEventWindow (symbol string, price float, volume int) externalTimeBatch(eventTime, 20 sec, 0) output expired events; This will processing events that arrive every 1 seconds from the eventTime. Starts on 0 th millisecond of an hour. EXAMPLE 3 define window cseEventWindow (symbol string, price float, volume int) externalTimeBatch(eventTime, 2 sec, eventTimestamp, 100) output expired events; This will processing events that arrive every 2 seconds from the eventTim. Considers the first event's eventTimestamp value as startTime. Waits 100 milliseconds for the arrival of a new event before flushing current batch.","title":"externalTimeBatch (Window)"},{"location":"docs/api/5.1.1/#frequent-window","text":"Deprecated This window returns the latest events with the most frequently occurred value for a given attribute(s). Frequency calculation for this window processor is based on Misra-Gries counting algorithm. Origin: siddhi-core:5.1.8 Syntax frequent( INT event.count) frequent( INT event.count, STRING attribute) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic event.count The number of most frequent events to be emitted to the stream. INT No No attribute The attributes to group the events. If no attributes are given, the concatenation of all the attributes of the event is considered. The concatenation of all the attributes of the event is considered. STRING Yes Yes Examples EXAMPLE 1 @info(name = 'query1') from purchase[price = 30]#window.frequent(2) select cardNo, price insert all events into PotentialFraud; This will returns the 2 most frequent events. EXAMPLE 2 @info(name = 'query1') from purchase[price = 30]#window.frequent(2, cardNo) select cardNo, price insert all events into PotentialFraud; This will returns the 2 latest events with the most frequently appeared card numbers.","title":"frequent (Window)"},{"location":"docs/api/5.1.1/#length-window","text":"A sliding length window that holds the last 'window.length' events at a given time, and gets updated for each arrival and expiry. Origin: siddhi-core:5.1.8 Syntax length( INT window.length) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.length The number of events that should be included in a sliding length window. INT No No Examples EXAMPLE 1 define window StockEventWindow (symbol string, price float, volume int) length(10) output all events; @info(name = 'query0') from StockEventStream insert into StockEventWindow; @info(name = 'query1') from StockEventWindow select symbol, sum(price) as price insert all events into outputStream ; This will process last 10 events in a sliding manner.","title":"length (Window)"},{"location":"docs/api/5.1.1/#lengthbatch-window","text":"A batch (tumbling) length window that holds and process a number of events as specified in the window.length. Origin: siddhi-core:5.1.8 Syntax lengthBatch( INT window.length) lengthBatch( INT window.length, BOOL stream.current.event) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.length The number of events the window should tumble. INT No No stream.current.event Let the window stream the current events out as and when they arrive to the window while expiring them in batches. false BOOL Yes No Examples EXAMPLE 1 define stream InputEventStream (symbol string, price float, volume int); @info(name = 'query1') from InputEventStream#lengthBatch(10) select symbol, sum(price) as price insert into OutputStream; This collect and process 10 events as a batch and output them. EXAMPLE 2 define stream InputEventStream (symbol string, price float, volume int); @info(name = 'query1') from InputEventStream#lengthBatch(10, true) select symbol, sum(price) as sumPrice insert into OutputStream; This window sends the arriving events directly to the output letting the sumPrice to increase gradually, after every 10 events it clears the window as a batch and resets the sumPrice to zero. EXAMPLE 3 define stream InputEventStream (symbol string, price float, volume int); define window StockEventWindow (symbol string, price float, volume int) lengthBatch(10) output all events; @info(name = 'query0') from InputEventStream insert into StockEventWindow; @info(name = 'query1') from StockEventWindow select symbol, sum(price) as price insert all events into OutputStream ; This uses an defined window to process 10 events as a batch and output all events.","title":"lengthBatch (Window)"},{"location":"docs/api/5.1.1/#lossyfrequent-window","text":"Deprecated This window identifies and returns all the events of which the current frequency exceeds the value specified for the supportThreshold parameter. Origin: siddhi-core:5.1.8 Syntax lossyFrequent( DOUBLE support.threshold) lossyFrequent( DOUBLE support.threshold, DOUBLE error.bound) lossyFrequent( DOUBLE support.threshold, DOUBLE error.bound, STRING attribute) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic support.threshold The support threshold value. DOUBLE No No error.bound The error bound value. support.threshold /10 DOUBLE Yes No attribute The attributes to group the events. If no attributes are given, the concatenation of all the attributes of the event is considered. The concatenation of all the attributes of the event is considered. STRING Yes Yes Examples EXAMPLE 1 define stream purchase (cardNo string, price float); define window purchaseWindow (cardNo string, price float) lossyFrequent(0.1, 0.01); @info(name = 'query0') from purchase[price = 30] insert into purchaseWindow; @info(name = 'query1') from purchaseWindow select cardNo, price insert all events into PotentialFraud; lossyFrequent(0.1, 0.01) returns all the events of which the current frequency exceeds 0.1, with an error bound of 0.01. EXAMPLE 2 define stream purchase (cardNo string, price float); define window purchaseWindow (cardNo string, price float) lossyFrequent(0.3, 0.05, cardNo); @info(name = 'query0') from purchase[price = 30] insert into purchaseWindow; @info(name = 'query1') from purchaseWindow select cardNo, price insert all events into PotentialFraud; lossyFrequent(0.3, 0.05, cardNo) returns all the events of which the cardNo attributes frequency exceeds 0.3, with an error bound of 0.05.","title":"lossyFrequent (Window)"},{"location":"docs/api/5.1.1/#session-window","text":"This is a session window that holds events that belong to a specific session. The events that belong to a specific session are identified by a grouping attribute (i.e., a session key). A session gap period is specified to determine the time period after which the session is considered to be expired. A new event that arrives with a specific value for the session key is matched with the session window with the same session key. There can be out of order and late arrival of events, these events can arrive after the session is expired, to include those events to the matching session key specify a latency time period that is less than the session gap period.To have aggregate functions with session windows, the events need to be grouped by the session key via a 'group by' clause. Origin: siddhi-core:5.1.8 Syntax session( INT|LONG|TIME window.session) session( INT|LONG|TIME window.session, STRING window.key) session( INT|LONG|TIME window.session, STRING window.key, INT|LONG|TIME window.allowed.latency) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.session The time period for which the session considered is valid. This is specified in seconds, minutes, or milliseconds (i.e., 'min', 'sec', or 'ms'. INT LONG TIME No No window.key The grouping attribute for events. default-key STRING Yes Yes window.allowed.latency This specifies the time period for which the session window is valid after the expiration of the session. The time period specified here should be less than the session time gap (which is specified via the 'window.session' parameter). 0 INT LONG TIME Yes No Examples EXAMPLE 1 define stream PurchaseEventStream (user string, item_number int, price float, quantity int); @info(name='query0) from PurchaseEventStream#window.session(5 sec, user, 2 sec) select * insert all events into OutputStream; This query processes events that arrive at the PurchaseEvent input stream. The 'user' attribute is the session key, and the session gap is 5 seconds. '2 sec' is specified as the allowed latency. Therefore, events with the matching user name that arrive 2 seconds after the expiration of the session are also considered when performing aggregations for the session identified by the given user name.","title":"session (Window)"},{"location":"docs/api/5.1.1/#sort-window","text":"This window holds a batch of events that equal the number specified as the windowLength and sorts them in the given order. Origin: siddhi-core:5.1.8 Syntax sort( INT window.length, STRING|DOUBLE|INT|LONG|FLOAT|LONG attribute) sort( INT window.length, STRING|DOUBLE|INT|LONG|FLOAT|LONG attribute, STRING order, STRING ...) sort( INT window.length, STRING|DOUBLE|INT|LONG|FLOAT|LONG attribute, STRING order, STRING|DOUBLE|INT|LONG|FLOAT|LONG attribute, STRING|DOUBLE|INT|LONG|FLOAT|LONG ...) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.length The size of the window length. INT No No attribute The attribute that should be checked for the order. The concatenation of all the attributes of the event is considered. STRING DOUBLE INT LONG FLOAT LONG No Yes order The order define as \"asc\" or \"desc\". asc STRING Yes No Examples EXAMPLE 1 define stream cseEventStream (symbol string, price float, volume long); define window cseEventWindow (symbol string, price float, volume long) sort(2,volume, 'asc'); @info(name = 'query0') from cseEventStream insert into cseEventWindow; @info(name = 'query1') from cseEventWindow select volume insert all events into outputStream ; sort(5, price, 'asc') keeps the events sorted by price in the ascending order. Therefore, at any given time, the window contains the 5 lowest prices.","title":"sort (Window)"},{"location":"docs/api/5.1.1/#time-window","text":"A sliding time window that holds events that arrived during the last windowTime period at a given time, and gets updated for each event arrival and expiry. Origin: siddhi-core:5.1.8 Syntax time( INT|LONG|TIME window.time) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.time The sliding time period for which the window should hold events. INT LONG TIME No No Examples EXAMPLE 1 define window cseEventWindow (symbol string, price float, volume int) time(20) output all events; @info(name = 'query0') from cseEventStream insert into cseEventWindow; @info(name = 'query1') from cseEventWindow select symbol, sum(price) as price insert all events into outputStream ; This will processing events that arrived within the last 20 milliseconds.","title":"time (Window)"},{"location":"docs/api/5.1.1/#timebatch-window","text":"A batch (tumbling) time window that holds and process events that arrive during 'window.time' period as a batch. Origin: siddhi-core:5.1.8 Syntax timeBatch( INT|LONG|TIME window.time) timeBatch( INT|LONG|TIME window.time, INT|LONG start.time) timeBatch( INT|LONG|TIME window.time, BOOL stream.current.event) timeBatch( INT|LONG|TIME window.time, INT|LONG start.time, BOOL stream.current.event) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.time The batch time period in which the window process the events. INT LONG TIME No No start.time This specifies an offset in milliseconds in order to start the window at a time different to the standard time. Timestamp of first event INT LONG Yes No stream.current.event Let the window stream the current events out as and when they arrive to the window while expiring them in batches. false BOOL Yes No Examples EXAMPLE 1 define stream InputEventStream (symbol string, price float, volume int); @info(name = 'query1') from InputEventStream#timeBatch(20 sec) select symbol, sum(price) as price insert into OutputStream; This collect and process incoming events as a batch every 20 seconds and output them. EXAMPLE 2 define stream InputEventStream (symbol string, price float, volume int); @info(name = 'query1') from InputEventStream#timeBatch(20 sec, true) select symbol, sum(price) as sumPrice insert into OutputStream; This window sends the arriving events directly to the output letting the sumPrice to increase gradually and on every 20 second interval it clears the window as a batch resetting the sumPrice to zero. EXAMPLE 3 define stream InputEventStream (symbol string, price float, volume int); define window StockEventWindow (symbol string, price float, volume int) timeBatch(20 sec) output all events; @info(name = 'query0') from InputEventStream insert into StockEventWindow; @info(name = 'query1') from StockEventWindow select symbol, sum(price) as price insert all events into OutputStream ; This uses an defined window to process events arrived every 20 seconds as a batch and output all events.","title":"timeBatch (Window)"},{"location":"docs/api/5.1.1/#timelength-window","text":"A sliding time window that, at a given time holds the last window.length events that arrived during last window.time period, and gets updated for every event arrival and expiry. Origin: siddhi-core:5.1.8 Syntax timeLength( INT|LONG|TIME window.time, INT window.length) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.time The sliding time period for which the window should hold events. INT LONG TIME No No window.length The number of events that should be be included in a sliding length window.. INT No No Examples EXAMPLE 1 define stream cseEventStream (symbol string, price float, volume int); define window cseEventWindow (symbol string, price float, volume int) timeLength(2 sec, 10); @info(name = 'query0') from cseEventStream insert into cseEventWindow; @info(name = 'query1') from cseEventWindow select symbol, price, volume insert all events into outputStream; window.timeLength(2 sec, 10) holds the last 10 events that arrived during last 2 seconds and gets updated for every event arrival and expiry.","title":"timeLength (Window)"},{"location":"docs/api/5.1.1/#js","text":"","title":"Js"},{"location":"docs/api/5.1.1/#eval-function","text":"This extension evaluates a given string and return the output according to the user specified data type. Origin: siddhi-script-js:5.0.2 Syntax INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT js:eval( STRING expression, STRING return.type) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic expression Any single line js expression or function. STRING No Yes return.type The return type of the evaluated expression. Supported types are int|long|float|double|bool|string. STRING No No Examples EXAMPLE 1 js:eval(\"700 800\", 'bool') In this example, the expression 700 800 will be evaluated and return result as false because user specified return type as bool.","title":"eval (Function)"},{"location":"docs/api/5.1.1/#json","text":"","title":"Json"},{"location":"docs/api/5.1.1/#group-aggregate-function","text":"This function aggregates the JSON elements and returns a JSON object by adding enclosing.element if it is provided. If enclosing.element is not provided it aggregate the JSON elements returns a JSON array. Origin: siddhi-execution-json:2.0.4 Syntax OBJECT json:group( STRING|OBJECT json) OBJECT json:group( STRING|OBJECT json, BOOL distinct) OBJECT json:group( STRING|OBJECT json, STRING enclosing.element) OBJECT json:group( STRING|OBJECT json, STRING enclosing.element, BOOL distinct) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json The JSON element that needs to be aggregated. STRING OBJECT No Yes enclosing.element The JSON element used to enclose the aggregated JSON elements. EMPTY_STRING STRING Yes Yes distinct This is used to only have distinct JSON elements in the concatenated JSON object/array that is returned. false BOOL Yes Yes Examples EXAMPLE 1 from InputStream#window.length(5) select json:group(\"json\") as groupedJSONArray input OutputStream; When we input events having values for the json as {\"date\":\"2013-11-19\",\"time\":\"10:30\"} and {\"date\":\"2013-11-19\",\"time\":\"12:20\"} , it returns [{\"date\":\"2013-11-19\",\"time\":\"10:30\"}{\"date\":\"2013-11-19\",\"time\":\"12:20\"}] to the 'OutputStream'. EXAMPLE 2 from InputStream#window.length(5) select json:group(\"json\", true) as groupedJSONArray input OutputStream; When we input events having values for the json as {\"date\":\"2013-11-19\",\"time\":\"10:30\"} and {\"date\":\"2013-11-19\",\"time\":\"10:30\"} , it returns [{\"date\":\"2013-11-19\",\"time\":\"10:30\"}] to the 'OutputStream'. EXAMPLE 3 from InputStream#window.length(5) select json:group(\"json\", \"result\") as groupedJSONArray input OutputStream; When we input events having values for the json as {\"date\":\"2013-11-19\",\"time\":\"10:30\"} and {\"date\":\"2013-11-19\",\"time\":\"12:20\"} , it returns {\"result\":[{\"date\":\"2013-11-19\",\"time\":\"10:30\"},{\"date\":\"2013-11-19\",\"time\":\"12:20\"}} to the 'OutputStream'. EXAMPLE 4 from InputStream#window.length(5) select json:group(\"json\", \"result\", true) as groupedJSONArray input OutputStream; When we input events having values for the json as {\"date\":\"2013-11-19\",\"time\":\"10:30\"} and {\"date\":\"2013-11-19\",\"time\":\"10:30\"} , it returns {\"result\":[{\"date\":\"2013-11-19\",\"time\":\"10:30\"}]} to the 'OutputStream'.","title":"group (Aggregate Function)"},{"location":"docs/api/5.1.1/#groupasobject-aggregate-function","text":"This function aggregates the JSON elements and returns a JSON object by adding enclosing.element if it is provided. If enclosing.element is not provided it aggregate the JSON elements returns a JSON array. Origin: siddhi-execution-json:2.0.4 Syntax OBJECT json:groupAsObject( STRING|OBJECT json) OBJECT json:groupAsObject( STRING|OBJECT json, BOOL distinct) OBJECT json:groupAsObject( STRING|OBJECT json, STRING enclosing.element) OBJECT json:groupAsObject( STRING|OBJECT json, STRING enclosing.element, BOOL distinct) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json The JSON element that needs to be aggregated. STRING OBJECT No Yes enclosing.element The JSON element used to enclose the aggregated JSON elements. EMPTY_STRING STRING Yes Yes distinct This is used to only have distinct JSON elements in the concatenated JSON object/array that is returned. false BOOL Yes Yes Examples EXAMPLE 1 from InputStream#window.length(5) select json:groupAsObject(\"json\") as groupedJSONArray input OutputStream; When we input events having values for the json as {\"date\":\"2013-11-19\",\"time\":\"10:30\"} and {\"date\":\"2013-11-19\",\"time\":\"12:20\"} , it returns [{\"date\":\"2013-11-19\",\"time\":\"10:30\"}{\"date\":\"2013-11-19\",\"time\":\"12:20\"}] to the 'OutputStream'. EXAMPLE 2 from InputStream#window.length(5) select json:groupAsObject(\"json\", true) as groupedJSONArray input OutputStream; When we input events having values for the json as {\"date\":\"2013-11-19\",\"time\":\"10:30\"} and {\"date\":\"2013-11-19\",\"time\":\"10:30\"} , it returns [{\"date\":\"2013-11-19\",\"time\":\"10:30\"}] to the 'OutputStream'. EXAMPLE 3 from InputStream#window.length(5) select json:groupAsObject(\"json\", \"result\") as groupedJSONArray input OutputStream; When we input events having values for the json as {\"date\":\"2013-11-19\",\"time\":\"10:30\"} and {\"date\":\"2013-11-19\",\"time\":\"12:20\"} , it returns {\"result\":[{\"date\":\"2013-11-19\",\"time\":\"10:30\"},{\"date\":\"2013-11-19\",\"time\":\"12:20\"}} to the 'OutputStream'. EXAMPLE 4 from InputStream#window.length(5) select json:groupAsObject(\"json\", \"result\", true) as groupedJSONArray input OutputStream; When we input events having values for the json as {\"date\":\"2013-11-19\",\"time\":\"10:30\"} and {\"date\":\"2013-11-19\",\"time\":\"10:30\"} , it returns {\"result\":[{\"date\":\"2013-11-19\",\"time\":\"10:30\"}]} to the 'OutputStream'.","title":"groupAsObject (Aggregate Function)"},{"location":"docs/api/5.1.1/#getbool-function","text":"Function retrieves the 'boolean' value specified in the given path of the JSON element. Origin: siddhi-execution-json:2.0.4 Syntax BOOL json:getBool( STRING|OBJECT json, STRING path) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json The JSON input containing boolean value. STRING OBJECT No Yes path The JSON path to fetch the boolean value. STRING No Yes Examples EXAMPLE 1 json:getBool(json,'$.married') If the json is the format {'name' : 'John', 'married' : true} , the function returns true as there is a matching boolean at .married /code . /p p /p span id=\"example-2\" class=\"md-typeset\" style=\"display: block; color: rgba(0, 0, 0, 0.54); font-size: 12.8px; font-weight: bold;\" EXAMPLE 2 /span json:getBool(json,'$.name') p /p p style=\"word-wrap: break-word;margin: 0;\" If the code json /code is the format code {'name' : 'John', 'married' : true} /code , the function returns code null /code as there is no matching boolean at code .married</code>.</p> <p></p> <span id=\"example-2\" class=\"md-typeset\" style=\"display: block; color: rgba(0, 0, 0, 0.54); font-size: 12.8px; font-weight: bold;\">EXAMPLE 2</span> <pre class=\"codehilite\"><code>json:getBool(json,'$.name')</code></pre> <p></p> <p style=\"word-wrap: break-word;margin: 0;\">If the <code>json</code> is the format <code>{'name' : 'John', 'married' : true}</code>, the function returns <code>null</code> as there is no matching boolean at <code> .name . EXAMPLE 3 json:getBool(json,'$.foo') If the json is the format {'name' : 'John', 'married' : true} , the function returns null as there is no matching element at $.foo .","title":"getBool (Function)"},{"location":"docs/api/5.1.1/#getdouble-function","text":"Function retrieves the 'double' value specified in the given path of the JSON element. Origin: siddhi-execution-json:2.0.4 Syntax DOUBLE json:getDouble( STRING|OBJECT json, STRING path) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json The JSON input containing double value. STRING OBJECT No Yes path The JSON path to fetch the double value. STRING No Yes Examples EXAMPLE 1 json:getDouble(json,'$.salary') If the json is the format {'name' : 'John', 'salary' : 12000.0} , the function returns 12000.0 as there is a matching double at .salary /code . /p p /p span id=\"example-2\" class=\"md-typeset\" style=\"display: block; color: rgba(0, 0, 0, 0.54); font-size: 12.8px; font-weight: bold;\" EXAMPLE 2 /span json:getDouble(json,'$.salary') p /p p style=\"word-wrap: break-word;margin: 0;\" If the code json /code is the format code {'name' : 'John', 'age' : 23} /code , the function returns code null /code as there are no matching element at code .salary</code>.</p> <p></p> <span id=\"example-2\" class=\"md-typeset\" style=\"display: block; color: rgba(0, 0, 0, 0.54); font-size: 12.8px; font-weight: bold;\">EXAMPLE 2</span> <pre class=\"codehilite\"><code>json:getDouble(json,'$.salary')</code></pre> <p></p> <p style=\"word-wrap: break-word;margin: 0;\">If the <code>json</code> is the format <code>{'name' : 'John', 'age' : 23}</code>, the function returns <code>null</code> as there are no matching element at <code> .salary . EXAMPLE 3 json:getDouble(json,'$.name') If the json is the format {'name' : 'John', 'age' : 23} , the function returns null as there are no matching double at $.name .","title":"getDouble (Function)"},{"location":"docs/api/5.1.1/#getfloat-function","text":"Function retrieves the 'float' value specified in the given path of the JSON element. Origin: siddhi-execution-json:2.0.4 Syntax FLOAT json:getFloat( STRING|OBJECT json, STRING path) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json The JSON input containing float value. STRING OBJECT No Yes path The JSON path to fetch the float value. STRING No Yes Examples EXAMPLE 1 json:getFloat(json,'$.salary') If the json is the format {'name' : 'John', 'salary' : 12000.0} , the function returns 12000 as there is a matching float at .salary /code . /p p /p span id=\"example-2\" class=\"md-typeset\" style=\"display: block; color: rgba(0, 0, 0, 0.54); font-size: 12.8px; font-weight: bold;\" EXAMPLE 2 /span json:getFloat(json,'$.salary') p /p p style=\"word-wrap: break-word;margin: 0;\" If the code json /code is the format code {'name' : 'John', 'age' : 23} /code , the function returns code null /code as there are no matching element at code .salary</code>.</p> <p></p> <span id=\"example-2\" class=\"md-typeset\" style=\"display: block; color: rgba(0, 0, 0, 0.54); font-size: 12.8px; font-weight: bold;\">EXAMPLE 2</span> <pre class=\"codehilite\"><code>json:getFloat(json,'$.salary')</code></pre> <p></p> <p style=\"word-wrap: break-word;margin: 0;\">If the <code>json</code> is the format <code>{'name' : 'John', 'age' : 23}</code>, the function returns <code>null</code> as there are no matching element at <code> .salary . EXAMPLE 3 json:getFloat(json,'$.name') If the json is the format {'name' : 'John', 'age' : 23} , the function returns null as there are no matching float at $.name .","title":"getFloat (Function)"},{"location":"docs/api/5.1.1/#getint-function","text":"Function retrieves the 'int' value specified in the given path of the JSON element. Origin: siddhi-execution-json:2.0.4 Syntax INT json:getInt( STRING|OBJECT json, STRING path) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json The JSON input containing int value. STRING OBJECT No Yes path The JSON path to fetch the int value. STRING No Yes Examples EXAMPLE 1 json:getInt(json,'$.age') If the json is the format {'name' : 'John', 'age' : 23} , the function returns 23 as there is a matching int at .age /code . /p p /p span id=\"example-2\" class=\"md-typeset\" style=\"display: block; color: rgba(0, 0, 0, 0.54); font-size: 12.8px; font-weight: bold;\" EXAMPLE 2 /span json:getInt(json,'$.salary') p /p p style=\"word-wrap: break-word;margin: 0;\" If the code json /code is the format code {'name' : 'John', 'age' : 23} /code , the function returns code null /code as there are no matching element at code .age</code>.</p> <p></p> <span id=\"example-2\" class=\"md-typeset\" style=\"display: block; color: rgba(0, 0, 0, 0.54); font-size: 12.8px; font-weight: bold;\">EXAMPLE 2</span> <pre class=\"codehilite\"><code>json:getInt(json,'$.salary')</code></pre> <p></p> <p style=\"word-wrap: break-word;margin: 0;\">If the <code>json</code> is the format <code>{'name' : 'John', 'age' : 23}</code>, the function returns <code>null</code> as there are no matching element at <code> .salary . EXAMPLE 3 json:getInt(json,'$.name') If the json is the format {'name' : 'John', 'age' : 23} , the function returns null as there are no matching int at $.name .","title":"getInt (Function)"},{"location":"docs/api/5.1.1/#getlong-function","text":"Function retrieves the 'long' value specified in the given path of the JSON element. Origin: siddhi-execution-json:2.0.4 Syntax LONG json:getLong( STRING|OBJECT json, STRING path) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json The JSON input containing long value. STRING OBJECT No Yes path The JSON path to fetch the long value. STRING No Yes Examples EXAMPLE 1 json:getLong(json,'$.age') If the json is the format {'name' : 'John', 'age' : 23} , the function returns 23 as there is a matching long at .age /code . /p p /p span id=\"example-2\" class=\"md-typeset\" style=\"display: block; color: rgba(0, 0, 0, 0.54); font-size: 12.8px; font-weight: bold;\" EXAMPLE 2 /span json:getLong(json,'$.salary') p /p p style=\"word-wrap: break-word;margin: 0;\" If the code json /code is the format code {'name' : 'John', 'age' : 23} /code , the function returns code null /code as there are no matching element at code .age</code>.</p> <p></p> <span id=\"example-2\" class=\"md-typeset\" style=\"display: block; color: rgba(0, 0, 0, 0.54); font-size: 12.8px; font-weight: bold;\">EXAMPLE 2</span> <pre class=\"codehilite\"><code>json:getLong(json,'$.salary')</code></pre> <p></p> <p style=\"word-wrap: break-word;margin: 0;\">If the <code>json</code> is the format <code>{'name' : 'John', 'age' : 23}</code>, the function returns <code>null</code> as there are no matching element at <code> .salary . EXAMPLE 3 json:getLong(json,'$.name') If the json is the format {'name' : 'John', 'age' : 23} , the function returns null as there are no matching long at $.name .","title":"getLong (Function)"},{"location":"docs/api/5.1.1/#getobject-function","text":"Function retrieves the object specified in the given path of the JSON element. Origin: siddhi-execution-json:2.0.4 Syntax OBJECT json:getObject( STRING|OBJECT json, STRING path) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json The JSON input containing the object. STRING OBJECT No Yes path The JSON path to fetch the object. STRING No Yes Examples EXAMPLE 1 json:getObject(json,'$.address') If the json is the format {'name' : 'John', 'address' : {'city' : 'NY', 'country' : 'USA'}} , the function returns {'city' : 'NY', 'country' : 'USA'} as there is a matching object at .address /code . /p p /p span id=\"example-2\" class=\"md-typeset\" style=\"display: block; color: rgba(0, 0, 0, 0.54); font-size: 12.8px; font-weight: bold;\" EXAMPLE 2 /span json:getObject(json,'$.age') p /p p style=\"word-wrap: break-word;margin: 0;\" If the code json /code is the format code {'name' : 'John', 'age' : 23} /code , the function returns code 23 /code as there is a matching object at code .address</code>.</p> <p></p> <span id=\"example-2\" class=\"md-typeset\" style=\"display: block; color: rgba(0, 0, 0, 0.54); font-size: 12.8px; font-weight: bold;\">EXAMPLE 2</span> <pre class=\"codehilite\"><code>json:getObject(json,'$.age')</code></pre> <p></p> <p style=\"word-wrap: break-word;margin: 0;\">If the <code>json</code> is the format <code>{'name' : 'John', 'age' : 23}</code>, the function returns <code>23</code> as there is a matching object at <code> .age . EXAMPLE 3 json:getObject(json,'$.salary') If the json is the format {'name' : 'John', 'age' : 23} , the function returns null as there are no matching element at $.salary .","title":"getObject (Function)"},{"location":"docs/api/5.1.1/#getstring-function","text":"Function retrieves value specified in the given path of the JSON element as a string. Origin: siddhi-execution-json:2.0.4 Syntax STRING json:getString( STRING|OBJECT json, STRING path) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json The JSON input containing value. STRING OBJECT No Yes path The JSON path to fetch the value. STRING No Yes Examples EXAMPLE 1 json:getString(json,'$.name') If the json is the format {'name' : 'John', 'age' : 23} , the function returns John as there is a matching string at .name /code . /p p /p span id=\"example-2\" class=\"md-typeset\" style=\"display: block; color: rgba(0, 0, 0, 0.54); font-size: 12.8px; font-weight: bold;\" EXAMPLE 2 /span json:getString(json,'$.salary') p /p p style=\"word-wrap: break-word;margin: 0;\" If the code json /code is the format code {'name' : 'John', 'age' : 23} /code , the function returns code null /code as there are no matching element at code .name</code>.</p> <p></p> <span id=\"example-2\" class=\"md-typeset\" style=\"display: block; color: rgba(0, 0, 0, 0.54); font-size: 12.8px; font-weight: bold;\">EXAMPLE 2</span> <pre class=\"codehilite\"><code>json:getString(json,'$.salary')</code></pre> <p></p> <p style=\"word-wrap: break-word;margin: 0;\">If the <code>json</code> is the format <code>{'name' : 'John', 'age' : 23}</code>, the function returns <code>null</code> as there are no matching element at <code> .salary . EXAMPLE 3 json:getString(json,'$.age') If the json is the format {'name' : 'John', 'age' : 23} , the function returns 23 as a string as there is a matching element at .age /code . /p p /p span id=\"example-4\" class=\"md-typeset\" style=\"display: block; color: rgba(0, 0, 0, 0.54); font-size: 12.8px; font-weight: bold;\" EXAMPLE 4 /span json:getString(json,'$.address') p /p p style=\"word-wrap: break-word;margin: 0;\" If the code json /code is the format code {'name' : 'John', 'address' : {'city' : 'NY', 'country' : 'USA'}} /code , the function returns code {'city' : 'NY', 'country' : 'USA'} /code as a string as there is a matching element at code .age</code>.</p> <p></p> <span id=\"example-4\" class=\"md-typeset\" style=\"display: block; color: rgba(0, 0, 0, 0.54); font-size: 12.8px; font-weight: bold;\">EXAMPLE 4</span> <pre class=\"codehilite\"><code>json:getString(json,'$.address')</code></pre> <p></p> <p style=\"word-wrap: break-word;margin: 0;\">If the <code>json</code> is the format <code>{'name' : 'John', 'address' : {'city' : 'NY', 'country' : 'USA'}}</code>, the function returns <code>{'city' : 'NY', 'country' : 'USA'}</code> as a string as there is a matching element at <code> .address .","title":"getString (Function)"},{"location":"docs/api/5.1.1/#isexists-function","text":"Function checks whether there is a JSON element present in the given path or not. Origin: siddhi-execution-json:2.0.4 Syntax BOOL json:isExists( STRING|OBJECT json, STRING path) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json The JSON input that needs to be searched for an elements. STRING OBJECT No Yes path The JSON path to check for the element. STRING No Yes Examples EXAMPLE 1 json:isExists(json, '$.name') If the json is the format {'name' : 'John', 'age' : 23} , the function returns true as there is an element in the given path. EXAMPLE 2 json:isExists(json, '$.salary') If the json is the format {'name' : 'John', 'age' : 23} , the function returns false as there is no element in the given path.","title":"isExists (Function)"},{"location":"docs/api/5.1.1/#setelement-function","text":"Function sets JSON element into a given JSON at the specific path. Origin: siddhi-execution-json:2.0.4 Syntax OBJECT json:setElement( STRING|OBJECT json, STRING path, STRING|BOOL|DOUBLE|FLOAT|INT|LONG|OBJECT json.element) OBJECT json:setElement( STRING|OBJECT json, STRING path, STRING|BOOL|DOUBLE|FLOAT|INT|LONG|OBJECT json.element, STRING key) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json The JSON to which a JSON element needs to be added/replaced. STRING OBJECT No Yes path The JSON path where the JSON element should be added/replaced. STRING No Yes json.element The JSON element being added. STRING BOOL DOUBLE FLOAT INT LONG OBJECT No Yes key The key to be used to refer the newly added element in the input JSON. Assumes the element is added to a JSON array, or the element selected by the JSON path will be updated. STRING Yes Yes Examples EXAMPLE 1 json:setElement(json, '$', \"{'country' : 'USA'}\", 'address') If the json is the format {'name' : 'John', 'married' : true} ,the function updates the json as {'name' : 'John', 'married' : true, 'address' : {'country' : 'USA'}} by adding 'address' element and returns the updated JSON. EXAMPLE 2 json:setElement(json, '$', 40, 'age') If the json is the format {'name' : 'John', 'married' : true} ,the function updates the json as {'name' : 'John', 'married' : true, 'age' : 40} by adding 'age' element and returns the updated JSON. EXAMPLE 3 json:setElement(json, '$', 45, 'age') If the json is the format {'name' : 'John', 'married' : true, 'age' : 40} , the function updates the json as {'name' : 'John', 'married' : true, 'age' : 45} by replacing 'age' element and returns the updated JSON. EXAMPLE 4 json:setElement(json, '$.items', 'book') If the json is the format {'name' : 'Stationary', 'items' : ['pen', 'pencil']} , the function updates the json as {'name' : 'John', 'items' : ['pen', 'pencil', 'book']} by adding 'book' in the items array and returns the updated JSON. EXAMPLE 5 json:setElement(json, '$.item', 'book') If the json is the format {'name' : 'Stationary', 'item' : 'pen'} , the function updates the json as {'name' : 'John', 'item' : 'book'} by replacing 'item' element and returns the updated JSON. EXAMPLE 6 json:setElement(json, '$.address', 'city', 'SF') If the json is the format {'name' : 'John', 'married' : true} ,the function will not update, but returns the original JSON as there are no valid path for $.address .","title":"setElement (Function)"},{"location":"docs/api/5.1.1/#toobject-function","text":"Function generate JSON object from the given JSON string. Origin: siddhi-execution-json:2.0.4 Syntax OBJECT json:toObject( STRING json) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json A valid JSON string that needs to be converted to a JSON object. STRING No Yes Examples EXAMPLE 1 json:toJson(json) This returns the JSON object corresponding to the given JSON string.","title":"toObject (Function)"},{"location":"docs/api/5.1.1/#tostring-function","text":"Function generates a JSON string corresponding to a given JSON object. Origin: siddhi-execution-json:2.0.4 Syntax STRING json:toString( OBJECT json) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json A valid JSON object to generates a JSON string. OBJECT No Yes Examples EXAMPLE 1 json:toString(json) This returns the JSON string corresponding to a given JSON object.","title":"toString (Function)"},{"location":"docs/api/5.1.1/#tokenize-stream-processor","text":"Stream processor tokenizes the given JSON into to multiple JSON string elements and sends them as separate events. Origin: siddhi-execution-json:2.0.4 Syntax json:tokenize( STRING|OBJECT json, STRING path) json:tokenize( STRING|OBJECT json, STRING path, BOOL fail.on.missing.attribute) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json The input JSON that needs to be tokenized. STRING OBJECT No Yes path The path of the set of elements that will be tokenized. STRING No Yes fail.on.missing.attribute If there are no element on the given path, when set to true the system will drop the event, and when set to false the system will pass 'null' value to the jsonElement output attribute. true BOOL Yes No Extra Return Attributes Name Description Possible Types jsonElement The JSON element retrieved based on the given path will be returned as a JSON string. If the 'path' selects a JSON array then the system returns each element in the array as a JSON string via a separate events. STRING Examples EXAMPLE 1 define stream InputStream (json string, path string); @info(name = 'query1') from InputStream#json:tokenizeAsObject(json, path) select path, jsonElement insert into OutputStream; If the input 'json' is {name:'John', enrolledSubjects:['Mathematics', 'Physics']} , and the 'path' is passed as .enrolledSubjects /code then for both the elements in the selected JSON array, it generates it generates events as code (' .enrolledSubjects</code> then for both the elements in the selected JSON array, it generates it generates events as <code>(' .enrolledSubjects', 'Mathematics') , and (' .enrolledSubjects', 'Physics') /code . br For the same input JSON, if the 'path' is passed as code .enrolledSubjects', 'Physics')</code>.<br>For the same input JSON, if the 'path' is passed as <code> .name then it will only produce one event (' .name', 'John') /code as the 'path' provided a single JSON element. /p p /p span id=\"example-2\" class=\"md-typeset\" style=\"display: block; color: rgba(0, 0, 0, 0.54); font-size: 12.8px; font-weight: bold;\" EXAMPLE 2 /span define stream InputStream (json string, path string); @info(name = 'query1') from InputStream#json:tokenizeAsObject(json, path, true) select path, jsonElement insert into OutputStream; p /p p style=\"word-wrap: break-word;margin: 0;\" If the input 'json' is code {name:'John', age:25} /code ,and the 'path' is passed as code .name', 'John')</code> as the 'path' provided a single JSON element.</p> <p></p> <span id=\"example-2\" class=\"md-typeset\" style=\"display: block; color: rgba(0, 0, 0, 0.54); font-size: 12.8px; font-weight: bold;\">EXAMPLE 2</span> <pre class=\"codehilite\"><code>define stream InputStream (json string, path string); @info(name = 'query1') from InputStream#json:tokenizeAsObject(json, path, true) select path, jsonElement insert into OutputStream;</code></pre> <p></p> <p style=\"word-wrap: break-word;margin: 0;\">If the input 'json' is <code>{name:'John', age:25}</code>,and the 'path' is passed as <code> .salary then the system will produce (' .salary', null) /code , as the 'fail.on.missing.attribute' is code true /code and there are no matching element for code .salary', null)</code>, as the 'fail.on.missing.attribute' is <code>true</code> and there are no matching element for <code> .salary .","title":"tokenize (Stream Processor)"},{"location":"docs/api/5.1.1/#tokenizeasobject-stream-processor","text":"Stream processor tokenizes the given JSON into to multiple JSON object elements and sends them as separate events. Origin: siddhi-execution-json:2.0.4 Syntax json:tokenizeAsObject( STRING|OBJECT json, STRING path) json:tokenizeAsObject( STRING|OBJECT json, STRING path, BOOL fail.on.missing.attribute) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json The input JSON that needs to be tokenized. STRING OBJECT No Yes path The path of the set of elements that will be tokenized. STRING No Yes fail.on.missing.attribute If there are no element on the given path, when set to true the system will drop the event, and when set to false the system will pass 'null' value to the jsonElement output attribute. true BOOL Yes No Extra Return Attributes Name Description Possible Types jsonElement The JSON element retrieved based on the given path will be returned as a JSON object. If the 'path' selects a JSON array then the system returns each element in the array as a JSON object via a separate events. OBJECT Examples EXAMPLE 1 define stream InputStream (json string, path string); @info(name = 'query1') from InputStream#json:tokenizeAsObject(json, path) select path, jsonElement insert into OutputStream; If the input 'json' is {name:'John', enrolledSubjects:['Mathematics', 'Physics']} , and the 'path' is passed as .enrolledSubjects /code then for both the elements in the selected JSON array, it generates it generates events as code (' .enrolledSubjects</code> then for both the elements in the selected JSON array, it generates it generates events as <code>(' .enrolledSubjects', 'Mathematics') , and (' .enrolledSubjects', 'Physics') /code . br For the same input JSON, if the 'path' is passed as code .enrolledSubjects', 'Physics')</code>.<br>For the same input JSON, if the 'path' is passed as <code> .name then it will only produce one event (' .name', 'John') /code as the 'path' provided a single JSON element. /p p /p span id=\"example-2\" class=\"md-typeset\" style=\"display: block; color: rgba(0, 0, 0, 0.54); font-size: 12.8px; font-weight: bold;\" EXAMPLE 2 /span define stream InputStream (json string, path string); @info(name = 'query1') from InputStream#json:tokenizeAsObject(json, path, true) select path, jsonElement insert into OutputStream; p /p p style=\"word-wrap: break-word;margin: 0;\" If the input 'json' is code {name:'John', age:25} /code ,and the 'path' is passed as code .name', 'John')</code> as the 'path' provided a single JSON element.</p> <p></p> <span id=\"example-2\" class=\"md-typeset\" style=\"display: block; color: rgba(0, 0, 0, 0.54); font-size: 12.8px; font-weight: bold;\">EXAMPLE 2</span> <pre class=\"codehilite\"><code>define stream InputStream (json string, path string); @info(name = 'query1') from InputStream#json:tokenizeAsObject(json, path, true) select path, jsonElement insert into OutputStream;</code></pre> <p></p> <p style=\"word-wrap: break-word;margin: 0;\">If the input 'json' is <code>{name:'John', age:25}</code>,and the 'path' is passed as <code> .salary then the system will produce (' .salary', null) /code , as the 'fail.on.missing.attribute' is code true /code and there are no matching element for code .salary', null)</code>, as the 'fail.on.missing.attribute' is <code>true</code> and there are no matching element for <code> .salary .","title":"tokenizeAsObject (Stream Processor)"},{"location":"docs/api/5.1.1/#list","text":"","title":"List"},{"location":"docs/api/5.1.1/#collect-aggregate-function","text":"Collects multiple values to construct a list. Origin: siddhi-execution-list:1.0.0 Syntax OBJECT list:collect( OBJECT|INT|LONG|FLOAT|DOUBLE|BOOL|STRING value) OBJECT list:collect( OBJECT|INT|LONG|FLOAT|DOUBLE|BOOL|STRING value, BOOL is.distinct) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic value Value of the list element OBJECT INT LONG FLOAT DOUBLE BOOL STRING No Yes is.distinct If true only distinct elements are collected false BOOL Yes Yes Examples EXAMPLE 1 from StockStream#window.lengthBatch(10) select list:collect(symbol) as stockSymbols insert into OutputStream; For the window expiry of 10 events, the collect() function will collect attributes of symbol to a single list and return as stockSymbols.","title":"collect (Aggregate Function)"},{"location":"docs/api/5.1.1/#merge-aggregate-function","text":"Collects multiple lists to merge as a single list. Origin: siddhi-execution-list:1.0.0 Syntax OBJECT list:merge( OBJECT list) OBJECT list:merge( OBJECT list, BOOL is.distinct) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic list List to be merged OBJECT No Yes is.distinct Whether to return list with distinct values false BOOL Yes Yes Examples EXAMPLE 1 from StockStream#window.lengthBatch(2) select list:merge(list) as stockSymbols insert into OutputStream; For the window expiry of 2 events, the merge() function will collect attributes of list and merge them to a single list, returned as stockSymbols.","title":"merge (Aggregate Function)"},{"location":"docs/api/5.1.1/#add-function","text":"Function returns the updated list after adding the given value. Origin: siddhi-execution-list:1.0.0 Syntax OBJECT list:add( OBJECT list, OBJECT|INT|LONG|FLOAT|DOUBLE|BOOL|STRING value) OBJECT list:add( OBJECT list, OBJECT|INT|LONG|FLOAT|DOUBLE|BOOL|STRING value, INT index) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic list The list to which the value should be added. OBJECT No Yes value The value to be added. OBJECT INT LONG FLOAT DOUBLE BOOL STRING No Yes index The index in which the value should to be added. last INT Yes Yes Examples EXAMPLE 1 list:add(stockSymbols, 'IBM') Function returns the updated list after adding the value IBM in the last index. EXAMPLE 2 list:add(stockSymbols, 'IBM', 0) Function returns the updated list after adding the value IBM in the 0 th index`.","title":"add (Function)"},{"location":"docs/api/5.1.1/#addall-function","text":"Function returns the updated list after adding all the values from the given list. Origin: siddhi-execution-list:1.0.0 Syntax OBJECT list:addAll( OBJECT to.list, OBJECT from.list) OBJECT list:addAll( OBJECT to.list, OBJECT from.list, BOOL is.distinct) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic to.list The list into which the values need to copied. OBJECT No Yes from.list The list from which the values are copied. OBJECT No Yes is.distinct If true returns list with distinct values false BOOL Yes Yes Examples EXAMPLE 1 list:putAll(toList, fromList) If toList contains values ('IBM', 'WSO2), and if fromList contains values ('IBM', 'XYZ') then the function returns updated toList with values ('IBM', 'WSO2', 'IBM', 'XYZ'). EXAMPLE 2 list:putAll(toList, fromList, true) If toList contains values ('IBM', 'WSO2), and if fromList contains values ('IBM', 'XYZ') then the function returns updated toList with values ('IBM', 'WSO2', 'XYZ').","title":"addAll (Function)"},{"location":"docs/api/5.1.1/#clear-function","text":"Function returns the cleared list. Origin: siddhi-execution-list:1.0.0 Syntax OBJECT list:clear( OBJECT list) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic list The list which needs to be cleared OBJECT No Yes Examples EXAMPLE 1 list:clear(stockDetails) Returns an empty list.","title":"clear (Function)"},{"location":"docs/api/5.1.1/#clone-function","text":"Function returns the cloned list. Origin: siddhi-execution-list:1.0.0 Syntax OBJECT list:clone( OBJECT list) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic list The list to which needs to be cloned. OBJECT No Yes Examples EXAMPLE 1 list:clone(stockSymbols) Function returns cloned list of stockSymbols.","title":"clone (Function)"},{"location":"docs/api/5.1.1/#contains-function","text":"Function checks whether the list contains the specific value. Origin: siddhi-execution-list:1.0.0 Syntax BOOL list:contains( OBJECT list, OBJECT|INT|LONG|FLOAT|DOUBLE|BOOL|STRING value) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic list The list that needs to be checked on whether it contains the value or not. OBJECT No Yes value The value that needs to be checked. OBJECT INT LONG FLOAT DOUBLE BOOL STRING No Yes Examples EXAMPLE 1 list:contains(stockSymbols, 'IBM') Returns 'true' if the stockSymbols list contains value IBM else it returns false .","title":"contains (Function)"},{"location":"docs/api/5.1.1/#containsall-function","text":"Function checks whether the list contains all the values in the given list. Origin: siddhi-execution-list:1.0.0 Syntax BOOL list:containsAll( OBJECT list, OBJECT given.list) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic list The list that needs to be checked on whether it contains all the values or not. OBJECT No Yes given.list The list which contains all the values to be checked. OBJECT No Yes Examples EXAMPLE 1 list:containsAll(stockSymbols, latestStockSymbols) Returns 'true' if the stockSymbols list contains values in latestStockSymbols else it returns false .","title":"containsAll (Function)"},{"location":"docs/api/5.1.1/#create-function","text":"Function creates a list containing all values provided. Origin: siddhi-execution-list:1.0.0 Syntax OBJECT list:create() OBJECT list:create( OBJECT|INT|LONG|FLOAT|DOUBLE|BOOL|STRING value1) OBJECT list:create( OBJECT|INT|LONG|FLOAT|DOUBLE|BOOL|STRING value1, OBJECT|INT|LONG|FLOAT|DOUBLE|BOOL|STRING ...) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic value1 Value 1 OBJECT INT LONG FLOAT DOUBLE BOOL STRING Yes Yes Examples EXAMPLE 1 list:create(1, 2, 3, 4, 5, 6) This returns a list with values 1 , 2 , 3 , 4 , 5 and 6 . EXAMPLE 2 list:create() This returns an empty list.","title":"create (Function)"},{"location":"docs/api/5.1.1/#get-function","text":"Function returns the value at the specific index, null if index is out of range. Origin: siddhi-execution-list:1.0.0 Syntax OBJECT|INT|LONG|FLOAT|DOUBLE|BOOL|STRING list:get( OBJECT list, INT index) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic list Attribute containing the list OBJECT No Yes index Index of the element INT No Yes Examples EXAMPLE 1 list:get(stockSymbols, 1) This returns the element in the 1 st index in the stockSymbols list.","title":"get (Function)"},{"location":"docs/api/5.1.1/#indexof-function","text":"Function returns the last index of the given element. Origin: siddhi-execution-list:1.0.0 Syntax INT list:indexOf( OBJECT list, OBJECT|INT|LONG|FLOAT|DOUBLE|BOOL|STRING value) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic list The list to be checked to get index of an element. OBJECT No Yes value Value for which last index needs to be identified. OBJECT INT LONG FLOAT DOUBLE BOOL STRING No Yes Examples EXAMPLE 1 list:indexOf(stockSymbols. `IBM`) Returns the last index of the element IBM if present else it returns -1.","title":"indexOf (Function)"},{"location":"docs/api/5.1.1/#isempty-function","text":"Function checks if the list is empty. Origin: siddhi-execution-list:1.0.0 Syntax BOOL list:isEmpty( OBJECT list) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic list The list that needs to be checked whether it's empty or not. OBJECT No Yes Examples EXAMPLE 1 list:isEmpty(stockSymbols) Returns 'true' if the stockSymbols list is empty else it returns false .","title":"isEmpty (Function)"},{"location":"docs/api/5.1.1/#islist-function","text":"Function checks if the object is type of a list. Origin: siddhi-execution-list:1.0.0 Syntax BOOL list:isList( OBJECT|INT|LONG|FLOAT|DOUBLE|BOOL|STRING arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The argument the need to be determined whether it's a list or not. OBJECT INT LONG FLOAT DOUBLE BOOL STRING No Yes Examples EXAMPLE 1 list:isList(stockSymbols) Returns 'true' if the stockSymbols is and an instance of java.util.List else it returns false .","title":"isList (Function)"},{"location":"docs/api/5.1.1/#lastindexof-function","text":"Function returns the index of the given value. Origin: siddhi-execution-list:1.0.0 Syntax INT list:lastIndexOf( OBJECT list, OBJECT|INT|LONG|FLOAT|DOUBLE|BOOL|STRING value) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic list The list to be checked to get index of an element. OBJECT No Yes value Value for which last index needs to be identified. OBJECT INT LONG FLOAT DOUBLE BOOL STRING No Yes Examples EXAMPLE 1 list:lastIndexOf(stockSymbols. `IBM`) Returns the last index of the element IBM if present else it returns -1.","title":"lastIndexOf (Function)"},{"location":"docs/api/5.1.1/#remove-function","text":"Function returns the updated list after removing the element with the specified value. Origin: siddhi-execution-list:1.0.0 Syntax OBJECT list:remove( OBJECT list, OBJECT|INT|LONG|FLOAT|DOUBLE|BOOL|STRING value) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic list The list that needs to be updated. OBJECT No Yes value The value of the element that needs to removed. OBJECT INT LONG FLOAT DOUBLE BOOL STRING No Yes Examples EXAMPLE 1 list:remove(stockSymbols, 'IBM') This returns the updated list, stockSymbols after stockSymbols the value IBM .","title":"remove (Function)"},{"location":"docs/api/5.1.1/#removeall-function","text":"Function returns the updated list after removing all the element with the specified list. Origin: siddhi-execution-list:1.0.0 Syntax OBJECT list:removeAll( OBJECT list, OBJECT given.list) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic list The list that needs to be updated. OBJECT No Yes given.list The list with all the elements that needs to removed. OBJECT No Yes Examples EXAMPLE 1 list:removeAll(stockSymbols, latestStockSymbols) This returns the updated list, stockSymbols after removing all the values in latestStockSymbols.","title":"removeAll (Function)"},{"location":"docs/api/5.1.1/#removebyindex-function","text":"Function returns the updated list after removing the element with the specified index. Origin: siddhi-execution-list:1.0.0 Syntax OBJECT list:removeByIndex( OBJECT list, INT index) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic list The list that needs to be updated. OBJECT No Yes index The index of the element that needs to removed. INT No Yes Examples EXAMPLE 1 list:removeByIndex(stockSymbols, 0) This returns the updated list, stockSymbols after removing value at 0 th index.","title":"removeByIndex (Function)"},{"location":"docs/api/5.1.1/#retainall-function","text":"Function returns the updated list after retaining all the elements in the specified list. Origin: siddhi-execution-list:1.0.0 Syntax OBJECT list:retainAll( OBJECT list, OBJECT given.list) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic list The list that needs to be updated. OBJECT No Yes given.list The list with all the elements that needs to reatined. OBJECT No Yes Examples EXAMPLE 1 list:retainAll(stockSymbols, latestStockSymbols) This returns the updated list, stockSymbols after retaining all the values in latestStockSymbols.","title":"retainAll (Function)"},{"location":"docs/api/5.1.1/#setvalue-function","text":"Function returns the updated list after replacing the element in the given index by the given value. Origin: siddhi-execution-list:1.0.0 Syntax OBJECT list:setValue( OBJECT list, INT index, OBJECT|INT|LONG|FLOAT|DOUBLE|BOOL|STRING value) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic list The list to which the value should be updated. OBJECT No Yes index The index in which the value should to be updated. INT No Yes value The value to be updated with. OBJECT INT LONG FLOAT DOUBLE BOOL STRING No Yes Examples EXAMPLE 1 list:set(stockSymbols, 0, 'IBM') Function returns the updated list after replacing the value at 0 th index with the value IBM","title":"setValue (Function)"},{"location":"docs/api/5.1.1/#size-function","text":"Function to return the size of the list. Origin: siddhi-execution-list:1.0.0 Syntax INT list:size( OBJECT list) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic list The list for which size should be returned. OBJECT No Yes Examples EXAMPLE 1 list:size(stockSymbols) Returns size of the stockSymbols list.","title":"size (Function)"},{"location":"docs/api/5.1.1/#sort-function","text":"Function returns lists sorted in ascending or descending order. Origin: siddhi-execution-list:1.0.0 Syntax OBJECT list:sort( OBJECT list) OBJECT list:sort( OBJECT list, STRING order) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic list The list which should be sorted. OBJECT No Yes order Order in which the list needs to be sorted (ASC/DESC/REV). REV STRING Yes No Examples EXAMPLE 1 list:sort(stockSymbols) Function returns the sorted list in ascending order. EXAMPLE 2 list:sort(stockSymbols, 'DESC') Function returns the sorted list in descending order.","title":"sort (Function)"},{"location":"docs/api/5.1.1/#tokenize-stream-processor_1","text":"Tokenize the list and return each key, value as new attributes in events Origin: siddhi-execution-list:1.0.0 Syntax list:tokenize( OBJECT list) list:tokenize( OBJECT list, OBJECT ...) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic list Array list which needs to be tokenized OBJECT No Yes Extra Return Attributes Name Description Possible Types index Index of an entry consisted in the list INT value Value of an entry consisted in the list OBJECT Examples EXAMPLE 1 list:tokenize(customList) If custom list contains ('WSO2', 'IBM', 'XYZ') elements, then tokenize function will return 3 events with value attributes WSO2, IBM and XYZ respectively.","title":"tokenize (Stream Processor)"},{"location":"docs/api/5.1.1/#map","text":"","title":"Map"},{"location":"docs/api/5.1.1/#collect-aggregate-function_1","text":"Collect multiple key-value pairs to construct a map. Only distinct keys are collected, if a duplicate key arrives, it overrides the old value Origin: siddhi-execution-map:5.0.5 Syntax OBJECT map:collect( INT|LONG|FLOAT|DOUBLE|FLOAT|BOOL|STRING key, OBJECT|INT|LONG|FLOAT|DOUBLE|BOOL|STRING value) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic key Key of the map entry INT LONG FLOAT DOUBLE FLOAT BOOL STRING No Yes value Value of the map entry OBJECT INT LONG FLOAT DOUBLE BOOL STRING No Yes Examples EXAMPLE 1 from StockStream#window.lengthBatch(10) select map:collect(symbol, price) as stockDetails insert into OutputStream; For the window expiry of 10 events, the collect() function will collect attributes of key and value to a single map and return as stockDetails.","title":"collect (Aggregate Function)"},{"location":"docs/api/5.1.1/#merge-aggregate-function_1","text":"Collect multiple maps to merge as a single map. Only distinct keys are collected, if a duplicate key arrives, it overrides the old value. Origin: siddhi-execution-map:5.0.5 Syntax OBJECT map:merge( OBJECT map) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic map Maps to be collected OBJECT No Yes Examples EXAMPLE 1 from StockStream#window.lengthBatch(2) select map:merge(map) as stockDetails insert into OutputStream; For the window expiry of 2 events, the merge() function will collect attributes of map and merge them to a single map, returned as stockDetails.","title":"merge (Aggregate Function)"},{"location":"docs/api/5.1.1/#clear-function_1","text":"Function returns the cleared map. Origin: siddhi-execution-map:5.0.5 Syntax OBJECT map:clear( OBJECT map) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic map The map which needs to be cleared OBJECT No Yes Examples EXAMPLE 1 map:clear(stockDetails) Returns an empty map.","title":"clear (Function)"},{"location":"docs/api/5.1.1/#clone-function_1","text":"Function returns the cloned map. Origin: siddhi-execution-map:5.0.5 Syntax OBJECT map:clone( OBJECT map) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic map The map to which needs to be cloned. OBJECT No Yes Examples EXAMPLE 1 map:clone(stockDetails) Function returns cloned map of stockDetails.","title":"clone (Function)"},{"location":"docs/api/5.1.1/#combinebykey-function","text":"Function returns the map after combining all the maps given as parameters, such that the keys, of all the maps will be matched with an Array list of values from each map respectively. Origin: siddhi-execution-map:5.0.5 Syntax OBJECT map:combineByKey( OBJECT map, OBJECT map) OBJECT map:combineByKey( OBJECT map, OBJECT map, OBJECT ...) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic map The map into which the key-values need to copied. OBJECT No Yes Examples EXAMPLE 1 map:combineByKey(map1, map2) If map2 contains key-value pairs ('symbol': 'wso2'), ('volume' : 100), and if map2 contains key-value pairs ('symbol': 'IBM'), ('price' : 12), then the function returns the map with key value pairs as follows, (symbol: ArrayList('wso2, 'IBM')), (volume: ArrayList(100, null)) and (price: ArrayList(null, 12))","title":"combineByKey (Function)"},{"location":"docs/api/5.1.1/#containskey-function","text":"Function checks if the map contains the key. Origin: siddhi-execution-map:5.0.5 Syntax BOOL map:containsKey( OBJECT map, INT|LONG|FLOAT|DOUBLE|BOOL|STRING key) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic map The map the needs to be checked on containing the key or not. OBJECT No Yes key The key to be checked. INT LONG FLOAT DOUBLE BOOL STRING No Yes Examples EXAMPLE 1 map:containsKey(stockDetails, '1234') Returns 'true' if the stockDetails map contains key 1234 else it returns false .","title":"containsKey (Function)"},{"location":"docs/api/5.1.1/#containsvalue-function","text":"Function checks if the map contains the value. Origin: siddhi-execution-map:5.0.5 Syntax BOOL map:containsValue( OBJECT map, INT|LONG|FLOAT|DOUBLE|BOOL|STRING value) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic map The map the needs to be checked on containing the value or not. OBJECT No Yes value The value to be checked. INT LONG FLOAT DOUBLE BOOL STRING No Yes Examples EXAMPLE 1 map:containsValue(stockDetails, 'IBM') Returns 'true' if the stockDetails map contains value IBM else it returns false .","title":"containsValue (Function)"},{"location":"docs/api/5.1.1/#create-function_1","text":"Function creates a map pairing the keys and their corresponding values. Origin: siddhi-execution-map:5.0.5 Syntax OBJECT map:create() OBJECT map:create( OBJECT|INT|LONG|FLOAT|DOUBLE|FLOAT|BOOL|STRING key1, OBJECT|INT|LONG|FLOAT|DOUBLE|FLOAT|BOOL|STRING value1) OBJECT map:create( OBJECT|INT|LONG|FLOAT|DOUBLE|FLOAT|BOOL|STRING key1, OBJECT|INT|LONG|FLOAT|DOUBLE|FLOAT|BOOL|STRING value1, OBJECT|INT|LONG|FLOAT|DOUBLE|FLOAT|BOOL|STRING ...) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic key1 Key 1 - OBJECT INT LONG FLOAT DOUBLE FLOAT BOOL STRING Yes Yes value1 Value 1 - OBJECT INT LONG FLOAT DOUBLE FLOAT BOOL STRING Yes Yes Examples EXAMPLE 1 map:create(1, 'one', 2, 'two', 3, 'three') This returns a map with keys 1 , 2 , 3 mapped with their corresponding values, one , two , three . EXAMPLE 2 map:create() This returns an empty map.","title":"create (Function)"},{"location":"docs/api/5.1.1/#createfromjson-function","text":"Function returns the map created by pairing the keys with their corresponding values given in the JSON string. Origin: siddhi-execution-map:5.0.5 Syntax OBJECT map:createFromJSON( STRING json.string) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json.string JSON as a string, which is used to create the map. STRING No Yes Examples EXAMPLE 1 map:createFromJSON(\"{\u2018symbol' : 'IBM', 'price' : 200, 'volume' : 100}\") This returns a map with the keys symbol , price , and volume , and their values, IBM , 200 and 100 respectively.","title":"createFromJSON (Function)"},{"location":"docs/api/5.1.1/#createfromxml-function","text":"Function returns the map created by pairing the keys with their corresponding values,given as an XML string. Origin: siddhi-execution-map:5.0.5 Syntax OBJECT map:createFromXML( STRING xml.string) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic xml.string The XML string, which is used to create the map. STRING No Yes Examples EXAMPLE 1 map:createFromXML(\" stock symbol IBM /symbol price 200 /price volume 100 /volume /stock \") This returns a map with the keys symbol , price , volume , and with their values IBM , 200 and 100 respectively.","title":"createFromXML (Function)"},{"location":"docs/api/5.1.1/#get-function_1","text":"Function returns the value corresponding to the given key from the map. Origin: siddhi-execution-map:5.0.5 Syntax OBJECT|INT|LONG|FLOAT|DOUBLE|FLOAT|BOOL|STRING map:get( OBJECT map, INT|LONG|FLOAT|DOUBLE|FLOAT|BOOL|STRING key) OBJECT|INT|LONG|FLOAT|DOUBLE|FLOAT|BOOL|STRING map:get( OBJECT map, INT|LONG|FLOAT|DOUBLE|FLOAT|BOOL|STRING key, OBJECT|INT|LONG|FLOAT|DOUBLE|FLOAT|BOOL|STRING default.value) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic map The map from where the value should be obtained. OBJECT No Yes key The key to fetch the value. INT LONG FLOAT DOUBLE FLOAT BOOL STRING No Yes default.value The value to be returned if the map does not have the key. OBJECT INT LONG FLOAT DOUBLE FLOAT BOOL STRING Yes Yes Examples EXAMPLE 1 map:get(companyMap, 1) If the companyMap has key 1 and value ABC in it's set of key value pairs. The function returns ABC . EXAMPLE 2 map:get(companyMap, 2) If the companyMap does not have any value for key 2 then the function returns null . EXAMPLE 3 map:get(companyMap, 2, 'two') If the companyMap does not have any value for key 2 then the function returns two .","title":"get (Function)"},{"location":"docs/api/5.1.1/#isempty-function_1","text":"Function checks if the map is empty. Origin: siddhi-execution-map:5.0.5 Syntax BOOL map:isEmpty( OBJECT map) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic map The map the need to be checked whether it's empty or not. OBJECT No Yes Examples EXAMPLE 1 map:isEmpty(stockDetails) Returns 'true' if the stockDetails map is empty else it returns false .","title":"isEmpty (Function)"},{"location":"docs/api/5.1.1/#ismap-function","text":"Function checks if the object is type of a map. Origin: siddhi-execution-map:5.0.5 Syntax BOOL map:isMap( OBJECT|INT|LONG|FLOAT|DOUBLE|FLOAT|BOOL|STRING arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The argument the need to be determined whether it's a map or not. OBJECT INT LONG FLOAT DOUBLE FLOAT BOOL STRING No Yes Examples EXAMPLE 1 map:isMap(stockDetails) Returns 'true' if the stockDetails is and an instance of java.util.Map else it returns false .","title":"isMap (Function)"},{"location":"docs/api/5.1.1/#keys-function","text":"Function to return the keys of the map as a list. Origin: siddhi-execution-map:5.0.5 Syntax OBJECT map:keys( OBJECT map) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic map The map from which list of keys to be returned. OBJECT No Yes Examples EXAMPLE 1 map:keys(stockDetails) Returns keys of the stockDetails map.","title":"keys (Function)"},{"location":"docs/api/5.1.1/#put-function","text":"Function returns the updated map after adding the given key-value pair. If the key already exist in the map the key is updated with the new value. Origin: siddhi-execution-map:5.0.5 Syntax OBJECT map:put( OBJECT map, OBJECT|INT|LONG|FLOAT|DOUBLE|FLOAT|BOOL|STRING key, OBJECT|INT|LONG|FLOAT|DOUBLE|FLOAT|BOOL|STRING value) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic map The map to which the value should be added. OBJECT No Yes key The key to be added. OBJECT INT LONG FLOAT DOUBLE FLOAT BOOL STRING No Yes value The value to be added. OBJECT INT LONG FLOAT DOUBLE FLOAT BOOL STRING No Yes Examples EXAMPLE 1 map:put(stockDetails , 'IBM' , '200') Function returns the updated map named stockDetails after adding the value 200 with the key IBM .","title":"put (Function)"},{"location":"docs/api/5.1.1/#putall-function","text":"Function returns the updated map after adding all the key-value pairs from another map. If there are duplicate keys, the key will be assigned new values from the map that's being copied. Origin: siddhi-execution-map:5.0.5 Syntax OBJECT map:putAll( OBJECT to.map, OBJECT from.map) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic to.map The map into which the key-values need to copied. OBJECT No Yes from.map The map from which the key-values are copied. OBJECT No Yes Examples EXAMPLE 1 map:putAll(toMap, fromMap) If toMap contains key-value pairs ('symbol': 'wso2'), ('volume' : 100), and if fromMap contains key-value pairs ('symbol': 'IBM'), ('price' : 12), then the function returns updated toMap with key-value pairs ('symbol': 'IBM'), ('price' : 12), ('volume' : 100).","title":"putAll (Function)"},{"location":"docs/api/5.1.1/#putifabsent-function","text":"Function returns the updated map after adding the given key-value pair if key is absent. Origin: siddhi-execution-map:5.0.5 Syntax OBJECT map:putIfAbsent( OBJECT map, INT|LONG|FLOAT|DOUBLE|BOOL|STRING key, INT|LONG|FLOAT|DOUBLE|BOOL|STRING value) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic map The map to which the value should be added. OBJECT No Yes key The key to be added. INT LONG FLOAT DOUBLE BOOL STRING No Yes value The value to be added. INT LONG FLOAT DOUBLE BOOL STRING No Yes Examples EXAMPLE 1 map:putIfAbsent(stockDetails , 1234 , 'IBM') Function returns the updated map named stockDetails after adding the value IBM with the key 1234 if key is absent from the original map.","title":"putIfAbsent (Function)"},{"location":"docs/api/5.1.1/#remove-function_1","text":"Function returns the updated map after removing the element with the specified key. Origin: siddhi-execution-map:5.0.5 Syntax OBJECT map:remove( OBJECT map, OBJECT|INT|LONG|FLOAT|DOUBLE|FLOAT|BOOL|STRING key) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic map The map that needs to be updated. OBJECT No Yes key The key of the element that needs to removed. OBJECT INT LONG FLOAT DOUBLE FLOAT BOOL STRING No Yes Examples EXAMPLE 1 map:remove(stockDetails, 1234) This returns the updated map, stockDetails after removing the key-value pair corresponding to the key 1234 .","title":"remove (Function)"},{"location":"docs/api/5.1.1/#replace-function","text":"Function returns the updated map after replacing the given key-value pair only if key is present. Origin: siddhi-execution-map:5.0.5 Syntax OBJECT map:replace( OBJECT map, INT|LONG|FLOAT|DOUBLE|FLOAT|BOOL|STRING key, INT|LONG|FLOAT|DOUBLE|BOOL|STRING value) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic map The map to which the key-value should be replaced. OBJECT No Yes key The key to be replaced. INT LONG FLOAT DOUBLE FLOAT BOOL STRING No Yes value The value to be replaced. INT LONG FLOAT DOUBLE BOOL STRING No Yes Examples EXAMPLE 1 map:replace(stockDetails , 1234 , 'IBM') Function returns the updated map named stockDetails after replacing the value IBM with the key 1234 if present.","title":"replace (Function)"},{"location":"docs/api/5.1.1/#replaceall-function","text":"Function returns the updated map after replacing all the key-value pairs from another map, if keys are present. Origin: siddhi-execution-map:5.0.5 Syntax OBJECT map:replaceAll( OBJECT to.map, OBJECT from.map) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic to.map The map into which the key-values need to copied. OBJECT No Yes from.map The map from which the key-values are copied. OBJECT No Yes Examples EXAMPLE 1 map:replaceAll(toMap, fromMap) If toMap contains key-value pairs ('symbol': 'wso2'), ('volume' : 100), and if fromMap contains key-value pairs ('symbol': 'IBM'), ('price' : 12), then the function returns updated toMap with key-value pairs ('symbol': 'IBM'), ('volume' : 100).","title":"replaceAll (Function)"},{"location":"docs/api/5.1.1/#size-function_1","text":"Function to return the size of the map. Origin: siddhi-execution-map:5.0.5 Syntax INT map:size( OBJECT map) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic map The map for which size should be returned. OBJECT No Yes Examples EXAMPLE 1 map:size(stockDetails) Returns size of the stockDetails map.","title":"size (Function)"},{"location":"docs/api/5.1.1/#tojson-function","text":"Function converts a map into a JSON object and returns the JSON as a string. Origin: siddhi-execution-map:5.0.5 Syntax STRING map:toJSON( OBJECT map) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic map The map that needs to be converted to JSON OBJECT No Yes Examples EXAMPLE 1 map:toJSON(company) If company is a map with key-value pairs, ('symbol': 'wso2'),('volume' : 100), and ('price', 200), it returns the JSON string {\"symbol\" : \"wso2\", \"volume\" : 100 , \"price\" : 200} .","title":"toJSON (Function)"},{"location":"docs/api/5.1.1/#toxml-function","text":"Function returns the map as an XML string. Origin: siddhi-execution-map:5.0.5 Syntax STRING map:toXML( OBJECT map) STRING map:toXML( OBJECT map, OBJECT|STRING root.element.name) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic map The map that needs to be converted to XML. OBJECT No Yes root.element.name The root element of the map. The XML root element will be ignored OBJECT STRING Yes Yes Examples EXAMPLE 1 toXML(company, 'abcCompany') If company is a map with key-value pairs, ('symbol' : 'wso2'), ('volume' : 100), and ('price' : 200), this function returns XML as a string, abcCompany symbol wso2 /symbol volume 100 /volume price 200 /price /abcCompany . EXAMPLE 2 toXML(company) If company is a map with key-value pairs, ('symbol' : 'wso2'), ('volume' : 100), and ('price' : 200), this function returns XML without root element as a string, symbol wso2 /symbol volume 100 /volume price 200 /price .","title":"toXML (Function)"},{"location":"docs/api/5.1.1/#values-function","text":"Function to return the values of the map. Origin: siddhi-execution-map:5.0.5 Syntax OBJECT map:values( OBJECT map) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic map The map from which list if values to be returned. OBJECT No Yes Examples EXAMPLE 1 map:values(stockDetails) Returns values of the stockDetails map.","title":"values (Function)"},{"location":"docs/api/5.1.1/#tokenize-stream-processor_2","text":"Tokenize the map and return each key, value as new attributes in events Origin: siddhi-execution-map:5.0.5 Syntax map:tokenize( OBJECT map) map:tokenize( OBJECT map, OBJECT ...) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic map Hash map containing key value pairs OBJECT No Yes Extra Return Attributes Name Description Possible Types key Key of an entry consisted in the map OBJECT value Value of an entry consisted in the map. If more than one map is given, then an Array List of values from each map is returned for the value attribute. OBJECT Examples EXAMPLE 1 define stream StockStream(symbol string, price float); from StockStream#window.lengthBatch(2) select map:collect(symbol, price) as symbolPriceMap insert into TempStream; from TempStream#map:tokenize(customMap) select key, value insert into SymbolStream; Based on the length batch window, symbolPriceMap will collect two events, and the map will then again tokenized to give 2 events with key and values being symbol name and price respectively.","title":"tokenize (Stream Processor)"},{"location":"docs/api/5.1.1/#math","text":"","title":"Math"},{"location":"docs/api/5.1.1/#percentile-aggregate-function","text":"This functions returns the pth percentile value of a given argument. Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:percentile( INT|LONG|FLOAT|DOUBLE arg, DOUBLE p) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value of the parameter whose percentile should be found. INT LONG FLOAT DOUBLE No Yes p Estimate of the percentile to be found (pth percentile) where p is any number greater than 0 or lesser than or equal to 100. DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (sensorId int, temperature double); from InValueStream select math:percentile(temperature, 97.0) as percentile insert into OutMediationStream; This function returns the percentile value based on the argument given. For example, math:percentile(temperature, 97.0) returns the 97 th percentile value of all the temperature events.","title":"percentile (Aggregate Function)"},{"location":"docs/api/5.1.1/#abs-function","text":"This function returns the absolute value of the given parameter. It wraps the java.lang.Math.abs() function. Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:abs( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The parameter whose absolute value is found. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:abs(inValue) as absValue insert into OutMediationStream; Irrespective of whether the 'invalue' in the input stream holds a value of abs(3) or abs(-3),the function returns 3 since the absolute value of both 3 and -3 is 3. The result directed to OutMediationStream stream.","title":"abs (Function)"},{"location":"docs/api/5.1.1/#acos-function","text":"If -1 = p1 = 1, this function returns the arc-cosine (inverse cosine) value of p1.If the domain is invalid, it returns NULL. The value returned is in radian scale. This function wraps the java.lang.Math.acos() function. Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:acos( FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose arc-cosine (inverse cosine) value is found. FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:acos(inValue) as acosValue insert into OutMediationStream; If the 'inValue' in the input stream is given, the function calculates the arc-cosine value of it and returns the arc-cosine value to the output stream, OutMediationStream. For example, acos(0.5) returns 1.0471975511965979.","title":"acos (Function)"},{"location":"docs/api/5.1.1/#asin-function","text":"If -1 = p1 = 1, this function returns the arc-sin (inverse sine) value of p1. If the domain is invalid, it returns NULL. The value returned is in radian scale. This function wraps the java.lang.Math.asin() function. Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:asin( FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose arc-sin (inverse sine) value is found. FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:asin(inValue) as asinValue insert into OutMediationStream; If the 'inValue' in the input stream is given, the function calculates the arc-sin value of it and returns the arc-sin value to the output stream, OutMediationStream. For example, asin(0.5) returns 0.5235987755982989.","title":"asin (Function)"},{"location":"docs/api/5.1.1/#atan-function","text":"1. If a single p1 is received, this function returns the arc-tangent (inverse tangent) value of p1 . 2. If p1 is received along with an optional p1 , it considers them as x and y coordinates and returns the arc-tangent (inverse tangent) value. The returned value is in radian scale. This function wraps the java.lang.Math.atan() function. Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:atan( INT|LONG|FLOAT|DOUBLE p1) DOUBLE math:atan( INT|LONG|FLOAT|DOUBLE p1, INT|LONG|FLOAT|DOUBLE p2) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose arc-tangent (inverse tangent) is found. If the optional second parameter is given this represents the x coordinate of the (x,y) coordinate pair. INT LONG FLOAT DOUBLE No Yes p2 This optional parameter represents the y coordinate of the (x,y) coordinate pair. 0D INT LONG FLOAT DOUBLE Yes Yes Examples EXAMPLE 1 define stream InValueStream (inValue1 double, inValue2 double); from InValueStream select math:atan(inValue1, inValue2) as convertedValue insert into OutMediationStream; If the 'inValue1' in the input stream is given, the function calculates the arc-tangent value of it and returns the arc-tangent value to the output stream, OutMediationStream. If both the 'inValue1' and 'inValue2' are given, then the function considers them to be x and y coordinates respectively and returns the calculated arc-tangent value to the output stream, OutMediationStream. For example, atan(12d, 5d) returns 1.1760052070951352.","title":"atan (Function)"},{"location":"docs/api/5.1.1/#bin-function","text":"This function returns a string representation of the p1 argument, that is of either 'integer' or 'long' data type, as an unsigned integer in base 2. It wraps the java.lang.Integer.toBinaryString and java.lang.Long.toBinaryString` methods. Origin: siddhi-execution-math:5.0.4 Syntax STRING math:bin( INT|LONG p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value in either 'integer' or 'long', that should be converted into an unsigned integer of base 2. INT LONG No Yes Examples EXAMPLE 1 define stream InValueStream (inValue long); from InValueStream select math:bin(inValue) as binValue insert into OutMediationStream; If the 'inValue' in the input stream is given, the function converts it into an unsigned integer in base 2 and directs the output to the output stream, OutMediationStream. For example, bin(9) returns '1001'.","title":"bin (Function)"},{"location":"docs/api/5.1.1/#cbrt-function","text":"This function returns the cube-root of 'p1' which is in radians. It wraps the java.lang.Math.cbrt() function. Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:cbrt( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose cube-root should be found. Input is required to be in radians. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:cbrt(inValue) as cbrtValue insert into OutMediationStream; If the 'inValue' is given, the function calculates the cube-root value for the same and directs the output to the output stream, OutMediationStream. For example, cbrt(17d) returns 2.5712815906582356.","title":"cbrt (Function)"},{"location":"docs/api/5.1.1/#ceil-function","text":"This function returns the smallest double value, i.e., the closest to the negative infinity, that is greater than or equal to the p1 argument, and is equal to a mathematical integer. It wraps the java.lang.Math.ceil() method. Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:ceil( FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose ceiling value is found. FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:ceil(inValue) as ceilingValue insert into OutMediationStream; This function calculates the ceiling value of the given 'inValue' and directs the result to 'OutMediationStream' output stream. For example, ceil(423.187d) returns 424.0.","title":"ceil (Function)"},{"location":"docs/api/5.1.1/#conv-function","text":"This function converts a from the fromBase base to the toBase base. Origin: siddhi-execution-math:5.0.4 Syntax STRING math:conv( STRING a, INT from.base, INT to.base) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic a The value whose base should be changed. Input should be given as a 'String'. STRING No Yes from.base The source base of the input parameter 'a'. INT No Yes to.base The target base that the input parameter 'a' should be converted into. INT No Yes Examples EXAMPLE 1 define stream InValueStream (inValue string,fromBase int,toBase int); from InValueStream select math:conv(inValue,fromBase,toBase) as convertedValue insert into OutMediationStream; If the 'inValue' in the input stream is given, and the base in which it currently resides in and the base to which it should be converted to is specified, the function converts it into a string in the target base and directs it to the output stream, OutMediationStream. For example, conv(\"7f\", 16, 10) returns \"127\".","title":"conv (Function)"},{"location":"docs/api/5.1.1/#copysign-function","text":"This function returns a value of an input with the received magnitude and sign of another input. It wraps the java.lang.Math.copySign() function. Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:copySign( INT|LONG|FLOAT|DOUBLE magnitude, INT|LONG|FLOAT|DOUBLE sign) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic magnitude The magnitude of this parameter is used in the output attribute. INT LONG FLOAT DOUBLE No Yes sign The sign of this parameter is used in the output attribute. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue1 double, inValue2 double); from InValueStream select math:copySign(inValue1,inValue2) as copysignValue insert into OutMediationStream; If two values are provided as 'inValue1' and 'inValue2', the function copies the magnitude and sign of the second argument into the first one and directs the result to the output stream, OutMediatonStream. For example, copySign(5.6d, -3.0d) returns -5.6.","title":"copySign (Function)"},{"location":"docs/api/5.1.1/#cos-function","text":"This function returns the cosine of p1 which is in radians. It wraps the java.lang.Math.cos() function. Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:cos( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose cosine value should be found.The input is required to be in radians. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:cos(inValue) as cosValue insert into OutMediationStream; If the 'inValue' is given, the function calculates the cosine value for the same and directs the output to the output stream, OutMediationStream. For example, cos(6d) returns 0.9601702866503661.","title":"cos (Function)"},{"location":"docs/api/5.1.1/#cosh-function","text":"This function returns the hyperbolic cosine of p1 which is in radians. It wraps the java.lang.Math.cosh() function. Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:cosh( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose hyperbolic cosine should be found. The input is required to be in radians. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:cosh(inValue) as cosValue insert into OutMediationStream; If the 'inValue' is given, the function calculates the hyperbolic cosine value for the same and directs the output to the output stream, OutMediationStream. For example, cosh (6d) returns 201.7156361224559.","title":"cosh (Function)"},{"location":"docs/api/5.1.1/#e-function","text":"This function returns the java.lang.Math.E constant, which is the closest double value to e, where e is the base of the natural logarithms. Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:e() Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:e() as eValue insert into OutMediationStream; This function returns the constant, 2.7182818284590452354 which is the closest double value to e and directs the output to 'OutMediationStream' output stream.","title":"e (Function)"},{"location":"docs/api/5.1.1/#exp-function","text":"This function returns the Euler's number e raised to the power of p1 . It wraps the java.lang.Math.exp() function. Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:exp( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The power that the Euler's number e is raised to. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:exp(inValue) as expValue insert into OutMediationStream; If the 'inValue' in the inputstream holds a value, this function calculates the corresponding Euler's number 'e' and directs it to the output stream, OutMediationStream. For example, exp(10.23) returns 27722.51006805505.","title":"exp (Function)"},{"location":"docs/api/5.1.1/#floor-function","text":"This function wraps the java.lang.Math.floor() function and returns the largest value, i.e., closest to the positive infinity, that is less than or equal to p1 , and is equal to a mathematical integer. Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:floor( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose floor value should be found. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:floor(inValue) as floorValue insert into OutMediationStream; This function calculates the floor value of the given 'inValue' input and directs the output to the 'OutMediationStream' output stream. For example, (10.23) returns 10.0.","title":"floor (Function)"},{"location":"docs/api/5.1.1/#getexponent-function","text":"This function returns the unbiased exponent that is used in the representation of p1 . This function wraps the java.lang.Math.getExponent() function. Origin: siddhi-execution-math:5.0.4 Syntax INT math:getExponent( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of whose unbiased exponent representation should be found. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:getExponent(inValue) as expValue insert into OutMediationStream; This function calculates the unbiased exponent of a given input, 'inValue' and directs the result to the 'OutMediationStream' output stream. For example, getExponent(60984.1) returns 15.","title":"getExponent (Function)"},{"location":"docs/api/5.1.1/#hex-function","text":"This function wraps the java.lang.Double.toHexString() function. It returns a hexadecimal string representation of the input, p1`. Origin: siddhi-execution-math:5.0.4 Syntax STRING math:hex( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose hexadecimal value should be found. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue int); from InValueStream select math:hex(inValue) as hexString insert into OutMediationStream; If the 'inValue' in the input stream is provided, the function converts this into its corresponding hexadecimal format and directs the output to the output stream, OutMediationStream. For example, hex(200) returns \"c8\".","title":"hex (Function)"},{"location":"docs/api/5.1.1/#isinfinite-function","text":"This function wraps the java.lang.Float.isInfinite() and java.lang.Double.isInfinite() and returns true if p1 is infinitely large in magnitude and false if otherwise. Origin: siddhi-execution-math:5.0.4 Syntax BOOL math:isInfinite( FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 This is the value of the parameter that the function determines to be either infinite or finite. FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue1 double,inValue2 int); from InValueStream select math:isInfinite(inValue1) as isInfinite insert into OutMediationStream; If the value given in the 'inValue' in the input stream is of infinitely large magnitude, the function returns the value, 'true' and directs the result to the output stream, OutMediationStream'. For example, isInfinite(java.lang.Double.POSITIVE_INFINITY) returns true.","title":"isInfinite (Function)"},{"location":"docs/api/5.1.1/#isnan-function","text":"This function wraps the java.lang.Float.isNaN() and java.lang.Double.isNaN() functions and returns true if p1 is NaN (Not-a-Number), and returns false if otherwise. Origin: siddhi-execution-math:5.0.4 Syntax BOOL math:isNan( FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter which the function determines to be either NaN or a number. FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue1 double,inValue2 int); from InValueStream select math:isNan(inValue1) as isNaN insert into OutMediationStream; If the 'inValue1' in the input stream has a value that is undefined, then the function considers it as an 'NaN' value and directs 'True' to the output stream, OutMediationStream. For example, isNan(java.lang.Math.log(-12d)) returns true.","title":"isNan (Function)"},{"location":"docs/api/5.1.1/#ln-function","text":"This function returns the natural logarithm (base e) of p1 . Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:ln( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose natural logarithm (base e) should be found. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:ln(inValue) as lnValue insert into OutMediationStream; If the 'inValue' in the input stream is given, the function calculates its natural logarithm (base e) and directs the results to the output stream, 'OutMeditionStream'. For example, ln(11.453) returns 2.438251704415579.","title":"ln (Function)"},{"location":"docs/api/5.1.1/#log-function","text":"This function returns the logarithm of the received number as per the given base . Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:log( INT|LONG|FLOAT|DOUBLE number, INT|LONG|FLOAT|DOUBLE base) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic number The value of the parameter whose base should be changed. INT LONG FLOAT DOUBLE No Yes base The base value of the ouput. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (number double, base double); from InValueStream select math:log(number, base) as logValue insert into OutMediationStream; If the number and the base to which it has to be converted into is given in the input stream, the function calculates the number to the base specified and directs the result to the output stream, OutMediationStream. For example, log(34, 2f) returns 5.08746284125034.","title":"log (Function)"},{"location":"docs/api/5.1.1/#log10-function","text":"This function returns the base 10 logarithm of p1 . Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:log10( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose base 10 logarithm should be found. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:log10(inValue) as lnValue insert into OutMediationStream; If the 'inValue' in the input stream is given, the function calculates the base 10 logarithm of the same and directs the result to the output stream, OutMediatioStream. For example, log10(19.234) returns 1.2840696117100832.","title":"log10 (Function)"},{"location":"docs/api/5.1.1/#log2-function","text":"This function returns the base 2 logarithm of p1 . Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:log2( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose base 2 logarithm should be found. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:log2(inValue) as lnValue insert into OutMediationStream; If the 'inValue' in the input stream is given, the function calculates the base 2 logarithm of the same and returns the value to the output stream, OutMediationStream. For example log2(91d) returns 6.507794640198696.","title":"log2 (Function)"},{"location":"docs/api/5.1.1/#max-function","text":"This function returns the greater value of p1 and p2 . Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:max( INT|LONG|FLOAT|DOUBLE p1, INT|LONG|FLOAT|DOUBLE p2) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 One of the input values to be compared in order to find the larger value of the two INT LONG FLOAT DOUBLE No Yes p2 The input value to be compared with 'p1' in order to find the larger value of the two. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue1 double,inValue2 int); from InValueStream select math:max(inValue1,inValue2) as maxValue insert into OutMediationStream; If two input values 'inValue1, and 'inValue2' are given, the function compares them and directs the larger value to the output stream, OutMediationStream. For example, max(123.67d, 91) returns 123.67.","title":"max (Function)"},{"location":"docs/api/5.1.1/#min-function","text":"This function returns the smaller value of p1 and p2 . Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:min( INT|LONG|FLOAT|DOUBLE p1, INT|LONG|FLOAT|DOUBLE p2) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 One of the input values that are to be compared in order to find the smaller value. INT LONG FLOAT DOUBLE No Yes p2 The input value that is to be compared with 'p1' in order to find the smaller value. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue1 double,inValue2 int); from InValueStream select math:min(inValue1,inValue2) as minValue insert into OutMediationStream; If two input values, 'inValue1' and 'inValue2' are given, the function compares them and directs the smaller value of the two to the output stream, OutMediationStream. For example, min(123.67d, 91) returns 91.","title":"min (Function)"},{"location":"docs/api/5.1.1/#oct-function","text":"This function converts the input parameter p1 to octal. Origin: siddhi-execution-math:5.0.4 Syntax STRING math:oct( INT|LONG p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose octal representation should be found. INT LONG No Yes Examples EXAMPLE 1 define stream InValueStream (inValue long); from InValueStream select math:oct(inValue) as octValue insert into OutMediationStream; If the 'inValue' in the input stream is given, this function calculates the octal value corresponding to the same and directs it to the output stream, OutMediationStream. For example, oct(99l) returns \"143\".","title":"oct (Function)"},{"location":"docs/api/5.1.1/#parsedouble-function","text":"This function returns the double value of the string received. Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:parseDouble( STRING p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that should be converted into a double value. STRING No Yes Examples EXAMPLE 1 define stream InValueStream (inValue string); from InValueStream select math:parseDouble(inValue) as output insert into OutMediationStream; If the 'inValue' in the input stream holds a value, this function converts it into the corresponding double value and directs it to the output stream, OutMediationStream. For example, parseDouble(\"123\") returns 123.0.","title":"parseDouble (Function)"},{"location":"docs/api/5.1.1/#parsefloat-function","text":"This function returns the float value of the received string. Origin: siddhi-execution-math:5.0.4 Syntax FLOAT math:parseFloat( STRING p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that should be converted into a float value. STRING No Yes Examples EXAMPLE 1 define stream InValueStream (inValue string); from InValueStream select math:parseFloat(inValue) as output insert into OutMediationStream; The function converts the input value given in 'inValue',into its corresponding float value and directs the result into the output stream, OutMediationStream. For example, parseFloat(\"123\") returns 123.0.","title":"parseFloat (Function)"},{"location":"docs/api/5.1.1/#parseint-function","text":"This function returns the integer value of the received string. Origin: siddhi-execution-math:5.0.4 Syntax INT math:parseInt( STRING p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that should be converted to an integer. STRING No Yes Examples EXAMPLE 1 define stream InValueStream (inValue string); from InValueStream select math:parseInt(inValue) as output insert into OutMediationStream; The function converts the 'inValue' into its corresponding integer value and directs the output to the output stream, OutMediationStream. For example, parseInt(\"123\") returns 123.","title":"parseInt (Function)"},{"location":"docs/api/5.1.1/#parselong-function","text":"This function returns the long value of the string received. Origin: siddhi-execution-math:5.0.4 Syntax LONG math:parseLong( STRING p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that should be converted to a long value. STRING No Yes Examples EXAMPLE 1 define stream InValueStream (inValue string); from InValueStream select math:parseLong(inValue) as output insert into OutMediationStream; The function converts the 'inValue' to its corresponding long value and directs the result to the output stream, OutMediationStream. For example, parseLong(\"123\") returns 123.","title":"parseLong (Function)"},{"location":"docs/api/5.1.1/#pi-function","text":"This function returns the java.lang.Math.PI constant, which is the closest value to pi, i.e., the ratio of the circumference of a circle to its diameter. Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:pi() Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:pi() as piValue insert into OutMediationStream; pi() always returns 3.141592653589793.","title":"pi (Function)"},{"location":"docs/api/5.1.1/#power-function","text":"This function raises the given value to a given power. Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:power( INT|LONG|FLOAT|DOUBLE value, INT|LONG|FLOAT|DOUBLE to.power) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic value The value that should be raised to the power of 'to.power' input parameter. INT LONG FLOAT DOUBLE No Yes to.power The power to which the 'value' input parameter should be raised. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue1 double, inValue2 double); from InValueStream select math:power(inValue1,inValue2) as powerValue insert into OutMediationStream; This function raises the 'inValue1' to the power of 'inValue2' and directs the output to the output stream, 'OutMediationStream. For example, (5.6d, 3.0d) returns 175.61599999999996.","title":"power (Function)"},{"location":"docs/api/5.1.1/#rand-function","text":"This returns a stream of pseudo-random numbers when a sequence of calls are sent to the rand() . Optionally, it is possible to define a seed, i.e., rand(seed) using which the pseudo-random numbers are generated. These functions internally use the java.util.Random class. Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:rand() DOUBLE math:rand( INT|LONG seed) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic seed An optional seed value that will be used to generate the random number sequence. defaultSeed INT LONG Yes Yes Examples EXAMPLE 1 define stream InValueStream (symbol string, price long, volume long); from InValueStream select symbol, math:rand() as randNumber select math:oct(inValue) as octValue insert into OutMediationStream; In the example given above, a random double value between 0 and 1 will be generated using math:rand().","title":"rand (Function)"},{"location":"docs/api/5.1.1/#round-function","text":"This function returns the value of the input argument rounded off to the closest integer/long value. Origin: siddhi-execution-math:5.0.4 Syntax INT|LONG math:round( FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that should be rounded off to the closest integer/long value. FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:round(inValue) as roundValue insert into OutMediationStream; The function rounds off 'inValue1' to the closest int/long value and directs the output to the output stream, 'OutMediationStream'. For example, round(3252.353) returns 3252.","title":"round (Function)"},{"location":"docs/api/5.1.1/#signum-function","text":"This returns +1, 0, or -1 for the given positive, zero and negative values respectively. This function wraps the java.lang.Math.signum() function. Origin: siddhi-execution-math:5.0.4 Syntax INT math:signum( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that should be checked to be positive, negative or zero. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:signum(inValue) as sign insert into OutMediationStream; The function evaluates the 'inValue' given to be positive, negative or zero and directs the result to the output stream, 'OutMediationStream'. For example, signum(-6.32d) returns -1.","title":"signum (Function)"},{"location":"docs/api/5.1.1/#sin-function","text":"This returns the sine of the value given in radians. This function wraps the java.lang.Math.sin() function. Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:sin( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose sine value should be found. Input is required to be in radians. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:sin(inValue) as sinValue insert into OutMediationStream; The function calculates the sine value of the given 'inValue' and directs the output to the output stream, 'OutMediationStream. For example, sin(6d) returns -0.27941549819892586.","title":"sin (Function)"},{"location":"docs/api/5.1.1/#sinh-function","text":"This returns the hyperbolic sine of the value given in radians. This function wraps the java.lang.Math.sinh() function. Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:sinh( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose hyperbolic sine value should be found. Input is required to be in radians. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:sinh(inValue) as sinhValue insert into OutMediationStream; This function calculates the hyperbolic sine value of 'inValue' and directs the output to the output stream, 'OutMediationStream'. For example, sinh(6d) returns 201.71315737027922.","title":"sinh (Function)"},{"location":"docs/api/5.1.1/#sqrt-function","text":"This function returns the square-root of the given value. It wraps the java.lang.Math.sqrt() s function. Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:sqrt( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose square-root value should be found. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:sqrt(inValue) as sqrtValue insert into OutMediationStream; The function calculates the square-root value of the 'inValue' and directs the output to the output stream, 'OutMediationStream'. For example, sqrt(4d) returns 2.","title":"sqrt (Function)"},{"location":"docs/api/5.1.1/#tan-function","text":"This function returns the tan of the given value in radians. It wraps the java.lang.Math.tan() function. Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:tan( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose tan value should be found. Input is required to be in radians. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:tan(inValue) as tanValue insert into OutMediationStream; This function calculates the tan value of the 'inValue' given and directs the output to the output stream, 'OutMediationStream'. For example, tan(6d) returns -0.29100619138474915.","title":"tan (Function)"},{"location":"docs/api/5.1.1/#tanh-function","text":"This function returns the hyperbolic tangent of the value given in radians. It wraps the java.lang.Math.tanh() function. Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:tanh( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose hyperbolic tangent value should be found. Input is required to be in radians. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:tanh(inValue) as tanhValue insert into OutMediationStream; If the 'inVaue' in the input stream is given, this function calculates the hyperbolic tangent value of the same and directs the output to 'OutMediationStream' stream. For example, tanh(6d) returns 0.9999877116507956.","title":"tanh (Function)"},{"location":"docs/api/5.1.1/#todegrees-function","text":"This function converts the value given in radians to degrees. It wraps the java.lang.Math.toDegrees() function. Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:toDegrees( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The input value in radians that should be converted to degrees. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:toDegrees(inValue) as degreesValue insert into OutMediationStream; The function converts the 'inValue' in the input stream from radians to degrees and directs the output to 'OutMediationStream' output stream. For example, toDegrees(6d) returns 343.77467707849394.","title":"toDegrees (Function)"},{"location":"docs/api/5.1.1/#toradians-function","text":"This function converts the value given in degrees to radians. It wraps the java.lang.Math.toRadians() function. Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:toRadians( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The input value in degrees that should be converted to radians. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:toRadians(inValue) as radiansValue insert into OutMediationStream; This function converts the input, from degrees to radians and directs the result to 'OutMediationStream' output stream. For example, toRadians(6d) returns 0.10471975511965977.","title":"toRadians (Function)"},{"location":"docs/api/5.1.1/#rdbms","text":"","title":"Rdbms"},{"location":"docs/api/5.1.1/#cud-stream-processor","text":"This function performs SQL CUD (INSERT, UPDATE, DELETE) queries on data sources. Note: This function to work data sources should be set at the Siddhi Manager level. Origin: siddhi-store-rdbms:7.0.2 Syntax rdbms:cud( STRING datasource.name, STRING query) rdbms:cud( STRING datasource.name, STRING query, STRING|BOOL|INT|DOUBLE|FLOAT|LONG parameter) rdbms:cud( STRING datasource.name, STRING query, STRING|BOOL|INT|DOUBLE|FLOAT|LONG parameter, STRING|BOOL|INT|DOUBLE|FLOAT|LONG ...) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic datasource.name The name of the datasource for which the query should be performed. If Siddhi is used as a Java/Python library the datasource should be explicitly set in the siddhi manager in order for the function to work. STRING No No query The update, delete, or insert query(formatted according to the relevant database type) that needs to be performed. STRING No Yes parameter If the second parameter is a parametrised SQL query, then siddhi attributes can be passed to set the values of the parameters STRING BOOL INT DOUBLE FLOAT LONG Yes Yes System Parameters Name Description Default Value Possible Parameters perform.CUD.operations If this parameter is set to 'true', the RDBMS CUD function is enabled to perform CUD operations. false true false Extra Return Attributes Name Description Possible Types numRecords The number of records manipulated by the query. INT Examples EXAMPLE 1 from TriggerStream#rdbms:cud(\"SAMPLE_DB\", \"UPDATE Customers_Table SET customerName='abc' where customerName='xyz'\") select numRecords insert into RecordStream; This query updates the events from the input stream named 'TriggerStream' with an additional attribute named 'numRecords', of which the value indicates the number of records manipulated. The updated events are inserted into an output stream named 'RecordStream'. EXAMPLE 2 from TriggerStream#rdbms:cud(\"SAMPLE_DB\", \"UPDATE Customers_Table SET customerName=? where customerName=?\", changedName, previousName) select numRecords insert into RecordStream; This query updates the events from the input stream named 'TriggerStream' with an additional attribute named 'numRecords', of which the value indicates the number of records manipulated. The updated events are inserted into an output stream named 'RecordStream'. Here the values of attributes changedName and previousName in the event will be set to the query.","title":"cud (Stream Processor)"},{"location":"docs/api/5.1.1/#query-stream-processor","text":"This function performs SQL retrieval queries on data sources. Note: This function to work data sources should be set at the Siddhi Manager level. Origin: siddhi-store-rdbms:7.0.2 Syntax rdbms:query( STRING datasource.name, STRING attribute.definition.list, STRING query) rdbms:query( STRING datasource.name, STRING attribute.definition.list, STRING query, STRING|BOOL|INT|DOUBLE|FLOAT|LONG parameter) rdbms:query( STRING datasource.name, STRING attribute.definition.list, STRING query, STRING|BOOL|INT|DOUBLE|FLOAT|LONG parameter, STRING|BOOL|INT|DOUBLE|FLOAT|LONG ...) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic datasource.name The name of the datasource for which the query should be performed. If Siddhi is used as a Java/Python library the datasource should be explicitly set in the siddhi manager in order for the function to work. STRING No No attribute.definition.list This is provided as a comma-separated list in the ' AttributeName AttributeType ' format. The SQL query is expected to return the attributes in the given order. e.g., If one attribute is defined here, the SQL query should return one column result set. If more than one column is returned, then the first column is processed. The Siddhi data types supported are 'STRING', 'INT', 'LONG', 'DOUBLE', 'FLOAT', and 'BOOL'. Mapping of the Siddhi data type to the database data type can be done as follows, Siddhi Datatype - Datasource Datatype STRING - CHAR , VARCHAR , LONGVARCHAR INT - INTEGER LONG - BIGINT DOUBLE - DOUBLE FLOAT - REAL BOOL - BIT STRING No No query The select query(formatted according to the relevant database type) that needs to be performed STRING No Yes parameter If the second parameter is a parametrised SQL query, then siddhi attributes can be passed to set the values of the parameters STRING BOOL INT DOUBLE FLOAT LONG Yes Yes Extra Return Attributes Name Description Possible Types attributeName The return attributes will be the ones defined in the parameter attribute.definition.list . STRING INT LONG DOUBLE FLOAT BOOL Examples EXAMPLE 1 from TriggerStream#rdbms:query('SAMPLE_DB', 'creditcardno string, country string, transaction string, amount int', 'select * from Transactions_Table') select creditcardno, country, transaction, amount insert into recordStream; Events inserted into recordStream includes all records matched for the query i.e an event will be generated for each record retrieved from the datasource. The event will include as additional attributes, the attributes defined in the attribute.definition.list (creditcardno, country, transaction, amount). EXAMPLE 2 from TriggerStream#rdbms:query('SAMPLE_DB', 'creditcardno string, country string,transaction string, amount int', 'select * from where country=?', countrySearchWord) select creditcardno, country, transaction, amount insert into recordStream; Events inserted into recordStream includes all records matched for the query i.e an event will be generated for each record retrieved from the datasource. The event will include as additional attributes, the attributes defined in the attribute.definition.list (creditcardno, country, transaction, amount). countrySearchWord value from the event will be set in the query when querying the datasource.","title":"query (Stream Processor)"},{"location":"docs/api/5.1.1/#regex","text":"","title":"Regex"},{"location":"docs/api/5.1.1/#find-function","text":"Finds the subsequence that matches the given regex pattern. Origin: siddhi-execution-regex:5.0.5 Syntax BOOL regex:find( STRING regex, STRING input.sequence) BOOL regex:find( STRING regex, STRING input.sequence, INT starting.index) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic regex A regular expression that is matched to a sequence in order to find the subsequence of the same. For example, \\d\\d(.*)WSO2 . STRING No Yes input.sequence The input sequence to be matched with the regular expression. For example, 21 products are produced by WSO2 . STRING No Yes starting.index The starting index of the input sequence from where the input sequence ismatched with the given regex pattern.For example, 10 . 0 INT Yes Yes Examples EXAMPLE 1 regex:find('\\d\\d(.*)WSO2', '21 products are produced by WSO2 currently') This method attempts to find the subsequence of the input.sequence that matches the regex pattern, \\d\\d(. )WSO2 . It returns true as a subsequence exists. EXAMPLE 2 regex:find('\\d\\d(.*)WSO2', '21 products are produced by WSO2.', 4) This method attempts to find the subsequence of the input.sequence that matches the regex pattern, \\d\\d(. )WSO2 starting from index 4 . It returns 'false' as subsequence does not exists.","title":"find (Function)"},{"location":"docs/api/5.1.1/#group-function","text":"Returns the subsequence captured by the given group during the regex match operation. Origin: siddhi-execution-regex:5.0.5 Syntax STRING regex:group( STRING regex, STRING input.sequence, INT group.id) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic regex A regular expression. For example, \\d\\d(.*)WSO2. STRING No Yes input.sequence The input sequence to be matched with the regular expression. For example, 2 1 products are produced by WSO2 . STRING No Yes group.id The given group id of the regex expression. For example, 2 . INT No Yes Examples EXAMPLE 1 regex:group('\\d\\d(.*)(WSO2.*)(WSO2.*)', '21 products are produced within 10 years by WSO2 currently by WSO2 employees', 3) Function returns 'WSO2 employees', the subsequence captured by the groupID 3 according to the regex pattern, \\d\\d(. )(WSO2. )(WSO2.*) .","title":"group (Function)"},{"location":"docs/api/5.1.1/#lookingat-function","text":"Matches the input.sequence from the beginning against the regex pattern, and unlike regex:matches() it does not require that the entire input.sequence be matched. Origin: siddhi-execution-regex:5.0.5 Syntax BOOL regex:lookingAt( STRING regex, STRING input.sequence) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic regex A regular expression. For example, \\d\\d(.*)WSO2 . STRING No Yes input.sequence The input sequence to be matched with the regular expression. For example, 21 products are produced by WSO2 . STRING No Yes Examples EXAMPLE 1 regex:lookingAt('\\d\\d(.*)(WSO2.*)', '21 products are produced by WSO2 currently in Sri Lanka') Function matches the input.sequence against the regex pattern, \\d\\d(. )(WSO2. ) from the beginning, and as it matches it returns true . EXAMPLE 2 regex:lookingAt('WSO2(.*)middleware(.*)', 'sample test string and WSO2 is situated in trace and it's a middleware company') Function matches the input.sequence against the regex pattern, WSO2(. )middleware(. ) from the beginning, and as it does not match it returns false .","title":"lookingAt (Function)"},{"location":"docs/api/5.1.1/#matches-function","text":"Matches the entire input.sequence against the regex pattern. Origin: siddhi-execution-regex:5.0.5 Syntax BOOL regex:matches( STRING regex, STRING input.sequence) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic regex A regular expression. For example, \\d\\d(.*)WSO2 . STRING No Yes input.sequence The input sequence to be matched with the regular expression. For example, 21 products are produced by WSO2 . STRING No Yes Examples EXAMPLE 1 regex:matches('WSO2(.*)middleware(.*)', 'WSO2 is situated in trace and its a middleware company') Function matches the entire input.sequence against WSO2(. )middleware(. ) regex pattern, and as it matches it returns true . EXAMPLE 2 regex:matches('WSO2(.*)middleware', 'WSO2 is situated in trace and its a middleware company') Function matches the entire input.sequence against WSO2(.*)middleware regex pattern. As it does not match it returns false .","title":"matches (Function)"},{"location":"docs/api/5.1.1/#reorder","text":"","title":"Reorder"},{"location":"docs/api/5.1.1/#akslack-stream-processor","text":"Stream processor performs reordering of out-of-order events optimized for a givenparameter using AQ-K-Slack algorithm . This is best for reordering events on attributes those are used for aggregations.data . Origin: siddhi-execution-reorder:5.0.3 Syntax reorder:akslack( LONG timestamp, INT|FLOAT|LONG|DOUBLE correlation.field) reorder:akslack( LONG timestamp, INT|FLOAT|LONG|DOUBLE correlation.field, LONG batch.size) reorder:akslack( LONG timestamp, INT|FLOAT|LONG|DOUBLE correlation.field, LONG batch.size, LONG timeout) reorder:akslack( LONG timestamp, INT|FLOAT|LONG|DOUBLE correlation.field, LONG batch.size, LONG timeout, LONG max.k) reorder:akslack( LONG timestamp, INT|FLOAT|LONG|DOUBLE correlation.field, LONG batch.size, LONG timeout, LONG max.k, BOOL discard.late.arrival) reorder:akslack( LONG timestamp, INT|FLOAT|LONG|DOUBLE correlation.field, LONG batch.size, LONG timeout, LONG max.k, BOOL discard.late.arrival, DOUBLE error.threshold, DOUBLE confidence.level) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic timestamp The event timestamp on which the events should be ordered. LONG No Yes correlation.field By monitoring the changes in this field Alpha K-Slack dynamically optimises its behavior. This field is used to calculate the runtime window coverage threshold, which represents the upper limit set for unsuccessfully handled late arrivals. INT FLOAT LONG DOUBLE No Yes batch.size The parameter 'batch.size' denotes the number of events that should be considered in the calculation of an alpha value. This should be greater than or equal to 15. 10,000 LONG Yes No timeout A timeout value in milliseconds, where the buffered events who are older than the given timeout period get flushed every second. -1 (timeout is infinite) LONG Yes No max.k The maximum K-Slack window threshold ('K' parameter). 9,223,372,036,854,775,807 (The maximum Long value) LONG Yes No discard.late.arrival If set to true the processor would discarded the out-of-order events arriving later than the K-Slack window, and in otherwise it allows the late arrivals to proceed. false BOOL Yes No error.threshold The error threshold to be applied in Alpha K-Slack algorithm. 0.03 (3%) DOUBLE Yes No confidence.level The confidence level to be applied in Alpha K-Slack algorithm. 0.95 (95%) DOUBLE Yes No Examples EXAMPLE 1 define stream StockStream (eventTime long, symbol string, volume long); @info(name = 'query1') from StockStream#reorder:akslack(eventTime, volume, 20)#window.time(5 min) select eventTime, symbol, sum(volume) as total insert into OutputStream; The query reorders events based on the 'eventTime' attribute value and optimises for aggregating 'volume' attribute considering last 20 events.","title":"akslack (Stream Processor)"},{"location":"docs/api/5.1.1/#kslack-stream-processor","text":"Stream processor performs reordering of out-of-order events using K-Slack algorithm . Origin: siddhi-execution-reorder:5.0.3 Syntax reorder:kslack( LONG timestamp) reorder:kslack( LONG timestamp, LONG timeout) reorder:kslack( LONG timestamp, BOOL discard.late.arrival) reorder:kslack( LONG timestamp, LONG timeout, LONG max.k) reorder:kslack( LONG timestamp, LONG timeout, BOOL discard.late.arrival) reorder:kslack( LONG timestamp, LONG timeout, LONG max.k, BOOL discard.late.arrival) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic timestamp The event timestamp on which the events should be ordered. LONG No Yes timeout A timeout value in milliseconds, where the buffered events who are older than the given timeout period get flushed every second. -1 (timeout is infinite) LONG Yes No max.k The maximum K-Slack window threshold ('K' parameter). 9,223,372,036,854,775,807 (The maximum Long value) LONG Yes No discard.late.arrival If set to true the processor would discarded the out-of-order events arriving later than the K-Slack window, and in otherwise it allows the late arrivals to proceed. false BOOL Yes No Examples EXAMPLE 1 define stream StockStream (eventTime long, symbol string, volume long); @info(name = 'query1') from StockStream#reorder:kslack(eventTime, 5000) select eventTime, symbol, volume insert into OutputStream; The query reorders events based on the 'eventTime' attribute value, and it forcefully flushes all the events who have arrived older than the given 'timeout' value ( 5000 milliseconds) every second.","title":"kslack (Stream Processor)"},{"location":"docs/api/5.1.1/#script","text":"","title":"Script"},{"location":"docs/api/5.1.1/#javascript-script","text":"This extension allows you to include JavaScript functions within the Siddhi Query Language. Origin: siddhi-script-js:5.0.2 Syntax define function FunctionName [javascript] return type { // Script code }; Examples EXAMPLE 1 define function concatJ[JavaScript] return string {\" var str1 = data[0]; var str2 = data[1]; var str3 = data[2]; var res = str1.concat(str2,str3); return res; }; This JS function will consume 3 var variables, concatenate them and will return as a string","title":"javascript (Script)"},{"location":"docs/api/5.1.1/#sink","text":"","title":"Sink"},{"location":"docs/api/5.1.1/#email-sink","text":"The email sink uses the 'smtp' server to publish events via emails. The events can be published in 'text', 'xml' or 'json' formats. The user can define email sink parameters in either the ' SP_HOME /conf/ PROFILE /deployment yaml' file or in the stream definition. The email sink first checks the stream definition for parameters, and if they are no configured there, it checks the 'deployment.yaml' file. If the parameters are not configured in either place, default values are considered for optional parameters. If you need to configure server system parameters that are not provided as options in the stream definition, then those parameters need to be defined them in the 'deployment.yaml' file under 'email sink properties'. For more information about the SMTP server parameters, see https://javaee.github.io/javamail/SMTP-Transport. Further, some email accounts are required to enable the 'access to less secure apps' option. For gmail accounts, you can enable this option via https://myaccount.google.com/lesssecureapps. Origin: siddhi-io-email:2.0.5 Syntax @sink(type=\"email\", username=\" STRING \", address=\" STRING \", password=\" STRING \", host=\" STRING \", port=\" INT \", ssl.enable=\" BOOL \", auth=\" BOOL \", content.type=\" STRING \", subject=\" STRING \", to=\" STRING \", cc=\" STRING \", bcc=\" STRING \", attachments=\" STRING \", connection.pool.size=\" INT \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic username The username of the email account that is used to send emails. e.g., 'abc' is the username of the 'abc@gmail.com' account. STRING No No address The address of the email account that is used to send emails. STRING No No password The password of the email account. STRING No No host The host name of the SMTP server. e.g., 'smtp.gmail.com' is a host name for a gmail account. The default value 'smtp.gmail.com' is only valid if the email account is a gmail account. smtp.gmail.com STRING Yes No port The port that is used to create the connection. '465' the default value is only valid is SSL is enabled. INT Yes No ssl.enable This parameter specifies whether the connection should be established via a secure connection or not. The value can be either 'true' or 'false'. If it is 'true', then the connection is establish via the 493 port which is a secure connection. true BOOL Yes No auth This parameter specifies whether to use the 'AUTH' command when authenticating or not. If the parameter is set to 'true', an attempt is made to authenticate the user using the 'AUTH' command. true BOOL Yes No content.type The content type can be either 'text/plain' or 'text/html'. text/plain STRING Yes No subject The subject of the mail to be send. STRING No Yes to The address of the 'to' recipient. If there are more than one 'to' recipients, then all the required addresses can be given as a comma-separated list. STRING No Yes cc The address of the 'cc' recipient. If there are more than one 'cc' recipients, then all the required addresses can be given as a comma-separated list. None STRING Yes No bcc The address of the 'bcc' recipient. If there are more than one 'bcc' recipients, then all the required addresses can be given as a comma-separated list. None STRING Yes No attachments File paths of the files that need to be attached to the email. These paths should be absolute paths. They can be either directories or files . If the path is to a directory, all the files located at the first level (i.e., not within another sub directory) are attached. None STRING Yes Yes connection.pool.size Number of concurrent Email client connections. 1 INT Yes No System Parameters Name Description Default Value Possible Parameters mail.smtp.ssl.trust If this parameter is se, and a socket factory has not been specified, it enables the use of a MailSSLSocketFactory. If this parameter is set to \" \", all the hosts are trusted. If it is set to a whitespace-separated list of hosts, only those specified hosts are trusted. If not, the hosts trusted depends on the certificate presented by the server. String mail.smtp.connectiontimeout The socket connection timeout value in milliseconds. infinite timeout Any Integer mail.smtp.timeout The socket I/O timeout value in milliseconds. infinite timeout Any Integer mail.smtp.from The email address to use for the SMTP MAIL command. This sets the envelope return address. Defaults to msg.getFrom() or InternetAddress.getLocalAddress(). Any valid email address mail.smtp.localport The local port number to bind to when creating the SMTP socket. Defaults to the port number picked by the Socket class. Any Integer mail.smtp.ehlo If this parameter is set to 'false', you must not attempt to sign in with the EHLO command. true true or false mail.smtp.auth.login.disable If this is set to 'true', it is not allowed to use the 'AUTH LOGIN' command. false true or false mail.smtp.auth.plain.disable If this parameter is set to 'true', it is not allowed to use the 'AUTH PLAIN' command. false true or false mail.smtp.auth.digest-md5.disable If this parameter is set to 'true', it is not allowed to use the 'AUTH DIGEST-MD5' command. false true or false mail.smtp.auth.ntlm.disable If this parameter is set to 'true', it is not allowed to use the 'AUTH NTLM' command false true or false mail.smtp.auth.ntlm.domain The NTLM authentication domain. None The valid NTLM authentication domain name. mail.smtp.auth.ntlm.flags NTLM protocol-specific flags. For more details, see http://curl.haxx.se/rfc/ntlm.html#theNtlmFlags. None Valid NTLM protocol-specific flags. mail.smtp.dsn.notify The NOTIFY option to the RCPT command. None Either 'NEVER', or a combination of 'SUCCESS', 'FAILURE', and 'DELAY' (separated by commas). mail.smtp.dsn.ret The 'RET' option to the 'MAIL' command. None Either 'FULL' or 'HDRS'. mail.smtp.sendpartial If this parameter is set to 'true' and a message is addressed to both valid and invalid addresses, the message is sent with a log that reports the partial failure with a 'SendFailedException' error. If this parameter is set to 'false' (which is default), the message is not sent to any of the recipients when the recipient lists contain one or more invalid addresses. false true or false mail.smtp.sasl.enable If this parameter is set to 'true', the system attempts to use the 'javax.security.sasl' package to choose an authentication mechanism for the login. false true or false mail.smtp.sasl.mechanisms Enter a space or a comma-separated list of SASL mechanism names that the system shouldt try to use. None mail.smtp.sasl.authorizationid The authorization ID to be used in the SASL authentication. If no value is specified, the authentication ID (i.e., username) is used. username Valid ID mail.smtp.sasl.realm The realm to be used with the 'DIGEST-MD5' authentication. None mail.smtp.quitwait If this parameter is set to 'false', the 'QUIT' command is issued and the connection is immediately closed. If this parameter is set to 'true' (which is default), the transport waits for the response to the QUIT command. false true or false mail.smtp.reportsuccess If this parameter is set to 'true', the transport to includes an 'SMTPAddressSucceededException' for each address to which the message is successfully delivered. false true or false mail.smtp.socketFactory If this parameter is set to a class that implements the 'javax.net.SocketFactory' interface, this class is used to create SMTP sockets. None Socket Factory mail.smtp.socketFactory.class If this parameter is set, it specifies the name of a class that implements the 'javax.net.SocketFactory interface'. This class is used to create SMTP sockets. None mail.smtp.socketFactory.fallback If this parameter is set to 'true', the failure to create a socket using the specified socket factory class causes the socket to be created using the 'java.net.Socket' class. true true or false mail.smtp.socketFactory.port This specifies the port to connect to when using the specified socket factory. 25 Valid port number mail.smtp.ssl.protocols This specifies the SSL protocols that need to be enabled for the SSL connections. None This parameter specifies a whitespace separated list of tokens that are acceptable to the 'javax.net.ssl.SSLSocket.setEnabledProtocols' method. mail.smtp.starttls.enable If this parameter is set to 'true', it is possible to issue the 'STARTTLS' command (if supported by the server) to switch the connection to a TLS-protected connection before issuing any login commands. false true or false mail.smtp.starttls.required If this parameter is set to 'true', it is required to use the 'STARTTLS' command. If the server does not support the 'STARTTLS' command, or if the command fails, the connection method will fail. false true or false mail.smtp.socks.host This specifies the host name of a SOCKS5 proxy server to be used for the connections to the mail server. None mail.smtp.socks.port This specifies the port number for the SOCKS5 proxy server. This needs to be used only if the proxy server is not using the standard port number 1080. 1080 valid port number mail.smtp.auth.ntlm.disable If this parameter is set to 'true', the AUTH NTLM command cannot be issued. false true or false mail.smtp.mailextension The extension string to be appended to the MAIL command. None mail.smtp.userset If this parameter is set to 'true', you should use the 'RSET' command instead of the 'NOOP' command in the 'isConnected' method. In some scenarios, 'sendmail' responds slowly after many 'NOOP' commands. This is avoided by using 'RSET' instead. false true or false Examples EXAMPLE 1 @sink(type='email', @map(type ='json'), username='sender.account', address='sender.account@gmail.com',password='account.password',subject='Alerts from Wso2 Stream Processor',to='{{email}}',)define stream FooStream (email string, loginId int, name string); This example illustrates how to publish events via an email sink based on the values provided for the mandatory parameters. As shown in the example, it publishes events from the 'FooStream' in 'json' format as emails to the specified 'to' recipients via the email sink. The email is sent from the 'sender.account@gmail.com' email address via a secure connection. EXAMPLE 2 @sink(type='email', @map(type ='json'), subject='Alerts from Wso2 Stream Processor',to='{{email}}',)define stream FooStream (email string, loginId int, name string); This example illustrates how to configure the query parameters and the system parameters in the 'deployment.yaml' file. Corresponding parameters need to be configured under 'email', and namespace:'sink' as follows: siddhi: extensions: - extension: name:'email' namespace:'sink' properties: username: sender's email username address: sender's email address password: sender's email password As shown in the example, events from the FooStream are published in 'json' format via the email sink as emails to the given 'to' recipients. The email is sent from the 'sender.account@gmail.com' address via a secure connection. EXAMPLE 3 @sink(type='email', @map(type ='json'), username='sender.account', address='sender.account@gmail.com',password='account.password',host='smtp.gmail.com',port='465',ssl.enable='true',auth='true',content.type='text/html',subject='Alerts from Wso2 Stream Processor-{{name}}',to='to1.account@gmail.com, to2.account@gmail.com',cc='cc1.account@gmail.com, cc2.account@gmail.com',bcc='bcc1.account@gmail.com)define stream FooStream (name string, age int, country string); This example illustrates how to publish events via the email sink. Events from the 'FooStream' stream are published in 'xml' format via the email sink as a text/html message and sent to the specified 'to', 'cc', and 'bcc' recipients via a secure connection. The 'name' namespace in the 'subject' attribute is the value of the 'name' parameter in the corresponding output event. EXAMPLE 4 @sink(type='email', @map(type ='json'), username='sender.account', address='sender.account@gmail.com',password='account.password',host='smtp.gmail.com',port='465',ssl.enable='true',auth='true',content.type='text/html',subject='Alerts from Wso2 Stream Processor-{{name}}',to='to1.account@gmail.com, to2.account@gmail.com',cc='cc1.account@gmail.com, cc2.account@gmail.com',bcc='bcc1.account@gmail.comattachments= '{{attachments}}')define stream FooStream (name string, age int, country string, attachments string); This example illustrates how to publish events via the email sink. Here, the email also contains attachments. Events from the FooStream are published in 'xml' format via the email sink as a 'text/html' message to the specified 'to','cc', and 'bcc' recipients via a secure connection. The 'name' namespace in the 'subject' attribute is the value for the 'name' parameter in the corresponding output event. The attachments included in the email message are the local files available in the path specified as the value for the 'attachments' attribute.","title":"email (Sink)"},{"location":"docs/api/5.1.1/#file-sink","text":"File Sink can be used to publish (write) event data which is processed within siddhi to files. Siddhi-io-file sink provides support to write both textual and binary data into files Origin: siddhi-io-file:2.0.3 Syntax @sink(type=\"file\", file.uri=\" STRING \", append=\" BOOL \", add.line.separator=\" BOOL \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic file.uri Used to specify the file for data to be written. STRING No Yes append This parameter is used to specify whether the data should be append to the file or not. If append = 'true', data will be write at the end of the file without changing the existing content. If file does not exist, a new fill will be crated and then data will be written. If append append = 'false', If given file exists, existing content will be deleted and then data will be written back to the file. If given file does not exist, a new file will be created and then data will be written on it. true BOOL Yes No add.line.separator This parameter is used to specify whether events added to the file should be separated by a newline. If add.event.separator= 'true',then a newline will be added after data is added to the file. true. (However, if csv mapper is used, it is false) BOOL Yes No Examples EXAMPLE 1 @sink(type='file', @map(type='json'), append='false', file.uri='/abc/{{symbol}}.txt') define stream BarStream (symbol string, price float, volume long); Under above configuration, for each event, a file will be generated if there's no such a file,and then data will be written to that file as json messagesoutput will looks like below. { \"event\":{ \"symbol\":\"WSO2\", \"price\":55.6, \"volume\":100 } }","title":"file (Sink)"},{"location":"docs/api/5.1.1/#grpc-sink","text":"This extension publishes event data encoded into GRPC Classes as defined in the user input jar. This extension has a default gRPC service classes added. The default service is called \"EventService\". Please find the protobuf definition here . If we want to use our custom gRPC services, we have to pack auto-generated gRPC service classes and protobuf classes into a jar file and add it into the project classpath (or to the jars folder in the siddhi-tooling folder if we use it with siddhi-tooling ). Please find the custom protobuf definition that uses in examples here . This grpc sink is used for scenarios where we send a request and don't expect a response back. I.e getting a google.protobuf.Empty response back. Origin: siddhi-io-grpc:1.0.5 Syntax @sink(type=\"grpc\", publisher.url=\" STRING \", headers=\" STRING \", idle.timeout=\" LONG \", keep.alive.time=\" LONG \", keep.alive.timeout=\" LONG \", keep.alive.without.calls=\" BOOL \", enable.retry=\" BOOL \", max.retry.attempts=\" INT \", retry.buffer.size=\" LONG \", per.rpc.buffer.size=\" LONG \", channel.termination.waiting.time=\" LONG \", truststore.file=\" STRING \", truststore.password=\" STRING \", truststore.algorithm=\" STRING \", tls.store.type=\" STRING \", keystore.file=\" STRING \", keystore.password=\" STRING \", keystore.algorithm=\" STRING \", enable.ssl=\" BOOL \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic publisher.url The url to which the outgoing events should be published via this extension. This url should consist the host hostPort, port, fully qualified service name, method name in the following format. grpc://0.0.0.0:9763/ serviceName / methodName For example: grpc://0.0.0.0:9763/org.wso2.grpc.EventService/consume STRING No No headers GRPC Request headers in format \"' key : value ',' key : value '\" . If header parameter is not provided just the payload is sent - STRING Yes No idle.timeout Set the duration in seconds without ongoing RPCs before going to idle mode. 1800 LONG Yes No keep.alive.time Sets the time in seconds without read activity before sending a keepalive ping. Keepalives can increase the load on services so must be used with caution. By default set to Long.MAX_VALUE which disables keep alive pinging. Long.MAX_VALUE LONG Yes No keep.alive.timeout Sets the time in seconds waiting for read activity after sending a keepalive ping. 20 LONG Yes No keep.alive.without.calls Sets whether keepalive will be performed when there are no outstanding RPC on a connection. false BOOL Yes No enable.retry Enables the retry mechanism provided by the gRPC library. false BOOL Yes No max.retry.attempts Sets max number of retry attempts. The total number of retry attempts for each RPC will not exceed this number even if service config may allow a higher number. 5 INT Yes No retry.buffer.size Sets the retry buffer size in bytes. If the buffer limit is exceeded, no RPC could retry at the moment, and in hedging case all hedges but one of the same RPC will cancel. 16777216 LONG Yes No per.rpc.buffer.size Sets the per RPC buffer limit in bytes used for retry. The RPC is not retriable if its buffer limit is exceeded. 1048576 LONG Yes No channel.termination.waiting.time The time in seconds to wait for the channel to become terminated, giving up if the timeout is reached. 5 LONG Yes No truststore.file the file path of truststore. If this is provided then server authentication is enabled - STRING Yes No truststore.password the password of truststore. If this is provided then the integrity of the keystore is checked - STRING Yes No truststore.algorithm the encryption algorithm to be used for server authentication - STRING Yes No tls.store.type TLS store type - STRING Yes No keystore.file the file path of keystore. If this is provided then client authentication is enabled - STRING Yes No keystore.password the password of keystore - STRING Yes No keystore.algorithm the encryption algorithm to be used for client authentication - STRING Yes No enable.ssl to enable ssl. If set to true and truststore.file is not given then it will be set to default carbon jks by default FALSE BOOL Yes No Examples EXAMPLE 1 @sink(type='grpc', publisher.url = 'grpc://134.23.43.35:8080/org.wso2.grpc.EventService/consume', @map(type='json')) define stream FooStream (message String); Here a stream named FooStream is defined with grpc sink. A grpc server should be running at 194.23.98.100 listening to port 8080. sink.id is set to 1 here. So we can write a source with sink.id 1 so that it will listen to responses for requests published from this stream. Note that since we are using EventService/consume the sink will be operating in default mode EXAMPLE 2 @sink(type='grpc', publisher.url = 'grpc://134.23.43.35:8080/org.wso2.grpc.EventService/consume', headers='{{headers}}', @map(type='json'), @payload('{{message}}')) define stream FooStream (message String, headers String); A similar example to above but with headers. Headers are also send into the stream as a data. In the sink headers dynamic property reads the value and sends it as MetaData with the request EXAMPLE 3 @sink(type='grpc', publisher.url = 'grpc://134.23.43.35:8080/org.wso2.grpc.MyService/send', @map(type='protobuf'), define stream FooStream (stringValue string, intValue int,longValue long,booleanValue bool,floatValue float,doubleValue double); Here a stream named FooStream is defined with grpc sink. A grpc server should be running at 134.23.43.35 listening to port 8080 since there is no mapper provided, attributes of stream definition should be as same as the attributes of protobuf message definition. EXAMPLE 4 @sink(type='grpc', publisher.url = 'grpc://134.23.43.35:8080/org.wso2.grpc.MyService/testMap', @map(type='protobuf'), define stream FooStream (stringValue string, intValue int,map object); Here a stream named FooStream is defined with grpc sink. A grpc server should be running at 134.23.43.35 listening to port 8080. The 'map object' in the stream definition defines that this stream is going to use Map object with grpc service. We can use any map object that extends 'java.util.AbstractMap' class. EXAMPLE 5 @sink(type='grpc', publisher.url = 'grpc://134.23.43.35:8080/org.wso2.grpc.MyService/testMap', @map(type='protobuf', @payload(stringValue='a',longValue='b',intValue='c',booleanValue='d',floatValue = 'e', doubleValue = 'f'))) define stream FooStream (a string, b long, c int,d bool,e float,f double); Here a stream named FooStream is defined with grpc sink. A grpc server should be running at 194.23.98.100 listening to port 8080. @payload is provided in this stream, therefore we can use any name for the attributes in the stream definition, but we should correctly map those names with protobuf message attributes. If we are planning to send metadata within a stream we should use @payload to map attributes to identify the metadata attribute and the protobuf attributes separately. EXAMPLE 6 @sink(type='grpc', publisher.url = 'grpc://194.23.98.100:8888/org.wso2.grpc.test.StreamService/clientStream', @map(type='protobuf')) define stream FooStream (stringValue string, intValue int,longValue long,booleanValue bool,floatValue float,doubleValue double); Here in the grpc sink, we are sending a stream of requests to the server that runs on 194.23.98.100 and port 8888. When we need to send a stream of requests from the grpc sink we have to define a client stream RPC method.Then the siddhi will identify whether it's a unary method or a stream method and send requests according to the method type.","title":"grpc (Sink)"},{"location":"docs/api/5.1.1/#grpc-call-sink","text":"This extension publishes event data encoded into GRPC Classes as defined in the user input jar. This extension has a default gRPC service classes jar added. The default service is called \"EventService\". Please find the protobuf definition here . If we want to use our custom gRPC services, we have to pack auto-generated gRPC service classes and protobuf classes into a jar file and add it into the project classpath (or to the jars folder in the siddhi-tooling folder if we use it with siddhi-tooling ). Please find the custom protobuf definition that uses in examples here . This grpc-call sink is used for scenarios where we send a request out and expect a response back. In default mode this will use EventService process method. grpc-call-response source is used to receive the responses. A unique sink.id is used to correlate between the sink and its corresponding source. Origin: siddhi-io-grpc:1.0.5 Syntax @sink(type=\"grpc-call\", publisher.url=\" STRING \", sink.id=\" INT \", headers=\" STRING \", idle.timeout=\" LONG \", keep.alive.time=\" LONG \", keep.alive.timeout=\" LONG \", keep.alive.without.calls=\" BOOL \", enable.retry=\" BOOL \", max.retry.attempts=\" INT \", retry.buffer.size=\" LONG \", per.rpc.buffer.size=\" LONG \", channel.termination.waiting.time=\" LONG \", max.inbound.message.size=\" LONG \", max.inbound.metadata.size=\" LONG \", truststore.file=\" STRING \", truststore.password=\" STRING \", truststore.algorithm=\" STRING \", tls.store.type=\" STRING \", keystore.file=\" STRING \", keystore.password=\" STRING \", keystore.algorithm=\" STRING \", enable.ssl=\" BOOL \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic publisher.url The url to which the outgoing events should be published via this extension. This url should consist the host hostPort, port, fully qualified service name, method name in the following format. grpc://0.0.0.0:9763/ serviceName / methodName For example: grpc://0.0.0.0:9763/org.wso2.grpc.EventService/consume STRING No No sink.id a unique ID that should be set for each grpc-call-sink. There is a 1:1 mapping between grpc-call sinks and grpc-call-response sources. Each sink has one particular source listening to the responses to requests published from that sink. So the same sink.id should be given when writing the source also. INT No No headers GRPC Request headers in format \"' key : value ',' key : value '\" . If header parameter is not provided just the payload is sent - STRING Yes No idle.timeout Set the duration in seconds without ongoing RPCs before going to idle mode. 1800 LONG Yes No keep.alive.time Sets the time in seconds without read activity before sending a keepalive ping. Keepalives can increase the load on services so must be used with caution. By default set to Long.MAX_VALUE which disables keep alive pinging. Long.MAX_VALUE LONG Yes No keep.alive.timeout Sets the time in seconds waiting for read activity after sending a keepalive ping. 20 LONG Yes No keep.alive.without.calls Sets whether keepalive will be performed when there are no outstanding RPC on a connection. false BOOL Yes No enable.retry Enables the retry and hedging mechanism provided by the gRPC library. false BOOL Yes No max.retry.attempts Sets max number of retry attempts. The total number of retry attempts for each RPC will not exceed this number even if service config may allow a higher number. 5 INT Yes No retry.buffer.size Sets the retry buffer size in bytes. If the buffer limit is exceeded, no RPC could retry at the moment, and in hedging case all hedges but one of the same RPC will cancel. 16777216 LONG Yes No per.rpc.buffer.size Sets the per RPC buffer limit in bytes used for retry. The RPC is not retriable if its buffer limit is exceeded. 1048576 LONG Yes No channel.termination.waiting.time The time in seconds to wait for the channel to become terminated, giving up if the timeout is reached. 5 LONG Yes No max.inbound.message.size Sets the maximum message size allowed to be received on the channel in bytes 4194304 LONG Yes No max.inbound.metadata.size Sets the maximum size of metadata allowed to be received in bytes 8192 LONG Yes No truststore.file the file path of truststore. If this is provided then server authentication is enabled - STRING Yes No truststore.password the password of truststore. If this is provided then the integrity of the keystore is checked - STRING Yes No truststore.algorithm the encryption algorithm to be used for server authentication - STRING Yes No tls.store.type TLS store type - STRING Yes No keystore.file the file path of keystore. If this is provided then client authentication is enabled - STRING Yes No keystore.password the password of keystore - STRING Yes No keystore.algorithm the encryption algorithm to be used for client authentication - STRING Yes No enable.ssl to enable ssl. If set to true and truststore.file is not given then it will be set to default carbon jks by default FALSE BOOL Yes No Examples EXAMPLE 1 @sink(type='grpc-call', publisher.url = 'grpc://194.23.98.100:8080/EventService/process', sink.id= '1', @map(type='json')) define stream FooStream (message String); @source(type='grpc-call-response', sink.id= '1') define stream BarStream (message String); Here a stream named FooStream is defined with grpc sink. A grpc server should be running at 194.23.98.100 listening to port 8080. sink.id is set to 1 here. So we can write a source with sink.id 1 so that it will listen to responses for requests published from this stream. Note that since we are using EventService/process the sink will be operating in default mode EXAMPLE 2 @sink(type='grpc-call', publisher.url = 'grpc://194.23.98.100:8080/EventService/process', sink.id= '1', @map(type='json')) define stream FooStream (message String); @source(type='grpc-call-response', sink.id= '1') define stream BarStream (message String); Here with the same FooStream definition we have added a BarStream which has a grpc-call-response source with the same sink.id 1. So the responses for calls sent from the FooStream will be added to BarStream. EXAMPLE 3 @sink(type='grpc-call', publisher.url = 'grpc://194.23.98.100:8888/org.wso2.grpc.test.MyService/process', sink.id= '1', @map(type='protobuf')) define stream FooStream (stringValue string, intValue int,longValue long,booleanValue bool,floatValue float,doubleValue double); @source(type='grpc-call-response', receiver.url = 'grpc://localhost:8888/org.wso2.grpc.MyService/process', sink.id= '1', @map(type='protobuf'))define stream FooStream (stringValue string, intValue int,longValue long,booleanValue bool,floatValue float,doubleValue double); Here a stream named FooStream is defined with grpc sink. A grpc server should be running at 194.23.98.100 listening to port 8080. We have added another stream called BarStream which is a grpc-call-response source with the same sink.id 1 and as same as FooStream definition. So the responses for calls sent from the FooStream will be added to BarStream. Since there is no mapping available in the stream definition attributes names should be as same as the attributes of the protobuf message definition. (Here the only reason we provide receiver.url in the grpc-call-response source is for protobuf mapper to map Response into a siddhi event, we can give any address and any port number in the URL, but we should provide the service name and the method name correctly) EXAMPLE 4 @sink(type='grpc-call', publisher.url = 'grpc://194.23.98.100:8888/org.wso2.grpc.test.MyService/process', sink.id= '1', @map(type='protobuf', @payload(stringValue='a',longValue='c',intValue='b',booleanValue='d',floatValue = 'e', doubleValue = 'f')))define stream FooStream (a string, b int,c long,d bool,e float,f double); @source(type='grpc-call-response', receiver.url = 'grpc://localhost:8888/org.wso2.grpc.test.MyService/process', sink.id= '1', @map(type='protobuf',@attributes(a = 'stringValue', b = 'intValue', c = 'longValue',d = 'booleanValue', e ='floatValue', f ='doubleValue')))define stream FooStream (a string, b int,c long,d bool,e float,f double); Here with the same FooStream definition we have added a BarStream which has a grpc-call-response source with the same sink.id 1. So the responses for calls sent from the FooStream will be added to BarStream. In this stream we provided mapping for both the sink and the source. so we can use any name for the attributes in the stream definition, but we have to map those attributes with correct protobuf attributes. As same as the grpc-sink, if we are planning to use metadata we should map the attributes.","title":"grpc-call (Sink)"},{"location":"docs/api/5.1.1/#grpc-service-response-sink","text":"This extension is used to send responses back to a gRPC client after receiving requests through grpc-service source. This correlates with the particular source using a unique source.id Origin: siddhi-io-grpc:1.0.5 Syntax @sink(type=\"grpc-service-response\", source.id=\" INT \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic source.id A unique id to identify the correct source to which this sink is mapped. There is a 1:1 mapping between source and sink INT No No Examples EXAMPLE 1 @sink(type='grpc-service-response', source.id='1', @map(type='json')) define stream BarStream (messageId String, message String); @source(type='grpc-service', url='grpc://134.23.43.35:8080/org.wso2.grpc.EventService/process', source.id='1', @map(type='json', @attributes(messageId='trp:messageId', message='message'))) define stream FooStream (messageId String, message String); from FooStream select * insert into BarStream; The grpc requests are received through the grpc-service sink. Each received event is sent back through grpc-service-source. This is just a passthrough through Siddhi as we are selecting everything from FooStream and inserting into BarStream.","title":"grpc-service-response (Sink)"},{"location":"docs/api/5.1.1/#http-sink","text":"HTTP sink publishes messages via HTTP or HTTPS protocols using methods such as POST, GET, PUT, and DELETE on formats text , XML and JSON . It can also publish to endpoints protected by basic authentication or OAuth 2.0. Origin: siddhi-io-http:2.2.0 Syntax @sink(type=\"http\", publisher.url=\" STRING \", basic.auth.username=\" STRING \", basic.auth.password=\" STRING \", https.truststore.file=\" STRING \", https.truststore.password=\" STRING \", oauth.username=\" STRING \", oauth.password=\" STRING \", consumer.key=\" STRING \", consumer.secret=\" STRING \", token.url=\" STRING \", refresh.token=\" STRING \", headers=\" STRING \", method=\" STRING \", socket.idle.timeout=\" INT \", chunk.disabled=\" BOOL \", ssl.protocol=\" STRING \", ssl.verification.disabled=\" BOOL \", tls.store.type=\" STRING \", ssl.configurations=\" STRING \", proxy.host=\" STRING \", proxy.port=\" STRING \", proxy.username=\" STRING \", proxy.password=\" STRING \", client.bootstrap.configurations=\" STRING \", max.pool.active.connections=\" INT \", min.pool.idle.connections=\" INT \", max.pool.idle.connections=\" INT \", min.evictable.idle.time=\" STRING \", time.between.eviction.runs=\" STRING \", max.wait.time=\" STRING \", test.on.borrow=\" BOOL \", test.while.idle=\" BOOL \", exhausted.action=\" INT \", hostname.verification.enabled=\" BOOL \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic publisher.url The URL to which the outgoing events should be published. Examples: http://localhost:8080/endpoint , https://localhost:8080/endpoint STRING No No basic.auth.username The username to be included in the authentication header when calling endpoints protected by basic authentication. basic.auth.password property should be also set when using this property. - STRING Yes No basic.auth.password The password to be included in the authentication header when calling endpoints protected by basic authentication. basic.auth.username property should be also set when using this property. - STRING Yes No https.truststore.file The file path of the client truststore when sending messages through https protocol. ${carbon.home}/resources/security/client-truststore.jks STRING Yes No https.truststore.password The password for the client-truststore. wso2carbon STRING Yes No oauth.username The username to be included in the authentication header when calling endpoints protected by OAuth 2.0. oauth.password property should be also set when using this property. - STRING Yes No oauth.password The password to be included in the authentication header when calling endpoints protected by OAuth 2.0. oauth.username property should be also set when using this property. - STRING Yes No consumer.key Consumer key used for calling endpoints protected by OAuth 2.0 - STRING Yes No consumer.secret Consumer secret used for calling endpoints protected by OAuth 2.0 - STRING Yes No token.url Token URL to generate a new access tokens when calling endpoints protected by OAuth 2.0 - STRING Yes No refresh.token Refresh token used for generating new access tokens when calling endpoints protected by OAuth 2.0 - STRING Yes No headers HTTP request headers in format \"' key : value ',' key : value '\" . When Content-Type header is not provided the system derives the Content-Type based on the provided sink mapper as following: - @map(type='xml') : application/xml - @map(type='json') : application/json - @map(type='text') : plain/text - @map(type='keyvalue') : application/x-www-form-urlencoded - For all other cases system defaults to plain/text Also the Content-Length header need not to be provided, as the system automatically defines it by calculating the size of the payload. Content-Type and Content-Length headers STRING Yes No method The HTTP method used for calling the endpoint. POST STRING Yes No socket.idle.timeout Socket timeout in millis. 6000 INT Yes No chunk.disabled Disable chunked transfer encoding. false BOOL Yes No ssl.protocol SSL/TLS protocol. TLS STRING Yes No ssl.verification.disabled Disable SSL verification. false BOOL Yes No tls.store.type TLS store type. JKS STRING Yes No ssl.configurations SSL/TSL configurations in format \"' key : value ',' key : value '\" . Some supported parameters: - SSL/TLS protocols: 'sslEnabledProtocols:TLSv1.1,TLSv1.2' - List of ciphers: 'ciphers:TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256' - Enable session creation: 'client.enable.session.creation:true' - Supported server names: 'server.suported.server.names:server' - Add HTTP SNIMatcher: 'server.supported.snimatchers:SNIMatcher' - STRING Yes No proxy.host Proxy server host - STRING Yes No proxy.port Proxy server port - STRING Yes No proxy.username Proxy server username - STRING Yes No proxy.password Proxy server password - STRING Yes No client.bootstrap.configurations Client bootstrap configurations in format \"' key : value ',' key : value '\" . Some supported configurations : - Client connect timeout in millis: 'client.bootstrap.connect.timeout:15000' - Client socket timeout in seconds: 'client.bootstrap.socket.timeout:15' - Client socket reuse: 'client.bootstrap.socket.reuse:true' - Enable TCP no delay: 'client.bootstrap.nodelay:true' - Enable client keep alive: 'client.bootstrap.keepalive:true' - Send buffer size: 'client.bootstrap.sendbuffersize:1048576' - Receive buffer size: 'client.bootstrap.recievebuffersize:1048576' - STRING Yes No max.pool.active.connections Maximum possible number of active connection per client pool. -1 INT Yes No min.pool.idle.connections Minimum number of idle connections that can exist per client pool. 0 INT Yes No max.pool.idle.connections Maximum number of idle connections that can exist per client pool. 100 INT Yes No min.evictable.idle.time Minimum time (in millis) a connection may sit idle in the client pool before it become eligible for eviction. 300000 STRING Yes No time.between.eviction.runs Time between two eviction operations (in millis) on the client pool. 30000 STRING Yes No max.wait.time The maximum time (in millis) the pool will wait (when there are no available connections) for a connection to be returned to the pool. 60000 STRING Yes No test.on.borrow Enable connections to be validated before being borrowed from the client pool. true BOOL Yes No test.while.idle Enable connections to be validated during the eviction operation (if any). true BOOL Yes No exhausted.action Action that should be taken when the maximum number of active connections are being used. This action should be indicated as an int and possible action values are following. 0 - Fail the request. 1 - Block the request, until a connection returns to the pool. 2 - Grow the connection pool size. 1 (Block when exhausted) INT Yes No hostname.verification.enabled Enable hostname verification. true BOOL Yes No System Parameters Name Description Default Value Possible Parameters clientBootstrapClientGroupSize Number of client threads to perform non-blocking read and write to one or more channels. (Number of available processors) * 2 Any positive integer clientBootstrapBossGroupSize Number of boss threads to accept incoming connections. Number of available processors Any positive integer clientBootstrapWorkerGroupSize Number of worker threads to accept the connections from boss threads and perform non-blocking read and write from one or more channels. (Number of available processors) * 2 Any positive integer trustStoreLocation The default truststore file path. ${carbon.home}/resources/security/client-truststore.jks Path to client truststore .jks file trustStorePassword The default truststore password. wso2carbon Truststore password as string Examples EXAMPLE 1 @sink(type = 'http', publisher.url = 'http://stocks.com/stocks', @map(type = 'json')) define stream StockStream (symbol string, price float, volume long); Events arriving on the StockStream will be published to the HTTP endpoint http://stocks.com/stocks using POST method with Content-Type application/json by converting those events to the default JSON format as following: { \"event\": { \"symbol\": \"FB\", \"price\": 24.5, \"volume\": 5000 } } EXAMPLE 2 @sink(type='http', publisher.url = 'http://localhost:8009/foo', client.bootstrap.configurations = \"'client.bootstrap.socket.timeout:20'\", max.pool.active.connections = '1', headers = \"{{headers}}\", @map(type='xml', @payload(\"\"\" stock {{payloadBody}} /stock \"\"\"))) define stream FooStream (payloadBody String, headers string); Events arriving on FooStream will be published to the HTTP endpoint http://localhost:8009/foo using POST method with Content-Type application/xml and setting payloadBody and header attribute values. If the payloadBody contains symbol WSO2 /symbol price 55.6 /price volume 100 /volume and header contains 'topic:foobar' values, then the system will generate an output with the body: stock symbol WSO2 /symbol price 55.6 /price volume 100 /volume /stock and HTTP headers: Content-Length:xxx , Content-Location:'xxx' , Content-Type:'application/xml' , HTTP_METHOD:'POST'","title":"http (Sink)"},{"location":"docs/api/5.1.1/#http-call-sink","text":"The http-call sink publishes messages to endpoints via HTTP or HTTPS protocols using methods such as POST, GET, PUT, and DELETE on formats text , XML or JSON and consume responses through its corresponding http-call-response source. It also supports calling endpoints protected with basic authentication or OAuth 2.0. Origin: siddhi-io-http:2.2.0 Syntax @sink(type=\"http-call\", publisher.url=\" STRING \", sink.id=\" STRING \", basic.auth.username=\" STRING \", basic.auth.password=\" STRING \", https.truststore.file=\" STRING \", https.truststore.password=\" STRING \", oauth.username=\" STRING \", oauth.password=\" STRING \", consumer.key=\" STRING \", consumer.secret=\" STRING \", token.url=\" STRING \", refresh.token=\" STRING \", headers=\" STRING \", method=\" STRING \", downloading.enabled=\" BOOL \", download.path=\" STRING \", blocking.io=\" BOOL \", socket.idle.timeout=\" INT \", chunk.disabled=\" BOOL \", ssl.protocol=\" STRING \", ssl.verification.disabled=\" BOOL \", ssl.configurations=\" STRING \", proxy.host=\" STRING \", proxy.port=\" STRING \", proxy.username=\" STRING \", proxy.password=\" STRING \", client.bootstrap.configurations=\" STRING \", max.pool.active.connections=\" INT \", min.pool.idle.connections=\" INT \", max.pool.idle.connections=\" INT \", min.evictable.idle.time=\" STRING \", time.between.eviction.runs=\" STRING \", max.wait.time=\" STRING \", test.on.borrow=\" BOOL \", test.while.idle=\" BOOL \", exhausted.action=\" INT \", hostname.verification.enabled=\" BOOL \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic publisher.url The URL which should be called. Examples: http://localhost:8080/endpoint , https://localhost:8080/endpoint STRING No No sink.id Identifier to correlate the http-call sink to its corresponding http-call-response sources to retrieved the responses. STRING No No basic.auth.username The username to be included in the authentication header when calling endpoints protected by basic authentication. basic.auth.password property should be also set when using this property. - STRING Yes No basic.auth.password The password to be included in the authentication header when calling endpoints protected by basic authentication. basic.auth.username property should be also set when using this property. - STRING Yes No https.truststore.file The file path of the client truststore when sending messages through https protocol. ${carbon.home}/resources/security/client-truststore.jks STRING Yes No https.truststore.password The password for the client-truststore. wso2carbon STRING Yes No oauth.username The username to be included in the authentication header when calling endpoints protected by OAuth 2.0. oauth.password property should be also set when using this property. - STRING Yes No oauth.password The password to be included in the authentication header when calling endpoints protected by OAuth 2.0. oauth.username property should be also set when using this property. - STRING Yes No consumer.key Consumer key used for calling endpoints protected by OAuth 2.0 - STRING Yes No consumer.secret Consumer secret used for calling endpoints protected by OAuth 2.0 - STRING Yes No token.url Token URL to generate a new access tokens when calling endpoints protected by OAuth 2.0 - STRING Yes No refresh.token Refresh token used for generating new access tokens when calling endpoints protected by OAuth 2.0 - STRING Yes No headers HTTP request headers in format \"' key : value ',' key : value '\" . When the Content-Type header is not provided the system decides the Content-Type based on the provided sink mapper as following: - @map(type='xml') : application/xml - @map(type='json') : application/json - @map(type='text') : plain/text - @map(type='keyvalue') : application/x-www-form-urlencoded - For all other cases system defaults to plain/text Also the Content-Length header need not to be provided, as the system automatically defines it by calculating the size of the payload. Content-Type and Content-Length headers STRING Yes No method The HTTP method used for calling the endpoint. POST STRING Yes No downloading.enabled Enable response received by the http-call-response source to be written to a file. When this is enabled the download.path property should be also set. false BOOL Yes No download.path The absolute file path along with the file name where the downloads should be saved. - STRING Yes Yes blocking.io Blocks the request thread until a response it received from HTTP call-response source before sending any other request. false BOOL Yes No socket.idle.timeout Socket timeout in millis. 6000 INT Yes No chunk.disabled Disable chunked transfer encoding. false BOOL Yes No ssl.protocol SSL/TLS protocol. TLS STRING Yes No ssl.verification.disabled Disable SSL verification. false BOOL Yes No ssl.configurations SSL/TSL configurations. Expected format \"' key : value ',' key : value '\" . Some supported parameters: - SSL/TLS protocols: 'sslEnabledProtocols:TLSv1.1,TLSv1.2' - List of ciphers: 'ciphers:TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256' - Enable session creation: 'client.enable.session.creation:true' - Supported server names: 'server.suported.server.names:server' - Add HTTP SNIMatcher: 'server.supported.snimatchers:SNIMatcher' - STRING Yes No proxy.host Proxy server host - STRING Yes No proxy.port Proxy server port - STRING Yes No proxy.username Proxy server username - STRING Yes No proxy.password Proxy server password - STRING Yes No client.bootstrap.configurations Client bootstrap configurations in format \"' key : value ',' key : value '\" . Some supported configurations : - Client connect timeout in millis: 'client.bootstrap.connect.timeout:15000' - Client socket timeout in seconds: 'client.bootstrap.socket.timeout:15' - Client socket reuse: 'client.bootstrap.socket.reuse:true' - Enable TCP no delay: 'client.bootstrap.nodelay:true' - Enable client keep alive: 'client.bootstrap.keepalive:true' - Send buffer size: 'client.bootstrap.sendbuffersize:1048576' - Receive buffer size: 'client.bootstrap.recievebuffersize:1048576' - STRING Yes No max.pool.active.connections Maximum possible number of active connection per client pool. -1 INT Yes No min.pool.idle.connections Minimum number of idle connections that can exist per client pool. 0 INT Yes No max.pool.idle.connections Maximum number of idle connections that can exist per client pool. 100 INT Yes No min.evictable.idle.time Minimum time (in millis) a connection may sit idle in the client pool before it become eligible for eviction. 300000 STRING Yes No time.between.eviction.runs Time between two eviction operations (in millis) on the client pool. 30000 STRING Yes No max.wait.time The maximum time (in millis) the pool will wait (when there are no available connections) for a connection to be returned to the pool. 60000 STRING Yes No test.on.borrow Enable connections to be validated before being borrowed from the client pool. true BOOL Yes No test.while.idle Enable connections to be validated during the eviction operation (if any). true BOOL Yes No exhausted.action Action that should be taken when the maximum number of active connections are being used. This action should be indicated as an int and possible action values are following. 0 - Fail the request. 1 - Block the request, until a connection returns to the pool. 2 - Grow the connection pool size. 1 (Block when exhausted) INT Yes No hostname.verification.enabled Enable hostname verification true BOOL Yes No System Parameters Name Description Default Value Possible Parameters clientBootstrapClientGroupSize Number of client threads to perform non-blocking read and write to one or more channels. (Number of available processors) * 2 Any positive integer clientBootstrapBossGroupSize Number of boss threads to accept incoming connections. Number of available processors Any positive integer clientBootstrapWorkerGroupSize Number of worker threads to accept the connections from boss threads and perform non-blocking read and write from one or more channels. (Number of available processors) * 2 Any positive integer trustStoreLocation The default truststore file path. ${carbon.home}/resources/security/client-truststore.jks Path to client truststore .jks file trustStorePassword The default truststore password. wso2carbon Truststore password as string Examples EXAMPLE 1 @sink(type='http-call', sink.id='foo', publisher.url='http://localhost:8009/foo', @map(type='xml', @payload('{{payloadBody}}'))) define stream FooStream (payloadBody string); @source(type='http-call-response', sink.id='foo', @map(type='text', regex.A='((.|\\n)*)', @attributes(headers='trp:headers', message='A[1]'))) define stream ResponseStream(message string, headers string); When events arrive in FooStream , http-call sink makes calls to endpoint on url http://localhost:8009/foo with POST method and Content-Type application/xml . If the event payloadBody attribute contains following XML: item name apple /name price 55 /price quantity 5 /quantity /item the http-call sink maps that and sends it to the endpoint. When endpoint sends a response it will be consumed by the corresponding http-call-response source correlated via the same sink.id foo and that will map the response message and send it via ResponseStream steam by assigning the message body as message attribute and response headers as headers attribute of the event. EXAMPLE 2 @sink(type='http-call', publisher.url='http://localhost:8005/files/{{name}}' downloading.enabled='true', download.path='{{downloadPath}}{{name}}', method='GET', sink.id='download', @map(type='json')) define stream DownloadRequestStream(name String, id int, downloadPath string); @source(type='http-call-response', sink.id='download', http.status.code='2\\\\d+', @map(type='text', regex.A='((.|\\n)*)', @attributes(name='trp:name', id='trp:id', file='A[1]'))) define stream ResponseStream2xx(name string, id string, file string); @source(type='http-call-response', sink.id='download', http.status.code='4\\\\d+', @map(type='text', regex.A='((.|\\n)*)', @attributes(errorMsg='A[1]'))) define stream ResponseStream4xx(errorMsg string); When events arrive in DownloadRequestStream with name : foo.txt , id : 75 and downloadPath : /user/download/ the http-call sink sends a GET request to the url http://localhost:8005/files/foo.txt to download the file to the given path /user/download/foo.txt and capture the response via its corresponding http-call-response source based on the response status code. If the response status code is in the range of 200 the message will be received by the http-call-response source associated with the ResponseStream2xx stream which expects http.status.code with regex 2\\d+ while downloading the file to the local file system on the path /user/download/foo.txt and mapping the response message having the absolute file path to event's file attribute. If the response status code is in the range of 400 then the message will be received by the http-call-response source associated with the ResponseStream4xx stream which expects http.status.code with regex 4\\d+ while mapping the error response to the errorMsg attribute of the event.","title":"http-call (Sink)"},{"location":"docs/api/5.1.1/#http-request-sink","text":"Deprecated (Use http-call sink instead). The http-request sink publishes messages to endpoints via HTTP or HTTPS protocols using methods such as POST, GET, PUT, and DELETE on formats text , XML or JSON and consume responses through its corresponding http-response source. It also supports calling endpoints protected with basic authentication or OAuth 2.0. Origin: siddhi-io-http:2.2.0 Syntax @sink(type=\"http-request\", publisher.url=\" STRING \", sink.id=\" STRING \", basic.auth.username=\" STRING \", basic.auth.password=\" STRING \", https.truststore.file=\" STRING \", https.truststore.password=\" STRING \", oauth.username=\" STRING \", oauth.password=\" STRING \", consumer.key=\" STRING \", consumer.secret=\" STRING \", token.url=\" STRING \", refresh.token=\" STRING \", headers=\" STRING \", method=\" STRING \", downloading.enabled=\" BOOL \", download.path=\" STRING \", blocking.io=\" BOOL \", socket.idle.timeout=\" INT \", chunk.disabled=\" BOOL \", ssl.protocol=\" STRING \", ssl.verification.disabled=\" BOOL \", ssl.configurations=\" STRING \", proxy.host=\" STRING \", proxy.port=\" STRING \", proxy.username=\" STRING \", proxy.password=\" STRING \", client.bootstrap.configurations=\" STRING \", max.pool.active.connections=\" INT \", min.pool.idle.connections=\" INT \", max.pool.idle.connections=\" INT \", min.evictable.idle.time=\" STRING \", time.between.eviction.runs=\" STRING \", max.wait.time=\" STRING \", test.on.borrow=\" BOOL \", test.while.idle=\" BOOL \", exhausted.action=\" INT \", hostname.verification.enabled=\" BOOL \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic publisher.url The URL which should be called. Examples: http://localhost:8080/endpoint , https://localhost:8080/endpoint STRING No No sink.id Identifier to correlate the http-request sink to its corresponding http-response sources to retrieved the responses. STRING No No basic.auth.username The username to be included in the authentication header when calling endpoints protected by basic authentication. basic.auth.password property should be also set when using this property. - STRING Yes No basic.auth.password The password to be included in the authentication header when calling endpoints protected by basic authentication. basic.auth.username property should be also set when using this property. - STRING Yes No https.truststore.file The file path of the client truststore when sending messages through https protocol. ${carbon.home}/resources/security/client-truststore.jks STRING Yes No https.truststore.password The password for the client-truststore. wso2carbon STRING Yes No oauth.username The username to be included in the authentication header when calling endpoints protected by OAuth 2.0. oauth.password property should be also set when using this property. - STRING Yes No oauth.password The password to be included in the authentication header when calling endpoints protected by OAuth 2.0. oauth.username property should be also set when using this property. - STRING Yes No consumer.key Consumer key used for calling endpoints protected by OAuth 2.0 - STRING Yes No consumer.secret Consumer secret used for calling endpoints protected by OAuth 2.0 - STRING Yes No token.url Token URL to generate a new access tokens when calling endpoints protected by OAuth 2.0 - STRING Yes No refresh.token Refresh token used for generating new access tokens when calling endpoints protected by OAuth 2.0 - STRING Yes No headers HTTP request headers in format \"' key : value ',' key : value '\" . When the Content-Type header is not provided the system decides the Content-Type based on the provided sink mapper as following: - @map(type='xml') : application/xml - @map(type='json') : application/json - @map(type='text') : plain/text - @map(type='keyvalue') : application/x-www-form-urlencoded - For all other cases system defaults to plain/text Also the Content-Length header need not to be provided, as the system automatically defines it by calculating the size of the payload. Content-Type and Content-Length headers STRING Yes No method The HTTP method used for calling the endpoint. POST STRING Yes No downloading.enabled Enable response received by the http-response source to be written to a file. When this is enabled the download.path property should be also set. false BOOL Yes No download.path The absolute file path along with the file name where the downloads should be saved. - STRING Yes Yes blocking.io Blocks the request thread until a response it received from HTTP call-response source before sending any other request. false BOOL Yes No socket.idle.timeout Socket timeout in millis. 6000 INT Yes No chunk.disabled Disable chunked transfer encoding. false BOOL Yes No ssl.protocol SSL/TLS protocol. TLS STRING Yes No ssl.verification.disabled Disable SSL verification. false BOOL Yes No ssl.configurations SSL/TSL configurations in format \"' key : value ',' key : value '\" . Some supported parameters: - SSL/TLS protocols: 'sslEnabledProtocols:TLSv1.1,TLSv1.2' - List of ciphers: 'ciphers:TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256' - Enable session creation: 'client.enable.session.creation:true' - Supported server names: 'server.suported.server.names:server' - Add HTTP SNIMatcher: 'server.supported.snimatchers:SNIMatcher' - STRING Yes No proxy.host Proxy server host - STRING Yes No proxy.port Proxy server port - STRING Yes No proxy.username Proxy server username - STRING Yes No proxy.password Proxy server password - STRING Yes No client.bootstrap.configurations Client bootstrap configurations in format \"' key : value ',' key : value '\" . Some supported configurations : - Client connect timeout in millis: 'client.bootstrap.connect.timeout:15000' - Client socket timeout in seconds: 'client.bootstrap.socket.timeout:15' - Client socket reuse: 'client.bootstrap.socket.reuse:true' - Enable TCP no delay: 'client.bootstrap.nodelay:true' - Enable client keep alive: 'client.bootstrap.keepalive:true' - Send buffer size: 'client.bootstrap.sendbuffersize:1048576' - Receive buffer size: 'client.bootstrap.recievebuffersize:1048576' - STRING Yes No max.pool.active.connections Maximum possible number of active connection per client pool. -1 INT Yes No min.pool.idle.connections Minimum number of idle connections that can exist per client pool. 0 INT Yes No max.pool.idle.connections Maximum number of idle connections that can exist per client pool. 100 INT Yes No min.evictable.idle.time Minimum time (in millis) a connection may sit idle in the client pool before it become eligible for eviction. 300000 STRING Yes No time.between.eviction.runs Time between two eviction operations (in millis) on the client pool. 30000 STRING Yes No max.wait.time The maximum time (in millis) the pool will wait (when there are no available connections) for a connection to be returned to the pool. 60000 STRING Yes No test.on.borrow Enable connections to be validated before being borrowed from the client pool. true BOOL Yes No test.while.idle Enable connections to be validated during the eviction operation (if any). true BOOL Yes No exhausted.action Action that should be taken when the maximum number of active connections are being used. This action should be indicated as an int and possible action values are following. 0 - Fail the request. 1 - Block the request, until a connection returns to the pool. 2 - Grow the connection pool size. 1 (Block when exhausted) INT Yes No hostname.verification.enabled Enable hostname verification true BOOL Yes No System Parameters Name Description Default Value Possible Parameters clientBootstrapClientGroupSize Number of client threads to perform non-blocking read and write to one or more channels. (Number of available processors) * 2 Any positive integer clientBootstrapBossGroupSize Number of boss threads to accept incoming connections. Number of available processors Any positive integer clientBootstrapWorkerGroupSize Number of worker threads to accept the connections from boss threads and perform non-blocking read and write from one or more channels. (Number of available processors) * 2 Any positive integer trustStoreLocation The default truststore file path. ${carbon.home}/resources/security/client-truststore.jks Path to client truststore .jks file trustStorePassword The default truststore password. wso2carbon Truststore password as string Examples EXAMPLE 1 @sink(type='http-request', sink.id='foo', publisher.url='http://localhost:8009/foo', @map(type='xml', @payload('{{payloadBody}}'))) define stream FooStream (payloadBody string); @source(type='http-response', sink.id='foo', @map(type='text', regex.A='((.|\\n)*)', @attributes(headers='trp:headers', message='A[1]'))) define stream ResponseStream(message string, headers string); When events arrive in FooStream , http-request sink makes calls to endpoint on url http://localhost:8009/foo with POST method and Content-Type application/xml . If the event payloadBody attribute contains following XML: item name apple /name price 55 /price quantity 5 /quantity /item the http-request sink maps that and sends it to the endpoint. When endpoint sends a response it will be consumed by the corresponding http-response source correlated via the same sink.id foo and that will map the response message and send it via ResponseStream steam by assigning the message body as message attribute and response headers as headers attribute of the event. EXAMPLE 2 @sink(type='http-request', publisher.url='http://localhost:8005/files/{{name}}' downloading.enabled='true', download.path='{{downloadPath}}{{name}}', method='GET', sink.id='download', @map(type='json')) define stream DownloadRequestStream(name String, id int, downloadPath string); @source(type='http-response', sink.id='download', http.status.code='2\\\\d+', @map(type='text', regex.A='((.|\\n)*)', @attributes(name='trp:name', id='trp:id', file='A[1]'))) define stream ResponseStream2xx(name string, id string, file string); @source(type='http-response', sink.id='download', http.status.code='4\\\\d+', @map(type='text', regex.A='((.|\\n)*)', @attributes(errorMsg='A[1]'))) define stream ResponseStream4xx(errorMsg string); When events arrive in DownloadRequestStream with name : foo.txt , id : 75 and downloadPath : /user/download/ the http-request sink sends a GET request to the url http://localhost:8005/files/foo.txt to download the file to the given path /user/download/foo.txt and capture the response via its corresponding http-response source based on the response status code. If the response status code is in the range of 200 the message will be received by the http-response source associated with the ResponseStream2xx stream which expects http.status.code with regex 2\\d+ while downloading the file to the local file system on the path /user/download/foo.txt and mapping the response message having the absolute file path to event's file attribute. If the response status code is in the range of 400 then the message will be received by the http-response source associated with the ResponseStream4xx stream which expects http.status.code with regex 4\\d+ while mapping the error response to the errorMsg attribute of the event.","title":"http-request (Sink)"},{"location":"docs/api/5.1.1/#http-response-sink","text":"Deprecated (Use http-service-response sink instead). The http-response sink send responses of the requests consumed by its corresponding http-request source, by mapping the response messages to formats such as text , XML and JSON . Origin: siddhi-io-http:2.2.0 Syntax @sink(type=\"http-response\", source.id=\" STRING \", message.id=\" STRING \", headers=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic source.id Identifier to correlate the http-response sink to its corresponding http-request source which consumed the request. STRING No No message.id Identifier to correlate the response with the request received by http-request source. STRING No Yes headers HTTP request headers in format \"' key : value ',' key : value '\" . When the Content-Type header is not provided the system decides the Content-Type based on the provided sink mapper as following: - @map(type='xml') : application/xml - @map(type='json') : application/json - @map(type='text') : plain/text - @map(type='keyvalue') : application/x-www-form-urlencoded - For all other cases system defaults to plain/text Also the Content-Length header need not to be provided, as the system automatically defines it by calculating the size of the payload. Content-Type and Content-Length headers STRING Yes No Examples EXAMPLE 1 @source(type='http-request', receiver.url='http://localhost:5005/add', source.id='adder', @map(type='json, @attributes(messageId='trp:messageId', value1='$.event.value1', value2='$.event.value2'))) define stream AddStream (messageId string, value1 long, value2 long); @sink(type='http-response', source.id='adder', message.id='{{messageId}}', @map(type = 'json')) define stream ResultStream (messageId string, results long); @info(name = 'query1') from AddStream select messageId, value1 + value2 as results insert into ResultStream; The http-request source on stream AddStream listens on url http://localhost:5005/stocks for JSON messages with format: { \"event\": { \"value1\": 3, \"value2\": 4 } } and when events arrive it maps to AddStream events and pass them to query query1 for processing. The query results produced on ResultStream are sent as a response via http-response sink with format: { \"event\": { \"results\": 7 } } Here the request and response are correlated by passing the messageId produced by the http-request to the respective http-response sink.","title":"http-response (Sink)"},{"location":"docs/api/5.1.1/#http-service-response-sink","text":"The http-service-response sink send responses of the requests consumed by its corresponding http-service source, by mapping the response messages to formats such as text , XML and JSON . Origin: siddhi-io-http:2.2.0 Syntax @sink(type=\"http-service-response\", source.id=\" STRING \", message.id=\" STRING \", headers=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic source.id Identifier to correlate the http-service-response sink to its corresponding http-service source which consumed the request. STRING No No message.id Identifier to correlate the response with the request received by http-service source. STRING No Yes headers HTTP request headers in format \"' key : value ',' key : value '\" . When the Content-Type header is not provided the system decides the Content-Type based on the provided sink mapper as following: - @map(type='xml') : application/xml - @map(type='json') : application/json - @map(type='text') : plain/text - @map(type='keyvalue') : application/x-www-form-urlencoded - For all other cases system defaults to plain/text Also the Content-Length header need not to be provided, as the system automatically defines it by calculating the size of the payload. Content-Type and Content-Length headers STRING Yes No Examples EXAMPLE 1 @source(type='http-service', receiver.url='http://localhost:5005/add', source.id='adder', @map(type='json, @attributes(messageId='trp:messageId', value1='$.event.value1', value2='$.event.value2'))) define stream AddStream (messageId string, value1 long, value2 long); @sink(type='http-service-response', source.id='adder', message.id='{{messageId}}', @map(type = 'json')) define stream ResultStream (messageId string, results long); @info(name = 'query1') from AddStream select messageId, value1 + value2 as results insert into ResultStream; The http-service source on stream AddStream listens on url http://localhost:5005/stocks for JSON messages with format: { \"event\": { \"value1\": 3, \"value2\": 4 } } and when events arrive it maps to AddStream events and pass them to query query1 for processing. The query results produced on ResultStream are sent as a response via http-service-response sink with format: { \"event\": { \"results\": 7 } } Here the request and response are correlated by passing the messageId produced by the http-service to the respective http-service-response sink.","title":"http-service-response (Sink)"},{"location":"docs/api/5.1.1/#inmemory-sink","text":"In-memory sink publishes events to In-memory sources that are subscribe to the same topic to which the sink publishes. This provides a way to connect multiple Siddhi Apps deployed under the same Siddhi Manager (JVM). Here both the publisher and subscriber should have the same event schema (stream definition) for successful data transfer. Origin: siddhi-core:5.1.8 Syntax @sink(type=\"inMemory\", topic=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic topic Event are delivered to allthe subscribers subscribed on this topic. STRING No No Examples EXAMPLE 1 @sink(type='inMemory', topic='Stocks', @map(type='passThrough')) define stream StocksStream (symbol string, price float, volume long); Here the StocksStream uses inMemory sink to emit the Siddhi events to all the inMemory sources deployed in the same JVM and subscribed to the topic Stocks .","title":"inMemory (Sink)"},{"location":"docs/api/5.1.1/#jms-sink","text":"JMS Sink allows users to subscribe to a JMS broker and publish JMS messages. Origin: siddhi-io-jms:2.0.3 Syntax @sink(type=\"jms\", destination=\" STRING \", connection.factory.jndi.name=\" STRING \", factory.initial=\" STRING \", provider.url=\" STRING \", connection.factory.type=\" STRING \", connection.username=\" STRING \", connection.password=\" STRING \", connection.factory.nature=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic destination Queue/Topic name which JMS Source should subscribe to STRING No Yes connection.factory.jndi.name JMS Connection Factory JNDI name. This value will be used for the JNDI lookup to find the JMS Connection Factory. QueueConnectionFactory STRING Yes No factory.initial Naming factory initial value STRING No No provider.url Java naming provider URL. Property for specifying configuration information for the service provider to use. The value of the property should contain a URL string (e.g. \"ldap://somehost:389\") STRING No No connection.factory.type Type of the connection connection factory. This can be either queue or topic. queue STRING Yes No connection.username username for the broker. None STRING Yes No connection.password Password for the broker None STRING Yes No connection.factory.nature Connection factory nature for the broker(cached/pooled). default STRING Yes No Examples EXAMPLE 1 @sink(type='jms', @map(type='xml'), factory.initial='org.apache.activemq.jndi.ActiveMQInitialContextFactory', provider.url='vm://localhost',destination='DAS_JMS_OUTPUT_TEST', connection.factory.type='topic',connection.factory.jndi.name='TopicConnectionFactory') define stream inputStream (name string, age int, country string); This example shows how to publish to an ActiveMQ topic. EXAMPLE 2 @sink(type='jms', @map(type='xml'), factory.initial='org.apache.activemq.jndi.ActiveMQInitialContextFactory', provider.url='vm://localhost',destination='DAS_JMS_OUTPUT_TEST') define stream inputStream (name string, age int, country string); This example shows how to publish to an ActiveMQ queue. Note that we are not providing properties like connection factory type","title":"jms (Sink)"},{"location":"docs/api/5.1.1/#kafka-sink","text":"A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to use the Kafka transport, the type parameter should have kafka as its value. Origin: siddhi-io-kafka:5.0.5 Syntax @sink(type=\"kafka\", bootstrap.servers=\" STRING \", topic=\" STRING \", partition.no=\" INT \", sequence.id=\" STRING \", key=\" STRING \", is.binary.message=\" BOOL \", optional.configuration=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No is.binary.message In order to send the binary events via kafka sink, this parameter is set to 'True'. null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='topic_with_partitions', partition.no='0', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka sink configuration publishes to 0 th partition of the topic named topic_with_partitions . EXAMPLE 2 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='{{symbol}}', partition.no='{{volume}}', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes dynamic topic and partitions that are taken from the Siddhi event. The value for partition.no is taken from the volume attribute, and the topic value is taken from the symbol attribute.","title":"kafka (Sink)"},{"location":"docs/api/5.1.1/#kafkamultidc-sink","text":"A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to publish events via the Kafka transport, and using two Kafka brokers to publish events to the same topic, the type parameter must have kafkaMultiDC as its value. Origin: siddhi-io-kafka:5.0.5 Syntax @sink(type=\"kafkaMultiDC\", bootstrap.servers=\" STRING \", topic=\" STRING \", sequence.id=\" STRING \", key=\" STRING \", partition.no=\" INT \", is.binary.message=\" BOOL \", optional.configuration=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma -separated values. There must be at least two servers in this list. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No is.binary.message In order to send the binary events via kafkaMultiDCSink, it is required to set this parameter to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink(type='kafkaMultiDC', topic='myTopic', partition.no='0',bootstrap.servers='host1:9092, host2:9092', @map(type='xml'))Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes to the default (i.e., 0 th ) partition of the brokers in two data centers","title":"kafkaMultiDC (Sink)"},{"location":"docs/api/5.1.1/#log-sink","text":"This is a sink that can be used as a logger. This will log the output events in the output stream with user specified priority and a prefix Origin: siddhi-core:5.1.8 Syntax @sink(type=\"log\", priority=\" STRING \", prefix=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic priority This will set the logger priority i.e log level. Accepted values are INFO, DEBUG, WARN, FATAL, ERROR, OFF, TRACE INFO STRING Yes No prefix This will be the prefix to the output message. If the output stream has event [2,4] and the prefix is given as \"Hello\" then the log will show \"Hello : [2,4]\" default prefix will be : STRING Yes No Examples EXAMPLE 1 @sink(type='log', prefix='My Log', priority='DEBUG') define stream BarStream (symbol string, price float, volume long) In this example BarStream uses log sink and the prefix is given as My Log. Also the priority is set to DEBUG. EXAMPLE 2 @sink(type='log', priority='DEBUG') define stream BarStream (symbol string, price float, volume long) In this example BarStream uses log sink and the priority is set to DEBUG. User has not specified prefix so the default prefix will be in the form Siddhi App Name : Stream Name EXAMPLE 3 @sink(type='log', prefix='My Log') define stream BarStream (symbol string, price float, volume long) In this example BarStream uses log sink and the prefix is given as My Log. User has not given a priority so it will be set to default INFO. EXAMPLE 4 @sink(type='log') define stream BarStream (symbol string, price float, volume long) In this example BarStream uses log sink. The user has not given prefix or priority so they will be set to their default values.","title":"log (Sink)"},{"location":"docs/api/5.1.1/#nats-sink","text":"NATS Sink allows users to subscribe to a NATS broker and publish messages. Origin: siddhi-io-nats:2.0.8 Syntax @sink(type=\"nats\", destination=\" STRING \", bootstrap.servers=\" STRING \", client.id=\" STRING \", cluster.id=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic destination Subject name which NATS sink should publish to. STRING No Yes bootstrap.servers The NATS based url of the NATS server. nats://localhost:4222 STRING Yes No client.id The identifier of the client publishing/connecting to the NATS broker. Should be unique for each client connecting to the server/cluster. None STRING Yes No cluster.id The identifier of the NATS server/cluster. test-cluster STRING Yes No Examples EXAMPLE 1 @sink(type='nats', @map(type='xml'), destination='SP_NATS_OUTPUT_TEST', bootstrap.servers='nats://localhost:4222',client.id='nats_client',server.id='test-cluster') define stream outputStream (name string, age int, country string); This example shows how to publish to a NATS subject with all supporting configurations. With the following configuration the sink identified as 'nats-client' will publish to a subject named as 'SP_NATS_OUTPUT_TEST' which resides in a nats instance with a cluster id of 'test-cluster', running in localhost and listening to the port 4222 for client connection. EXAMPLE 2 @sink(type='nats', @map(type='xml'), destination='SP_NATS_OUTPUT_TEST') define stream outputStream (name string, age int, country string); This example shows how to publish to a NATS subject with mandatory configurations. With the following configuration the sink identified with an auto generated client id will publish to a subject named as 'SP_NATS_OUTPUT_TEST' which resides in a nats instance with a cluster id of 'test-cluster', running in localhost and listening to the port 4222 for client connection.","title":"nats (Sink)"},{"location":"docs/api/5.1.1/#prometheus-sink","text":"This sink publishes events processed by Siddhi into Prometheus metrics and exposes them to the Prometheus server at the specified URL. The created metrics can be published to Prometheus via 'server' or 'pushGateway', depending on your preference. The metric types that are supported by the Prometheus sink are 'counter', 'gauge', 'histogram', and 'summary'. The values and labels of the Prometheus metrics can be updated through the events. Origin: siddhi-io-prometheus:2.1.0 Syntax @sink(type=\"prometheus\", job=\" STRING \", publish.mode=\" STRING \", push.url=\" STRING \", server.url=\" STRING \", metric.type=\" STRING \", metric.help=\" STRING \", metric.name=\" STRING \", buckets=\" STRING \", quantiles=\" STRING \", quantile.error=\" DOUBLE \", value.attribute=\" STRING \", push.operation=\" STRING \", grouping.key=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic job This parameter specifies the job name of the metric. This must be the same job name that is defined in the Prometheus configuration file. siddhiJob STRING Yes No publish.mode The mode in which the metrics need to be exposed to the Prometheus server.The possible publishing modes are 'server' and 'pushgateway'.The server mode exposes the metrics through an HTTP server at the specified URL, and the 'pushGateway' mode pushes the metrics to the pushGateway that needs to be running at the specified URL. server STRING Yes No push.url This parameter specifies the target URL of the Prometheus pushGateway. This is the URL at which the pushGateway must be listening. This URL needs to be defined in the Prometheus configuration file as a target before it can be used here. http://localhost:9091 STRING Yes No server.url This parameter specifies the URL where the HTTP server is initiated to expose metrics in the 'server' publish mode. This URL needs to be defined in the Prometheus configuration file as a target before it can be used here. http://localhost:9080 STRING Yes No metric.type The type of Prometheus metric that needs to be created at the sink. The supported metric types are 'counter', 'gauge',c'histogram' and 'summary'. STRING No No metric.help A brief description of the metric and its purpose. STRING Yes No metric.name This parameter allows you to assign a preferred name for the metric. The metric name must match the regex format, i.e., [a-zA-Z_:][a-zA-Z0-9_:]*. STRING Yes No buckets The bucket values preferred by the user for histogram metrics. The bucket values must be in the 'string' format with each bucket value separated by a comma as shown in the example below. \"2,4,6,8\" null STRING Yes No quantiles This parameter allows you to specify quantile values for summary metrics as preferred. The quantile values must be in the 'string' format with each quantile value separated by a comma as shown in the example below. \"0.5,0.75,0.95\" null STRING Yes No quantile.error The error tolerance value for calculating quantiles in summary metrics. This must be a positive value, but less than 1. 0.001 DOUBLE Yes No value.attribute The name of the attribute in the stream definition that specifies the metric value. The defined 'value' attribute must be included in the stream definition. The system increases the metric value for the counter and gauge metric types by the value of the 'value attribute. The system observes the value of the 'value' attribute for the calculations of 'summary' and 'histogram' metric types. value STRING Yes No push.operation This parameter defines the mode for pushing metrics to the pushGateway. The available push operations are 'push' and 'pushadd'. The operations differ according to the existing metrics in pushGateway where 'push' operation replaces the existing metrics, and 'pushadd' operation only updates the newly created metrics. pushadd STRING Yes No grouping.key This parameter specifies the grouping key of created metrics in key-value pairs. The grouping key is used only in pushGateway mode in order to distinguish the metrics from already existing metrics. The expected format of the grouping key is as follows: \"'key1:value1','key2:value2'\" STRING Yes No System Parameters Name Description Default Value Possible Parameters jobName This property specifies the default job name for the metric. This job name must be the same as the job name defined in the Prometheus configuration file. siddhiJob Any string publishMode The default publish mode for the Prometheus sink for exposing metrics to the Prometheus server. The mode can be either 'server' or 'pushgateway'. server server or pushgateway serverURL This property configures the URL where the HTTP server is initiated to expose metrics. This URL needs to be defined in the Prometheus configuration file as a target to be identified by Prometheus before it can be used here. By default, the HTTP server is initiated at 'http://localhost:9080'. http://localhost:9080 Any valid URL pushURL This property configures the target URL of the Prometheus pushGateway (where the pushGateway needs to listen). This URL needs to be defined in the Prometheus configuration file as a target to be identified by Prometheus before it can be used here. http://localhost:9091 Any valid URL groupingKey This property configures the grouping key of created metrics in key-value pairs. Grouping key is used only in pushGateway mode in order to distinguish these metrics from already existing metrics under the same job. The expected format of the grouping key is as follows: \"'key1:value1','key2:value2'\" . null Any key value pairs in the supported format Examples EXAMPLE 1 @sink(type='prometheus',job='fooOrderCount', server.url ='http://localhost:9080', publish.mode='server', metric.type='counter', metric.help= 'Number of foo orders', @map(type='keyvalue')) define stream FooCountStream (Name String, quantity int, value int); In the above example, the Prometheus-sink creates a counter metric with the stream name and defined attributes as labels. The metric is exposed through an HTTP server at the target URL. EXAMPLE 2 @sink(type='prometheus',job='inventoryLevel', push.url='http://localhost:9080', publish.mode='pushGateway', metric.type='gauge', metric.help= 'Current level of inventory', @map(type='keyvalue')) define stream InventoryLevelStream (Name String, value int); In the above example, the Prometheus-sink creates a gauge metric with the stream name and defined attributes as labels.The metric is pushed to the Prometheus pushGateway at the target URL.","title":"prometheus (Sink)"},{"location":"docs/api/5.1.1/#rabbitmq-sink","text":"The rabbitmq sink pushes the events into a rabbitmq broker using the AMQP protocol Origin: siddhi-io-rabbitmq:3.0.2 Syntax @sink(type=\"rabbitmq\", uri=\" STRING \", heartbeat=\" INT \", exchange.name=\" STRING \", exchange.type=\" STRING \", exchange.durable.enabled=\" BOOL \", exchange.autodelete.enabled=\" BOOL \", delivery.mode=\" INT \", content.type=\" STRING \", content.encoding=\" STRING \", priority=\" INT \", correlation.id=\" STRING \", reply.to=\" STRING \", expiration=\" STRING \", message.id=\" STRING \", timestamp=\" STRING \", type=\" STRING \", user.id=\" STRING \", app.id=\" STRING \", routing.key=\" STRING \", headers=\" STRING \", tls.enabled=\" BOOL \", tls.truststore.path=\" STRING \", tls.truststore.password=\" STRING \", tls.truststore.type=\" STRING \", tls.version=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic uri The URI that used to connect to an AMQP server. If no URI is specified, an error is logged in the CLI.e.g., amqp://guest:guest , amqp://guest:guest@localhost:5672 STRING No No heartbeat The period of time (in seconds) after which the peer TCP connection should be considered unreachable (down) by RabbitMQ and client libraries. 60 INT Yes No exchange.name The name of the exchange that decides what to do with a message it sends.If the exchange.name already exists in the RabbitMQ server, then the system uses that exchange.name instead of redeclaring. STRING No Yes exchange.type The type of the exchange.name. The exchange types available are direct , fanout , topic and headers . For a detailed description of each type, see RabbitMQ - AMQP Concepts direct STRING Yes Yes exchange.durable.enabled If this is set to true , the exchange remains declared even if the broker restarts. false BOOL Yes Yes exchange.autodelete.enabled If this is set to true , the exchange is automatically deleted when it is not used anymore. false BOOL Yes Yes delivery.mode This determines whether the connection should be persistent or not. The value must be either 1 or 2 .If the delivery.mode = 1, then the connection is not persistent. If the delivery.mode = 2, then the connection is persistent. 1 INT Yes No content.type The message content type. This should be the MIME content type. null STRING Yes No content.encoding The message content encoding. The value should be MIME content encoding. null STRING Yes No priority Specify a value within the range 0 to 9 in this parameter to indicate the message priority. 0 INT Yes Yes correlation.id The message correlated to the current message. e.g., The request to which this message is a reply. When a request arrives, a message describing the task is pushed to the queue by the front end server. After that the frontend server blocks to wait for a response message with the same correlation ID. A pool of worker machines listen on queue, and one of them picks up the task, performs it, and returns the result as message. Once a message with right correlation ID arrives, thefront end server continues to return the response to the caller. null STRING Yes Yes reply.to This is an anonymous exclusive callback queue. When the RabbitMQ receives a message with the reply.to property, it sends the response to the mentioned queue. This is commonly used to name a reply queue (or any other identifier that helps a consumer application to direct its response). null STRING Yes No expiration The expiration time after which the message is deleted. The value of the expiration field describes the TTL (Time To Live) period in milliseconds. null STRING Yes No message.id The message identifier. If applications need to identify messages, it is recommended that they use this attribute instead of putting it into the message payload. null STRING Yes Yes timestamp Timestamp of the moment when the message was sent. If you do not specify a value for this parameter, the system automatically generates the current date and time as the timestamp value. The format of the timestamp value is dd/mm/yyyy . current timestamp STRING Yes No type The type of the message. e.g., The type of the event or the command represented by the message. null STRING Yes No user.id The user ID specified here is verified by RabbitMQ against theuser name of the actual connection. This is an optional parameter. null STRING Yes No app.id The identifier of the application that produced the message. null STRING Yes No routing.key The key based on which the excahnge determines how to route the message to the queue. The routing key is similar to an address for the message. empty STRING Yes Yes headers The headers of the message. The attributes used for routing are taken from the this paremeter. A message is considered matching if the value of the header equals the value specified upon binding. null STRING Yes Yes tls.enabled This parameter specifies whether an encrypted communication channel should be established or not. When this parameter is set to true , the tls.truststore.path and tls.truststore.password parameters are initialized. false BOOL Yes No tls.truststore.path The file path to the location of the truststore of the client that sends the RabbitMQ events via the AMQP protocol. A custom client-truststore can be specified if required. If a custom truststore is not specified, then the system uses the default client-trustore in the {carbon.home}/resources/security /code directory. /p /td td style=\"vertical-align: top\" {carbon.home}/resources/security</code> directory.</p></td> <td style=\"vertical-align: top\"> /resources/security/client-truststore.jks STRING Yes No tls.truststore.password The password for the client-truststore. A custom password can be specified if required. If no custom password is specified, then the system uses wso2carbon as the default password. wso2carbon STRING Yes No tls.truststore.type The type of the truststore. JKS STRING Yes No tls.version The version of the tls/ssl. SSL STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink(type ='rabbitmq', uri = 'amqp://guest:guest@localhost:5672', exchange.name = 'direct', routing.key= 'direct', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes events to the direct exchange with the direct exchange type and the directTest routing key.","title":"rabbitmq (Sink)"},{"location":"docs/api/5.1.1/#s3-sink","text":"S3 sink publishes events as Amazon AWS S3 buckets. Origin: siddhi-io-s3:1.0.2 Syntax @sink(type=\"s3\", credential.provider.class=\" STRING \", aws.access.key=\" STRING \", aws.secret.key=\" STRING \", bucket.name=\" STRING \", aws.region=\" STRING \", versioning.enabled=\" BOOL \", object.path=\" STRING \", storage.class=\" STRING \", content.type=\" STRING \", bucket.acl=\" STRING \", node.id=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic credential.provider.class AWS credential provider class to be used. If blank along with the username and the password, default credential provider will be used. EMPTY_STRING STRING Yes No aws.access.key AWS access key. This cannot be used along with the credential.provider.class EMPTY_STRING STRING Yes No aws.secret.key AWS secret key. This cannot be used along with the credential.provider.class EMPTY_STRING STRING Yes No bucket.name Name of the S3 bucket STRING No No aws.region The region to be used to create the bucket EMPTY_STRING STRING Yes No versioning.enabled Flag to enable versioning support in the bucket false BOOL Yes No object.path Path for each S3 object STRING No Yes storage.class AWS storage class standard STRING Yes No content.type Content type of the event application/octet-stream STRING Yes Yes bucket.acl Access control list for the bucket EMPTY_STRING STRING Yes No node.id The node ID of the current publisher. This needs to be unique for each publisher instance as it may cause object overwrites while uploading the objects to same S3 bucket from different publishers. EMPTY_STRING STRING Yes No Examples EXAMPLE 1 @sink(type='s3', bucket.name='user-stream-bucket',object.path='bar/users', credential.provider='software.amazon.awssdk.auth.credentials.ProfileCredentialsProvider', flush.size='3', @map(type='json', enclosing.element='$.user', @payload(\"\"\"{\"name\": \"{{name}}\", \"age\": {{age}}}\"\"\"))) define stream UserStream(name string, age int); This creates a S3 bucket named 'user-stream-bucket'. Then this will collect 3 events together and create a JSON object and save that in S3.","title":"s3 (Sink)"},{"location":"docs/api/5.1.1/#tcp-sink","text":"A Siddhi application can be configured to publish events via the TCP transport by adding the @Sink(type = 'tcp') annotation at the top of an event stream definition. Origin: siddhi-io-tcp:3.0.4 Syntax @sink(type=\"tcp\", url=\" STRING \", sync=\" STRING \", tcp.no.delay=\" BOOL \", keep.alive=\" BOOL \", worker.threads=\" INT|LONG \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic url The URL to which outgoing events should be published via TCP. The URL should adhere to tcp:// host : port / context format. STRING No No sync This parameter defines whether the events should be published in a synchronized manner or not. If sync = 'true', then the worker will wait for the ack after sending the message. Else it will not wait for an ack. false STRING Yes Yes tcp.no.delay This is to specify whether to disable Nagle algorithm during message passing. If tcp.no.delay = 'true', the execution of Nagle algorithm will be disabled in the underlying TCP logic. Hence there will be no delay between two successive writes to the TCP connection. Else there can be a constant ack delay. true BOOL Yes No keep.alive This property defines whether the server should be kept alive when there are no connections available. true BOOL Yes No worker.threads Number of threads to publish events. 10 INT LONG Yes No Examples EXAMPLE 1 @Sink(type = 'tcp', url='tcp://localhost:8080/abc', sync='true' @map(type='binary')) define stream Foo (attribute1 string, attribute2 int); A sink of type 'tcp' has been defined. All events arriving at Foo stream via TCP transport will be sent to the url tcp://localhost:8080/abc in a synchronous manner.","title":"tcp (Sink)"},{"location":"docs/api/5.1.1/#sinkmapper","text":"","title":"Sinkmapper"},{"location":"docs/api/5.1.1/#avro-sink-mapper","text":"This extension is a Siddhi Event to Avro Message output mapper.Transports that publish messages to Avro sink can utilize this extension to convert Siddhi events to Avro messages. You can either specify the Avro schema or provide the schema registry URL and the schema reference ID as parameters in the stream definition. If no Avro schema is specified, a flat Avro schema of the 'record' type is generated with the stream attributes as schema fields. Origin: siddhi-map-avro:2.0.6 Syntax @sink(..., @map(type=\"avro\", schema.def=\" STRING \", schema.registry=\" STRING \", schema.id=\" STRING \") QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic schema.def This specifies the required Avro schema to be used to convert Siddhi events to Avro messages. The schema needs to be specified as a quoted JSON string. STRING No No schema.registry This specifies the URL of the schema registry. STRING No No schema.id This specifies the ID of the avro schema. This ID is the global ID that is returned from the schema registry when posting the schema to the registry. The specified ID is used to retrieve the schema from the schema registry. STRING No No Examples EXAMPLE 1 @sink(type='inMemory', topic='stock', @map(type='avro',schema.def = \"\"\"{\"type\":\"record\",\"name\":\"stock\",\"namespace\":\"stock.example\",\"fields\":[{\"name\":\"symbol\",\"type\":\"string\"},{\"name\":\"price\",\"type\":\"float\"},{\"name\":\"volume\",\"type\":\"long\"}]}\"\"\")) define stream StockStream (symbol string, price float, volume long); The above configuration performs a default Avro mapping that generates an Avro message as an output ByteBuffer. EXAMPLE 2 @sink(type='inMemory', topic='stock', @map(type='avro',schema.registry = 'http://localhost:8081', schema.id ='22',@payload(\"\"\"{\"Symbol\":{{symbol}},\"Price\":{{price}},\"Volume\":{{volume}}}\"\"\" ))) define stream StockStream (symbol string, price float, volume long); The above configuration performs a custom Avro mapping that generates an Avro message as an output ByteBuffer. The Avro schema is retrieved from the given schema registry (localhost:8081) using the schema ID provided.","title":"avro (Sink Mapper)"},{"location":"docs/api/5.1.1/#binary-sink-mapper","text":"This section explains how to map events processed via Siddhi in order to publish them in the binary format. Origin: siddhi-map-binary:2.0.4 Syntax @sink(..., @map(type=\"binary\") Examples EXAMPLE 1 @sink(type='inMemory', topic='WSO2', @map(type='binary')) define stream FooStream (symbol string, price float, volume long); This will publish Siddhi event in binary format.","title":"binary (Sink Mapper)"},{"location":"docs/api/5.1.1/#csv-sink-mapper","text":"This output mapper extension allows you to convert Siddhi events processed by the WSO2 SP to CSV message before publishing them. You can either use custom placeholder to map a custom CSV message or use pre-defined CSV format where event conversion takes place without extra configurations. Origin: siddhi-map-csv:2.0.3 Syntax @sink(..., @map(type=\"csv\", delimiter=\" STRING \", header=\" BOOL \", event.grouping.enabled=\" BOOL \") QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic delimiter This parameter used to separate the output CSV data, when converting a Siddhi event to CSV format, , STRING Yes No header This parameter specifies whether the CSV messages will be generated with header or not. If this parameter is set to true, message will be generated with header false BOOL Yes No event.grouping.enabled If this parameter is set to true , events are grouped via a line.separator when multiple events are received. It is required to specify a value for the System.lineSeparator() when the value for this parameter is true . false BOOL Yes No Examples EXAMPLE 1 @sink(type='inMemory', topic='{{symbol}}', @map(type='csv')) define stream BarStream (symbol string, price float, volume long); Above configuration will perform a default CSV output mapping, which will generate output as follows: WSO2,55.6,100 OS supported line separator If header is true and delimiter is \"-\", then the output will be as follows: symbol-price-volume OS supported line separator WSO2-55.6-100 OS supported line separator EXAMPLE 2 @sink(type='inMemory', topic='{{symbol}}', @map(type='csv',header='true',delimiter='-',@payload(symbol='0',price='2',volume='1')))define stream BarStream (symbol string, price float,volume long); Above configuration will perform a custom CSV mapping. Here, user can add custom place order in the @payload. The place order indicates that where the attribute name's value will be appear in the output message, The output will be produced output as follows: WSO2,100,55.6 If header is true and delimiter is \"-\", then the output will be as follows: symbol-price-volume WSO2-55.6-100 OS supported line separator If event grouping is enabled, then the output is as follows: WSO2-55.6-100 OS supported line separator WSO2-55.6-100 OS supported line separator WSO2-55.6-100 OS supported line separator","title":"csv (Sink Mapper)"},{"location":"docs/api/5.1.1/#json-sink-mapper","text":"This extension is an Event to JSON output mapper. Transports that publish messages can utilize this extension to convert Siddhi events to JSON messages. You can either send a pre-defined JSON format or a custom JSON message. Origin: siddhi-map-json:5.0.5 Syntax @sink(..., @map(type=\"json\", validate.json=\" BOOL \", enclosing.element=\" STRING \") QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic validate.json If this property is set to true , it enables JSON validation for the JSON messages generated. When validation is carried out, messages that do not adhere to proper JSON standards are dropped. This property is set to 'false' by default. false BOOL Yes No enclosing.element This specifies the enclosing element to be used if multiple events are sent in the same JSON message. Siddhi treats the child elements of the given enclosing element as events and executes JSON expressions on them. If an enclosing.element is not provided, the multiple event scenario is disregarded and JSON path is evaluated based on the root element. $ STRING Yes No Examples EXAMPLE 1 @sink(type='inMemory', topic='stock', @map(type='json')) define stream FooStream (symbol string, price float, volume long); Above configuration does a default JSON input mapping that generates the output given below. { \"event\":{ \"symbol\":WSO2, \"price\":55.6, \"volume\":100 } } EXAMPLE 2 @sink(type='inMemory', topic='{{symbol}}', @map(type='json', enclosing.element='$.portfolio', validate.json='true', @payload( \"\"\"{\"StockData\":{\"Symbol\":\"{{symbol}}\",\"Price\":{{price}}}\"\"\"))) define stream BarStream (symbol string, price float, volume long); The above configuration performs a custom JSON mapping that generates the following JSON message as the output. {\"portfolio\":{ \"StockData\":{ \"Symbol\":WSO2, \"Price\":55.6 } } }","title":"json (Sink Mapper)"},{"location":"docs/api/5.1.1/#keyvalue-sink-mapper","text":"The Event to Key-Value Map output mapper extension allows you to convert Siddhi events processed by WSO2 SP to key-value map events before publishing them. You can either use pre-defined keys where conversion takes place without extra configurations, or use custom keys with which the messages can be published. Origin: siddhi-map-keyvalue:2.0.5 Syntax @sink(..., @map(type=\"keyvalue\") Examples EXAMPLE 1 @sink(type='inMemory', topic='stock', @map(type='keyvalue')) define stream FooStream (symbol string, price float, volume long); This query performs a default Key-Value output mapping. The expected output is something similar to the following: symbol:'WSO2' price : 55.6f volume: 100L EXAMPLE 2 @sink(type='inMemory', topic='stock', @map(type='keyvalue', @payload(a='symbol',b='price',c='volume'))) define stream FooStream (symbol string, price float, volume long); This query performs a custom Key-Value output mapping where values are passed as objects. Values for symbol , price , and volume attributes are published with the keys a , b and c respectively. The expected output is a map similar to the following: a:'WSO2' b : 55.6f c: 100L EXAMPLE 3 @sink(type='inMemory', topic='stock', @map(type='keyvalue', @payload(a='{{symbol}} is here',b='`price`',c='volume'))) define stream FooStream (symbol string, price float, volume long); This query performs a custom Key-Value output mapping where the values of the a and b attributes are strings and c is object. The expected output should be a Map similar to the following: a:'WSO2 is here' b : 'price' c: 100L","title":"keyvalue (Sink Mapper)"},{"location":"docs/api/5.1.1/#passthrough-sink-mapper","text":"Pass-through mapper passed events (Event[]) through without any mapping or modifications. Origin: siddhi-core:5.1.8 Syntax @sink(..., @map(type=\"passThrough\") Examples EXAMPLE 1 @sink(type='inMemory', @map(type='passThrough')) define stream BarStream (symbol string, price float, volume long); In the following example BarStream uses passThrough outputmapper which emit Siddhi event directly without any transformation into sink.","title":"passThrough (Sink Mapper)"},{"location":"docs/api/5.1.1/#protobuf-sink-mapper","text":"This output mapper allows you to convert Events to protobuf messages before publishing them. To work with this mapper you have to add auto-generated protobuf classes to the project classpath. When you use this output mapper, you can either define stream attributes as the same names as the protobuf message attributes or you can use custom mapping to map stream definition attributes with the protobuf attributes..Please find the sample proto definition here Origin: siddhi-map-protobuf:1.0.2 Syntax @sink(..., @map(type=\"protobuf\", class=\" STRING \") QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic class This specifies the class name of the protobuf message class, If sink type is grpc then it's not necessary to provide this parameter. - STRING Yes No Examples EXAMPLE 1 @sink(type='grpc', url = 'grpc://localhost:2000/org.wso2.grpc.test.MyService/process @map(type='protobuf')) define stream BarStream (stringValue string, intValue int,longValue long,booleanValue bool,floatValue float,doubleValue double) Above definition will map BarStream values into the protobuf message type of the 'process' method in 'MyService' service EXAMPLE 2 @sink(type='grpc', url = 'grpc://localhost:2000/org.wso2.grpc.test.MyService/process @map(type='protobuf'), @payload(stringValue='a',longValue='b',intValue='c',booleanValue='d',floatValue = 'e', doubleValue = 'f'))) define stream BarStream (a string, b long, c int,d bool,e float,f double); The above definition will map BarStream values to request message type of the 'process' method in 'MyService' service. and stream values will map like this, - value of 'a' will be assign 'stringValue' variable in the message class - value of 'b' will be assign 'longValue' variable in the message class - value of 'c' will be assign 'intValue' variable in the message class - value of 'd' will be assign 'booleanValue' variable in the message class - value of 'e' will be assign 'floatValue' variable in the message class - value of 'f' will be assign 'doubleValue' variable in the message class EXAMPLE 3 @sink(type='grpc', url = 'grpc://localhost:2000/org.wso2.grpc.test.MyService/testMap' @map(type='protobuf')) define stream BarStream (stringValue string,intValue int,map object); The above definition will map BarStream values to request message type of the 'testMap' method in 'MyService' service and since there is an object data type is inthe stream(map object) , mapper will assume that 'map' is an instance of 'java.util.Map' class, otherwise it will throws and error. EXAMPLE 4 @sink(type='inMemory', topic='test01', @map(type='protobuf', class='org.wso2.grpc.test.Request')) define stream BarStream (stringValue string, intValue int,longValue long,booleanValue bool,floatValue float,doubleValue double); The above definition will map BarStream values to 'org.wso2.grpc.test.Request'protobuf class type. If sink type is not a grpc, sink is expecting to get the mapping protobuf class from the 'class' parameter in the @map extension","title":"protobuf (Sink Mapper)"},{"location":"docs/api/5.1.1/#text-sink-mapper","text":"This extension is a Event to Text output mapper. Transports that publish text messages can utilize this extension to convert the Siddhi events to text messages. Users can use a pre-defined text format where event conversion is carried out without any additional configurations, or use custom placeholder(using {{ and }} ) to map custom text messages. Again, you can also enable mustache based custom mapping. In mustache based custom mapping you can use custom placeholder (using {{ and }} or {{{ and }}} ) to map custom text. In mustache based custom mapping, all variables are HTML escaped by default. For example: is replaced with amp; \" is replaced with quot; = is replaced with #61; If you want to return unescaped HTML, use the triple mustache {{{ instead of double {{ . Origin: siddhi-map-text:2.0.4 Syntax @sink(..., @map(type=\"text\", event.grouping.enabled=\" BOOL \", delimiter=\" STRING \", new.line.character=\" STRING \", mustache.enabled=\" BOOL \") QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic event.grouping.enabled If this parameter is set to true , events are grouped via a delimiter when multiple events are received. It is required to specify a value for the delimiter parameter when the value for this parameter is true . false BOOL Yes No delimiter This parameter specifies how events are separated when a grouped event is received. This must be a whole line and not a single character. ~ ~ ~ ~ STRING Yes No new.line.character This attribute indicates the new line character of the event that is expected to be received. This is used mostly when communication between 2 types of operating systems is expected. For example, Linux uses \\n whereas Windows uses \\r\\n as the end of line character. \\n STRING Yes No mustache.enabled If this parameter is set to true , then mustache mapping gets enabled forcustom text mapping. false BOOL Yes No Examples EXAMPLE 1 @sink(type='inMemory', topic='stock', @map(type='text')) define stream FooStream (symbol string, price float, volume long); This query performs a default text input mapping. The expected output is as follows: symbol:\"WSO2\", price:55.6, volume:100 EXAMPLE 2 @sink(type='inMemory', topic='stock', @map(type='text', event.grouping.enabled='true')) define stream FooStream (symbol string, price float, volume long); This query performs a default text input mapping with event grouping. The expected output is as follows: symbol:\"WSO2\", price:55.6, volume:100 ~ ~ ~ ~ symbol:\"WSO2\", price:55.6, volume:100 EXAMPLE 3 @sink(type='inMemory', topic='stock', @map(type='text', @payload(\"SensorID : {{symbol}}/{{volume}}, SensorPrice : Rs{{price}}/=, Value : {{volume}}ml\"))) define stream FooStream (symbol string, price float, volume long); This query performs a custom text mapping. The expected output is as follows: SensorID : wso2/100, SensorPrice : Rs1000/=, Value : 100ml for the following siddhi event. {wso2,1000,100} EXAMPLE 4 @sink(type='inMemory', topic='stock', @map(type='text', event.grouping.enabled='true', @payload(\"Stock price of {{symbol}} is {{price}}\"))) define stream FooStream (symbol string, price float, volume long); This query performs a custom text mapping with event grouping. The expected output is as follows: Stock price of WSO2 is 55.6 ~ ~ ~ ~ Stock price of WSO2 is 55.6 ~ ~ ~ ~ Stock price of WSO2 is 55.6 for the following siddhi event. {WSO2,55.6,10} EXAMPLE 5 @sink(type='inMemory', topic='stock', @map(type='text', mustache.enabled='true', @payload(\"SensorID : {{{symbol}}}/{{{volume}}}, SensorPrice : Rs{{{price}}}/=, Value : {{{volume}}}ml\"))) define stream FooStream (symbol string, price float, volume long); This query performs a custom text mapping to return unescaped HTML. The expected output is as follows: SensorID : a b/100, SensorPrice : Rs1000/=, Value : 100ml for the following siddhi event. {a b,1000,100}","title":"text (Sink Mapper)"},{"location":"docs/api/5.1.1/#xml-sink-mapper","text":"This mapper converts Siddhi output events to XML before they are published via transports that publish in XML format. Users can either send a pre-defined XML format or a custom XML message containing event data. Origin: siddhi-map-xml:5.0.3 Syntax @sink(..., @map(type=\"xml\", validate.xml=\" BOOL \", enclosing.element=\" STRING \") QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic validate.xml This parameter specifies whether the XML messages generated should be validated or not. If this parameter is set to true, messages that do not adhere to proper XML standards are dropped. false BOOL Yes No enclosing.element When an enclosing element is specified, the child elements (e.g., the immediate child elements) of that element are considered as events. This is useful when you need to send multiple events in a single XML message. When an enclosing element is not specified, one XML message per every event will be emitted without enclosing. None in custom mapping and events in default mapping STRING Yes No Examples EXAMPLE 1 @sink(type='inMemory', topic='stock', @map(type='xml')) define stream FooStream (symbol string, price float, volume long); Above configuration will do a default XML input mapping which will generate below output events event symbol WSO2 /symbol price 55.6 /price volume 100 /volume /event /events EXAMPLE 2 @sink(type='inMemory', topic='{{symbol}}', @map(type='xml', enclosing.element=' portfolio ', validate.xml='true', @payload( \" StockData Symbol {{symbol}} /Symbol Price {{price}} /Price /StockData \"))) define stream BarStream (symbol string, price float, volume long); Above configuration will perform a custom XML mapping. Inside @payload you can specify the custom template that you want to send the messages out and addd placeholders to places where you need to add event attributes.Above config will produce below output XML message portfolio StockData Symbol WSO2 /Symbol Price 55.6 /Price /StockData /portfolio","title":"xml (Sink Mapper)"},{"location":"docs/api/5.1.1/#source","text":"","title":"Source"},{"location":"docs/api/5.1.1/#cdc-source","text":"The CDC source receives events when change events (i.e., INSERT, UPDATE, DELETE) are triggered for a database table. Events are received in the 'key-value' format. There are two modes you could perform CDC: Listening mode and Polling mode. In polling mode, the datasource is periodically polled for capturing the changes. The polling period can be configured. In polling mode, you can only capture INSERT and UPDATE changes. On listening mode, the Source will keep listening to the Change Log of the database and notify in case a change has taken place. Here, you are immediately notified about the change, compared to polling mode. The key values of the map of a CDC change event are as follows. For 'listening' mode: For insert: Keys are specified as columns of the table. For delete: Keys are followed by the specified table columns. This is achieved via 'before_'. e.g., specifying 'before_X' results in the key being added before the column named 'X'. For update: Keys are followed followed by the specified table columns. This is achieved via 'before_'. e.g., specifying 'before_X' results in the key being added before the column named 'X'. For 'polling' mode: Keys are specified as the columns of the table.#### Preparations required for working with Oracle Databases in listening mode Using the extension in Windows, Mac OSX and AIX are pretty straight forward inorder to achieve the required behaviour please follow the steps given below - Download the compatible version of oracle instantclient for the database version from here and extract - Extract and set the environment variable LD_LIBRARY_PATH to the location of instantclient which was exstracted as shown below export LD_LIBRARY_PATH= path to the instant client location - Inside the instantclient folder which was download there are two jars xstreams.jar and ojdbc version .jar convert them to OSGi bundles using the tools which were provided in the distribution /bin for converting the ojdbc.jar use the tool spi-provider.sh|bat and for the conversion of xstreams.jar use the jni-provider.sh as shown below(Note: this way of converting Xstreams jar is applicable only for Linux environments for other OSs this step is not required and converting it through the jartobundle.sh tool is enough) ./jni-provider.sh input-jar destination comma seperated native library names once ojdbc and xstreams jars are converted to OSGi copy the generated jars to the distribution /lib . Currently siddhi-io-cdc only supports the oracle database distributions 12 and above See parameter: mode for supported databases and change events. Origin: siddhi-io-cdc:2.0.4 Syntax @source(type=\"cdc\", url=\" STRING \", mode=\" STRING \", jdbc.driver.name=\" STRING \", username=\" STRING \", password=\" STRING \", pool.properties=\" STRING \", datasource.name=\" STRING \", table.name=\" STRING \", polling.column=\" STRING \", polling.interval=\" INT \", operation=\" STRING \", connector.properties=\" STRING \", database.server.id=\" STRING \", database.server.name=\" STRING \", wait.on.missed.record=\" BOOL \", missed.record.waiting.timeout=\" INT \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic url The connection URL to the database. F=The format used is: 'jdbc:mysql:// host : port / database_name ' STRING No No mode Mode to capture the change data. The type of events that can be received, and the required parameters differ based on the mode. The mode can be one of the following: 'polling': This mode uses a column named 'polling.column' to monitor the given table. It captures change events of the 'RDBMS', 'INSERT, and 'UPDATE' types. 'listening': This mode uses logs to monitor the given table. It currently supports change events only of the 'MySQL', 'INSERT', 'UPDATE', and 'DELETE' types. listening STRING Yes No jdbc.driver.name The driver class name for connecting the database. It is required to specify a value for this parameter when the mode is 'polling'. STRING Yes No username The username to be used for accessing the database. This user needs to have the 'SELECT', 'RELOAD', 'SHOW DATABASES', 'REPLICATION SLAVE', and 'REPLICATION CLIENT'privileges for the change data capturing table (specified via the 'table.name' parameter). To operate in the polling mode, the user needs 'SELECT' privileges. STRING No No password The password of the username you specified for accessing the database. STRING No No pool.properties The pool parameters for the database connection can be specified as key-value pairs. STRING Yes No datasource.name Name of the wso2 datasource to connect to the database. When datasource name is provided, the URL, username and password are not needed. A datasource based connection is given more priority over the URL based connection. This parameter is applicable only when the mode is set to 'polling', and it can be applied only when you use this extension with WSO2 Stream Processor. STRING Yes No table.name The name of the table that needs to be monitored for data changes. STRING No No polling.column The column name that is polled to capture the change data. It is recommended to have a TIMESTAMP field as the 'polling.column' in order to capture the inserts and updates. Numeric auto-incremental fields and char fields can also be used as 'polling.column'. However, note that fields of these types only support insert change capturing, and the possibility of using a char field also depends on how the data is input. It is required to enter a value for this parameter only when the mode is 'polling'. STRING Yes No polling.interval The time interval (specified in seconds) to poll the given table for changes. This parameter is applicable only when the mode is set to 'polling'. 1 INT Yes No operation The change event operation you want to carry out. Possible values are 'insert', 'update' or 'delete'. This parameter is not case sensitive. It is required to specify a value only when the mode is 'listening'. STRING No No connector.properties Here, you can specify Debezium connector properties as a comma-separated string. The properties specified here are given more priority over the parameters. This parameter is applicable only for the 'listening' mode. Empty_String STRING Yes No database.server.id An ID to be used when joining MySQL database cluster to read the bin log. This should be a unique integer between 1 to 2^32. This parameter is applicable only when the mode is 'listening'. Random integer between 5400 and 6400 STRING Yes No database.server.name A logical name that identifies and provides a namespace for the database server. This parameter is applicable only when the mode is 'listening'. {host}_{port} STRING Yes No wait.on.missed.record Indicates whether the process needs to wait on missing/out-of-order records. When this flag is set to 'true' the process will be held once it identifies a missing record. The missing recrod is identified by the sequence of the polling.column value. This can be used only with number fields and not recommended to use with time values as it will not be sequential. This should be enabled ONLY where the records can be written out-of-order, (eg. concurrent writers) as this degrades the performance. false BOOL Yes No missed.record.waiting.timeout The timeout (specified in seconds) to retry for missing/out-of-order record. This should be used along with the wait.on.missed.record parameter. If the parameter is not set, the process will indefinitely wait for the missing record. -1 INT Yes No Examples EXAMPLE 1 @source(type = 'cdc' , url = 'jdbc:mysql://localhost:3306/SimpleDB', username = 'cdcuser', password = 'pswd4cdc', table.name = 'students', operation = 'insert', @map(type='keyvalue', @attributes(id = 'id', name = 'name'))) define stream inputStream (id string, name string); In this example, the CDC source listens to the row insertions that are made in the 'students' table with the column name, and the ID. This table belongs to the 'SimpleDB' MySQL database that can be accessed via the given URL. EXAMPLE 2 @source(type = 'cdc' , url = 'jdbc:mysql://localhost:3306/SimpleDB', username = 'cdcuser', password = 'pswd4cdc', table.name = 'students', operation = 'update', @map(type='keyvalue', @attributes(id = 'id', name = 'name', before_id = 'before_id', before_name = 'before_name'))) define stream inputStream (before_id string, id string, before_name string , name string); In this example, the CDC source listens to the row updates that are made in the 'students' table. This table belongs to the 'SimpleDB' MySQL database that can be accessed via the given URL. EXAMPLE 3 @source(type = 'cdc' , url = 'jdbc:mysql://localhost:3306/SimpleDB', username = 'cdcuser', password = 'pswd4cdc', table.name = 'students', operation = 'delete', @map(type='keyvalue', @attributes(before_id = 'before_id', before_name = 'before_name'))) define stream inputStream (before_id string, before_name string); In this example, the CDC source listens to the row deletions made in the 'students' table. This table belongs to the 'SimpleDB' database that can be accessed via the given URL. EXAMPLE 4 @source(type = 'cdc', mode='polling', polling.column = 'id', jdbc.driver.name = 'com.mysql.jdbc.Driver', url = 'jdbc:mysql://localhost:3306/SimpleDB', username = 'cdcuser', password = 'pswd4cdc', table.name = 'students', @map(type='keyvalue'), @attributes(id = 'id', name = 'name')) define stream inputStream (id int, name string); In this example, the CDC source polls the 'students' table for inserts. 'id' that is specified as the polling colum' is an auto incremental field. The connection to the database is made via the URL, username, password, and the JDBC driver name. EXAMPLE 5 @source(type = 'cdc', mode='polling', polling.column = 'id', datasource.name = 'SimpleDB', table.name = 'students', @map(type='keyvalue'), @attributes(id = 'id', name = 'name')) define stream inputStream (id int, name string); In this example, the CDC source polls the 'students' table for inserts. The given polling column is a char column with the 'S001, S002, ... .' pattern. The connection to the database is made via a data source named 'SimpleDB'. Note that the 'datasource.name' parameter works only with the Stream Processor. EXAMPLE 6 @source(type = 'cdc', mode='polling', polling.column = 'last_updated', datasource.name = 'SimpleDB', table.name = 'students', @map(type='keyvalue')) define stream inputStream (name string); In this example, the CDC source polls the 'students' table for inserts and updates. The polling column is a timestamp field. EXAMPLE 7 @source(type='cdc', jdbc.driver.name='com.mysql.jdbc.Driver', url='jdbc:mysql://localhost:3306/SimpleDB', username='cdcuser', password='pswd4cdc', table.name='students', mode='polling', polling.column='id', operation='insert', wait.on.missed.record='true', missed.record.waiting.timeout='10', @map(type='keyvalue'), @attributes(batch_no='batch_no', item='item', qty='qty')) define stream inputStream (id int, name string); In this example, the CDC source polls the 'students' table for inserts. The polling column is a numeric field. This source expects the records in the database to be written concurrently/out-of-order so it waits if it encounters a missing record. If the record doesn't appear within 10 seconds it resumes the process. EXAMPLE 8 @source(type = 'cdc', url = 'jdbc:oracle:thin://localhost:1521/ORCLCDB', username='c##xstrm', password='xs', table.name='DEBEZIUM.sweetproductiontable', operation = 'insert', connector.properties='oracle.outserver.name=DBZXOUT,oracle.pdb=ORCLPDB1' @map(type = 'keyvalue')) define stream insertSweetProductionStream (ID int, NAME string, WEIGHT int); In this example, the CDC source connect to an Oracle database and listens for insert queries of sweetproduction table","title":"cdc (Source)"},{"location":"docs/api/5.1.1/#email-source","text":"The 'Email' source allows you to receive events via emails. An 'Email' source can be configured using the 'imap' or 'pop3' server to receive events. This allows you to filter the messages that satisfy the criteria specified under the 'search term' option. The email source parameters can be defined in either the ' SP_HOME /conf/ PROFILE /deployment yaml' file or the stream definition. If the parameter configurations are not available in either place, the default values are considered (i.e., if default values are available). If you need to configure server system parameters that are not provided as options in the stream definition, they need to be defined in the 'deployment yaml' file under 'email source properties'. For more information about 'imap' and 'pop3' server system parameters, see the following. JavaMail Reference Implementation - IMAP Store JavaMail Reference Implementation - POP3 Store Store Origin: siddhi-io-email:2.0.5 Syntax @source(type=\"email\", username=\" STRING \", password=\" STRING \", store=\" STRING \", host=\" STRING \", port=\" INT \", folder=\" STRING \", search.term=\" STRING \", polling.interval=\" LONG \", action.after.processed=\" STRING \", folder.to.move=\" STRING \", content.type=\" STRING \", ssl.enable=\" BOOL \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic username The user name of the email account. e.g., 'wso2mail' is the username of the 'wso2mail@gmail.com' mail account. STRING No No password The password of the email account STRING No No store The store type that used to receive emails. Possible values are 'imap' and 'pop3'. imap STRING Yes No host The host name of the server (e.g., 'imap.gmail.com' is the host name for a gmail account with an IMAP store.). The default value 'imap.gmail.com' is only valid if the email account is a gmail account with IMAP enabled. If store type is 'imap', then the default value is 'imap.gmail.com'. If the store type is 'pop3', then thedefault value is 'pop3.gmail.com'. STRING Yes No port The port that is used to create the connection. '993', the default value is valid only if the store is 'imap' and ssl-enabled. INT Yes No folder The name of the folder to which the emails should be fetched. INBOX STRING Yes No search.term The option that includes conditions such as key-value pairs to search for emails. In a string search term, the key and the value should be separated by a semicolon (';'). Each key-value pair must be within inverted commas (' '). The string search term can define multiple comma-separated key-value pairs. This string search term currently supports only the 'subject', 'from', 'to', 'bcc', and 'cc' keys. e.g., if you enter 'subject:DAS, from:carbon, bcc:wso2', the search term creates a search term instance that filters emails that contain 'DAS' in the subject, 'carbon' in the 'from' address, and 'wso2' in one of the 'bcc' addresses. The string search term carries out sub string matching that is case-sensitive. If '@' in included in the value for any key other than the 'subject' key, it checks for an address that is equal to the value given. e.g., If you search for 'abc@', the string search terms looks for an address that contains 'abc' before the '@' symbol. None STRING Yes No polling.interval This defines the time interval in seconds at which th email source should poll the account to check for new mail arrivals.in seconds. 600 LONG Yes No action.after.processed The action to be performed by the email source for the processed mail. Possible values are as follows: 'FLAGGED': Sets the flag as 'flagged'. 'SEEN': Sets the flag as 'read'. 'ANSWERED': Sets the flag as 'answered'. 'DELETE': Deletes tha mail after the polling cycle. 'MOVE': Moves the mail to the folder specified in the 'folder.to.move' parameter. If the folder specified is 'pop3', then the only option available is 'DELETE'. NONE STRING Yes No folder.to.move The name of the folder to which the mail must be moved once it is processed. If the action after processing is 'MOVE', it is required to specify a value for this parameter. STRING No No content.type The content type of the email. It can be either 'text/plain' or 'text/html.' text/plain STRING Yes No ssl.enable If this is set to 'true', a secure port is used to establish the connection. The possible values are 'true' and 'false'. true BOOL Yes No System Parameters Name Description Default Value Possible Parameters mail.imap.partialfetch This determines whether the IMAP partial-fetch capability should be used. true true or false mail.imap.fetchsize The partial fetch size in bytes. 16K value in bytes mail.imap.peek If this is set to 'true', the IMAP PEEK option should be used when fetching body parts to avoid setting the 'SEEN' flag on messages. The default value is 'false'. This can be overridden on a per-message basis by the 'setPeek method' in 'IMAPMessage'. false true or false mail.imap.connectiontimeout The socket connection timeout value in milliseconds. This timeout is implemented by 'java.net.Socket'. infinity timeout Any Integer value mail.imap.timeout The socket read timeout value in milliseconds. This timeout is implemented by 'java.net.Socket'. infinity timeout Any Integer value mail.imap.writetimeout The socket write timeout value in milliseconds. This timeout is implemented by using a 'java.util.concurrent.ScheduledExecutorService' per connection that schedules a thread to close the socket if the timeout period elapses. Therefore, the overhead of using this timeout is one thread per connection. infinity timeout Any Integer value mail.imap.statuscachetimeout The timeout value in milliseconds for the cache of 'STATUS' command response. 1000ms Time out in miliseconds mail.imap.appendbuffersize The maximum size of a message to buffer in memory when appending to an IMAP folder. None Any Integer value mail.imap.connectionpoolsize The maximum number of available connections in the connection pool. 1 Any Integer value mail.imap.connectionpooltimeout The timeout value in milliseconds for connection pool connections. 45000ms Any Integer mail.imap.separatestoreconnection If this parameter is set to 'true', it indicates that a dedicated store connection needs to be used for store commands. true true or false mail.imap.auth.login.disable If this is set to 'true', it is not possible to use the non-standard 'AUTHENTICATE LOGIN' command instead of the plain 'LOGIN' command. false true or false mail.imap.auth.plain.disable If this is set to 'true', the 'AUTHENTICATE PLAIN' command cannot be used. false true or false mail.imap.auth.ntlm.disable If true, prevents use of the AUTHENTICATE NTLM command. false true or false mail.imap.proxyauth.user If the server supports the PROXYAUTH extension, this property specifies the name of the user to act as. Authentication to log in to the server is carried out using the administrator's credentials. After authentication, the IMAP provider issues the 'PROXYAUTH' command with the user name specified in this property. None Valid string value mail.imap.localaddress The local address (host name) to bind to when creating the IMAP socket. Defaults to the address picked by the Socket class. Valid string value mail.imap.localport The local port number to bind to when creating the IMAP socket. Defaults to the port number picked by the Socket class. Valid String value mail.imap.sasl.enable If this parameter is set to 'true', the system attempts to use the 'javax.security.sasl' package to choose an authentication mechanism for the login. false true or false mail.imap.sasl.mechanisms A list of SASL mechanism names that the system should to try to use. The names can be separated by spaces or commas. None Valid string value mail.imap.sasl.authorizationid The authorization ID to use in the SASL authentication. If this parameter is not set, the authentication ID (username) is used. Valid string value mail.imap.sasl.realm The realm to use with SASL authentication mechanisms that require a realm, such as 'DIGEST-MD5'. None Valid string value mail.imap.auth.ntlm.domain The NTLM authentication domain. None Valid string value The NTLM authentication domain. NTLM protocol-specific flags. None Valid integer value mail.imap.socketFactory If this parameter is set to a class that implements the 'javax.net.SocketFactory' interface, this class is used to create IMAP sockets. None Valid SocketFactory mail.imap.socketFactory.class If this parameter is set, it specifies the name of a class that implements the 'javax.net.SocketFactory' interface. This class is used to create IMAP sockets. None Valid string mail.imap.socketFactory.fallback If this parameter is set to 'true', failure to create a socket using the specified socket factory class results in the socket being created using the 'java.net.Socket' class. true true or false mail.imap.socketFactory.port This specifies the port to connect to when using the specified socket factory. If this parameter is not set, the default port is used. 143 Valid Integer mail.imap.ssl.checkserveridentity If this parameter is set to 'true', the system checks the server identity as specified by RFC 2595. false true or false mail.imap.ssl.trust If this parameter is set and a socket factory has not been specified, it enables the use of a 'MailSSLSocketFactory'. If this parameter is set to ' ', all the hosts are trusted. If this parameter specifies list of hosts separated by white spaces, only those hosts are trusted. If the parameter is not set to any of the values mentioned above, trust depends on the certificate presented by the server. Valid String mail.imap.ssl.socketFactory If this parameter is set to a class that extends the 'javax.net.ssl.SSLSocketFactory' class this class is used to create IMAP SSL sockets. None SSL Socket Factory mail.imap.ssl.socketFactory.class If this parameter is set, it specifies the name of a class that extends the 'javax.net.ssl.SSLSocketFactory' class. This class is used to create IMAP SSL sockets. None Valid String mail.imap.ssl.socketFactory.port This specifies the port to connect to when using the specified socket factory. the default port 993 is used. valid port number mail.imap.ssl.protocols This specifies the SSL protocols that are enabled for SSL connections. The property value is a whitespace-separated list of tokens acceptable to the 'javax.net.ssl.SSLSocket.setEnabledProtocols' method. None Valid string mail.imap.starttls.enable If this parameter is set to 'true', it is possible to use the 'STARTTLS' command (if supported by the server) to switch the connection to a TLS-protected connection before issuing any login commands. false true or false mail.imap.socks.host This specifies the host name of a 'SOCKS5' proxy server that is used to connect to the mail server. None Valid String mail.imap.socks.port This specifies the port number for the 'SOCKS5' proxy server. This is needed if the proxy server is not using the standard port number 1080. 1080 Valid String mail.imap.minidletime This property sets the delay in milliseconds. 10 milliseconds time in seconds (Integer) mail.imap.enableimapevents If this property is set to 'true', it enables special IMAP-specific events to be delivered to the 'ConnectionListener' of the store. The unsolicited responses received during the idle method of the store are sent as connection events with 'IMAPStore.RESPONSE' as the type. The event's message is the raw IMAP response string. false true or false mail.imap.folder.class The class name of a subclass of 'com.sun.mail.imap.IMAPFolder'. The subclass can be used to provide support for additional IMAP commands. The subclass must have public constructors of the form 'public MyIMAPFolder'(String fullName, char separator, IMAPStore store, Boolean isNamespace) and public 'MyIMAPFolder'(ListInfo li, IMAPStore store) None Valid String mail.pop3.connectiontimeout The socket connection timeout value in milliseconds. Infinite timeout Integer value mail.pop3.timeout The socket I/O timeout value in milliseconds. Infinite timeout Integer value mail.pop3.message.class The class name of a subclass of 'com.sun.mail.pop3.POP3Message'. None Valid String mail.pop3.localaddress The local address (host name) to bind to when creating the POP3 socket. Defaults to the address picked by the Socket class. Valid String mail.pop3.localport The local port number to bind to when creating the POP3 socket. Defaults to the port number picked by the Socket class. Valid port number mail.pop3.apop.enable If this parameter is set to 'true', use 'APOP' instead of 'USER/PASS' to log in to the 'POP3' server (if the 'POP3' server supports 'APOP'). APOP sends a digest of the password instead of clearing the text password. false true or false mail.pop3.socketFactory If this parameter is set to a class that implements the 'javax.net.SocketFactory' interface, this class is used to create 'POP3' sockets. None Socket Factory mail.pop3.socketFactory.class If this parameter is set, it specifies the name of a class that implements the 'javax.net.SocketFactory' interface. This class is used to create 'POP3' sockets. None Valid String mail.pop3.socketFactory.fallback If this parameter is set to 'true', failure to create a socket using the specified socket factory class results in the socket being created using the 'java.net.Socket' class. false true or false mail.pop3.socketFactory.port This specifies the port to connect to when using the specified socket factory. Default port Valid port number mail.pop3.ssl.checkserveridentity If this parameter is set to 'true', check the server identity as specified by RFC 2595. false true or false mail.pop3.ssl.trust If this parameter is set and a socket factory has not been specified, it is possible to use a 'MailSSLSocketFactory'. If this parameter is set to ' ', all the hosts are trusted. If the parameter is set to a whitespace-separated list of hosts, only those hosts are trusted. If the parameter is not set to any of the values mentioned above, trust depends on the certificate presented by the server. Valid String mail.pop3.ssl.socketFactory If this parameter is set to a class that extends the 'javax.net.ssl.SSLSocketFactory' class, this class is used to create 'POP3' SSL sockets. None SSL Socket Factory mail.pop3.ssl.checkserveridentity If this parameter is set to 'true', the system checks the server identity as specified by 'RFC 2595'. false true or false mail.pop3.ssl.trust If this parameter is set and a socket factory has not been specified, it is possible to use a 'MailSSLSocketFactory'. If this parameter is set to '*', all the hosts are trusted. If the parameter is set to a whitespace-separated list of hosts, only those hosts are trusted. Trust depends on the certificate presented by the server. Valid String mail.pop3.ssl.socketFactory If this parameter is set to a class that extends the 'javax.net.ssl.SSLSocketFactory' class, this class is used to create 'POP3 SSL' sockets. None SSL Socket Factory mail.pop3.ssl.socketFactory.class If this parameter is set, it specifies the name of a class that extends the 'javax.net.ssl.SSLSocketFactory' class. This class is used to create 'POP3 SSL' sockets. None Valid String mail.pop3.ssl.socketFactory.p This parameter pecifies the port to connect to when using the specified socket factory. 995 Valid Integer mail.pop3.ssl.protocols This parameter specifies the SSL protocols that are enabled for SSL connections. The property value is a whitespace-separated list of tokens acceptable to the 'javax.net.ssl.SSLSocket.setEnabledProtocols' method. None Valid String mail.pop3.starttls.enable If this parameter is set to 'true', it is possible to use the 'STLS' command (if supported by the server) to switch the connection to a TLS-protected connection before issuing any login commands. false true or false mail.pop3.starttls.required If this parameter is set to 'true', it is required to use the 'STLS' command. The connect method fails if the server does not support the 'STLS' command or if the command fails. false true or false mail.pop3.socks.host This parameter specifies the host name of a 'SOCKS5' proxy server that can be used to connect to the mail server. None Valid String mail.pop3.socks.port This parameter specifies the port number for the 'SOCKS5' proxy server. None Valid String mail.pop3.disabletop If this parameter is set to 'true', the 'POP3 TOP' command is not used to fetch message headers. false true or false mail.pop3.forgettopheaders If this parameter is set to 'true', the headers that might have been retrieved using the 'POP3 TOP' command is forgotten and replaced by the headers retrieved when the 'POP3 RETR' command is executed. false true or false mail.pop3.filecache.enable If this parameter is set to 'true', the 'POP3' provider caches message data in a temporary file instead of caching them in memory. Messages are only added to the cache when accessing the message content. Message headers are always cached in memory (on demand). The file cache is removed when the folder is closed or the JVM terminates. false true or false mail.pop3.filecache.dir If the file cache is enabled, this property is used to override the default directory used by the JDK for temporary files. None Valid String mail.pop3.cachewriteto This parameter controls the behavior of the 'writeTo' method on a 'POP3' message object. If the parameter is set to 'true', the message content has not been cached yet, and the 'ignoreList' is null, the message is cached before being written. If not, the message is streamed directly to the output stream without being cached. false true or false mail.pop3.keepmessagecontent If this property is set to 'true', a hard reference to the cached content is retained, preventing the memory from being reused until the folder is closed, or until the cached content is explicitly invalidated (using the 'invalidate' method). false true or false Examples EXAMPLE 1 @source(type='email', @map(type='xml'), username='receiver.account', password='account.password',)define stream inputStream (name string, age int, country string); This example illustrates how to receive events in 'xml' format via the email source. In this example, only the required parameters are defined in the stream definition. The default values are taken for the other parameters. The search term is not defined, and therefore, all the new messages in the inbox folder are polled and taken. EXAMPLE 2 @source(type='email', @map(type='xml'), username='receiver.account', password='account.password',store = 'imap',host = 'imap.gmail.com',port = '993',searchTerm = 'subject:Stream Processor, from: from.account@ , cc: cc.account',polling.interval='500',action.after.processed='DELETE',content.type='text/html,)define stream inputStream (name string, age int, country string); This example illustrates how to receive events in 'xml' format via the email source. The email source polls the mail account every 500 seconds to check whether any new mails have arrived. It processes new mails only if they satisfy the conditions specified for the email search term (the value for 'from' of the email message should be 'from.account@. host name ', and the message should contain 'cc.account' in the cc receipient list and the word 'Stream Processor' in the mail subject). in this example, the action after processing is 'DELETE'. Therefore,after processing the event, corresponding mail is deleted from the mail folder.","title":"email (Source)"},{"location":"docs/api/5.1.1/#file-source","text":"File Source provides the functionality for user to feed data to siddhi from files. Both text and binary files are supported by file source. Origin: siddhi-io-file:2.0.3 Syntax @source(type=\"file\", dir.uri=\" STRING \", file.uri=\" STRING \", mode=\" STRING \", tailing=\" BOOL \", action.after.process=\" STRING \", action.after.failure=\" STRING \", move.after.process=\" STRING \", move.after.failure=\" STRING \", begin.regex=\" STRING \", end.regex=\" STRING \", file.polling.interval=\" STRING \", dir.polling.interval=\" STRING \", timeout=\" STRING \", file.read.wait.timeout=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic dir.uri Used to specify a directory to be processed. All the files inside this directory will be processed. Only one of 'dir.uri' and 'file.uri' should be provided. This uri MUST have the respective protocol specified. STRING No No file.uri Used to specify a file to be processed. Only one of 'dir.uri' and 'file.uri' should be provided. This uri MUST have the respective protocol specified. STRING No No mode This parameter is used to specify how files in given directory should.Possible values for this parameter are, 1. TEXT.FULL : to read a text file completely at once. 2. BINARY.FULL : to read a binary file completely at once. 3. LINE : to read a text file line by line. 4. REGEX : to read a text file and extract data using a regex. line STRING Yes No tailing This can either have value true or false. By default it will be true. This attribute allows user to specify whether the file should be tailed or not. If tailing is enabled, the first file of the directory will be tailed. Also tailing should not be enabled in 'binary.full' or 'text.full' modes. true BOOL Yes No action.after.process This parameter is used to specify the action which should be carried out after processing a file in the given directory. It can be either DELETE or MOVE and default value will be 'DELETE'. If the action.after.process is MOVE, user must specify the location to move consumed files using 'move.after.process' parameter. delete STRING Yes No action.after.failure This parameter is used to specify the action which should be carried out if a failure occurred during the process. It can be either DELETE or MOVE and default value will be 'DELETE'. If the action.after.failure is MOVE, user must specify the location to move consumed files using 'move.after.failure' parameter. delete STRING Yes No move.after.process If action.after.process is MOVE, user must specify the location to move consumed files using 'move.after.process' parameter. This should be the absolute path of the file that going to be created after moving is done. This uri MUST have the respective protocol specified. STRING No No move.after.failure If action.after.failure is MOVE, user must specify the location to move consumed files using 'move.after.failure' parameter. This should be the absolute path of the file that going to be created after moving is done. This uri MUST have the respective protocol specified. STRING No No begin.regex This will define the regex to be matched at the beginning of the retrieved content. None STRING Yes No end.regex This will define the regex to be matched at the end of the retrieved content. None STRING Yes No file.polling.interval This parameter is used to specify the time period (in milliseconds) of a polling cycle for a file. 1000 STRING Yes No dir.polling.interval This parameter is used to specify the time period (in milliseconds) of a polling cycle for a directory. 1000 STRING Yes No timeout This parameter is used to specify the maximum time period (in milliseconds) for waiting until a file is processed. 5000 STRING Yes No file.read.wait.timeout This parameter is used to specify the maximum time period (in milliseconds) till it waits before retrying to read the full file content. 1000 STRING Yes No Examples EXAMPLE 1 @source(type='file', mode='text.full', tailing='false' dir.uri='file://abc/xyz', action.after.process='delete', @map(type='json')) define stream FooStream (symbol string, price float, volume long); Under above configuration, all the files in directory will be picked and read one by one. In this case, it's assumed that all the files contains json valid json strings with keys 'symbol','price' and 'volume'. Once a file is read, its content will be converted to an event using siddhi-map-json extension and then, that event will be received to the FooStream. Finally, after reading is finished, the file will be deleted. EXAMPLE 2 @source(type='file', mode='files.repo.line', tailing='true', dir.uri='file://abc/xyz', @map(type='json')) define stream FooStream (symbol string, price float, volume long); Under above configuration, the first file in directory '/abc/xyz' will be picked and read line by line. In this case, it is assumed that the file contains lines json strings. For each line, line content will be converted to an event using siddhi-map-json extension and then, that event will be received to the FooStream. Once file content is completely read, it will keep checking whether a new entry is added to the file or not. If such entry is added, it will be immediately picked up and processed.","title":"file (Source)"},{"location":"docs/api/5.1.1/#grpc-source","text":"This extension starts a grpc server during initialization time. The server listens to requests from grpc stubs. This source has a default mode of operation and custom user defined grpc service mode. By default this uses EventService. Please find the proto definition here . In the default mode this source will use EventService consume method. If we want to use our custom gRPC services, we have to pack auto-generated gRPC service classes and protobuf classes into a jar file and add it into the project classpath (or to the jars folder in the siddhi-tooling folder if we use it with siddhi-tooling ). Please find the custom protobuf definition that uses in examples here . This method will receive requests and injects them into stream through a mapper. Origin: siddhi-io-grpc:1.0.5 Syntax @source(type=\"grpc\", receiver.url=\" STRING \", max.inbound.message.size=\" INT \", max.inbound.metadata.size=\" INT \", server.shutdown.waiting.time=\" LONG \", truststore.file=\" STRING \", truststore.password=\" STRING \", truststore.algorithm=\" STRING \", tls.store.type=\" STRING \", keystore.file=\" STRING \", keystore.password=\" STRING \", keystore.algorithm=\" STRING \", enable.ssl=\" BOOL \", threadpool.size=\" INT \", threadpool.buffer.size=\" INT \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic receiver.url The url which can be used by a client to access the grpc server in this extension. This url should consist the host hostPort, port, fully qualified service name, method name in the following format. grpc://0.0.0.0:9763/ serviceName / methodName For example: grpc://0.0.0.0:9763/org.wso2.grpc.EventService/consume STRING No No max.inbound.message.size Sets the maximum message size in bytes allowed to be received on the server. 4194304 INT Yes No max.inbound.metadata.size Sets the maximum size of metadata in bytes allowed to be received. 8192 INT Yes No server.shutdown.waiting.time The time in seconds to wait for the server to shutdown, giving up if the timeout is reached. 5 LONG Yes No truststore.file the file path of truststore. If this is provided then server authentication is enabled - STRING Yes No truststore.password the password of truststore. If this is provided then the integrity of the keystore is checked - STRING Yes No truststore.algorithm the encryption algorithm to be used for server authentication - STRING Yes No tls.store.type TLS store type - STRING Yes No keystore.file the file path of keystore. If this is provided then client authentication is enabled - STRING Yes No keystore.password the password of keystore - STRING Yes No keystore.algorithm the encryption algorithm to be used for client authentication - STRING Yes No enable.ssl to enable ssl. If set to true and truststore.file is not given then it will be set to default carbon jks by default FALSE BOOL Yes No threadpool.size Sets the maximum size of threadpool dedicated to serve requests at the gRPC server 100 INT Yes No threadpool.buffer.size Sets the maximum size of threadpool buffer server 100 INT Yes No System Parameters Name Description Default Value Possible Parameters keyStoreFile Path of the key store file src/main/resources/security/wso2carbon.jks valid path for a key store file keyStorePassword This is the password used with key store file wso2carbon valid password for the key store file keyStoreAlgorithm The encryption algorithm to be used for client authentication SunX509 - trustStoreFile This is the trust store file with the path src/main/resources/security/client-truststore.jks - trustStorePassword This is the password used with trust store file wso2carbon valid password for the trust store file trustStoreAlgorithm the encryption algorithm to be used for server authentication SunX509 - Examples EXAMPLE 1 @source(type='grpc', receiver.url='grpc://localhost:8888/org.wso2.grpc.EventService/consume', @map(type='json')) define stream BarStream (message String); Here the port is given as 8888. So a grpc server will be started on port 8888 and the server will expose EventService. This is the default service packed with the source. In EventService the consume method is EXAMPLE 2 @source(type='grpc', receiver.url='grpc://localhost:8888/org.wso2.grpc.EventService/consume', @map(type='json', @attributes(name='trp:name', age='trp:age', message='message'))) define stream BarStream (message String, name String, age int); Here we are getting headers sent with the request as transport properties and injecting them into the stream. With each request a header will be sent in MetaData in the following format: 'Name:John', 'Age:23' EXAMPLE 3 @source(type='grpc', receiver.url='grpc://localhost:8888/org.wso2.grpc.MyService/send', @map(type='protobuf')) define stream BarStream (stringValue string, intValue int,longValue long,booleanValue bool,floatValue float,doubleValue double); Here the port is given as 8888. So a grpc server will be started on port 8888 and sever will keep listening to the 'send' RPC method in the 'MyService' service. EXAMPLE 4 @source(type='grpc', receiver.url='grpc://localhost:8888/org.wso2.grpc.MyService/send', @map(type='protobuf', @attributes(a = 'stringValue', b = 'intValue', c = 'longValue',d = 'booleanValue', e ='floatValue', f ='doubleValue'))) define stream BarStream (a string ,c long,b int, d bool,e float,f double); Here the port is given as 8888. So a grpc server will be started on port 8888 and sever will keep listening to the 'send' method in the 'MyService' service. Since we provide mapping in the stream we can use any names for stream attributes, but we have to map those names with correct protobuf message attributes' names. If we want to send metadata, we should map the attributes. EXAMPLE 5 @source(type='grpc', receiver.url='grpc://localhost:8888/org.wso2.grpc.StreamService/clientStream', @map(type='protobuf')) define stream BarStream (stringValue string, intValue int,longValue long,booleanValue bool,floatValue float,doubleValue double); Here we receive a stream of requests to the grpc source. Whenever we want to use streaming with grpc source, we have to define the RPC method as client streaming method (look at the sample proto file provided in the resource folder here ), when we define a stream method siddhi will identify it as a stream RPC method and ready to accept stream of request from the client.","title":"grpc (Source)"},{"location":"docs/api/5.1.1/#grpc-call-response-source","text":"This grpc source receives responses received from gRPC server for requests sent from a grpc-call sink. The source will receive responses for sink with the same sink.id. For example if you have a gRPC sink with sink.id 15 then we need to set the sink.id as 15 in the source to receives responses. Sinks and sources have 1:1 mapping Origin: siddhi-io-grpc:1.0.5 Syntax @source(type=\"grpc-call-response\", sink.id=\" INT \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic sink.id a unique ID that should be set for each grpc-call source. There is a 1:1 mapping between grpc-call sinks and grpc-call-response sources. Each sink has one particular source listening to the responses to requests published from that sink. So the same sink.id should be given when writing the sink also. INT No No Examples EXAMPLE 1 @source(type='grpc-call-response', sink.id= '1') define stream BarStream (message String);@sink(type='grpc-call', publisher.url = 'grpc://194.23.98.100:8080/EventService/process', sink.id= '1', @map(type='json')) define stream FooStream (message String); Here we are listening to responses for requests sent from the sink with sink.id 1 will be received here. The results will be injected into BarStream","title":"grpc-call-response (Source)"},{"location":"docs/api/5.1.1/#grpc-service-source","text":"This extension implements a grpc server for receiving and responding to requests. During initialization time a grpc server is started on the user specified port exposing the required service as given in the url. This source also has a default mode and a user defined grpc service mode. By default this uses EventService. Please find the proto definition here In the default mode this will use the EventService process method. If we want to use our custom gRPC services, we have to pack auto-generated gRPC service classes and protobuf classes into a jar file and add it into the project classpath (or to the jars folder in the siddhi-tooling folder if we use it with siddhi-tooling ). Please find the custom protobuf definition that uses in examples here . This accepts grpc message class Event as defined in the EventService proto. This uses GrpcServiceResponse sink to send reponses back in the same Event message format. Origin: siddhi-io-grpc:1.0.5 Syntax @source(type=\"grpc-service\", receiver.url=\" STRING \", max.inbound.message.size=\" INT \", max.inbound.metadata.size=\" INT \", service.timeout=\" INT \", server.shutdown.waiting.time=\" LONG \", truststore.file=\" STRING \", truststore.password=\" STRING \", truststore.algorithm=\" STRING \", tls.store.type=\" STRING \", keystore.file=\" STRING \", keystore.password=\" STRING \", keystore.algorithm=\" STRING \", enable.ssl=\" BOOL \", threadpool.size=\" INT \", threadpool.buffer.size=\" INT \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic receiver.url The url which can be used by a client to access the grpc server in this extension. This url should consist the host hostPort, port, fully qualified service name, method name in the following format. grpc://0.0.0.0:9763/ serviceName / methodName For example: grpc://0.0.0.0:9763/org.wso2.grpc.EventService/consume STRING No No max.inbound.message.size Sets the maximum message size in bytes allowed to be received on the server. 4194304 INT Yes No max.inbound.metadata.size Sets the maximum size of metadata in bytes allowed to be received. 8192 INT Yes No service.timeout The period of time in milliseconds to wait for siddhi to respond to a request received. After this time period of receiving a request it will be closed with an error message. 10000 INT Yes No server.shutdown.waiting.time The time in seconds to wait for the server to shutdown, giving up if the timeout is reached. 5 LONG Yes No truststore.file the file path of truststore. If this is provided then server authentication is enabled - STRING Yes No truststore.password the password of truststore. If this is provided then the integrity of the keystore is checked - STRING Yes No truststore.algorithm the encryption algorithm to be used for server authentication - STRING Yes No tls.store.type TLS store type - STRING Yes No keystore.file the file path of keystore. If this is provided then client authentication is enabled - STRING Yes No keystore.password the password of keystore - STRING Yes No keystore.algorithm the encryption algorithm to be used for client authentication - STRING Yes No enable.ssl to enable ssl. If set to true and truststore.file is not given then it will be set to default carbon jks by default FALSE BOOL Yes No threadpool.size Sets the maximum size of threadpool dedicated to serve requests at the gRPC server 100 INT Yes No threadpool.buffer.size Sets the maximum size of threadpool buffer server 100 INT Yes No System Parameters Name Description Default Value Possible Parameters keyStoreFile This is the key store file with the path src/main/resources/security/wso2carbon.jks valid path for a key store file keyStorePassword This is the password used with key store file wso2carbon valid password for the key store file keyStoreAlgorithm The encryption algorithm to be used for client authentication SunX509 - trustStoreFile This is the trust store file with the path src/main/resources/security/client-truststore.jks - trustStorePassword This is the password used with trust store file wso2carbon valid password for the trust store file trustStoreAlgorithm the encryption algorithm to be used for server authentication SunX509 - Examples EXAMPLE 1 @source(type='grpc-service', receiver.url='grpc://localhost:8888/org.wso2.grpc.EventService/process', source.id='1', @map(type='json', @attributes(messageId='trp:messageId', message='message'))) define stream FooStream (messageId String, message String); Here a grpc server will be started at port 8888. The process method of EventService will be exposed for clients. source.id is set as 1. So a grpc-service-response sink with source.id = 1 will send responses back for requests received to this source. Note that it is required to specify the transport property messageId since we need to correlate the request message with the response. EXAMPLE 2 @sink(type='grpc-service-response', source.id='1', @map(type='json')) define stream BarStream (messageId String, message String); @source(type='grpc-service', receiver.url='grpc://134.23.43.35:8080/org.wso2.grpc.EventService/process', source.id='1', @map(type='json', @attributes(messageId='trp:messageId', message='message'))) define stream FooStream (messageId String, message String); from FooStream select * insert into BarStream; The grpc requests are received through the grpc-service sink. Each received event is sent back through grpc-service-source. This is just a passthrough through Siddhi as we are selecting everything from FooStream and inserting into BarStream. EXAMPLE 3 @source(type='grpc-service', source.id='1' receiver.url='grpc://locanhost:8888/org.wso2.grpc.EventService/consume', @map(type='json', @attributes(name='trp:name', age='trp:age', message='message'))) define stream BarStream (message String, name String, age int); Here we are getting headers sent with the request as transport properties and injecting them into the stream. With each request a header will be sent in MetaData in the following format: 'Name:John', 'Age:23' EXAMPLE 4 @sink(type='grpc-service-response', source.id='1', message.id='{{messageId}}', @map(type='protobuf', @payload(stringValue='a',intValue='b',longValue='c',booleanValue='d',floatValue = 'e', doubleValue ='f'))) define stream BarStream (a string,messageId string, b int,c long,d bool,e float,f double); @source(type='grpc-service', receiver.url='grpc://134.23.43.35:8888/org.wso2.grpc.test.MyService/process', source.id='1', @map(type='protobuf', @attributes(messageId='trp:message.id', a = 'stringValue', b = 'intValue', c = 'longValue',d = 'booleanValue', e = 'floatValue', f ='doubleValue'))) define stream FooStream (a string,messageId string, b int,c long,d bool,e float,f double); from FooStream select * insert into BarStream; Here a grpc server will be started at port 8888. The process method of the MyService will be exposed to the clients. 'source.id' is set as 1. So a grpc-service-response sink with source.id = 1 will send responses back for requests received to this source. Note that it is required to specify the transport property messageId since we need to correlate the request message with the response and also we should map stream attributes with correct protobuf message attributes even they define using the same name as protobuf message attributes.","title":"grpc-service (Source)"},{"location":"docs/api/5.1.1/#http-source","text":"HTTP source receives POST requests via HTTP and HTTPS protocols in format such as text , XML and JSON . It also supports basic authentication to ensure events are received from authorized users/systems. The request headers and properties can be accessed via transport properties in the format trp: header . Origin: siddhi-io-http:2.2.0 Syntax @source(type=\"http\", receiver.url=\" STRING \", basic.auth.enabled=\" STRING \", worker.count=\" INT \", socket.idle.timeout=\" INT \", ssl.verify.client=\" STRING \", ssl.protocol=\" STRING \", tls.store.type=\" STRING \", ssl.configurations=\" STRING \", request.size.validation.configurations=\" STRING \", header.validation.configurations=\" STRING \", server.bootstrap.configurations=\" STRING \", trace.log.enabled=\" BOOL \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic receiver.url The URL on which events should be received. To enable SSL use https protocol in the url. http://0.0.0.0:9763/ appNAme / streamName STRING Yes No basic.auth.enabled This only works in VM, Docker and Kubernetes. Where when enabled it authenticates each request using the Authorization:'Basic encodeBase64(username:Password)' header. false STRING Yes No worker.count The number of active worker threads to serve the incoming events. By default the value is set to 1 to ensure events are processed in the same order they arrived. By increasing this value, higher performance can be achieved in the expense of loosing event ordering. 1 INT Yes No socket.idle.timeout Idle timeout for HTTP connection in millis. 120000 INT Yes No ssl.verify.client The type of client certificate verification. Supported values are require , optional . - STRING Yes No ssl.protocol SSL/TLS protocol. TLS STRING Yes No tls.store.type TLS store type. JKS STRING Yes No ssl.configurations SSL/TSL configurations in format \"' key : value ',' key : value '\" . Some supported parameters: - SSL/TLS protocols: 'sslEnabledProtocols:TLSv1.1,TLSv1.2' - List of ciphers: 'ciphers:TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256' - Enable session creation: 'client.enable.session.creation:true' - Supported server names: 'server.suported.server.names:server' - Add HTTP SNIMatcher: 'server.supported.snimatchers:SNIMatcher' - STRING Yes No request.size.validation.configurations Configurations to validate the HTTP request size. Expected format \"' key : value ',' key : value '\" . Some supported configurations : - Enable request size validation: 'request.size.validation:true' If request size is validated - Maximum request size: 'request.size.validation.maximum.value:2048' - Response status code when request size validation fails: 'request.size.validation.reject.status.code:401' - Response message when request size validation fails: 'request.size.validation.reject.message:Message is bigger than the valid size' - Response Content-Type when request size validation fails: 'request.size.validation.reject.message.content.type:plain/text' - STRING Yes No header.validation.configurations Configurations to validate HTTP headers. Expected format \"' key : value ',' key : value '\" . Some supported configurations : - Enable header size validation: 'header.size.validation:true' If header size is validated - Maximum length of initial line: 'header.validation.maximum.request.line:4096' - Maximum length of all headers: 'header.validation.maximum.size:8192' - Maximum length of the content or each chunk: 'header.validation.maximum.chunk.size:8192' - Response status code when header validation fails: 'header.validation.reject.status.code:401' - Response message when header validation fails: 'header.validation.reject.message:Message header is bigger than the valid size' - Response Content-Type when header validation fails: 'header.validation.reject.message.content.type:plain/text' - STRING Yes No server.bootstrap.configurations Server bootstrap configurations in format \"' key : value ',' key : value '\" . Some supported configurations : - Server connect timeout in millis: 'server.bootstrap.connect.timeout:15000' - Server socket timeout in seconds: 'server.bootstrap.socket.timeout:15' - Enable TCP no delay: 'server.bootstrap.nodelay:true' - Enable server keep alive: 'server.bootstrap.keepalive:true' - Send buffer size: 'server.bootstrap.sendbuffersize:1048576' - Receive buffer size: 'server.bootstrap.recievebuffersize:1048576' - Number of connections queued: 'server.bootstrap.socket.backlog:100' - STRING Yes No trace.log.enabled Enable trace log for traffic monitoring. false BOOL Yes No System Parameters Name Description Default Value Possible Parameters serverBootstrapBossGroupSize Number of boss threads to accept incoming connections. Number of available processors Any positive integer serverBootstrapWorkerGroupSize Number of worker threads to accept the connections from boss threads and perform non-blocking read and write from one or more channels. (Number of available processors) * 2 Any positive integer serverBootstrapClientGroupSize Number of client threads to perform non-blocking read and write to one or more channels. (Number of available processors) * 2 Any positive integer defaultHost The default host of the transport. 0.0.0.0 Any valid host defaultScheme The default protocol. http http https defaultHttpPort The default HTTP port when default scheme is http . 8280 Any valid port defaultHttpsPort The default HTTPS port when default scheme is https . 8243 Any valid port keyStoreLocation The default keystore file path. ${carbon.home}/resources/security/wso2carbon.jks Path to .jks file keyStorePassword The default keystore password. wso2carbon Keystore password as string Examples EXAMPLE 1 @app.name('StockProcessor') @source(type='http', @map(type = 'json')) define stream StockStream (symbol string, price float, volume long); Above HTTP source listeners on url http://0.0.0.0:9763/StockProcessor/StockStream for JSON messages on the format: { \"event\": { \"symbol\": \"FB\", \"price\": 24.5, \"volume\": 5000 } } It maps the incoming messages and sends them to StockStream for processing. EXAMPLE 2 @source(type='http', receiver.url='http://localhost:5005/stocks', @map(type = 'xml')) define stream StockStream (symbol string, price float, volume long); Above HTTP source listeners on url http://localhost:5005/stocks for JSON messages on the format: events event symbol Fb /symbol price 55.6 /price volume 100 /volume /event /events It maps the incoming messages and sends them to StockStream for processing.","title":"http (Source)"},{"location":"docs/api/5.1.1/#http-call-response-source","text":"The http-call-response source receives the responses for the calls made by its corresponding http-call sink, and maps them from formats such as text , XML and JSON . To handle messages with different http status codes having different formats, multiple http-call-response sources are allowed to associate with a single http-call sink. It allows accessing the attributes of the event that initiated the call, and the response headers and properties via transport properties in the format trp: attribute name and trp: header/property respectively. Origin: siddhi-io-http:2.2.0 Syntax @source(type=\"http-call-response\", sink.id=\" STRING \", http.status.code=\" STRING \", allow.streaming.responses=\" BOOL \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic sink.id Identifier to correlate the http-call-response source with its corresponding http-call sink that published the messages. STRING No No http.status.code The matching http responses status code regex, that is used to filter the the messages which will be processed by the source.Eg: http.status.code = '200' , http.status.code = '4\\d+' 200 STRING Yes No allow.streaming.responses Enable consuming responses on a streaming manner. false BOOL Yes No Examples EXAMPLE 1 @sink(type='http-call', method='POST', publisher.url='http://localhost:8005/registry/employee', sink.id='employee-info', @map(type='json')) define stream EmployeeRequestStream (name string, id int); @source(type='http-call-response', sink.id='employee-info', http.status.code='2\\\\d+', @map(type='json', @attributes(name='trp:name', id='trp:id', location='$.town', age='$.age'))) define stream EmployeeResponseStream(name string, id int, location string, age int); @source(type='http-call-response', sink.id='employee-info', http.status.code='4\\\\d+', @map(type='text', regex.A='((.|\\n)*)', @attributes(error='A[1]'))) define stream EmployeeErrorStream(error string); When events arrive in EmployeeRequestStream , http-call sink makes calls to endpoint on url http://localhost:8005/registry/employee with POST method and Content-Type application/json . If the arriving event has attributes name : John and id : 1423 it will send a message with default JSON mapping as follows: { \"event\": { \"name\": \"John\", \"id\": 1423 } } When the endpoint responds with status code in the range of 200 the message will be received by the http-call-response source associated with the EmployeeResponseStream stream, because it is correlated with the sink by the same sink.id employee-info and as that expects messages with http.status.code in regex format 2\\d+ . If the response message is in the format { \"town\": \"NY\", \"age\": 24 } the source maps the location and age attributes by executing JSON path on the message and maps the name and id attributes by extracting them from the request event via as transport properties. If the response status code is in the range of 400 then the message will be received by the http-call-response source associated with the EmployeeErrorStream stream, because it is correlated with the sink by the same sink.id employee-info and it expects messages with http.status.code in regex format 4\\d+ , and maps the error response to the error attribute of the event.","title":"http-call-response (Source)"},{"location":"docs/api/5.1.1/#http-request-source","text":"Deprecated (Use http-service source instead). The http-request source receives POST requests via HTTP and HTTPS protocols in format such as text , XML and JSON and sends responses via its corresponding http-response sink correlated through a unique source.id . For request and response correlation, it generates a messageId upon each incoming request and expose it via transport properties in the format trp:messageId to correlate them with the responses at the http-response sink. The request headers and properties can be accessed via transport properties in the format trp: header . It also supports basic authentication to ensure events are received from authorized users/systems. Origin: siddhi-io-http:2.2.0 Syntax @source(type=\"http-request\", receiver.url=\" STRING \", source.id=\" STRING \", connection.timeout=\" INT \", basic.auth.enabled=\" STRING \", worker.count=\" INT \", socket.idle.timeout=\" INT \", ssl.verify.client=\" STRING \", ssl.protocol=\" STRING \", tls.store.type=\" STRING \", ssl.configurations=\" STRING \", request.size.validation.configurations=\" STRING \", header.validation.configurations=\" STRING \", server.bootstrap.configurations=\" STRING \", trace.log.enabled=\" BOOL \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic receiver.url The URL on which events should be received. To enable SSL use https protocol in the url. http://0.0.0.0:9763/ appNAme / streamName STRING Yes No source.id Identifier to correlate the http-request source to its corresponding http-response sinks to send responses. STRING No No connection.timeout Connection timeout in millis. The system will send a timeout, if a corresponding response is not sent by an associated http-response sink within the given time. 120000 INT Yes No basic.auth.enabled This only works in VM, Docker and Kubernetes. Where when enabled it authenticates each request using the Authorization:'Basic encodeBase64(username:Password)' header. false STRING Yes No worker.count The number of active worker threads to serve the incoming events. By default the value is set to 1 to ensure events are processed in the same order they arrived. By increasing this value, higher performance can be achieved in the expense of loosing event ordering. 1 INT Yes No socket.idle.timeout Idle timeout for HTTP connection in millis. 120000 INT Yes No ssl.verify.client The type of client certificate verification. Supported values are require , optional . - STRING Yes No ssl.protocol SSL/TLS protocol. TLS STRING Yes No tls.store.type TLS store type. JKS STRING Yes No ssl.configurations SSL/TSL configurations in format \"' key : value ',' key : value '\" . Some supported parameters: - SSL/TLS protocols: 'sslEnabledProtocols:TLSv1.1,TLSv1.2' - List of ciphers: 'ciphers:TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256' - Enable session creation: 'client.enable.session.creation:true' - Supported server names: 'server.suported.server.names:server' - Add HTTP SNIMatcher: 'server.supported.snimatchers:SNIMatcher' - STRING Yes No request.size.validation.configurations Configurations to validate the HTTP request size. Expected format \"' key : value ',' key : value '\" . Some supported configurations : - Enable request size validation: 'request.size.validation:true' If request size is validated - Maximum request size: 'request.size.validation.maximum.value:2048' - Response status code when request size validation fails: 'request.size.validation.reject.status.code:401' - Response message when request size validation fails: 'request.size.validation.reject.message:Message is bigger than the valid size' - Response Content-Type when request size validation fails: 'request.size.validation.reject.message.content.type:plain/text' - STRING Yes No header.validation.configurations Configurations to validate HTTP headers. Expected format \"' key : value ',' key : value '\" . Some supported configurations : - Enable header size validation: 'header.size.validation:true' If header size is validated - Maximum length of initial line: 'header.validation.maximum.request.line:4096' - Maximum length of all headers: 'header.validation.maximum.size:8192' - Maximum length of the content or each chunk: 'header.validation.maximum.chunk.size:8192' - Response status code when header validation fails: 'header.validation.reject.status.code:401' - Response message when header validation fails: 'header.validation.reject.message:Message header is bigger than the valid size' - Response Content-Type when header validation fails: 'header.validation.reject.message.content.type:plain/text' - STRING Yes No server.bootstrap.configurations Server bootstrap configurations in format \"' key : value ',' key : value '\" . Some supported configurations : - Server connect timeout in millis: 'server.bootstrap.connect.timeout:15000' - Server socket timeout in seconds: 'server.bootstrap.socket.timeout:15' - Enable TCP no delay: 'server.bootstrap.nodelay:true' - Enable server keep alive: 'server.bootstrap.keepalive:true' - Send buffer size: 'server.bootstrap.sendbuffersize:1048576' - Receive buffer size: 'server.bootstrap.recievebuffersize:1048576' - Number of connections queued: 'server.bootstrap.socket.backlog:100' - STRING Yes No trace.log.enabled Enable trace log for traffic monitoring. false BOOL Yes No System Parameters Name Description Default Value Possible Parameters serverBootstrapBossGroupSize Number of boss threads to accept incoming connections. Number of available processors Any positive integer serverBootstrapWorkerGroupSize Number of worker threads to accept the connections from boss threads and perform non-blocking read and write from one or more channels. (Number of available processors) * 2 Any positive integer serverBootstrapClientGroupSize Number of client threads to perform non-blocking read and write to one or more channels. (Number of available processors) * 2 Any positive integer defaultHost The default host of the transport. 0.0.0.0 Any valid host defaultScheme The default protocol. http http https defaultHttpPort The default HTTP port when default scheme is http . 8280 Any valid port defaultHttpsPort The default HTTPS port when default scheme is https . 8243 Any valid port keyStoreLocation The default keystore file path. ${carbon.home}/resources/security/wso2carbon.jks Path to .jks file keyStorePassword The default keystore password. wso2carbon Keystore password as string Examples EXAMPLE 1 @source(type='http-request', receiver.url='http://localhost:5005/add', source.id='adder', @map(type='json, @attributes(messageId='trp:messageId', value1='$.event.value1', value2='$.event.value2'))) define stream AddStream (messageId string, value1 long, value2 long); @sink(type='http-response', source.id='adder', message.id='{{messageId}}', @map(type = 'json')) define stream ResultStream (messageId string, results long); @info(name = 'query1') from AddStream select messageId, value1 + value2 as results insert into ResultStream; Above sample listens events on http://localhost:5005/stocks url for JSON messages on the format: { \"event\": { \"value1\": 3, \"value2\": 4 } } Map the vents into AddStream, process the events through query query1 , and sends the results produced on ResultStream via http-response sink on the message format: { \"event\": { \"results\": 7 } }","title":"http-request (Source)"},{"location":"docs/api/5.1.1/#http-response-source","text":"Deprecated (Use http-call-response source instead). The http-response source receives the responses for the calls made by its corresponding http-request sink, and maps them from formats such as text , XML and JSON . To handle messages with different http status codes having different formats, multiple http-response sources are allowed to associate with a single http-request sink. It allows accessing the attributes of the event that initiated the call, and the response headers and properties via transport properties in the format trp: attribute name and trp: header/property respectively. Origin: siddhi-io-http:2.2.0 Syntax @source(type=\"http-response\", sink.id=\" STRING \", http.status.code=\" STRING \", allow.streaming.responses=\" BOOL \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic sink.id Identifier to correlate the http-response source with its corresponding http-request sink that published the messages. STRING No No http.status.code The matching http responses status code regex, that is used to filter the the messages which will be processed by the source.Eg: http.status.code = '200' , http.status.code = '4\\d+' 200 STRING Yes No allow.streaming.responses Enable consuming responses on a streaming manner. false BOOL Yes No Examples EXAMPLE 1 @sink(type='http-request', method='POST', publisher.url='http://localhost:8005/registry/employee', sink.id='employee-info', @map(type='json')) define stream EmployeeRequestStream (name string, id int); @source(type='http-response', sink.id='employee-info', http.status.code='2\\\\d+', @map(type='json', @attributes(name='trp:name', id='trp:id', location='$.town', age='$.age'))) define stream EmployeeResponseStream(name string, id int, location string, age int); @source(type='http-response', sink.id='employee-info', http.status.code='4\\\\d+', @map(type='text', regex.A='((.|\\n)*)', @attributes(error='A[1]'))) define stream EmployeeErrorStream(error string); When events arrive in EmployeeRequestStream , http-request sink makes calls to endpoint on url http://localhost:8005/registry/employee with POST method and Content-Type application/json . If the arriving event has attributes name : John and id : 1423 it will send a message with default JSON mapping as follows: { \"event\": { \"name\": \"John\", \"id\": 1423 } } When the endpoint responds with status code in the range of 200 the message will be received by the http-response source associated with the EmployeeResponseStream stream, because it is correlated with the sink by the same sink.id employee-info and as that expects messages with http.status.code in regex format 2\\d+ . If the response message is in the format { \"town\": \"NY\", \"age\": 24 } the source maps the location and age attributes by executing JSON path on the message and maps the name and id attributes by extracting them from the request event via as transport properties. If the response status code is in the range of 400 then the message will be received by the http-response source associated with the EmployeeErrorStream stream, because it is correlated with the sink by the same sink.id employee-info and it expects messages with http.status.code in regex format 4\\d+ , and maps the error response to the error attribute of the event.","title":"http-response (Source)"},{"location":"docs/api/5.1.1/#http-service-source","text":"The http-service source receives POST requests via HTTP and HTTPS protocols in format such as text , XML and JSON and sends responses via its corresponding http-service-response sink correlated through a unique source.id . For request and response correlation, it generates a messageId upon each incoming request and expose it via transport properties in the format trp:messageId to correlate them with the responses at the http-service-response sink. The request headers and properties can be accessed via transport properties in the format trp: header . It also supports basic authentication to ensure events are received from authorized users/systems. Origin: siddhi-io-http:2.2.0 Syntax @source(type=\"http-service\", receiver.url=\" STRING \", source.id=\" STRING \", connection.timeout=\" INT \", basic.auth.enabled=\" STRING \", worker.count=\" INT \", socket.idle.timeout=\" INT \", ssl.verify.client=\" STRING \", ssl.protocol=\" STRING \", tls.store.type=\" STRING \", ssl.configurations=\" STRING \", request.size.validation.configurations=\" STRING \", header.validation.configurations=\" STRING \", server.bootstrap.configurations=\" STRING \", trace.log.enabled=\" BOOL \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic receiver.url The URL on which events should be received. To enable SSL use https protocol in the url. http://0.0.0.0:9763/ appNAme / streamName STRING Yes No source.id Identifier to correlate the http-service source to its corresponding http-service-response sinks to send responses. STRING No No connection.timeout Connection timeout in millis. The system will send a timeout, if a corresponding response is not sent by an associated http-service-response sink within the given time. 120000 INT Yes No basic.auth.enabled This only works in VM, Docker and Kubernetes. Where when enabled it authenticates each request using the Authorization:'Basic encodeBase64(username:Password)' header. false STRING Yes No worker.count The number of active worker threads to serve the incoming events. By default the value is set to 1 to ensure events are processed in the same order they arrived. By increasing this value, higher performance can be achieved in the expense of loosing event ordering. 1 INT Yes No socket.idle.timeout Idle timeout for HTTP connection in millis. 120000 INT Yes No ssl.verify.client The type of client certificate verification. Supported values are require , optional . - STRING Yes No ssl.protocol SSL/TLS protocol. TLS STRING Yes No tls.store.type TLS store type. JKS STRING Yes No ssl.configurations SSL/TSL configurations in format \"' key : value ',' key : value '\" . Some supported parameters: - SSL/TLS protocols: 'sslEnabledProtocols:TLSv1.1,TLSv1.2' - List of ciphers: 'ciphers:TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256' - Enable session creation: 'client.enable.session.creation:true' - Supported server names: 'server.suported.server.names:server' - Add HTTP SNIMatcher: 'server.supported.snimatchers:SNIMatcher' - STRING Yes No request.size.validation.configurations Configurations to validate the HTTP request size. Expected format \"' key : value ',' key : value '\" . Some supported configurations : - Enable request size validation: 'request.size.validation:true' If request size is validated - Maximum request size: 'request.size.validation.maximum.value:2048' - Response status code when request size validation fails: 'request.size.validation.reject.status.code:401' - Response message when request size validation fails: 'request.size.validation.reject.message:Message is bigger than the valid size' - Response Content-Type when request size validation fails: 'request.size.validation.reject.message.content.type:plain/text' - STRING Yes No header.validation.configurations Configurations to validate HTTP headers. Expected format \"' key : value ',' key : value '\" . Some supported configurations : - Enable header size validation: 'header.size.validation:true' If header size is validated - Maximum length of initial line: 'header.validation.maximum.request.line:4096' - Maximum length of all headers: 'header.validation.maximum.size:8192' - Maximum length of the content or each chunk: 'header.validation.maximum.chunk.size:8192' - Response status code when header validation fails: 'header.validation.reject.status.code:401' - Response message when header validation fails: 'header.validation.reject.message:Message header is bigger than the valid size' - Response Content-Type when header validation fails: 'header.validation.reject.message.content.type:plain/text' - STRING Yes No server.bootstrap.configurations Server bootstrap configurations in format \"' key : value ',' key : value '\" . Some supported configurations : - Server connect timeout in millis: 'server.bootstrap.connect.timeout:15000' - Server socket timeout in seconds: 'server.bootstrap.socket.timeout:15' - Enable TCP no delay: 'server.bootstrap.nodelay:true' - Enable server keep alive: 'server.bootstrap.keepalive:true' - Send buffer size: 'server.bootstrap.sendbuffersize:1048576' - Receive buffer size: 'server.bootstrap.recievebuffersize:1048576' - Number of connections queued: 'server.bootstrap.socket.backlog:100' - STRING Yes No trace.log.enabled Enable trace log for traffic monitoring. false BOOL Yes No System Parameters Name Description Default Value Possible Parameters serverBootstrapBossGroupSize Number of boss threads to accept incoming connections. Number of available processors Any positive integer serverBootstrapWorkerGroupSize Number of worker threads to accept the connections from boss threads and perform non-blocking read and write from one or more channels. (Number of available processors) * 2 Any positive integer serverBootstrapClientGroupSize Number of client threads to perform non-blocking read and write to one or more channels. (Number of available processors) * 2 Any positive integer defaultHost The default host of the transport. 0.0.0.0 Any valid host defaultScheme The default protocol. http http https defaultHttpPort The default HTTP port when default scheme is http . 8280 Any valid port defaultHttpsPort The default HTTPS port when default scheme is https . 8243 Any valid port keyStoreLocation The default keystore file path. ${carbon.home}/resources/security/wso2carbon.jks Path to .jks file keyStorePassword The default keystore password. wso2carbon Keystore password as string Examples EXAMPLE 1 @source(type='http-service', receiver.url='http://localhost:5005/add', source.id='adder', @map(type='json, @attributes(messageId='trp:messageId', value1='$.event.value1', value2='$.event.value2'))) define stream AddStream (messageId string, value1 long, value2 long); @sink(type='http-service-response', source.id='adder', message.id='{{messageId}}', @map(type = 'json')) define stream ResultStream (messageId string, results long); @info(name = 'query1') from AddStream select messageId, value1 + value2 as results insert into ResultStream; Above sample listens events on http://localhost:5005/stocks url for JSON messages on the format: { \"event\": { \"value1\": 3, \"value2\": 4 } } Map the vents into AddStream, process the events through query query1 , and sends the results produced on ResultStream via http-service-response sink on the message format: { \"event\": { \"results\": 7 } }","title":"http-service (Source)"},{"location":"docs/api/5.1.1/#inmemory-source","text":"In-memory source subscribes to a topic to consume events which are published on the same topic by In-memory sinks. This provides a way to connect multiple Siddhi Apps deployed under the same Siddhi Manager (JVM). Here both the publisher and subscriber should have the same event schema (stream definition) for successful data transfer. Origin: siddhi-core:5.1.8 Syntax @source(type=\"inMemory\", topic=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic topic Subscribes to the events sent on the given topic. STRING No No Examples EXAMPLE 1 @source(type='inMemory', topic='Stocks', @map(type='passThrough')) define stream StocksStream (symbol string, price float, volume long); Here the StocksStream uses inMemory source to consume events published on the topic Stocks by the inMemory sinks deployed in the same JVM.","title":"inMemory (Source)"},{"location":"docs/api/5.1.1/#jms-source","text":"JMS Source allows users to subscribe to a JMS broker and receive JMS messages. It has the ability to receive Map messages and Text messages. Origin: siddhi-io-jms:2.0.3 Syntax @source(type=\"jms\", destination=\" STRING \", connection.factory.jndi.name=\" STRING \", factory.initial=\" STRING \", provider.url=\" STRING \", connection.factory.type=\" STRING \", worker.count=\" INT \", connection.username=\" STRING \", connection.password=\" STRING \", retry.interval=\" INT \", retry.count=\" INT \", use.receiver=\" BOOL \", subscription.durable=\" BOOL \", connection.factory.nature=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic destination Queue/Topic name which JMS Source should subscribe to STRING No No connection.factory.jndi.name JMS Connection Factory JNDI name. This value will be used for the JNDI lookup to find the JMS Connection Factory. QueueConnectionFactory STRING Yes No factory.initial Naming factory initial value STRING No No provider.url Java naming provider URL. Property for specifying configuration information for the service provider to use. The value of the property should contain a URL string (e.g. \"ldap://somehost:389\") STRING No No connection.factory.type Type of the connection connection factory. This can be either queue or topic. queue STRING Yes No worker.count Number of worker threads listening on the given queue/topic. 1 INT Yes No connection.username username for the broker. None STRING Yes No connection.password Password for the broker None STRING Yes No retry.interval Interval between each retry attempt in case of connection failure in milliseconds. 10000 INT Yes No retry.count Number of maximum reties that will be attempted in case of connection failure with broker. 5 INT Yes No use.receiver Implementation to be used when consuming JMS messages. By default transport will use MessageListener and tweaking this property will make make use of MessageReceiver false BOOL Yes No subscription.durable Property to enable durable subscription. false BOOL Yes No connection.factory.nature Connection factory nature for the broker. default STRING Yes No Examples EXAMPLE 1 @source(type='jms', @map(type='json'), factory.initial='org.apache.activemq.jndi.ActiveMQInitialContextFactory', provider.url='tcp://localhost:61616',destination='DAS_JMS_TEST', connection.factory.type='topic',connection.factory.jndi.name='TopicConnectionFactory') define stream inputStream (name string, age int, country string); This example shows how to connect to an ActiveMQ topic and receive messages. EXAMPLE 2 @source(type='jms', @map(type='json'), factory.initial='org.apache.activemq.jndi.ActiveMQInitialContextFactory', provider.url='tcp://localhost:61616',destination='DAS_JMS_TEST' ) define stream inputStream (name string, age int, country string); This example shows how to connect to an ActiveMQ queue and receive messages. Note that we are not providing properties like connection factory type","title":"jms (Source)"},{"location":"docs/api/5.1.1/#kafka-source","text":"A Kafka source receives events to be processed by WSO2 SP from a topic with a partition for a Kafka cluster. The events received can be in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. Origin: siddhi-io-kafka:5.0.5 Syntax @source(type=\"kafka\", bootstrap.servers=\" STRING \", topic.list=\" STRING \", group.id=\" STRING \", threading.option=\" STRING \", partition.no.list=\" STRING \", seq.enabled=\" BOOL \", is.binary.message=\" BOOL \", topic.offsets.map=\" STRING \", enable.auto.commit=\" BOOL \", optional.configuration=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This specifies the list of Kafka servers to which the Kafka source must listen. This list can be provided as a set of comma-separated values. e.g., localhost:9092,localhost:9093 STRING No No topic.list This specifies the list of topics to which the source must listen. This list can be provided as a set of comma-separated values. e.g., topic_one,topic_two STRING No No group.id This is an ID to identify the Kafka source group. The group ID ensures that sources with the same topic and partition that are in the same group do not receive the same event. STRING No No threading.option This specifies whether the Kafka source is to be run on a single thread, or in multiple threads based on a condition. Possible values are as follows: single.thread : To run the Kafka source on a single thread. topic.wise : To use a separate thread per topic. partition.wise : To use a separate thread per partition. STRING No No partition.no.list The partition number list for the given topic. This is provided as a list of comma-separated values. e.g., 0,1,2, . 0 STRING Yes No seq.enabled If this parameter is set to true , the sequence of the events received via the source is taken into account. Therefore, each event should contain a sequence number as an attribute value to indicate the sequence. false BOOL Yes No is.binary.message In order to receive binary events via the Kafka source,it is required to setthis parameter to 'True'. false BOOL Yes No topic.offsets.map This parameter specifies reading offsets for each topic and partition. The value for this parameter is specified in the following format: topic = offset , topic = offset , When an offset is defined for a topic, the Kafka source skips reading the message with the number specified as the offset as well as all the messages sent previous to that message. If the offset is not defined for a specific topic it reads messages from the beginning. e.g., stocks=100,trades=50 reads from the 101th message of the stocks topic, and from the 51 st message of the trades topic. null STRING Yes No enable.auto.commit This parameter specifies whether to commit offsets automatically. By default, as the Siddhi Kafka source reads messages from Kafka, it will periodically(Default value is set to 1000ms. You can configure it with auto.commit.interval.ms property as an optional.configuration ) commit its current offset (defined as the offset of the next message to be read) for the partitions it is reading from back to Kafka. To guarantee at-least-once processing, we recommend you to enable Siddhi Periodic State Persistence when enable.auto.commit property is set to true . When you would like more control over exactly when offsets are committed, you can set enable.auto.commit to false and Siddhi will commit the offset once the records are successfully processed at the Source. When enable.auto.commit is set to false , manual committing would introduce a latency during consumption. true BOOL Yes No optional.configuration This parameter contains all the other possible configurations that the consumer is created with. e.g., ssl.keystore.type:JKS,batch.size:200 . null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic,kafka_topic2', group.id='test', threading.option='partition.wise', bootstrap.servers='localhost:9092', partition.no.list='0,1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This kafka source configuration listens to the kafka_topic and kafka_topic2 topics with 0 and 1 partitions. A thread is created for each topic and partition combination. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . EXAMPLE 2 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic', group.id='test', threading.option='single.thread', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka source configuration listens to the kafka_topic topic for the default partition because no partition.no.list is defined. Only one thread is created for the topic. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream .","title":"kafka (Source)"},{"location":"docs/api/5.1.1/#kafkamultidc-source","text":"The Kafka Multi-Datacenter(DC) source receives records from the same topic in brokers deployed in two different kafka clusters. It filters out all the duplicate messages and ensuresthat the events are received in the correct order using sequential numbering. It receives events in formats such as TEXT , XML JSON and Binary`.The Kafka Source creates the default partition '0' for a given topic, if the topic has not yet been created in the Kafka cluster. Origin: siddhi-io-kafka:5.0.5 Syntax @source(type=\"kafkaMultiDC\", bootstrap.servers=\" STRING \", topic=\" STRING \", partition.no=\" INT \", is.binary.message=\" BOOL \", optional.configuration=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This contains the kafka server list which the kafka source listens to. This is given using comma-separated values. eg: 'localhost:9092,localhost:9093' STRING No No topic This is the topic that the source listens to. eg: 'topic_one' STRING No No partition.no This is the partition number of the given topic. 0 INT Yes No is.binary.message In order to receive the binary events via the Kafka Multi-DC source, the value of this parameter needs to be set to 'True'. false BOOL Yes No optional.configuration This contains all the other possible configurations with which the consumer can be created.eg: producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source(type='kafkaMultiDC', topic='kafka_topic', bootstrap.servers='host1:9092,host1:9093', partition.no='1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; The following query listens to 'kafka_topic' topic, deployed in the broker host1:9092 and host1:9093, with partition 1. A thread is created for each broker. The receiving xml events are mapped to a siddhi event and sent to the FooStream.","title":"kafkaMultiDC (Source)"},{"location":"docs/api/5.1.1/#nats-source","text":"NATS Source allows users to subscribe to a NATS broker and receive messages. It has the ability to receive all the message types supported by NATS. Origin: siddhi-io-nats:2.0.8 Syntax @source(type=\"nats\", destination=\" STRING \", bootstrap.servers=\" STRING \", client.id=\" STRING \", cluster.id=\" STRING \", queue.group.name=\" STRING \", durable.name=\" STRING \", subscription.sequence=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic destination Subject name which NATS Source should subscribe to. STRING No No bootstrap.servers The NATS based url of the NATS server. nats://localhost:4222 STRING Yes No client.id The identifier of the client subscribing/connecting to the NATS broker. None STRING Yes No cluster.id The identifier of the NATS server/cluster. test-cluster STRING Yes No queue.group.name This can be used when there is a requirement to share the load of a NATS subject. Clients belongs to the same queue group share the subscription load. None STRING Yes No durable.name This can be used to subscribe to a subject from the last acknowledged message when a client or connection failure happens. The client can be uniquely identified using the tuple (client.id, durable.name). None STRING Yes No subscription.sequence This can be used to subscribe to a subject from a given number of message sequence. All the messages from the given point of sequence number will be passed to the client. If not provided then the either the persisted value or 0 will be used. None STRING Yes No Examples EXAMPLE 1 @source(type='nats', @map(type='text'), destination='SP_NATS_INPUT_TEST', bootstrap.servers='nats://localhost:4222',client.id='nats_client',server.id='test-cluster',queue.group.name = 'group_nats',durable.name = 'nats-durable',subscription.sequence = '100') define stream inputStream (name string, age int, country string); This example shows how to subscribe to a NATS subject with all supporting configurations.With the following configuration the source identified as 'nats-client' will subscribes to a subject named as 'SP_NATS_INPUT_TEST' which resides in a nats instance with a cluster id of 'test-cluster', running in localhost and listening to the port 4222 for client connection. This subscription will receive all the messages from 100 th in the subject. EXAMPLE 2 @source(type='nats', @map(type='text'), destination='SP_NATS_INPUT_TEST', ) define stream inputStream (name string, age int, country string); This example shows how to subscribe to a NATS subject with mandatory configurations.With the following configuration the source identified with an auto generated client id will subscribes to a subject named as 'SP_NATS_INTPUT_TEST' which resides in a nats instance with a cluster id of 'test-cluster', running in localhost and listening to the port 4222 for client connection. This will receive all available messages in the subject EXAMPLE 3 @source(type='nats', @map(type='json', @attributes(name='$.name', age='$.age', country='$.country', sequenceNum='trp:sequenceNumber')), destination='SIDDHI_NATS_SOURCE_TEST_DEST', client.id='nats_client', bootstrap.servers='nats://localhost:4222', cluster.id='test-cluster') define stream inputStream (name string, age int, country string, sequenceNum string); This example shows how to pass NATS Streaming sequence number to the event.","title":"nats (Source)"},{"location":"docs/api/5.1.1/#prometheus-source","text":"This source consumes Prometheus metrics that are exported from a specified URL as Siddhi events by sending HTTP requests to the URL. Based on the source configuration, it analyzes metrics from the text response and sends them as Siddhi events through key-value mapping.The user can retrieve metrics of the 'including', 'counter', 'gauge', 'histogram', and 'summary' types. The source retrieves the metrics from a text response of the target. Therefore, it is you need to use 'string' as the attribute type for the attributes that correspond with the Prometheus metric labels. Further, the Prometheus metric value is passed through the event as 'value'. This requires you to include an attribute named 'value' in the stream definition. The supported types for the 'value' attribute are 'INT', 'LONG', 'FLOAT', and 'DOUBLE'. Origin: siddhi-io-prometheus:2.1.0 Syntax @source(type=\"prometheus\", target.url=\" STRING \", scrape.interval=\" INT \", scrape.timeout=\" INT \", scheme=\" STRING \", metric.name=\" STRING \", metric.type=\" STRING \", username=\" STRING \", password=\" STRING \", client.truststore.file=\" STRING \", client.truststore.password=\" STRING \", headers=\" STRING \", job=\" STRING \", instance=\" STRING \", grouping.key=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic target.url This property specifies the target URL to which the Prometheus metrics are exported in the 'TEXT' format. STRING No No scrape.interval This property specifies the time interval in seconds within which the source should send an HTTP request to the specified target URL. 60 INT Yes No scrape.timeout This property is the time duration in seconds for a scrape request to get timed-out if the server at the URL does not respond. 10 INT Yes No scheme This property specifies the scheme of the target URL. The supported schemes are 'HTTP' and 'HTTPS'. HTTP STRING Yes No metric.name This property specifies the name of the metrics that are to be fetched. The metric name must match the regex format, i.e., '[a-zA-Z_:][a-zA-Z0-9_:]* '. Stream name STRING Yes No metric.type This property specifies the type of the Prometheus metric that is required to be fetched. The supported metric types are 'counter', 'gauge',\" 'histogram', and 'summary'. STRING No No username This property specifies the username that needs to be added in the authorization header of the HTTP request if basic authentication is enabled at the target. It is required to specify both the username and the password to enable basic authentication. If you do not provide a value for one or both of these parameters, an error is logged in the console. STRING Yes No password This property specifies the password that needs to be added in the authorization header of the HTTP request if basic authentication is enabled at the target. It is required to specify both the username and the password to enable basic authentication. If you do not provide a value for one or both of these parameters, an error is logged in the console. STRING Yes No client.truststore.file The file path to the location of the truststore to which the client needs to send HTTPS requests via the 'HTTPS' protocol. STRING Yes No client.truststore.password The password for the client-truststore. This is required to send HTTPS requests. A custom password can be specified if required. STRING Yes No headers Headers that need to be included as HTTP request headers in the request. The format of the supported input is as follows, \"'header1:value1','header2:value2'\" STRING Yes No job This property defines the job name of the exported Prometheus metrics that needs to be fetched. STRING Yes No instance This property defines the instance of the exported Prometheus metrics that needs to be fetched. STRING Yes No grouping.key This parameter specifies the grouping key of the required metrics in key-value pairs. The grouping key is used if the metrics are exported by Prometheus 'pushGateway' in order to distinguish those metrics from already existing metrics. The expected format of the grouping key is as follows: \"'key1:value1','key2:value2'\" STRING Yes No System Parameters Name Description Default Value Possible Parameters scrapeInterval The default time interval in seconds for the Prometheus source to send HTTP requests to the target URL. 60 Any integer value scrapeTimeout The default time duration (in seconds) for an HTTP request to time-out if the server at the URL does not respond. 10 Any integer value scheme The scheme of the target for the Prometheus source to send HTTP requests. The supported schemes are 'HTTP' and 'HTTPS'. HTTP HTTP or HTTPS username The username that needs to be added in the authorization header of the HTTP request if basic authentication is enabled at the target. It is required to specify both the username and password to enable basic authentication. If you do not specify a value for one or both of these parameters, an error is logged in the console. Any string password The password that needs to be added in the authorization header of the HTTP request if basic authentication is enabled at the target. It is required to specify both the username and password to enable basic authentication. If you do not specify a value for one or both of these parameters, an error is logged in the console. Any string trustStoreFile The default file path to the location of truststore that the client needs to access in order to send HTTPS requests through 'HTTPS' protocol. ${carbon.home}/resources/security/client-truststore.jks Any valid path for the truststore file trustStorePassword The default password for the client-truststore that the client needs to access in order to send HTTPS requests through 'HTTPS' protocol. wso2carbon Any string headers The headers that need to be included as HTTP request headers in the scrape request. The format of the supported input is as follows, \"'header1:value1','header2:value2'\" Any valid http headers job The default job name of the exported Prometheus metrics that needs to be fetched. Any valid job name instance The default instance of the exported Prometheus metrics that needs to be fetched. Any valid instance name groupingKey The default grouping key of the required Prometheus metrics in key-value pairs. The grouping key is used if the metrics are exported by the Prometheus pushGateway in order to distinguish these metrics from already existing metrics. The expected format of the grouping key is as follows: \"'key1:value1','key2:value2'\" Any valid grouping key pairs Examples EXAMPLE 1 @source(type= 'prometheus', target.url= 'http://localhost:9080/metrics', metric.type= 'counter', metric.name= 'sweet_production_counter', @map(type= 'keyvalue')) define stream FooStream1(metric_name string, metric_type string, help string, subtype string, name string, quantity string, value double); In this example, the Prometheus source sends an HTTP request to the 'target.url' and analyzes the response. From the analyzed response, the source retrieves the Prometheus counter metrics with the 'sweet_production_counter' nameand converts the filtered metrics into Siddhi events using the key-value mapper. The generated maps have keys and values as follows: metric_name - sweet_production_counter metric_type - counter help - help_string_of_metric subtype - null name - value_of_label_name quantity - value_of_label_quantity value - value_of_metric EXAMPLE 2 @source(type= 'prometheus', target.url= 'http://localhost:9080/metrics', metric.type= 'summary', metric.name= 'sweet_production_summary', @map(type= 'keyvalue')) define stream FooStream2(metric_name string, metric_type string, help string, subtype string, name string, quantity string, quantile string, value double); In this example, the Prometheus source sends an HTTP request to the 'target.url' and analyzes the response. From the analysed response, the source retrieves the Prometheus summary metrics with the 'sweet_production_summary' nameand converts the filtered metrics into Siddhi events using the key-value mapper. The generated maps have keys and values as follows: metric_name - sweet_production_summary metric_type - summary help - help_string_of_metric subtype - 'sum'/'count'/'null' name - value_of_label_name quantity - value_of_label_quantity quantile - value of the quantile value - value_of_metric EXAMPLE 3 @source(type= 'prometheus', target.url= 'http://localhost:9080/metrics', metric.type= 'histogram', metric.name= 'sweet_production_histogram', @map(type= 'keyvalue')) define stream FooStream3(metric_name string, metric_type string, help string, subtype string, name string, quantity string, le string, value double); In this example, the prometheus source sends an HTTP request to the 'target.url' and analyzes the response. From the analyzed response, the source retrieves the Prometheus histogram metrics with the 'sweet_production_histogram' name and converts the filtered metrics into Siddhi events using the key-value mapper. The generated maps have keys and values as follows, metric_name - sweet_production_histogram metric_type - histogram help - help_string_of_metric subtype - 'sum'/'count'/'bucket' name - value_of_label_name quantity - value_of_label_quantity le - value of the bucket value - value_of_metric","title":"prometheus (Source)"},{"location":"docs/api/5.1.1/#rabbitmq-source","text":"The rabbitmq source receives the events from the rabbitmq broker via the AMQP protocol. Origin: siddhi-io-rabbitmq:3.0.2 Syntax @source(type=\"rabbitmq\", uri=\" STRING \", heartbeat=\" INT \", exchange.name=\" STRING \", exchange.type=\" STRING \", exchange.durable.enabled=\" BOOL \", exchange.autodelete.enabled=\" BOOL \", routing.key=\" STRING \", headers=\" STRING \", queue.name=\" STRING \", queue.durable.enabled=\" BOOL \", queue.exclusive.enabled=\" BOOL \", queue.autodelete.enabled=\" BOOL \", tls.enabled=\" BOOL \", tls.truststore.path=\" STRING \", tls.truststore.password=\" STRING \", tls.truststore.type=\" STRING \", tls.version=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic uri The URI that is used to connect to an AMQP server. If no URI is specified,an error is logged in the CLI.e.g., amqp://guest:guest , amqp://guest:guest@localhost:5672 STRING No No heartbeat The period of time (in seconds) after which the peer TCP connection should be considered unreachable (down) by RabbitMQ and client libraries. 60 INT Yes No exchange.name The name of the exchange that decides what to do with a message it receives.If the exchange.name already exists in the RabbitMQ server, then the system uses that exchange.name instead of redeclaring. STRING No No exchange.type The type of the exchange name. The exchange types available are direct , fanout , topic and headers . For a detailed description of each type, see RabbitMQ - AMQP Concepts . direct STRING Yes No exchange.durable.enabled If this is set to true , the exchange remains declared even if the broker restarts. false BOOL Yes No exchange.autodelete.enabled If this is set to true , the exchange is automatically deleted when it is not used anymore. false BOOL Yes No routing.key The key based on which the exchange determines how to route the message to queues. The routing key is like an address for the message. The routing.key must be initialized when the value for the exchange.type parameter is direct or topic . empty STRING Yes No headers The headers of the message. The attributes used for routing are taken from the this paremeter. A message is considered matching if the value of the header equals the value specified upon binding. null STRING Yes No queue.name A queue is a buffer that stores messages. If the queue name already exists in the RabbitMQ server, then the system usees that queue name instead of redeclaring it. If no value is specified for this parameter, the system uses the unique queue name that is automatically generated by the RabbitMQ server. system generated queue name STRING Yes No queue.durable.enabled If this parameter is set to true , the queue remains declared even if the broker restarts false BOOL Yes No queue.exclusive.enabled If this parameter is set to true , the queue is exclusive for the current connection. If it is set to false , it is also consumable by other connections. false BOOL Yes No queue.autodelete.enabled If this parameter is set to true , the queue is automatically deleted when it is not used anymore. false BOOL Yes No tls.enabled This parameter specifies whether an encrypted communication channel should be established or not. When this parameter is set to true , the tls.truststore.path and tls.truststore.password parameters are initialized. false BOOL Yes No tls.truststore.path The file path to the location of the truststore of the client that receives the RabbitMQ events via the AMQP protocol. A custom client-truststore can be specified if required. If a custom truststore is not specified, then the system uses the default client-trustore in the {carbon.home}/resources/security /code directory. /p /td td style=\"vertical-align: top\" {carbon.home}/resources/security</code> directory.</p></td> <td style=\"vertical-align: top\"> /resources/security/client-truststore.jks STRING Yes No tls.truststore.password The password for the client-truststore. A custom password can be specified if required. If no custom password is specified, then the system uses wso2carbon as the default password. wso2carbon STRING Yes No tls.truststore.type The type of the truststore. JKS STRING Yes No tls.version The version of the tls/ssl. SSL STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @source(type ='rabbitmq', uri = 'amqp://guest:guest@localhost:5672', exchange.name = 'direct', routing.key= 'direct', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query receives events from the direct exchange with the direct exchange type, and the directTest routing key.","title":"rabbitmq (Source)"},{"location":"docs/api/5.1.1/#tcp-source","text":"A Siddhi application can be configured to receive events via the TCP transport by adding the @Source(type = 'tcp') annotation at the top of an event stream definition. When this is defined the associated stream will receive events from the TCP transport on the host and port defined in the system. Origin: siddhi-io-tcp:3.0.4 Syntax @source(type=\"tcp\", context=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic context The URL 'context' that should be used to receive the events. / STRING Yes No System Parameters Name Description Default Value Possible Parameters host Tcp server host. 0.0.0.0 Any valid host or IP port Tcp server port. 9892 Any integer representing valid port receiver.threads Number of threads to receive connections. 10 Any positive integer worker.threads Number of threads to serve events. 10 Any positive integer tcp.no.delay This is to specify whether to disable Nagle algorithm during message passing. If tcp.no.delay = 'true', the execution of Nagle algorithm will be disabled in the underlying TCP logic. Hence there will be no delay between two successive writes to the TCP connection. Else there can be a constant ack delay. true true false keep.alive This property defines whether the server should be kept alive when there are no connections available. true true false Examples EXAMPLE 1 @Source(type = 'tcp', context='abc', @map(type='binary')) define stream Foo (attribute1 string, attribute2 int ); Under this configuration, events are received via the TCP transport on default host,port, abc context, and they are passed to Foo stream for processing.","title":"tcp (Source)"},{"location":"docs/api/5.1.1/#sourcemapper","text":"","title":"Sourcemapper"},{"location":"docs/api/5.1.1/#avro-source-mapper","text":"This extension is an Avro to Event input mapper. Transports that accept Avro messages can utilize this extension to convert the incoming Avro messages to Siddhi events. The Avro schema to be used for creating Avro messages can be specified as a parameter in the stream definition. If no Avro schema is specified, a flat avro schema of the 'record' type is generated with the stream attributes as schema fields. The generated/specified Avro schema is used to convert Avro messages to Siddhi events. Origin: siddhi-map-avro:2.0.6 Syntax @source(..., @map(type=\"avro\", schema.def=\" STRING \", schema.registry=\" STRING \", schema.id=\" STRING \", fail.on.missing.attribute=\" BOOL \") QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic schema.def This specifies the schema of the Avro message. The full schema used to create the Avro message needs to be specified as a quoted JSON string. STRING No No schema.registry This specifies the URL of the schema registry. STRING No No schema.id This specifies the ID of the Avro schema. This ID is the global ID that is returned from the schema registry when posting the schema to the registry. The schema is retrieved from the schema registry via the specified ID. STRING No No fail.on.missing.attribute If this parameter is set to 'true', a JSON execution failing or returning a null value results in that message being dropped by the system. If this parameter is set to 'false', a JSON execution failing or returning a null value results in the system being prompted to send the event with a null value to Siddhi so that the user can handle it as required (i.e., by assigning a default value. true BOOL Yes No Examples EXAMPLE 1 @source(type='inMemory', topic='user', @map(type='avro', schema .def = \"\"\"{\"type\":\"record\",\"name\":\"userInfo\",\"namespace\":\"user.example\",\"fields\":[{\"name\":\"name\",\"type\":\"string\"}, {\"name\":\"age\",\"type\":\"int\"}]}\"\"\")) define stream UserStream (name string, age int ); The above Siddhi query performs a default Avro input mapping. The input Avro message that contains user information is converted to a Siddhi event. The expected input is a byte array or ByteBuffer. EXAMPLE 2 @source(type='inMemory', topic='user', @map(type='avro', schema .def = \"\"\"{\"type\":\"record\",\"name\":\"userInfo\",\"namespace\":\"avro.userInfo\",\"fields\":[{\"name\":\"username\",\"type\":\"string\"}, {\"name\":\"age\",\"type\":\"int\"}]}\"\"\",@attributes(name=\"username\",age=\"age\"))) define stream userStream (name string, age int ); The above Siddhi query performs a custom Avro input mapping. The input Avro message that contains user information is converted to a Siddhi event. The expected input is a byte array or ByteBuffer. EXAMPLE 3 @source(type='inMemory', topic='user', @map(type='avro',schema.registry='http://192.168.2.5:9090', schema.id='1',@attributes(name=\"username\",age=\"age\"))) define stream UserStream (name string, age int ); The above Siddhi query performs a custom Avro input mapping. The input Avro message that contains user information is converted to a Siddhi event via the schema retrieved from the given schema registry(localhost:8081). The expected input is a byte array or ByteBuffer.","title":"avro (Source Mapper)"},{"location":"docs/api/5.1.1/#binary-source-mapper","text":"This extension is a binary input mapper that converts events received in binary format to Siddhi events before they are processed. Origin: siddhi-map-binary:2.0.4 Syntax @source(..., @map(type=\"binary\") Examples EXAMPLE 1 @source(type='inMemory', topic='WSO2', @map(type='binary'))define stream FooStream (symbol string, price float, volume long); This query performs a mapping to convert an event of the binary format to a Siddhi event.","title":"binary (Source Mapper)"},{"location":"docs/api/5.1.1/#csv-source-mapper","text":"This extension is used to convert CSV message to Siddhi event input mapper. You can either receive pre-defined CSV message where event conversion takes place without extra configurations,or receive custom CSV message where a custom place order to map from custom CSV message. Origin: siddhi-map-csv:2.0.3 Syntax @source(..., @map(type=\"csv\", delimiter=\" STRING \", header.present=\" BOOL \", fail.on.unknown.attribute=\" BOOL \", event.grouping.enabled=\" BOOL \") QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic delimiter When converting a CSV format message to Siddhi event, this parameter indicatesinput CSV message's data should be split by this parameter , STRING Yes No header.present When converting a CSV format message to Siddhi event, this parameter indicates whether CSV message has header or not. This can either have value true or false.If it's set to false then it indicates that CSV message has't header. false BOOL Yes No fail.on.unknown.attribute This parameter specifies how unknown attributes should be handled. If it's set to true and one or more attributes don't havevalues, then SP will drop that message. If this parameter is set to false , the Stream Processor adds the required attribute's values to such events with a null value and the event is converted to a Siddhi event. true BOOL Yes No event.grouping.enabled This parameter specifies whether event grouping is enabled or not. To receive a group of events together and generate multiple events, this parameter must be set to true . false BOOL Yes No Examples EXAMPLE 1 @source(type='inMemory', topic='stock', @map(type='csv')) define stream FooStream (symbol string, price float, volume int); Above configuration will do a default CSV input mapping. Expected input will look like below: WSO2 ,55.6 , 100OR \"WSO2,No10,Palam Groove Rd,Col-03\" ,55.6 , 100If header.present is true and delimiter is \"-\", then the input is as follows: symbol-price-volumeWSO2-55.6-100 EXAMPLE 2 @source(type='inMemory', topic='stock', @map(type='csv',header='true', @attributes(symbol = \"2\", price = \"0\", volume = \"1\"))) define stream FooStream (symbol string, price float, volume long); Above configuration will perform a custom CSV mapping. Here, user can add place order of each attribute in the @attribute. The place order indicates where the attribute name's value has appeared in the input.Expected input will look like below: 55.6,100,WSO2 OR55.6,100,\"WSO2,No10,Palm Groove Rd,Col-03\" If header is true and delimiter is \"-\", then the output is as follows: price-volume-symbol 55.6-100-WSO2 If group events is enabled then input should be as follows: price-volume-symbol 55.6-100-WSO2System.lineSeparator() 55.6-100-IBMSystem.lineSeparator() 55.6-100-IFSSystem.lineSeparator()","title":"csv (Source Mapper)"},{"location":"docs/api/5.1.1/#json-source-mapper","text":"This extension is a JSON-to-Event input mapper. Transports that accept JSON messages can utilize this extension to convert an incoming JSON message into a Siddhi event. Users can either send a pre-defined JSON format, where event conversion happens without any configurations, or use the JSON path to map from a custom JSON message. In default mapping, the JSON string of the event can be enclosed by the element \"event\", though optional. Origin: siddhi-map-json:5.0.5 Syntax @source(..., @map(type=\"json\", enclosing.element=\" STRING \", fail.on.missing.attribute=\" BOOL \") QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic enclosing.element This is used to specify the enclosing element when sending multiple events in the same JSON message. Mapper treats the child elements of a given enclosing element as events and executes the JSON path expressions on these child elements. If the enclosing.element is not provided then the multiple-event scenario is disregarded and the JSON path is evaluated based on the root element. $ STRING Yes No fail.on.missing.attribute This parameter allows users to handle unknown attributes.The value of this can either be true or false. By default it is true. If a JSON execution fails or returns null, mapper drops that message. However, setting this property to false prompts mapper to send an event with a null value to Siddhi, where users can handle it as required, ie., assign a default value.) true BOOL Yes No Examples EXAMPLE 1 @source(type='inMemory', topic='stock', @map(type='json')) define stream FooStream (symbol string, price float, volume long); This configuration performs a default JSON input mapping. For a single event, the input is required to be in one of the following formats: { \"event\":{ \"symbol\":\"WSO2\", \"price\":55.6, \"volume\":100 } } or { \"symbol\":\"WSO2\", \"price\":55.6, \"volume\":100 } EXAMPLE 2 @source(type='inMemory', topic='stock', @map(type='json')) define stream FooStream (symbol string, price float, volume long); This configuration performs a default JSON input mapping. For multiple events, the input is required to be in one of the following formats: [ {\"event\":{\"symbol\":\"WSO2\",\"price\":55.6,\"volume\":100}}, {\"event\":{\"symbol\":\"WSO2\",\"price\":56.6,\"volume\":99}}, {\"event\":{\"symbol\":\"WSO2\",\"price\":57.6,\"volume\":80}} ] or [ {\"symbol\":\"WSO2\",\"price\":55.6,\"volume\":100}, {\"symbol\":\"WSO2\",\"price\":56.6,\"volume\":99}, {\"symbol\":\"WSO2\",\"price\":57.6,\"volume\":80} ] EXAMPLE 3 @source(type='inMemory', topic='stock', @map(type='json', enclosing.element=\"$.portfolio\", @attributes(symbol = \"company.symbol\", price = \"price\", volume = \"volume\"))) This configuration performs a custom JSON mapping. For a single event, the expected input is similar to the one shown below: { \"portfolio\":{ \"stock\":{ \"volume\":100, \"company\":{ \"symbol\":\"WSO2\" }, \"price\":55.6 } } } EXAMPLE 4 @source(type='inMemory', topic='stock', @map(type='json', enclosing.element=\"$.portfolio\", @attributes(symbol = \"stock.company.symbol\", price = \"stock.price\", volume = \"stock.volume\"))) define stream FooStream (symbol string, price float, volume long); The configuration performs a custom JSON mapping. For multiple events, expected input looks as follows. .{\"portfolio\": [ {\"stock\":{\"volume\":100,\"company\":{\"symbol\":\"wso2\"},\"price\":56.6}}, {\"stock\":{\"volume\":200,\"company\":{\"symbol\":\"wso2\"},\"price\":57.6}} ] }","title":"json (Source Mapper)"},{"location":"docs/api/5.1.1/#keyvalue-source-mapper","text":"Key-Value Map to Event input mapper extension allows transports that accept events as key value maps to convert those events to Siddhi events. You can either receive pre-defined keys where conversion takes place without extra configurations, or use custom keys to map from the message. Origin: siddhi-map-keyvalue:2.0.5 Syntax @source(..., @map(type=\"keyvalue\", fail.on.missing.attribute=\" BOOL \") QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic fail.on.missing.attribute If this parameter is set to true , if an event arrives without a matching key for a specific attribute in the connected stream, it is dropped and not processed by the Stream Processor. If this parameter is set to false the Stream Processor adds the required key to such events with a null value, and the event is converted to a Siddhi event so that you could handle them as required before they are further processed. true BOOL Yes No Examples EXAMPLE 1 @source(type='inMemory', topic='stock', @map(type='keyvalue')) define stream FooStream (symbol string, price float, volume long); This query performs a default key value input mapping. The expected input is a map similar to the following: symbol: 'WSO2' price: 55.6f volume: 100 EXAMPLE 2 @source(type='inMemory', topic='stock', @map(type='keyvalue', fail.on.missing.attribute='true', @attributes(symbol = 's', price = 'p', volume = 'v')))define stream FooStream (symbol string, price float, volume long); This query performs a custom key value input mapping. The matching keys for the symbol , price and volume attributes are be s , p , and v respectively. The expected input is a map similar to the following: s: 'WSO2' p: 55.6 v: 100","title":"keyvalue (Source Mapper)"},{"location":"docs/api/5.1.1/#passthrough-source-mapper","text":"Pass-through mapper passed events (Event[]) through without any mapping or modifications. Origin: siddhi-core:5.1.8 Syntax @source(..., @map(type=\"passThrough\") Examples EXAMPLE 1 @source(type='tcp', @map(type='passThrough')) define stream BarStream (symbol string, price float, volume long); In this example BarStream uses passThrough inputmapper which passes the received Siddhi event directly without any transformation into source.","title":"passThrough (Source Mapper)"},{"location":"docs/api/5.1.1/#protobuf-source-mapper","text":"This input mapper allows you to convert protobuf messages into Events. To work with this input mapper you have to add auto-generated protobuf classes to the project classpath. When you use this input mapper, you can either define stream attributes as the same names as the protobuf message attributes or you can use custom mapping to map stream definition attributes with the protobuf attributes..Please find the sample proto definition here Origin: siddhi-map-protobuf:1.0.2 Syntax @source(..., @map(type=\"protobuf\", class=\" STRING \") QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic class This specifies the class name of the protobuf message class, If sink type is grpc then it's not necessary to provide this field. - STRING Yes No Examples EXAMPLE 1 source(type='grpc', receiver.url = 'grpc://localhost: 2000/org.wso2.grpc.test.MyService/process', @map(type='protobuf')) define stream FooStream (stringValue string, intValue int,longValue long,booleanValue bool,floatValue float,doubleValue double); Above definition will convert the protobuf messages that are received to this source into siddhi events. EXAMPLE 2 source(type='grpc', receiver.url = 'grpc://localhost: 2000/org.wso2.grpc.test.MyService/process', @map(type='protobuf', @attributes(a = 'stringValue', b = 'intValue', c = 'longValue',d = 'booleanValue',' e = floatValue', f ='doubleValue'))) define stream FooStream (a string ,c long,b int, d bool,e float,f double); Above definition will convert the protobuf messages that are received to this source into siddhi events. since there's a mapping available for the stream, protobuf message object will be map like this, -'stringValue' of the protobuf message will be assign to the 'a' attribute of the stream - 'intValue' of the protobuf message will be assign to the 'b' attribute of the stream - 'longValue' of the protobuf message will be assign to the 'c' attribute of the stream - 'booleanValue' of the protobuf message will be assign to the 'd' attribute of the stream - 'floatValue' of the protobuf message will be assign to the 'e' attribute of the stream - 'doubleValue' of the protobuf message will be assign to the 'f' attribute of the stream EXAMPLE 3 source(type='grpc', receiver.url = 'grpc://localhost: 2000/org.wso2.grpc.test.MyService/testMap', @map(type='protobuf')) define stream FooStream (stringValue string ,intValue int,map object); Above definition will convert the protobuf messages that are received to this source into siddhi events. since there's an object type attribute available in the stream (map object), mapper will assume that object is an instance of 'java.util.Map' class. otherwise mapper will throws an exception EXAMPLE 4 @source(type='inMemory', topic='test01', @map(type='protobuf', class='org.wso2.grpc.test.Request')) define stream FooStream (stringValue string, intValue int,longValue long,booleanValue bool,floatValue float,doubleValue double); The above definition will convert the 'org.wso2.grpc.test.Request' type protobuf messages into siddhi events. If we did not provide the 'receiver.url' in the stream definition we have to provide the protobuf class name in the 'class' parameter inside @map.","title":"protobuf (Source Mapper)"},{"location":"docs/api/5.1.1/#text-source-mapper","text":"This extension is a text to Siddhi event input mapper. Transports that accept text messages can utilize this extension to convert the incoming text message to Siddhi event. Users can either use a pre-defined text format where event conversion happens without any additional configurations, or specify a regex to map a text message using custom configurations. Origin: siddhi-map-text:2.0.4 Syntax @source(..., @map(type=\"text\", regex.groupid=\" STRING \", fail.on.missing.attribute=\" BOOL \", event.grouping.enabled=\" BOOL \", delimiter=\" STRING \", new.line.character=\" STRING \") QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic regex.groupid This parameter specifies a regular expression group. The groupid can be any capital letter (e.g., regex.A,regex.B .. etc). You can specify any number of regular expression groups. In the attribute annotation, you need to map all attributes to the regular expression group with the matching group index. If you need to to enable custom mapping, it is required to specifythe matching group for each and every attribute. STRING No No fail.on.missing.attribute This parameter specifies how unknown attributes should be handled. If it is set to true a message is dropped if its execution fails, or if one or more attributes do not have values. If this parameter is set to false , null values are assigned to attributes with missing values, and messages with such attributes are not dropped. true BOOL Yes No event.grouping.enabled This parameter specifies whether event grouping is enabled or not. To receive a group of events together and generate multiple events, this parameter must be set to true . false BOOL Yes No delimiter This parameter specifies how events must be separated when multiple events are received. This must be whole line and not a single character. ~ ~ ~ ~ STRING Yes No new.line.character This attribute indicates the new line character of the event that is expected to be received. This is used mostly when communication between 2 types of operating systems is expected. For example, Linux uses \\n as the end of line character whereas windows uses \\r\\n . \\n STRING Yes No Examples EXAMPLE 1 @source(type='inMemory', topic='stock', @map(type='text')) define stream FooStream (symbol string, price float, volume long); This query performs a default text input mapping. The expected input is as follows: symbol:\"WSO2\", price:55.6, volume:100 OR symbol:'WSO2', price:55.6, volume:100 If group events is enabled then input should be as follows: symbol:\"WSO2\", price:55.6, volume:100 ~ ~ ~ ~ symbol:\"WSO2\", price:55.6, volume:100 EXAMPLE 2 @source(type='inMemory', topic='stock', @map(type='text', fail.on.missing.attribute = 'true', regex.A='(\\w+)\\s([-0-9]+)',regex.B='volume\\s([-0-9]+)', @attributes(symbol = 'A[1]',price = 'A[2]',volume = 'B'))) define stream FooStream (symbol string, price float, volume long); This query performs a custom text mapping. The expected input is as follows: wos2 550 volume 100 If group events is enabled then input should be as follows: wos2 550 volume 100 ~ ~ ~ ~ wos2 550 volume 100 ~ ~ ~ ~ wos2 550 volume 100","title":"text (Source Mapper)"},{"location":"docs/api/5.1.1/#xml-source-mapper","text":"This mapper converts XML input to Siddhi event. Transports which accepts XML messages can utilize this extension to convert the incoming XML message to Siddhi event. Users can either send a pre-defined XML format where event conversion will happen without any configs or can use xpath to map from a custom XML message. Origin: siddhi-map-xml:5.0.3 Syntax @source(..., @map(type=\"xml\", namespaces=\" STRING \", enclosing.element=\" STRING \", fail.on.missing.attribute=\" BOOL \") QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic namespaces Used to provide namespaces used in the incoming XML message beforehand to configure xpath expressions. User can provide a comma separated list. If these are not provided xpath evaluations will fail None STRING Yes No enclosing.element Used to specify the enclosing element in case of sending multiple events in same XML message. WSO2 DAS will treat the child element of given enclosing element as events and execute xpath expressions on child elements. If enclosing.element is not provided multiple event scenario is disregarded and xpaths will be evaluated with respect to root element. Root element STRING Yes No fail.on.missing.attribute This can either have value true or false. By default it will be true. This attribute allows user to handle unknown attributes. By default if an xpath execution fails or returns null DAS will drop that message. However setting this property to false will prompt DAS to send and event with null value to Siddhi where user can handle it accordingly(ie. Assign a default value) True BOOL Yes No Examples EXAMPLE 1 @source(type='inMemory', topic='stock', @map(type='xml')) define stream FooStream (symbol string, price float, volume long); Above configuration will do a default XML input mapping. Expected input will look like below. events event symbol WSO2 /symbol price 55.6 /price volume 100 /volume /event /events EXAMPLE 2 @source(type='inMemory', topic='stock', @map(type='xml', namespaces = \"dt=urn:schemas-microsoft-com:datatypes\", enclosing.element=\"//portfolio\", @attributes(symbol = \"company/symbol\", price = \"price\", volume = \"volume\"))) define stream FooStream (symbol string, price float, volume long); Above configuration will perform a custom XML mapping. In the custom mapping user can add xpath expressions representing each event attribute using @attribute annotation. Expected input will look like below. portfolio xmlns:dt=\"urn:schemas-microsoft-com:datatypes\" stock exchange=\"nasdaq\" volume 100 /volume company symbol WSO2 /symbol /company price dt:type=\"number\" 55.6 /price /stock /portfolio","title":"xml (Source Mapper)"},{"location":"docs/api/5.1.1/#store","text":"","title":"Store"},{"location":"docs/api/5.1.1/#mongodb-store","text":"Using this extension a MongoDB Event Table can be configured to persist events in a MongoDB of user's choice. Origin: siddhi-store-mongodb:2.0.3 Syntax @Store(type=\"mongodb\", mongodb.uri=\" STRING \", collection.name=\" STRING \", secure.connection=\" STRING \", trust.store=\" STRING \", trust.store.password=\" STRING \", key.store=\" STRING \", key.store.password=\" STRING \") @PrimaryKey(\"PRIMARY_KEY\") @Index(\"INDEX\") QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic mongodb.uri The MongoDB URI for the MongoDB data store. The uri must be of the format mongodb://[username:password@]host1[:port1][,hostN[:portN]][/[database][?options]] The options specified in the uri will override any connection options specified in the deployment yaml file. Note: The user should have read permissions to the admindb as well as read/write permissions to the database accessed. STRING No No collection.name The name of the collection in the store this Event Table should be persisted as. Name of the siddhi event table. STRING Yes No secure.connection Describes enabling the SSL for the mongodb connection false STRING Yes No trust.store File path to the trust store. {carbon.home}/resources/security/client-truststore.jks /td td style=\"vertical-align: top\" STRING /td td style=\"vertical-align: top\" Yes /td td style=\"vertical-align: top\" No /td /tr tr td style=\"vertical-align: top\" trust.store.password /td td style=\"vertical-align: top; word-wrap: break-word\" p style=\"word-wrap: break-word;margin: 0;\" Password to access the trust store /p /td td style=\"vertical-align: top\" wso2carbon /td td style=\"vertical-align: top\" STRING /td td style=\"vertical-align: top\" Yes /td td style=\"vertical-align: top\" No /td /tr tr td style=\"vertical-align: top\" key.store /td td style=\"vertical-align: top; word-wrap: break-word\" p style=\"word-wrap: break-word;margin: 0;\" File path to the keystore. /p /td td style=\"vertical-align: top\" {carbon.home}/resources/security/client-truststore.jks</td> <td style=\"vertical-align: top\">STRING</td> <td style=\"vertical-align: top\">Yes</td> <td style=\"vertical-align: top\">No</td> </tr> <tr> <td style=\"vertical-align: top\">trust.store.password</td> <td style=\"vertical-align: top; word-wrap: break-word\"><p style=\"word-wrap: break-word;margin: 0;\">Password to access the trust store</p></td> <td style=\"vertical-align: top\">wso2carbon</td> <td style=\"vertical-align: top\">STRING</td> <td style=\"vertical-align: top\">Yes</td> <td style=\"vertical-align: top\">No</td> </tr> <tr> <td style=\"vertical-align: top\">key.store</td> <td style=\"vertical-align: top; word-wrap: break-word\"><p style=\"word-wrap: break-word;margin: 0;\">File path to the keystore.</p></td> <td style=\"vertical-align: top\"> /resources/security/client-truststore.jks STRING Yes No key.store.password Password to access the keystore wso2carbon STRING Yes No System Parameters Name Description Default Value Possible Parameters applicationName Sets the logical name of the application using this MongoClient. The application name may be used by the client to identify the application to the server, for use in server logs, slow query logs, and profile collection. null the logical name of the application using this MongoClient. The UTF-8 encoding may not exceed 128 bytes. cursorFinalizerEnabled Sets whether cursor finalizers are enabled. true true false requiredReplicaSetName The name of the replica set null the logical name of the replica set sslEnabled Sets whether to initiate connection with TSL/SSL enabled. true: Initiate the connection with TLS/SSL. false: Initiate the connection without TLS/SSL. false true false trustStore File path to the trust store. {carbon.home}/resources/security/client-truststore.jks /td td style=\"vertical-align: top\" Any valid file path. /td /tr tr td style=\"vertical-align: top\" trustStorePassword /td td style=\"vertical-align: top;\" p style=\"word-wrap: break-word;margin: 0;\" Password to access the trust store /p /td td style=\"vertical-align: top\" wso2carbon /td td style=\"vertical-align: top\" Any valid password. /td /tr tr td style=\"vertical-align: top\" keyStore /td td style=\"vertical-align: top;\" p style=\"word-wrap: break-word;margin: 0;\" File path to the keystore. /p /td td style=\"vertical-align: top\" {carbon.home}/resources/security/client-truststore.jks</td> <td style=\"vertical-align: top\">Any valid file path.</td> </tr> <tr> <td style=\"vertical-align: top\">trustStorePassword</td> <td style=\"vertical-align: top;\"><p style=\"word-wrap: break-word;margin: 0;\">Password to access the trust store</p></td> <td style=\"vertical-align: top\">wso2carbon</td> <td style=\"vertical-align: top\">Any valid password.</td> </tr> <tr> <td style=\"vertical-align: top\">keyStore</td> <td style=\"vertical-align: top;\"><p style=\"word-wrap: break-word;margin: 0;\">File path to the keystore.</p></td> <td style=\"vertical-align: top\"> /resources/security/client-truststore.jks Any valid file path. keyStorePassword Password to access the keystore wso2carbon Any valid password. connectTimeout The time in milliseconds to attempt a connection before timing out. 10000 Any positive integer connectionsPerHost The maximum number of connections in the connection pool. 100 Any positive integer minConnectionsPerHost The minimum number of connections in the connection pool. 0 Any natural number maxConnectionIdleTime The maximum number of milliseconds that a connection can remain idle in the pool before being removed and closed. A zero value indicates no limit to the idle time. A pooled connection that has exceeded its idle time will be closed and replaced when necessary by a new connection. 0 Any positive integer maxWaitTime The maximum wait time in milliseconds that a thread may wait for a connection to become available. A value of 0 means that it will not wait. A negative value means to wait indefinitely 120000 Any integer threadsAllowedToBlockForConnectionMultiplier The maximum number of connections allowed per host for this MongoClient instance. Those connections will be kept in a pool when idle. Once the pool is exhausted, any operation requiring a connection will block waiting for an available connection. 100 Any natural number maxConnectionLifeTime The maximum life time of a pooled connection. A zero value indicates no limit to the life time. A pooled connection that has exceeded its life time will be closed and replaced when necessary by a new connection. 0 Any positive integer socketKeepAlive Sets whether to keep a connection alive through firewalls false true false socketTimeout The time in milliseconds to attempt a send or receive on a socket before the attempt times out. Default 0 means never to timeout. 0 Any natural integer writeConcern The write concern to use. acknowledged acknowledged w1 w2 w3 unacknowledged fsynced journaled replica_acknowledged normal safe majority fsync_safe journal_safe replicas_safe readConcern The level of isolation for the reads from replica sets. default local majority linearizable readPreference Specifies the replica set read preference for the connection. primary primary secondary secondarypreferred primarypreferred nearest localThreshold The size (in milliseconds) of the latency window for selecting among multiple suitable MongoDB instances. 15 Any natural number serverSelectionTimeout Specifies how long (in milliseconds) to block for server selection before throwing an exception. A value of 0 means that it will timeout immediately if no server is available. A negative value means to wait indefinitely. 30000 Any integer heartbeatSocketTimeout The socket timeout for connections used for the cluster heartbeat. A value of 0 means that it will timeout immediately if no cluster member is available. A negative value means to wait indefinitely. 20000 Any integer heartbeatConnectTimeout The connect timeout for connections used for the cluster heartbeat. A value of 0 means that it will timeout immediately if no cluster member is available. A negative value means to wait indefinitely. 20000 Any integer heartbeatFrequency Specify the interval (in milliseconds) between checks, counted from the end of the previous check until the beginning of the next one. 10000 Any positive integer minHeartbeatFrequency Sets the minimum heartbeat frequency. In the event that the driver has to frequently re-check a server's availability, it will wait at least this long since the previous check to avoid wasted effort. 500 Any positive integer Examples EXAMPLE 1 @Store(type=\"mongodb\",mongodb.uri=\"mongodb://admin:admin@localhost/Foo\") @PrimaryKey(\"symbol\") @Index(\"volume:1\", {background:true,unique:true}\") define table FooTable (symbol string, price float, volume long); This will create a collection called FooTable for the events to be saved with symbol as Primary Key(unique index at mongoDB level) and index for the field volume will be created in ascending order with the index option to create the index in the background. Note: @PrimaryKey: This specifies a list of comma-separated values to be treated as unique fields in the table. Each record in the table must have a unique combination of values for the fields specified here. @Index: This specifies the fields that must be indexed at the database level. You can specify multiple values as a come-separated list. A single value to be in the format, FieldName : SortOrder . The last element is optional through which a valid index options can be passed. SortOrder : 1 for Ascending -1 for Descending. Optional, with default value as 1. IndexOptions : Index Options must be defined inside curly brackets. Options must follow the standard mongodb index options format. https://docs.mongodb.com/manual/reference/method/db.collection.createIndex/ Example 1: @Index( 'symbol:1' , '{\"unique\":true}' ) Example 2: @Index( 'symbol' , '{\"unique\":true}' ) Example 3: @Index( 'symbol:1' , 'volume:-1' , '{\"unique\":true}' )","title":"mongodb (Store)"},{"location":"docs/api/5.1.1/#rdbms-store","text":"This extension assigns data sources and connection instructions to event tables. It also implements read-write operations on connected data sources. Origin: siddhi-store-rdbms:7.0.2 Syntax @Store(type=\"rdbms\", jdbc.url=\" STRING \", username=\" STRING \", password=\" STRING \", jdbc.driver.name=\" STRING \", pool.properties=\" STRING \", jndi.resource=\" STRING \", datasource=\" STRING \", table.name=\" STRING \", field.length=\" STRING \", table.check.query=\" STRING \", use.collation=\" BOOL \") @PrimaryKey(\"PRIMARY_KEY\") @Index(\"INDEX\") QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic jdbc.url The JDBC URL via which the RDBMS data store is accessed. STRING No No username The username to be used to access the RDBMS data store. STRING No No password The password to be used to access the RDBMS data store. STRING No No jdbc.driver.name The driver class name for connecting the RDBMS data store. STRING No No pool.properties Any pool parameters for the database connection must be specified as key-value pairs. null STRING Yes No jndi.resource The name of the JNDI resource through which the connection is attempted. If this is found, the pool properties described above are not taken into account and the connection is attempted via JNDI lookup instead. null STRING Yes No datasource The name of the Carbon datasource that should be used for creating the connection with the database. If this is found, neither the pool properties nor the JNDI resource name described above are taken into account and the connection is attempted via Carbon datasources instead. Only works in Siddhi Distribution null STRING Yes No table.name The name with which the event table should be persisted in the store. If no name is specified via this parameter, the event table is persisted with the same name as the Siddhi table. The table name defined in the Siddhi App query. STRING Yes No field.length The number of characters that the values for fields of the 'STRING' type in the table definition must contain. Each required field must be provided as a comma-separated list of key-value pairs in the ' field.name : length ' format. If this is not specified, the default number of characters specific to the database type is considered. null STRING Yes No table.check.query This query will be used to check whether the table is exist in the given database. But the provided query should return an SQLException if the table does not exist in the database. Furthermore if the provided table is a database view, and it is not exists in the database a table from given name will be created in the database The tableCheckQuery which define in store rdbms configs STRING Yes No use.collation This property allows users to use collation for string attirbutes. By default it's false and binary collation is not used. Currently 'latin1_bin' and 'SQL_Latin1_General_CP1_CS_AS' are used as collations for MySQL and Microsoft SQL database types respectively. false BOOL Yes No System Parameters Name Description Default Value Possible Parameters {{RDBMS-Name}}.maxVersion The latest version supported for {{RDBMS-Name}}. 0 N/A {{RDBMS-Name}}.minVersion The earliest version supported for {{RDBMS-Name}}. 0 N/A {{RDBMS-Name}}.tableCheckQuery The template query for the 'check table' operation in {{RDBMS-Name}}. H2 : CREATE TABLE {{TABLE_NAME}} ({{COLUMNS, PRIMARY_KEYS}}) MySQL : CREATE TABLE {{TABLE_NAME}} ({{COLUMNS, PRIMARY_KEYS}}) Oracle : CREATE TABLE {{TABLE_NAME}} ({{COLUMNS, PRIMARY_KEYS}}) Microsoft SQL Server : CREATE TABLE {{TABLE_NAME}} ({{COLUMNS, PRIMARY_KEYS}}) PostgreSQL : CREATE TABLE {{TABLE_NAME}} ({{COLUMNS, PRIMARY_KEYS}}) DB2. : CREATE TABLE {{TABLE_NAME}} ({{COLUMNS, PRIMARY_KEYS}}) N/A {{RDBMS-Name}}.tableCreateQuery The template query for the 'create table' operation in {{RDBMS-Name}}. H2 : SELECT 1 FROM {{TABLE_NAME}} LIMIT 1 MySQL : SELECT 1 FROM {{TABLE_NAME}} LIMIT 1 Oracle : SELECT 1 FROM {{TABLE_NAME}} WHERE rownum=1 Microsoft SQL Server : SELECT TOP 1 1 from {{TABLE_NAME}} PostgreSQL : SELECT 1 FROM {{TABLE_NAME}} LIMIT 1 DB2. : SELECT 1 FROM {{TABLE_NAME}} FETCH FIRST 1 ROWS ONLY N/A {{RDBMS-Name}}.indexCreateQuery The template query for the 'create index' operation in {{RDBMS-Name}}. H2 : CREATE INDEX {{TABLE_NAME}}_INDEX ON {{TABLE_NAME}} ({{INDEX_COLUMNS}}) MySQL : CREATE INDEX {{TABLE_NAME}}_INDEX ON {{TABLE_NAME}} ({{INDEX_COLUMNS}}) Oracle : CREATE INDEX {{TABLE_NAME}}_INDEX ON {{TABLE_NAME}} ({{INDEX_COLUMNS}}) Microsoft SQL Server : CREATE INDEX {{TABLE_NAME}}_INDEX ON {{TABLE_NAME}} ({{INDEX_COLUMNS}}) {{TABLE_NAME}} ({{INDEX_COLUMNS}}) PostgreSQL : CREATE INDEX {{TABLE_NAME}}_INDEX ON {{TABLE_NAME}} ({{INDEX_COLUMNS}}) DB2. : CREATE INDEX {{TABLE_NAME}}_INDEX ON {{TABLE_NAME}} ({{INDEX_COLUMNS}}) N/A {{RDBMS-Name}}.recordInsertQuery The template query for the 'insert record' operation in {{RDBMS-Name}}. H2 : INSERT INTO {{TABLE_NAME}} ({{COLUMNS}}) VALUES ({{Q}}) MySQL : INSERT INTO {{TABLE_NAME}} ({{COLUMNS}}) VALUES ({{Q}}) Oracle : INSERT INTO {{TABLE_NAME}} ({{COLUMNS}}) VALUES ({{Q}}) Microsoft SQL Server : INSERT INTO {{TABLE_NAME}} ({{COLUMNS}}) VALUES ({{Q}}) PostgreSQL : INSERT INTO {{TABLE_NAME}} ({{COLUMNS}}) VALUES ({{Q}}) DB2. : INSERT INTO {{TABLE_NAME}} ({{COLUMNS}}) VALUES ({{Q}}) N/A {{RDBMS-Name}}.recordUpdateQuery The template query for the 'update record' operation in {{RDBMS-Name}}. H2 : UPDATE {{TABLE_NAME}} SET {{COLUMNS_AND_VALUES}} {{CONDITION}} MySQL : UPDATE {{TABLE_NAME}} SET {{COLUMNS_AND_VALUES}} {{CONDITION}} Oracle : UPDATE {{TABLE_NAME}} SET {{COLUMNS_AND_VALUES}} {{CONDITION}} Microsoft SQL Server : UPDATE {{TABLE_NAME}} SET {{COLUMNS_AND_VALUES}} {{CONDITION}} PostgreSQL : UPDATE {{TABLE_NAME}} SET {{COLUMNS_AND_VALUES}} {{CONDITION}} DB2. : UPDATE {{TABLE_NAME}} SET {{COLUMNS_AND_VALUES}} {{CONDITION}} N/A {{RDBMS-Name}}.recordSelectQuery The template query for the 'select record' operation in {{RDBMS-Name}}. H2 : SELECT * FROM {{TABLE_NAME}} {{CONDITION}} MySQL : SELECT * FROM {{TABLE_NAME}} {{CONDITION}} Oracle : SELECT * FROM {{TABLE_NAME}} {{CONDITION}} Microsoft SQL Server : SELECT * FROM {{TABLE_NAME}} {{CONDITION}} PostgreSQL : SELECT * FROM {{TABLE_NAME}} {{CONDITION}} DB2. : SELECT * FROM {{TABLE_NAME}} {{CONDITION}} N/A {{RDBMS-Name}}.recordExistsQuery The template query for the 'check record existence' operation in {{RDBMS-Name}}. H2 : SELECT TOP 1 1 FROM {{TABLE_NAME}} {{CONDITION}} MySQL : SELECT 1 FROM {{TABLE_NAME}} {{CONDITION}} Oracle : SELECT COUNT(1) INTO existence FROM {{TABLE_NAME}} {{CONDITION}} Microsoft SQL Server : SELECT TOP 1 FROM {{TABLE_NAME}} {{CONDITION}} PostgreSQL : SELECT 1 FROM {{TABLE_NAME}} {{CONDITION}} LIMIT 1 DB2. : SELECT 1 FROM {{TABLE_NAME}} {{CONDITION}} FETCH FIRST 1 ROWS ONLY N/A {{RDBMS-Name}}.recordDeleteQuery The query for the 'delete record' operation in {{RDBMS-Name}}. H2 : DELETE FROM {{TABLE_NAME}} {{CONDITION}} MySQL : DELETE FROM {{TABLE_NAME}} {{CONDITION}} Oracle : DELETE FROM {{TABLE_NAME}} {{CONDITION}} Microsoft SQL Server : DELETE FROM {{TABLE_NAME}} {{CONDITION}} PostgreSQL : DELETE FROM {{TABLE_NAME}} {{CONDITION}} DB2. : DELETE FROM {{TABLE_NAME}} {{CONDITION}} N/A {{RDBMS-Name}}.stringSize This defines the length for the string fields in {{RDBMS-Name}}. H2 : 254 MySQL : 254 Oracle : 254 Microsoft SQL Server : 254 PostgreSQL : 254 DB2. : 254 N/A {{RDBMS-Name}}.fieldSizeLimit This defines the field size limit for select/switch to big string type from the default string type if the 'bigStringType' is available in field type list. H2 : N/A MySQL : N/A Oracle : 2000 Microsoft SQL Server : N/A PostgreSQL : N/A DB2. : N/A 0 = n = INT_MAX {{RDBMS-Name}}.batchSize This defines the batch size when operations are performed for batches of events. H2 : 1000 MySQL : 1000 Oracle : 1000 Microsoft SQL Server : 1000 PostgreSQL : 1000 DB2. : 1000 N/A {{RDBMS-Name}}.batchEnable This specifies whether 'Update' and 'Insert' operations can be performed for batches of events or not. H2 : true MySQL : true Oracle (versions 12.0 and less) : false Oracle (versions 12.1 and above) : true Microsoft SQL Server : true PostgreSQL : true DB2. : true N/A {{RDBMS-Name}}.transactionSupported This is used to specify whether the JDBC connection that is used supports JDBC transactions or not. H2 : true MySQL : true Oracle : true Microsoft SQL Server : true PostgreSQL : true DB2. : true N/A {{RDBMS-Name}}.typeMapping.binaryType This is used to specify the binary data type. An attribute defines as 'object' type in Siddhi stream will be stored into RDBMS with this type. H2 : BLOB MySQL : BLOB Oracle : BLOB Microsoft SQL Server : VARBINARY(max) PostgreSQL : BYTEA DB2. : BLOB(64000) N/A {{RDBMS-Name}}.typeMapping.booleanType This is used to specify the boolean data type. An attribute defines as 'bool' type in Siddhi stream will be stored into RDBMS with this type. H2 : TINYINT(1) MySQL : TINYINT(1) Oracle : NUMBER(1) Microsoft SQL Server : BIT PostgreSQL : BOOLEAN DB2. : SMALLINT N/A {{RDBMS-Name}}.typeMapping.doubleType This is used to specify the double data type. An attribute defines as 'double' type in Siddhi stream will be stored into RDBMS with this type. H2 : DOUBLE MySQL : DOUBLE Oracle : NUMBER(19,4) Microsoft SQL Server : FLOAT(32) PostgreSQL : DOUBLE PRECISION DB2. : DOUBLE N/A {{RDBMS-Name}}.typeMapping.floatType This is used to specify the float data type. An attribute defines as 'float' type in Siddhi stream will be stored into RDBMS with this type. H2 : FLOAT MySQL : FLOAT Oracle : NUMBER(19,4) Microsoft SQL Server : REAL PostgreSQL : REAL DB2. : REAL N/A {{RDBMS-Name}}.typeMapping.integerType This is used to specify the integer data type. An attribute defines as 'int' type in Siddhi stream will be stored into RDBMS with this type. H2 : INTEGER MySQL : INTEGER Oracle : NUMBER(10) Microsoft SQL Server : INTEGER PostgreSQL : INTEGER DB2. : INTEGER N/A {{RDBMS-Name}}.typeMapping.longType This is used to specify the long data type. An attribute defines as 'long' type in Siddhi stream will be stored into RDBMS with this type. H2 : BIGINT MySQL : BIGINT Oracle : NUMBER(19) Microsoft SQL Server : BIGINT PostgreSQL : BIGINT DB2. : BIGINT N/A {{RDBMS-Name}}.typeMapping.stringType This is used to specify the string data type. An attribute defines as 'string' type in Siddhi stream will be stored into RDBMS with this type. H2 : VARCHAR(stringSize) MySQL : VARCHAR(stringSize) Oracle : VARCHAR(stringSize) Microsoft SQL Server : VARCHAR(stringSize) PostgreSQL : VARCHAR(stringSize) DB2. : VARCHAR(stringSize) N/A {{RDBMS-Name}}.typeMapping.bigStringType This is used to specify the big string data type. An attribute defines as 'string' type in Siddhi stream and field.length define in the annotation is greater than the fieldSizeLimit, will be stored into RDBMS with this type. H2 : N/A MySQL : N/A Oracle : CLOB Microsoft SQL Server : N/A PostgreSQL : N/A DB2.* : N/A N/A Examples EXAMPLE 1 @Store(type=\"rdbms\", jdbc.url=\"jdbc:mysql://localhost:3306/stocks\", username=\"root\", password=\"root\", jdbc.driver.name=\"com.mysql.jdbc.Driver\",field.length=\"symbol:100\") @PrimaryKey(\"id\", \"symbol\") @Index(\"volume\") define table StockTable (id string, symbol string, price float, volume long); The above example creates an event table named 'StockTable' in the database if it does not already exist (with four attributes named id , symbol , price , and volume of the types 'string', 'string', 'float', and 'long' respectively). The connection is made as specified by the parameters configured for the '@Store' annotation. The @PrimaryKey() and @Index() annotations can be used to define primary keys or indexes for the table and they follow Siddhi query syntax. RDBMS store supports having more than one attributes in the @PrimaryKey or @Index annotations. In this example a composite Primary key of both attributes id and symbol will be created. EXAMPLE 2 @Store(type=\"rdbms\", jdbc.url=\"jdbc:mysql://localhost:3306/das\", username=\"root\", password=\"root\" , jdbc.driver.name=\"org.h2.Driver\",field.length=\"symbol:100\") @PrimaryKey(\"symbol\") @Index(\"symbol\") define table StockTable (symbol string, price float, volume long); define stream InputStream (symbol string, volume long); from InputStream as a join StockTable as b on str:contains(b.symbol, a.symbol) select a.symbol as symbol, b.volume as volume insert into FooStream; The above example creates an event table named 'StockTable' in the database if it does not already exist (with three attributes named 'symbol', 'price', and 'volume' of the types 'string', 'float' and 'long' respectively). Then the table is joined with a stream named 'InputStream' based on a condition. The following operations are included in the condition: [ AND, OR, Comparisons( = = == !=), IS NULL, NOT, str:contains(Table Column , Stream Attribute or Search.String)] EXAMPLE 3 @Store(type=\"rdbms\", jdbc.url=\"jdbc:mysql://localhost:3306/das\", table.name=\"StockTable\", username=\"root\", password=\"root\" , jdbc.driver.name=\"org.h2.Driver\", field.length=\"symbol:100\", table.check.query=\"SELECT 1 FROM StockTable LIMIT 1\") @PrimaryKey(\"symbol\") @Index(\"symbol\") define table StockTable (symbol string, price float, volume long); define stream InputStream (symbol string, volume long); from InputStream as a join StockTable as b on str:contains(b.symbol, a.symbol) select a.symbol as symbol, b.volume as volume insert into FooStream; The above example creates an event table named 'StockTable' in the database if it does not already exist (with three attributes named 'symbol', 'price', and 'volume' of the types 'string', 'float' and 'long' respectively). Then the table is joined with a stream named 'InputStream' based on a condition. The following operations are included in the condition: [ AND, OR, Comparisons( = = == !=), IS NULL, NOT, str:contains(Table Column , Stream Attribute or Search.String)]","title":"rdbms (Store)"},{"location":"docs/api/5.1.1/#redis-store","text":"This extension assigns data source and connection instructions to event tables. It also implements read write operations on connected datasource. This extension only can be used to read the data which persisted using the same extension since unique implementation has been used to map the relational data in to redis's key and value representation Origin: siddhi-store-redis:3.1.1 Syntax @Store(type=\"redis\", table.name=\" STRING \", cluster.mode=\" BOOL \", nodes=\" STRING \", ttl.seconds=\" LONG \", ttl.on.update=\" BOOL \", ttl.on.read=\" BOOL \") @PrimaryKey(\"PRIMARY_KEY\") @Index(\"INDEX\") QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic table.name The name with which the event table should be persisted in the store. If noname is specified via this parameter, the event table is persisted with the same name as the Siddhi table. The tale name defined in the siddhi app STRING Yes No cluster.mode This will decide the redis mode. if this is false, client will connect to a single redis node. false BOOL No No nodes host, port and the password of the node(s).In single node mode node details can be provided as follows- \"node='hosts:port@password'\" In clustered mode host and port of all the master nodes should be provided separated by a comma(,). As an example \"nodes = 'localhost:30001,localhost:30002'\". localhost:6379@root STRING Yes No ttl.seconds Time to live in seconds for each record -1 LONG Yes No ttl.on.update Set ttl on row update false BOOL Yes No ttl.on.read Set ttl on read rows false BOOL Yes No Examples EXAMPLE 1 @store(type='redis',nodes='localhost:6379@root',table.name='fooTable',cluster.mode=false)define table fooTable(time long, date String) Above example will create a redis table with the name fooTable and work on asingle redis node. EXAMPLE 2 @Store(type='redis', table.name='SweetProductionTable', nodes='localhost:30001,localhost:30002,localhost:30003', cluster.mode='true') @primaryKey('symbol') @index('price') define table SweetProductionTable (symbol string, price float, volume long); Above example demonstrate how to use the redis extension to connect in to redis cluster. Please note that, as nodes all the master node's host and port should be provided in order to work correctly. In clustered node password will not besupported EXAMPLE 3 @store(type='redis',nodes='localhost:6379@root',table.name='fooTable', ttl.seconds='30', ttl.onUpdate='true', ttl.onRead='true')define table fooTable(time long, date String) Above example will create a redis table with the name fooTable and work on asingle redis node. All rows inserted, updated or read will have its ttl set to 30 seconds","title":"redis (Store)"},{"location":"docs/api/5.1.1/#str","text":"","title":"Str"},{"location":"docs/api/5.1.1/#groupconcat-aggregate-function","text":"This function aggregates the received events by concatenating the keys in those events using a separator, e.g.,a comma (,) or a hyphen (-), and returns the concatenated key string. Origin: siddhi-execution-string:5.0.7 Syntax STRING str:groupConcat( STRING key) STRING str:groupConcat( STRING key, STRING ...) STRING str:groupConcat( STRING key, STRING separator, BOOL distinct) STRING str:groupConcat( STRING key, STRING separator, BOOL distinct, STRING order) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic key The string that needs to be aggregated. STRING No Yes separator The separator that separates each string key after concatenating the keys. , STRING Yes Yes distinct This is used to only have distinct values in the concatenated string that is returned. false BOOL Yes Yes order This parameter accepts 'ASC' or 'DESC' strings to sort the string keys in either ascending or descending order respectively. No order STRING Yes Yes Examples EXAMPLE 1 from InputStream#window.time(5 min) select str:groupConcat(\"key\") as groupedKeys input OutputStream; When we input events having values for the key as 'A' , 'B' , 'S' , 'C' , 'A' , it returns \"A,B,S,C,A\" to the 'OutputStream'. EXAMPLE 2 from InputStream#window.time(5 min) select groupConcat(\"key\",\"-\",true,\"ASC\") as groupedKeys input OutputStream; When we input events having values for the key as 'A' , 'B' , 'S' , 'C' , 'A' , specify the seperator as hyphen and choose the order to be ascending, the function returns \"A-B-C-S\" to the 'OutputStream'.","title":"groupConcat (Aggregate Function)"},{"location":"docs/api/5.1.1/#charat-function","text":"This function returns the 'char' value that is present at the given index position. of the input string. Origin: siddhi-execution-string:5.0.7 Syntax STRING str:charAt( STRING input.value, INT index) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.value The input string of which the char value at the given position needs to be returned. STRING No Yes index The variable that specifies the index of the char value that needs to be returned. INT No Yes Examples EXAMPLE 1 charAt(\"WSO2\", 1) In this case, the functiion returns the character that exists at index 1. Hence, it returns 'S'.","title":"charAt (Function)"},{"location":"docs/api/5.1.1/#coalesce-function_1","text":"This returns the first input parameter value of the given argument, that is not null. Origin: siddhi-execution-string:5.0.7 Syntax INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT str:coalesce( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg, INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT ...) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg It can have one or more input parameters in any data type. However, all the specified parameters are required to be of the same type. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No Yes Examples EXAMPLE 1 coalesce(null, \"BBB\", \"CCC\") This returns the first input parameter that is not null. In this example, it returns \"BBB\".","title":"coalesce (Function)"},{"location":"docs/api/5.1.1/#concat-function","text":"This function returns a string value that is obtained as a result of concatenating two or more input string values. Origin: siddhi-execution-string:5.0.7 Syntax STRING str:concat( STRING arg, STRING ...) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg This can have two or more string type input parameters. STRING No Yes Examples EXAMPLE 1 concat(\"D533\", \"8JU^\", \"XYZ\") This returns a string value by concatenating two or more given arguments. In the example shown above, it returns \"D5338JU^XYZ\".","title":"concat (Function)"},{"location":"docs/api/5.1.1/#contains-function_1","text":"This function returns true if the input.string contains the specified sequence of char values in the search.string . Origin: siddhi-execution-string:5.0.7 Syntax BOOL str:contains( STRING input.string, STRING search.string) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string Input string value. STRING No Yes search.string The string value to be searched for in the input.string . STRING No Yes Examples EXAMPLE 1 contains(\"21 products are produced by WSO2 currently\", \"WSO2\") This returns a boolean value as the output. In this case, it returns true .","title":"contains (Function)"},{"location":"docs/api/5.1.1/#equalsignorecase-function","text":"This returns a boolean value by comparing two strings lexicographically without considering the letter case. Origin: siddhi-execution-string:5.0.7 Syntax BOOL str:equalsIgnoreCase( STRING arg1, STRING arg2) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg1 The first input string argument. STRING No Yes arg2 The second input string argument. This is compared with the first argument. STRING No Yes Examples EXAMPLE 1 equalsIgnoreCase(\"WSO2\", \"wso2\") This returns a boolean value as the output. In this scenario, it returns \"true\".","title":"equalsIgnoreCase (Function)"},{"location":"docs/api/5.1.1/#filltemplate-function","text":"fillTemplate(string, map) will replace all the keys in the string using values in the map. fillTemplate(string, r1, r2 ..) replace all the entries {{1}}, {{2}}, {{3}} with r1 , r2, r3. Origin: siddhi-execution-string:5.0.7 Syntax STRING str:fillTemplate( STRING template, STRING|INT|LONG|DOUBLE|FLOAT|BOOL replacement.type, STRING|INT|LONG|DOUBLE|FLOAT|BOOL ...) STRING str:fillTemplate( STRING template, OBJECT map) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic template The string with templated fields that needs to be filled with the given strings. The format of the templated fields should be as follows: {{KEY}} where 'KEY' is a STRING if you are using fillTemplate(string, map) {{KEY}} where 'KEY' is an INT if you are using fillTemplate(string, r1, r2 ..) This KEY is used to map the values STRING No Yes replacement.type A set of arguments with any type string|int|long|double|float|bool. - STRING INT LONG DOUBLE FLOAT BOOL Yes Yes map A map with key-value pairs to be replaced. - OBJECT Yes Yes Examples EXAMPLE 1 str:fillTemplate(\"{{prize}} 100 {{salary}} 10000\", map:create('prize', 300, 'salary', 10000)) In this example, the template is '{{prize}} 100 {{salary}} 10000'.Here, the templated string {{prize}} is replaced with the value corresponding to the 'prize' key in the given map. Likewise salary replace with the salary value of the map EXAMPLE 2 str:fillTemplate(\"{{1}} 100 {{2}} 10000\", 200, 300) In this example, the template is '{{1}} 100 {{2}} 10000'.Here, the templated string {{1}} is replaced with the corresponding 1 st value 200. Likewise {{2}} replace with the 300","title":"fillTemplate (Function)"},{"location":"docs/api/5.1.1/#hex-function_1","text":"This function returns a hexadecimal string by converting each byte of each character in the input string to two hexadecimal digits. Origin: siddhi-execution-string:5.0.7 Syntax STRING str:hex( STRING input.string) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string The input string to derive the hexadecimal value. STRING No Yes Examples EXAMPLE 1 hex(\"MySQL\") This returns the hexadecimal value of the input.string. In this scenario, the output is \"4d7953514c\".","title":"hex (Function)"},{"location":"docs/api/5.1.1/#length-function","text":"Returns the length of the input string. Origin: siddhi-execution-string:5.0.7 Syntax INT str:length( STRING input.string) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string The input string to derive the length. STRING No Yes Examples EXAMPLE 1 length(\"Hello World\") This outputs the length of the provided string. In this scenario, the, output is 11 .","title":"length (Function)"},{"location":"docs/api/5.1.1/#lower-function","text":"Converts the capital letters in the input string to the equivalent simple letters. Origin: siddhi-execution-string:5.0.7 Syntax STRING str:lower( STRING input.string) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string The input string to convert to the lower case (i.e., equivalent simple letters). STRING No Yes Examples EXAMPLE 1 lower(\"WSO2 cep \") This converts the capital letters in the input.string to the equivalent simple letters. In this scenario, the output is \"wso2 cep \".","title":"lower (Function)"},{"location":"docs/api/5.1.1/#regexp-function","text":"Returns a boolean value based on the matchability of the input string and the given regular expression. Origin: siddhi-execution-string:5.0.7 Syntax BOOL str:regexp( STRING input.string, STRING regex) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string The input string to match with the given regular expression. STRING No Yes regex The regular expression to be matched with the input string. STRING No Yes Examples EXAMPLE 1 regexp(\"WSO2 abcdh\", \"WSO(.*h)\") This returns a boolean value after matching regular expression with the given string. In this scenario, it returns \"true\" as the output.","title":"regexp (Function)"},{"location":"docs/api/5.1.1/#repeat-function","text":"Repeats the input string for a specified number of times. Origin: siddhi-execution-string:5.0.7 Syntax STRING str:repeat( STRING input.string, INT times) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string The input string that is repeated the number of times as defined by the user. STRING No Yes times The number of times the input.string needs to be repeated . INT No Yes Examples EXAMPLE 1 repeat(\"StRing 1\", 3) This returns a string value by repeating the string for a specified number of times. In this scenario, the output is \"StRing 1StRing 1StRing 1\".","title":"repeat (Function)"},{"location":"docs/api/5.1.1/#replaceall-function_1","text":"Finds all the substrings of the input string that matches with the given expression, and replaces them with the given replacement string. Origin: siddhi-execution-string:5.0.7 Syntax STRING str:replaceAll( STRING input.string, STRING regex, STRING replacement.string) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string The input string to be replaced. STRING No Yes regex The regular expression to be matched with the input string. STRING No Yes replacement.string The string with which each substring that matches the given expression should be replaced. STRING No Yes Examples EXAMPLE 1 replaceAll(\"hello hi hello\", 'hello', 'test') This returns a string after replacing the substrings of the input string with the replacement string. In this scenario, the output is \"test hi test\" .","title":"replaceAll (Function)"},{"location":"docs/api/5.1.1/#replacefirst-function","text":"Finds the first substring of the input string that matches with the given regular expression, and replaces itwith the given replacement string. Origin: siddhi-execution-string:5.0.7 Syntax STRING str:replaceFirst( STRING input.string, STRING regex, STRING replacement.string) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string The input string that should be replaced. STRING No Yes regex The regular expression with which the input string should be matched. STRING No Yes replacement.string The string with which the first substring of input string that matches the regular expression should be replaced. STRING No Yes Examples EXAMPLE 1 replaceFirst(\"hello WSO2 A hello\", 'WSO2(.*)A', 'XXXX') This returns a string after replacing the first substring with the given replacement string. In this scenario, the output is \"hello XXXX hello\".","title":"replaceFirst (Function)"},{"location":"docs/api/5.1.1/#reverse-function","text":"Returns the input string in the reverse order character-wise and string-wise. Origin: siddhi-execution-string:5.0.7 Syntax STRING str:reverse( STRING input.string) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string The input string to be reversed. STRING No Yes Examples EXAMPLE 1 reverse(\"Hello World\") This outputs a string value by reversing the incoming input.string . In this scenario, the output is \"dlroW olleH\".","title":"reverse (Function)"},{"location":"docs/api/5.1.1/#split-function","text":"Splits the input.string into substrings using the value parsed in the split.string and returns the substring at the position specified in the group.number . Origin: siddhi-execution-string:5.0.7 Syntax STRING str:split( STRING input.string, STRING split.string, INT group.number) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string The input string to be replaced. STRING No Yes split.string The string value to be used to split the input.string . STRING No Yes group.number The index of the split group INT No Yes Examples EXAMPLE 1 split(\"WSO2,ABM,NSFT\", \",\", 0) This splits the given input.string by given split.string and returns the string in the index given by group.number. In this scenario, the output will is \"WSO2\".","title":"split (Function)"},{"location":"docs/api/5.1.1/#strcmp-function","text":"Compares two strings lexicographically and returns an integer value. If both strings are equal, 0 is returned. If the first string is lexicographically greater than the second string, a positive value is returned. If the first string is lexicographically greater than the second string, a negative value is returned. Origin: siddhi-execution-string:5.0.7 Syntax INT str:strcmp( STRING arg1, STRING arg2) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg1 The first input string argument. STRING No Yes arg2 The second input string argument that should be compared with the first argument lexicographically. STRING No Yes Examples EXAMPLE 1 strcmp(\"AbCDefghiJ KLMN\", 'Hello') This compares two strings lexicographically and outputs an integer value.","title":"strcmp (Function)"},{"location":"docs/api/5.1.1/#substr-function","text":"Returns a substring of the input string by considering a subset or all of the following factors: starting index, length, regular expression, and regex group number. Origin: siddhi-execution-string:5.0.7 Syntax STRING str:substr( STRING input.string, INT begin.index) STRING str:substr( STRING input.string, INT begin.index, INT length) STRING str:substr( STRING input.string, STRING regex) STRING str:substr( STRING input.string, STRING regex, INT group.number) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string The input string to be processed. STRING No Yes begin.index Starting index to consider for the substring. - INT Yes Yes length The length of the substring. input.string .length - begin.index INT Yes Yes regex The regular expression that should be matched with the input string. - STRING Yes Yes group.number The regex group number 0 INT Yes Yes Examples EXAMPLE 1 substr(\"AbCDefghiJ KLMN\", 4) This outputs the substring based on the given begin.index . In this scenario, the output is \"efghiJ KLMN\". EXAMPLE 2 substr(\"AbCDefghiJ KLMN\", 2, 4) This outputs the substring based on the given begin.index and length. In this scenario, the output is \"CDef\". EXAMPLE 3 substr(\"WSO2D efghiJ KLMN\", '^WSO2(.*)') This outputs the substring by applying the regex. In this scenario, the output is \"WSO2D efghiJ KLMN\". EXAMPLE 4 substr(\"WSO2 cep WSO2 XX E hi hA WSO2 heAllo\", 'WSO2(.*)A(.*)', 2) This outputs the substring by applying the regex and considering the group.number . In this scenario, the output is \" ello\".","title":"substr (Function)"},{"location":"docs/api/5.1.1/#trim-function","text":"Returns a copy of the input string without the leading and trailing whitespace (if any). Origin: siddhi-execution-string:5.0.7 Syntax STRING str:trim( STRING input.string) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string The input string that needs to be trimmed. STRING No Yes Examples EXAMPLE 1 trim(\" AbCDefghiJ KLMN \") This returns a copy of the input.string with the leading and/or trailing white-spaces omitted. In this scenario, the output is \"AbCDefghiJ KLMN\".","title":"trim (Function)"},{"location":"docs/api/5.1.1/#unhex-function","text":"Returns a string by converting the hexadecimal characters in the input string. Origin: siddhi-execution-string:5.0.7 Syntax STRING str:unhex( STRING input.string) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string The hexadecimal input string that needs to be converted to string. STRING No Yes Examples EXAMPLE 1 unhex(\"4d7953514c\") This converts the hexadecimal value to string.","title":"unhex (Function)"},{"location":"docs/api/5.1.1/#upper-function","text":"Converts the simple letters in the input string to the equivalent capital/block letters. Origin: siddhi-execution-string:5.0.7 Syntax STRING str:upper( STRING input.string) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string The input string that should be converted to the upper case (equivalent capital/block letters). STRING No Yes Examples EXAMPLE 1 upper(\"Hello World\") This converts the simple letters in the input.string to theequivalent capital letters. In this scenario, the output is \"HELLO WORLD\".","title":"upper (Function)"},{"location":"docs/api/5.1.1/#tokenize-stream-processor_3","text":"This function splits the input string into tokens using a given regular expression and returns the split tokens. Origin: siddhi-execution-string:5.0.7 Syntax str:tokenize( STRING input.string, STRING regex) str:tokenize( STRING input.string, STRING regex, BOOL distinct) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string The input string which needs to be split. STRING No Yes regex The string value which is used to tokenize the 'input.string'. STRING No Yes distinct This flag is used to return only distinct values. false BOOL Yes Yes Extra Return Attributes Name Description Possible Types token The attribute which contains a single token. STRING Examples EXAMPLE 1 define stream inputStream (str string); @info(name = 'query1') from inputStream#str:tokenize(str , ',') select token insert into outputStream; This query performs tokenization on the given string. If the str is \"Android,Windows8,iOS\", then the string is split into 3 events containing the token attribute values, i.e., Android , Windows8 and iOS .","title":"tokenize (Stream Processor)"},{"location":"docs/api/5.1.1/#time","text":"","title":"Time"},{"location":"docs/api/5.1.1/#currentdate-function","text":"Function returns the system time in yyyy-MM-dd format. Origin: siddhi-execution-time:5.0.4 Syntax STRING time:currentDate() Examples EXAMPLE 1 time:currentDate() Returns the current date in the yyyy-MM-dd format, such as 2019-06-21 .","title":"currentDate (Function)"},{"location":"docs/api/5.1.1/#currenttime-function","text":"Function returns system time in the HH ss format. Origin: siddhi-execution-time:5.0.4 Syntax STRING time:currentTime() Examples EXAMPLE 1 time:currentTime() Returns the current date in the HH ss format, such as 15:23:24 .","title":"currentTime (Function)"},{"location":"docs/api/5.1.1/#currenttimestamp-function","text":"When no argument is provided, function returns the system current timestamp in yyyy-MM-dd HH ss format, and when a timezone is provided as an argument, it converts and return the current system time to the given timezone format. Origin: siddhi-execution-time:5.0.4 Syntax STRING time:currentTimestamp() STRING time:currentTimestamp( STRING timezone) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic timezone The timezone to which the current time need to be converted. For example, Asia/Kolkata , PST . Get the supported timezone IDs from here System timezone STRING Yes No Examples EXAMPLE 1 time:currentTimestamp() Returns current system time in yyyy-MM-dd HH ss format, such as 2019-03-31 14:07:00 . EXAMPLE 2 time:currentTimestamp('Asia/Kolkata') Returns current system time converted to 'Asia/Kolkata' timezone yyyy-MM-dd HH ss format, such as 2019-03-31 19:07:00 . Get the supported timezone IDs from here EXAMPLE 3 time:currentTimestamp('CST') Returns current system time converted to 'CST' timezone yyyy-MM-dd HH ss format, such as 2019-03-31 02:07:00 . Get the supported timezone IDs from here","title":"currentTimestamp (Function)"},{"location":"docs/api/5.1.1/#date-function","text":"Extracts the date part of a date or date-time and return it in yyyy-MM-dd format. Origin: siddhi-execution-time:5.0.4 Syntax STRING time:date( STRING date.value, STRING date.format) STRING time:date( STRING date.value) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic date.value The value of the date. For example, 2014-11-11 13:23:44.657 , 2014-11-11 , 13:23:44.657 . STRING No Yes date.format The format of the date value provided. For example, yyyy/MM/dd HH ss.SSS . yyyy-MM-dd HH:mm:ss.SSS STRING Yes Yes Examples EXAMPLE 1 time:date('2014/11/11 13:23:44', 'yyyy/MM/dd HH:mm:ss') Extracts the date and returns 2014-11-11 . EXAMPLE 2 time:date('2014-11-23 13:23:44.345') Extracts the date and returns 2014-11-13 . EXAMPLE 3 time:date('13:23:44', 'HH:mm:ss') Extracts the date and returns 1970-01-01 .","title":"date (Function)"},{"location":"docs/api/5.1.1/#dateadd-function","text":"Adds the specified time interval to a date. Origin: siddhi-execution-time:5.0.4 Syntax STRING time:dateAdd( STRING date.value, INT expr, STRING unit) STRING time:dateAdd( LONG timestamp.in.milliseconds, INT expr, STRING unit) STRING time:dateAdd( STRING date.value, INT expr, STRING unit, STRING date.format) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic date.value The value of the date. For example, 2014-11-11 13:23:44.657 , 2014-11-11 , 13:23:44.657 . - STRING Yes Yes expr The amount by which the selected part of the date should be incremented. For example 2 , 5 , 10 , etc. INT No Yes unit This is the part of the date that needs to be modified. For example, MINUTE , HOUR , MONTH , YEAR , QUARTER , WEEK , DAY , SECOND . STRING No No date.format The format of the date value provided. For example, yyyy-MM-dd HH ss.SSS . yyyy-MM-dd HH:mm:ss.SSS STRING Yes Yes timestamp.in.milliseconds The date value in milliseconds. For example, 1415712224000L . - LONG Yes Yes Examples EXAMPLE 1 time:dateAdd('2014-11-11 13:23:44.657', 5, 'YEAR', 'yyyy-MM-dd HH:mm:ss.SSS') Adds five years to the given date value and returns 2019-11-11 13:23:44.657 . EXAMPLE 2 time:dateAdd('2014-11-11 13:23:44.657', 5, 'YEAR') Adds five years to the given date value and returns 2019-11-11 13:23:44.657 using the default date.format yyyy-MM-dd HH ss.SSS . EXAMPLE 3 time:dateAdd( 1415712224000L, 1, 'HOUR') Adds one hour and 1415715824000 as a string .","title":"dateAdd (Function)"},{"location":"docs/api/5.1.1/#datediff-function","text":"Returns difference between two dates in days. Origin: siddhi-execution-time:5.0.4 Syntax INT time:dateDiff( STRING date.value1, STRING date.value2, STRING date.format1, STRING date.format2) INT time:dateDiff( STRING date.value1, STRING date.value2) INT time:dateDiff( LONG timestamp.in.milliseconds1, LONG timestamp.in.milliseconds2) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic date.value1 The value of the first date parameter. For example, 2014-11-11 13:23:44.657 , 2014-11-11 , 13:23:44.657 . - STRING Yes Yes date.value2 The value of the second date parameter. For example, 2014-11-11 13:23:44.657 , 2014-11-11 , 13:23:44.657 . - STRING Yes Yes date.format1 The format of the first date value provided. For example, yyyy-MM-dd HH ss.SSS . yyyy-MM-dd HH:mm:ss.SSS STRING Yes Yes date.format2 The format of the second date value provided. For example, yyyy-MM-dd HH ss.SSS . yyyy-MM-dd HH:mm:ss.SSS STRING Yes Yes timestamp.in.milliseconds1 The first date value in milliseconds from the epoch. For example, 1415712224000L . - LONG Yes Yes timestamp.in.milliseconds2 The second date value in milliseconds from the epoch. For example, 1415712224000L . - LONG Yes Yes Examples EXAMPLE 1 time:dateDiff('2014-11-11 13:23:44', '2014-11-9 13:23:44', 'yyyy-MM-dd HH:mm:ss', 'yyyy-MM-dd HH:mm:ss') Returns the date difference between the two given dates as 2 . EXAMPLE 2 time:dateDiff('2014-11-13 13:23:44', '2014-11-9 13:23:44') Returns the date difference between the two given dates as 4 . EXAMPLE 3 time:dateDiff(1415692424000L, 1412841224000L) Returns the date difference between the two given dates as 33 .","title":"dateDiff (Function)"},{"location":"docs/api/5.1.1/#dateformat-function","text":"Formats the data in string or milliseconds format to the given date format. Origin: siddhi-execution-time:5.0.4 Syntax STRING time:dateFormat( STRING date.value, STRING date.target.format, STRING date.source.format) STRING time:dateFormat( STRING date.value, STRING date.target.format) STRING time:dateFormat( LONG timestamp.in.milliseconds, STRING date.target.format) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic date.value The value of the date. For example, 2014-11-11 13:23:44.657 , 2014-11-11 , 13:23:44.657 . - STRING Yes Yes date.target.format The format of the date into which the date value needs to be converted. For example, yyyy/MM/dd HH ss . STRING No Yes date.source.format The format input date.value.For example, yyyy-MM-dd HH ss.SSS . yyyy-MM-dd HH:mm:ss.SSS STRING Yes Yes timestamp.in.milliseconds The date value in milliseconds from the epoch. For example, 1415712224000L . - LONG Yes Yes Examples EXAMPLE 1 time:dateFormat('2014/11/11 13:23:44', 'mm:ss', 'yyyy/MM/dd HH:mm:ss') Converts date based on the target date format mm:ss and returns 23:44 . EXAMPLE 2 time:dateFormat('2014-11-11 13:23:44', 'HH:mm:ss') Converts date based on the target date format HH ss and returns 13:23:44 . EXAMPLE 3 time:dateFormat(1415692424000L, 'yyyy-MM-dd') Converts date in millisecond based on the target date format yyyy-MM-dd and returns 2014-11-11 .","title":"dateFormat (Function)"},{"location":"docs/api/5.1.1/#datesub-function","text":"Subtracts the specified time interval from the given date. Origin: siddhi-execution-time:5.0.4 Syntax STRING time:dateSub( STRING date.value, INT expr, STRING unit) STRING time:dateSub( STRING date.value, INT expr, STRING unit, STRING date.format) STRING time:dateSub( LONG timestamp.in.milliseconds, INT expr, STRING unit) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic date.value The value of the date. For example, 2014-11-11 13:23:44.657 , 2014-11-11 , 13:23:44.657 . - STRING Yes Yes expr The amount by which the selected part of the date should be decremented. For example 2 , 5 , 10 , etc. INT No Yes unit This is the part of the date that needs to be modified. For example, MINUTE , HOUR , MONTH , YEAR , QUARTER , WEEK , DAY , SECOND . STRING No No date.format The format of the date value provided. For example, yyyy-MM-dd HH ss.SSS . yyyy-MM-dd HH:mm:ss.SSS STRING Yes Yes timestamp.in.milliseconds The date value in milliseconds. For example, 1415712224000L . - LONG Yes Yes Examples EXAMPLE 1 time:dateSub('2019-11-11 13:23:44.657', 5, 'YEAR', 'yyyy-MM-dd HH:mm:ss.SSS') Subtracts five years to the given date value and returns 2014-11-11 13:23:44.657 . EXAMPLE 2 time:dateSub('2019-11-11 13:23:44.657', 5, 'YEAR') Subtracts five years to the given date value and returns 2014-11-11 13:23:44.657 using the default date.format yyyy-MM-dd HH ss.SSS . EXAMPLE 3 time:dateSub( 1415715824000L, 1, 'HOUR') Subtracts one hour and 1415712224000 as a string .","title":"dateSub (Function)"},{"location":"docs/api/5.1.1/#dayofweek-function","text":"Extracts the day on which a given date falls. Origin: siddhi-execution-time:5.0.4 Syntax STRING time:dayOfWeek( STRING date.value, STRING date.format) STRING time:dayOfWeek( STRING date.value) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic date.value The value of the date. For example, 2014-11-11 13:23:44.657 , 2014-11-11 , 13:23:44.657 . STRING No Yes date.format The format of the date value provided. For example, yyyy/MM/dd HH ss.SSS . yyyy-MM-dd HH:mm:ss.SSS STRING Yes Yes Examples EXAMPLE 1 time:date('2014/12/11 13:23:44', 'yyyy/MM/dd HH:mm:ss') Extracts the date and returns Thursday . EXAMPLE 2 time:date('2014-11-11 13:23:44.345') Extracts the date and returns Tuesday .","title":"dayOfWeek (Function)"},{"location":"docs/api/5.1.1/#extract-function","text":"Function extracts a date unit from the date. Origin: siddhi-execution-time:5.0.4 Syntax INT time:extract( STRING unit, STRING date.value) INT time:extract( STRING unit, STRING date.value, STRING date.format) INT time:extract( STRING unit, STRING date.value, STRING date.format, STRING locale) INT time:extract( LONG timestamp.in.milliseconds, STRING unit) INT time:extract( LONG timestamp.in.milliseconds, STRING unit, STRING locale) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic unit This is the part of the date that needs to be modified. For example, MINUTE , HOUR , MONTH , YEAR , QUARTER , WEEK , DAY , SECOND . STRING No No date.value The value of the date. For example, 2014-11-11 13:23:44.657 , 2014-11-11 , 13:23:44.657 . - STRING Yes Yes date.format The format of the date value provided. For example, yyyy-MM-dd HH ss.SSS . yyyy-MM-dd HH:mm:ss.SSS STRING Yes Yes timestamp.in.milliseconds The date value in milliseconds. For example, 1415712224000L . - LONG Yes Yes locale Represents a specific geographical, political or cultural region. For example en_US and fr_FR Current default locale set in the Java Virtual Machine. STRING Yes No Examples EXAMPLE 1 time:extract('YEAR', '2019/11/11 13:23:44.657', 'yyyy/MM/dd HH:mm:ss.SSS') Extracts the year amount and returns 2019 . EXAMPLE 2 time:extract('DAY', '2019-11-12 13:23:44.657') Extracts the day amount and returns 12 . EXAMPLE 3 time:extract(1394556804000L, 'HOUR') Extracts the hour amount and returns 22 .","title":"extract (Function)"},{"location":"docs/api/5.1.1/#timestampinmilliseconds-function","text":"Returns the system time or the given time in milliseconds. Origin: siddhi-execution-time:5.0.4 Syntax LONG time:timestampInMilliseconds() LONG time:timestampInMilliseconds( STRING date.value, STRING date.format) LONG time:timestampInMilliseconds( STRING date.value) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic date.value The value of the date. For example, 2014-11-11 13:23:44.657 , 2014-11-11 , 13:23:44.657 . Current system time STRING Yes Yes date.format The format of the date value provided. For example, yyyy/MM/dd HH ss.SSS . yyyy-MM-dd HH:mm:ss.SSS STRING Yes Yes Examples EXAMPLE 1 time:timestampInMilliseconds() Returns the system current time in milliseconds. EXAMPLE 2 time:timestampInMilliseconds('2007-11-30 10:30:19', 'yyyy-MM-DD HH:MM:SS') Converts 2007-11-30 10:30:19 in yyyy-MM-DD HH:MM:SS format to milliseconds as 1170131400019 . EXAMPLE 3 time:timestampInMilliseconds('2007-11-30 10:30:19.000') Converts 2007-11-30 10:30:19 in yyyy-MM-DD HH:MM:ss.SSS format to milliseconds as 1196398819000 .","title":"timestampInMilliseconds (Function)"},{"location":"docs/api/5.1.1/#utctimestamp-function","text":"Function returns the system current time in UTC timezone with yyyy-MM-dd HH ss format. Origin: siddhi-execution-time:5.0.4 Syntax STRING time:utcTimestamp() Examples EXAMPLE 1 time:utcTimestamp() Returns the system current time in UTC timezone with yyyy-MM-dd HH ss format, and a sample output will be like 2019-07-03 09:58:34 .","title":"utcTimestamp (Function)"},{"location":"docs/api/5.1.1/#unique","text":"","title":"Unique"},{"location":"docs/api/5.1.1/#deduplicate-stream-processor","text":"Removes duplicate events based on the unique.key parameter that arrive within the time.interval gap from one another. Origin: siddhi-execution-unique:5.0.5 Syntax unique:deduplicate( INT|LONG|FLOAT|BOOL|DOUBLE|STRING unique.key, INT|LONG time.interval) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic unique.key Parameter to uniquely identify events. INT LONG FLOAT BOOL DOUBLE STRING No Yes time.interval The sliding time period within which the duplicate events are dropped. INT LONG No No Examples EXAMPLE 1 define stream TemperatureStream (sensorId string, temperature double) from TemperatureStream#unique:deduplicate(sensorId, 30 sec) select * insert into UniqueTemperatureStream; Query that removes duplicate events of TemperatureStream stream based on sensorId attribute when they arrive within 30 seconds.","title":"deduplicate (Stream Processor)"},{"location":"docs/api/5.1.1/#ever-window","text":"Window that retains the latest events based on a given unique keys. When a new event arrives with the same key it replaces the one that exist in the window. b This function is not recommended to be used when the maximum number of unique attributes are undefined, as there is a risk of system going out to memory /b . Origin: siddhi-execution-unique:5.0.5 Syntax unique:ever( INT|LONG|FLOAT|BOOL|DOUBLE|STRING unique.key) unique:ever( INT|LONG|FLOAT|BOOL|DOUBLE|STRING unique.key, INT|LONG|FLOAT|BOOL|DOUBLE|STRING ...) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic unique.key The attribute used to checked for uniqueness. INT LONG FLOAT BOOL DOUBLE STRING No Yes Examples EXAMPLE 1 define stream LoginEvents (timestamp long, ip string); from LoginEvents#window.unique:ever(ip) select count(ip) as ipCount insert events into UniqueIps; Query collects all unique events based on the ip attribute by retaining the latest unique events from the LoginEvents stream. Then the query counts the unique ip s arrived so far and outputs the ipCount via the UniqueIps stream. EXAMPLE 2 define stream DriverChangeStream (trainID string, driver string); from DriverChangeStream#window.unique:ever(trainID) select trainID, driver insert expired events into PreviousDriverChangeStream; Query collects all unique events based on the trainID attribute by retaining the latest unique events from the DriverChangeStream stream. The query outputs the previous unique event stored in the window as the expired events are emitted via PreviousDriverChangeStream stream. EXAMPLE 3 define stream StockStream (symbol string, price float); define stream PriceRequestStream(symbol string); from StockStream#window.unique:ever(symbol) as s join PriceRequestStream as p on s.symbol == p.symbol select s.symbol as symbol, s.price as price insert events into PriceResponseStream; Query stores the last unique event for each symbol attribute of StockStream stream, and joins them with events arriving on the PriceRequestStream for equal symbol attributes to fetch the latest price for each requested symbol and output via PriceResponseStream stream.","title":"ever (Window)"},{"location":"docs/api/5.1.1/#externaltimebatch-window_1","text":"This is a batch (tumbling) time window that is determined based on an external time, i.e., time stamps that are specified via an attribute in the events. It holds the latest unique events that arrived during the last window time period. The unique events are determined based on the value for a specified unique key parameter. When a new event arrives within the time window with a value for the unique key parameter that is the same as that of an existing event in the window, the existing event expires and it is replaced by the new event. Origin: siddhi-execution-unique:5.0.5 Syntax unique:externalTimeBatch( INT|LONG|FLOAT|BOOL|DOUBLE|STRING unique.key, LONG time.stamp, INT|LONG window.time) unique:externalTimeBatch( INT|LONG|FLOAT|BOOL|DOUBLE|STRING unique.key, LONG time.stamp, INT|LONG window.time, INT start.time) unique:externalTimeBatch( INT|LONG|FLOAT|BOOL|DOUBLE|STRING unique.key, LONG time.stamp, INT|LONG window.time, INT start.time, INT|LONG time.out) unique:externalTimeBatch( INT|LONG|FLOAT|BOOL|DOUBLE|STRING unique.key, LONG time.stamp, INT|LONG window.time, INT start.time, INT|LONG time.out, BOOL replace.time.stamp.with.batch.end.time) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic unique.key The attribute that should be checked for uniqueness. INT LONG FLOAT BOOL DOUBLE STRING No Yes time.stamp The time which the window determines as the current time and acts upon. The value of this parameter should be monotonically increasing. LONG No Yes window.time The sliding time period for which the window should hold events. INT LONG No No start.time This specifies an offset in milliseconds in order to start the window at a time different to the standard time. Timestamp of first event INT Yes No time.out Time to wait for arrival of a new event, before flushing and returning the output for events belonging to a specific batch. The system waits till an event from the next batch arrives to flush the current batch INT LONG Yes No replace.time.stamp.with.batch.end.time Replaces the 'timestamp' value with the corresponding batch end time stamp. false BOOL Yes No Examples EXAMPLE 1 define stream LoginEvents (timestamp long, ip string); from LoginEvents#window.unique:externalTimeBatch(ip, timestamp, 1 sec, 0, 2 sec) select timestamp, ip, count() as total insert into UniqueIps ; In this query, the window holds the latest unique events that arrive from the 'LoginEvent' stream during each second. The latest events are determined based on the external time stamp. At a given time, all the events held in the window have unique values for the 'ip' and monotonically increasing values for 'timestamp' attributes. The events in the window are inserted into the 'UniqueIps' output stream. The system waits for 2 seconds for the arrival of a new event before flushing the current batch.","title":"externalTimeBatch (Window)"},{"location":"docs/api/5.1.1/#first-window","text":"This is a window that holds only the first set of unique events according to the unique key parameter. When a new event arrives with a key that is already in the window, that event is not processed by the window. Origin: siddhi-execution-unique:5.0.5 Syntax unique:first( INT|LONG|FLOAT|BOOL|DOUBLE|STRING unique.key) unique:first( INT|LONG|FLOAT|BOOL|DOUBLE|STRING unique.key, INT|LONG|FLOAT|BOOL|DOUBLE|STRING ...) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic unique.key The attribute that should be checked for uniqueness. If there is more than one parameter to check for uniqueness, it can be specified as an array separated by commas. INT LONG FLOAT BOOL DOUBLE STRING No Yes Examples EXAMPLE 1 define stream LoginEvents (timeStamp long, ip string); from LoginEvents#window.unique:first(ip) insert into UniqueIps ; This returns the first set of unique items that arrive from the 'LoginEvents' stream, and returns them to the 'UniqueIps' stream. The unique events are only those with a unique value for the 'ip' attribute.","title":"first (Window)"},{"location":"docs/api/5.1.1/#firstlengthbatch-window","text":"This is a batch (tumbling) window that holds a specific number of unique events (depending on which events arrive first). The unique events are selected based on a specific parameter that is considered as the unique key. When a new event arrives with a value for the unique key parameter that matches the same of an existing event in the window, that event is not processed by the window. Origin: siddhi-execution-unique:5.0.5 Syntax unique:firstLengthBatch( INT|LONG|FLOAT|BOOL|DOUBLE|STRING unique.key, INT window.length) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic unique.key The attribute that should be checked for uniqueness. INT LONG FLOAT BOOL DOUBLE STRING No Yes window.length The number of events the window should tumble. INT No No Examples EXAMPLE 1 define window CseEventWindow (symbol string, price float, volume int) from CseEventStream#window.unique:firstLengthBatch(symbol, 10) select symbol, price, volume insert all events into OutputStream ; The window in this configuration holds the first unique events from the 'CseEventStream' stream every second, and outputs them all into the the 'OutputStream' stream. All the events in a window during a given second should have a unique value for the 'symbol' attribute.","title":"firstLengthBatch (Window)"},{"location":"docs/api/5.1.1/#firsttimebatch-window","text":"A batch-time or tumbling window that holds the unique events according to the unique key parameters that have arrived within the time period of that window and gets updated for each such time window. When a new event arrives with a key which is already in the window, that event is not processed by the window. Origin: siddhi-execution-unique:5.0.5 Syntax unique:firstTimeBatch( INT|LONG|FLOAT|BOOL|DOUBLE|STRING unique.key, INT|LONG window.time) unique:firstTimeBatch( INT|LONG|FLOAT|BOOL|DOUBLE|STRING unique.key, INT|LONG window.time, INT|LONG start.time) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic unique.key The attribute that should be checked for uniqueness. INT LONG FLOAT BOOL DOUBLE STRING No Yes window.time The sliding time period for which the window should hold events. INT LONG No No start.time This specifies an offset in milliseconds in order to start the window at a time different to the standard time. Timestamp of the first event. INT LONG Yes No Examples EXAMPLE 1 define stream CseEventStream (symbol string, price float, volume int) from CseEventStream#window.unique:firstTimeBatch(symbol,1 sec) select symbol, price, volume insert all events into OutputStream ; This holds the first unique events that arrive from the 'cseEventStream' input stream during each second, based on the symbol,as a batch, and returns all the events to the 'OutputStream'.","title":"firstTimeBatch (Window)"},{"location":"docs/api/5.1.1/#length-window_1","text":"This is a sliding length window that holds the events of the latest window length with the unique key and gets updated for the expiry and arrival of each event. When a new event arrives with the key that is already there in the window, then the previous event expires and new event is kept within the window. Origin: siddhi-execution-unique:5.0.5 Syntax unique:length( INT|LONG|FLOAT|BOOL|DOUBLE|STRING unique.key, INT window.length) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic unique.key The attribute that should be checked for uniqueness. INT LONG FLOAT BOOL DOUBLE STRING No Yes window.length The number of events that should be included in a sliding length window. INT No No Examples EXAMPLE 1 define stream CseEventStream (symbol string, price float, volume int) from CseEventStream#window.unique:length(symbol,10) select symbol, price, volume insert all events into OutputStream; In this configuration, the window holds the latest 10 unique events. The latest events are selected based on the symbol attribute. If the 'CseEventStream' receives an event for which the value for the symbol attribute is the same as that of an existing event in the window, the existing event is replaced by the new event. All the events are returned to the 'OutputStream' event stream once an event expires or is added to the window.","title":"length (Window)"},{"location":"docs/api/5.1.1/#lengthbatch-window_1","text":"This is a batch (tumbling) window that holds a specified number of latest unique events. The unique events are determined based on the value for a specified unique key parameter. The window is updated for every window length, i.e., for the last set of events of the specified number in a tumbling manner. When a new event arrives within the window length having the same value for the unique key parameter as an existing event in the window, the previous event is replaced by the new event. Origin: siddhi-execution-unique:5.0.5 Syntax unique:lengthBatch( INT|LONG|FLOAT|BOOL|DOUBLE|STRING unique.key, INT window.length) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic unique.key The attribute that should be checked for uniqueness. INT LONG FLOAT BOOL DOUBLE STRING No Yes window.length The number of events the window should tumble. INT No No Examples EXAMPLE 1 define window CseEventWindow (symbol string, price float, volume int) from CseEventStream#window.unique:lengthBatch(symbol, 10) select symbol, price, volume insert expired events into OutputStream ; In this query, the window at any give time holds the last 10 unique events from the 'CseEventStream' stream. Each of the 10 events within the window at a given time has a unique value for the symbol attribute. If a new event has the same value for the symbol attribute as an existing event within the window length, the existing event expires and it is replaced by the new event. The query returns expired individual events as well as expired batches of events to the 'OutputStream' stream.","title":"lengthBatch (Window)"},{"location":"docs/api/5.1.1/#time-window_1","text":"This is a sliding time window that holds the latest unique events that arrived during the previous time window. The unique events are determined based on the value for a specified unique key parameter. The window is updated with the arrival and expiry of each event. When a new event that arrives within a window time period has the same value for the unique key parameter as an existing event in the window, the previous event is replaced by the new event. Origin: siddhi-execution-unique:5.0.5 Syntax unique:time( INT|LONG|FLOAT|BOOL|DOUBLE|STRING unique.key, INT|LONG window.time) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic unique.key The attribute that should be checked for uniqueness. INT LONG FLOAT BOOL DOUBLE STRING No Yes window.time The sliding time period for which the window should hold events. INT LONG No No Examples EXAMPLE 1 define stream CseEventStream (symbol string, price float, volume int) from CseEventStream#window.unique:time(symbol, 1 sec) select symbol, price, volume insert expired events into OutputStream ; In this query, the window holds the latest unique events that arrived within the last second from the 'CseEventStream', and returns the expired events to the 'OutputStream' stream. During any given second, each event in the window should have a unique value for the 'symbol' attribute. If a new event that arrives within the same second has the same value for the symbol attribute as an existing event in the window, the existing event expires.","title":"time (Window)"},{"location":"docs/api/5.1.1/#timebatch-window_1","text":"This is a batch (tumbling) time window that is updated with the latest events based on a unique key parameter. If a new event that arrives within the time period of a windowhas a value for the key parameter which matches that of an existing event, the existing event expires and it is replaced by the latest event. Origin: siddhi-execution-unique:5.0.5 Syntax unique:timeBatch( INT|LONG|FLOAT|BOOL|DOUBLE|STRING unique.key, INT|LONG window.time) unique:timeBatch( INT|LONG|FLOAT|BOOL|DOUBLE|STRING unique.key, INT|LONG window.time, INT|LONG start.time) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic unique.key The attribute that should be checked for uniqueness. INT LONG FLOAT BOOL DOUBLE STRING No Yes window.time The tumbling time period for which the window should hold events. INT LONG No No start.time This specifies an offset in milliseconds in order to start the window at a time different to the standard time. Timestamp of first event INT LONG Yes No Examples EXAMPLE 1 define stream CseEventStream (symbol string, price float, volume int) from CseEventStream#window.unique:timeBatch(symbol, 1 sec) select symbol, price, volume insert all events into OutputStream ; This window holds the latest unique events that arrive from the 'CseEventStream' at a given time, and returns all the events to the 'OutputStream' stream. It is updated every second based on the latest values for the 'symbol' attribute.","title":"timeBatch (Window)"},{"location":"docs/api/5.1.1/#timelengthbatch-window","text":"This is a batch or tumbling time length window that is updated with the latest events based on a unique key parameter. The window tumbles upon the elapse of the time window, or when a number of unique events have arrived. If a new event that arrives within the period of the window has a value for the key parameter which matches the value of an existing event, the existing event expires and it is replaced by the new event. Origin: siddhi-execution-unique:5.0.5 Syntax unique:timeLengthBatch( INT|LONG|FLOAT|BOOL|DOUBLE|STRING unique.key, INT|LONG window.time, INT window.length) unique:timeLengthBatch( INT|LONG|FLOAT|BOOL|DOUBLE|STRING unique.key, INT|LONG window.time, INT|LONG start.time, INT window.length) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic unique.key The attribute that should be checked for uniqueness. INT LONG FLOAT BOOL DOUBLE STRING No Yes window.time The sliding time period for which the window should hold the events. INT LONG No No start.time This specifies an offset in milliseconds in order to start the window at a time different to the standard time. Timestamp of first event INT LONG Yes No window.length The number of events the window should tumble. INT No No Examples EXAMPLE 1 define stream CseEventStream (symbol string, price float, volume int) from CseEventStream#window.unique:timeLengthBatch(symbol, 1 sec, 20) select symbol, price, volume insert all events into OutputStream; This window holds the latest unique events that arrive from the 'CseEventStream' at a given time, and returns all the events to the 'OutputStream' stream. It is updated every second based on the latest values for the 'symbol' attribute.","title":"timeLengthBatch (Window)"},{"location":"docs/api/5.1.1/#unitconversion","text":"","title":"Unitconversion"},{"location":"docs/api/5.1.1/#mmtokm-function","text":"This converts the input given in megameters into kilometers. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:MmTokm( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from megameters into kilometers. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:MmTokm(1) The megameter value '1' is converted into kilometers as '1000.0' .","title":"MmTokm (Function)"},{"location":"docs/api/5.1.1/#cmtoft-function","text":"This converts the input given in centimeters into feet. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:cmToft( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from centimeters into feet. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:cmToft(100) The centimeters value '100' is converted into feet as '3.280' .","title":"cmToft (Function)"},{"location":"docs/api/5.1.1/#cmtoin-function","text":"This converts the input given in centimeters into inches. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:cmToin( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from centimeters into inches. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:cmToin(100) Input centimeters value '100' is converted into inches as '39.37'.","title":"cmToin (Function)"},{"location":"docs/api/5.1.1/#cmtokm-function","text":"This converts the input value given in centimeters into kilometers. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:cmTokm( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from centimeters into kilometers. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:cmTokm(100) The centimeters value '100' is converted into kilometers as '0.001'.","title":"cmTokm (Function)"},{"location":"docs/api/5.1.1/#cmtom-function","text":"This converts the input given in centimeters into meters. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:cmTom( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from centimeters into meters. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:cmTom(100) The centimeters value '100' is converted into meters as '1.0' .","title":"cmTom (Function)"},{"location":"docs/api/5.1.1/#cmtomi-function","text":"This converts the input given in centimeters into miles. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:cmTomi( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from centimeters into miles. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:cmTomi(10000) The centimeters value '10000' is converted into miles as '0.062' .","title":"cmTomi (Function)"},{"location":"docs/api/5.1.1/#cmtomm-function","text":"This converts the input given in centimeters into millimeters. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:cmTomm( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from centimeters into millimeters. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:cmTomm(1) The centimeter value '1' is converted into millimeters as '10.0' .","title":"cmTomm (Function)"},{"location":"docs/api/5.1.1/#cmtonm-function","text":"This converts the input given in centimeters into nanometers. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:cmTonm( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from centimeters into nanometers. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:cmTonm(1) The centimeter value '1' is converted into nanometers as '10000000' .","title":"cmTonm (Function)"},{"location":"docs/api/5.1.1/#cmtoum-function","text":"This converts the input in centimeters into micrometers. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:cmToum( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from centimeters into micrometers. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:cmToum(100) The centimeters value '100' is converted into micrometers as '1000000.0' .","title":"cmToum (Function)"},{"location":"docs/api/5.1.1/#cmtoyd-function","text":"This converts the input given in centimeters into yards. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:cmToyd( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from centimeters into yards. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:cmToyd(1) The centimeter value '1' is converted into yards as '0.01' .","title":"cmToyd (Function)"},{"location":"docs/api/5.1.1/#dtoh-function","text":"This converts the input given in days into hours. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:dToh( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from days into hours. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:dToh(1) The day value '1' is converted into hours as '24.0'.","title":"dToh (Function)"},{"location":"docs/api/5.1.1/#gtokg-function","text":"This converts the input given in grams into kilograms. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:gTokg( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from grams into kilograms. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:gTokg(1000) The grams value '1000' is converted into kilogram as '1.0' .","title":"gTokg (Function)"},{"location":"docs/api/5.1.1/#gtomg-function","text":"This converts the input given in grams into milligrams. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:gTomg( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from grams into milligrams. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:gTomg(1) The gram value '1' is converted into milligrams as '1000.0' .","title":"gTomg (Function)"},{"location":"docs/api/5.1.1/#gtoug-function","text":"This converts the input given in grams into micrograms. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:gToug( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from grams into micrograms. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:gToug(1) The gram value '1' is converted into micrograms as '1000000.0' .","title":"gToug (Function)"},{"location":"docs/api/5.1.1/#htom-function","text":"This converts the input given in hours into minutes. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:hTom( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from hours into minutes. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:hTom(1) The hour value '1' is converted into minutes as '60.0' .","title":"hTom (Function)"},{"location":"docs/api/5.1.1/#htos-function","text":"This converts the input given in hours into seconds. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:hTos( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from hours into seconds. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:hTos(1) The hour value '1' is converted into seconds as '3600.0'.","title":"hTos (Function)"},{"location":"docs/api/5.1.1/#kgtolt-function","text":"This converts the input given in kilograms into imperial tons. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:kgToLT( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from kilograms into imperial tons. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:kgToLT(1000) The kilograms value '1000' is converted into imperial tons as '0.9842' .","title":"kgToLT (Function)"},{"location":"docs/api/5.1.1/#kgtost-function","text":"This converts the input given in kilograms into US tons. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:kgToST( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from kilograms into US tons. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:kgToST(1000) The kilograms value '1000 is converted into US tons as '1.10' .","title":"kgToST (Function)"},{"location":"docs/api/5.1.1/#kgtog-function","text":"This converts the input given in kilograms into grams. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:kgTog( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from kilograms into grams. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:kgTog(1) The kilogram value '1' is converted into grams as '1000'.","title":"kgTog (Function)"},{"location":"docs/api/5.1.1/#kgtolb-function","text":"This converts the input given in kilograms into pounds. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:kgTolb( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from kilograms into pounds. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:kgTolb(1) The kilogram value '1' is converted into pounds as '2.2' .","title":"kgTolb (Function)"},{"location":"docs/api/5.1.1/#kgtooz-function","text":"This converts the input given in kilograms into ounces. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:kgTooz( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from kilograms into ounces. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:kgTooz(1) The kilogram value '1' is converted into ounces as ' 35.274' .","title":"kgTooz (Function)"},{"location":"docs/api/5.1.1/#kgtost-function_1","text":"This converts the input given in kilograms into imperial stones. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:kgTost( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from kilograms into imperial stones. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:kgTost(1) The kilogram value '1' is converted into imperial stones as '0.157' .","title":"kgTost (Function)"},{"location":"docs/api/5.1.1/#kgtot-function","text":"This converts the input given in kilograms into tonnes. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:kgTot( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from kilograms into tonnes. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:kgTot(1) The kilogram value '1' is converted into tonnes as '0.001' .","title":"kgTot (Function)"},{"location":"docs/api/5.1.1/#kmtocm-function","text":"This converts the input given in kilometers into centimeters. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:kmTocm( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from kilometers into centimeters. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:kmTocm(1) The kilometer value '1' is converted into centimeters as '100000.0' .","title":"kmTocm (Function)"},{"location":"docs/api/5.1.1/#kmtoft-function","text":"This converts the input given in kilometers into feet. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:kmToft( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from kilometers into feet. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:kmToft(1) The kilometer value '1' is converted into feet as '3280.8' .","title":"kmToft (Function)"},{"location":"docs/api/5.1.1/#kmtoin-function","text":"This converts the input given in kilometers into inches. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:kmToin( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from kilometers into inches. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:kmToin(1) The kilometer value '1' is converted into inches as '39370.08' .","title":"kmToin (Function)"},{"location":"docs/api/5.1.1/#kmtom-function","text":"This converts the input given in kilometers into meters. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:kmTom( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from kilometers into meters. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:kmTom(1) The kilometer value '1' is converted into meters as '1000.0' .","title":"kmTom (Function)"},{"location":"docs/api/5.1.1/#kmtomi-function","text":"This converts the input given in kilometers into miles. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:kmTomi( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from kilometers into miles. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:kmTomi(1) The kilometer value '1' is converted into miles as '0.621' .","title":"kmTomi (Function)"},{"location":"docs/api/5.1.1/#kmtomm-function","text":"This converts the input given in kilometers into millimeters. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:kmTomm( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from kilometers into millimeters. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:kmTomm(1) The kilometer value '1' is converted into millimeters as '1000000.0' .","title":"kmTomm (Function)"},{"location":"docs/api/5.1.1/#kmtonm-function","text":"This converts the input given in kilometers into nanometers. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:kmTonm( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from kilometers into nanometers. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:kmTonm(1) The kilometer value '1' is converted into nanometers as '1000000000000.0' .","title":"kmTonm (Function)"},{"location":"docs/api/5.1.1/#kmtoum-function","text":"This converts the input given in kilometers into micrometers. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:kmToum( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from kilometers into micrometers. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:kmToum(1) The kilometer value '1' is converted into micrometers as '1000000000.0' .","title":"kmToum (Function)"},{"location":"docs/api/5.1.1/#kmtoyd-function","text":"This converts the input given in kilometers into yards. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:kmToyd( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from kilometers into yards. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:kmToyd(1) The kilometer value '1' is converted into yards as '1093.6' .","title":"kmToyd (Function)"},{"location":"docs/api/5.1.1/#ltom3-function","text":"This converts the input given in liters into cubic meters. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:lTom3( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from liters into cubic meters. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:lTom3(1000) The liters value '1000' is converted into cubic meters as '1' .","title":"lTom3 (Function)"},{"location":"docs/api/5.1.1/#ltoml-function","text":"This converts the input given in liters into milliliters. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:lToml( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from liters into milliliters. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:lToml(1) The liter value '1' is converted into milliliters as '1000.0' .","title":"lToml (Function)"},{"location":"docs/api/5.1.1/#m3tol-function","text":"This converts the input given in cubic meters into liters. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:m3Tol( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from meters into liters. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:m3Tol(1) The cubic meter value '1' is converted into liters as '1000.0' .","title":"m3Tol (Function)"},{"location":"docs/api/5.1.1/#mtocm-function","text":"This converts the input given in meters into centimeters. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:mTocm( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from meters into centimeters. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:mTocm(1) The meter value '1' is converted to centimeters as '100.0' .","title":"mTocm (Function)"},{"location":"docs/api/5.1.1/#mtoft-function","text":"This converts the input given in meters into feet. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:mToft( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from meters into feet. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:mToft(1) The meter value '1' is converted into feet as '3.280' .","title":"mToft (Function)"},{"location":"docs/api/5.1.1/#mtomm-function","text":"This converts the input given in meters into millimeters. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:mTomm( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from meters into millimeters. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:mTomm(1) The meter value '1' is converted into millimeters as '1000.0' .","title":"mTomm (Function)"},{"location":"docs/api/5.1.1/#mtos-function","text":"This converts the input given in minutes into seconds. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:mTos( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from minutes into seconds. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:mTos(1) The minute value '1' is converted into seconds as '60.0' .","title":"mTos (Function)"},{"location":"docs/api/5.1.1/#mtoyd-function","text":"This converts the input given in meters into yards. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:mToyd( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from meters into yards. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:mToyd(1) The meter value '1' is converted into yards as '1.093' .","title":"mToyd (Function)"},{"location":"docs/api/5.1.1/#mitokm-function","text":"This converts the input given in miles into kilometers. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:miTokm( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from miles into kilometers. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:miTokm(1) The mile value '1' is converted into kilometers as '1.6' .","title":"miTokm (Function)"},{"location":"docs/api/5.1.1/#mltol-function","text":"This converts the input given in milliliters into liters. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:mlTol( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from milliliters into liters. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:mlTol(1000) The milliliters value '1000' is converted into liters as '1'.","title":"mlTol (Function)"},{"location":"docs/api/5.1.1/#stoms-function","text":"This converts the input given in seconds into milliseconds. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:sToms( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from seconds into milliseconds. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:sToms(1) The second value '1' is converted into milliseconds as '1000.0' .","title":"sToms (Function)"},{"location":"docs/api/5.1.1/#stons-function","text":"This converts the input given in seconds into nanoseconds. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:sTons( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from seconds into nanoseconds. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:sTons(1) The second value '1' is converted into nanoseconds as '1000000000.0' .","title":"sTons (Function)"},{"location":"docs/api/5.1.1/#stous-function","text":"This converts the input given in seconds into microseconds. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:sTous( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from seconds into microseconds. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:sTous(1) The second value '1' is converted into microseconds as '1000000.0' .","title":"sTous (Function)"},{"location":"docs/api/5.1.1/#ttog-function","text":"This converts the input given in tonnes into grams. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:tTog( INT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from Tonnes into grams. INT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:tTog(1) The tonne value '1' is converted into grams as '1000000.0' .","title":"tTog (Function)"},{"location":"docs/api/5.1.1/#ttokg-function","text":"This converts the input given in tonnes into kilograms. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:tTokg( INT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from tonnes into kilograms. INT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:tTokg(inValue) The tonne value is converted into kilograms as '1000.0' .","title":"tTokg (Function)"},{"location":"docs/api/5.1.1/#ytod-function","text":"This converts the given input in years into days. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:yTod( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from years into days. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:yTod(1) The year value '1' is converted into days as '365.2525' .","title":"yTod (Function)"},{"location":"docs/api/latest/","text":"API Docs - v5.1.1 Core and (Aggregate Function) Returns the results of AND operation for all the events. Origin: siddhi-core:5.1.8 Syntax BOOL and( BOOL arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be AND operation. BOOL No Yes Examples EXAMPLE 1 from cscStream#window.lengthBatch(10) select and(isFraud) as isFraudTransaction insert into alertStream; This will returns the result for AND operation of isFraud values as a boolean value for event chunk expiry by window length batch. avg (Aggregate Function) Calculates the average for all the events. Origin: siddhi-core:5.1.8 Syntax DOUBLE avg( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that need to be averaged. INT LONG DOUBLE FLOAT No Yes Examples EXAMPLE 1 from fooStream#window.timeBatch select avg(temp) as avgTemp insert into barStream; avg(temp) returns the average temp value for all the events based on their arrival and expiry. count (Aggregate Function) Returns the count of all the events. Origin: siddhi-core:5.1.8 Syntax LONG count() LONG count( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg This function accepts one parameter. It can belong to any one of the available types. INT LONG DOUBLE FLOAT STRING BOOL OBJECT Yes Yes Examples EXAMPLE 1 from fooStream#window.timeBatch(10 sec) select count() as count insert into barStream; This will return the count of all the events for time batch in 10 seconds. distinctCount (Aggregate Function) This returns the count of distinct occurrences for a given arg. Origin: siddhi-core:5.1.8 Syntax LONG distinctCount( INT|LONG|DOUBLE|FLOAT|STRING arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The object for which the number of distinct occurences needs to be counted. INT LONG DOUBLE FLOAT STRING No Yes Examples EXAMPLE 1 from fooStream select distinctcount(pageID) as count insert into barStream; distinctcount(pageID) for the following output returns '3' when the available values are as follows. \"WEB_PAGE_1\" \"WEB_PAGE_1\" \"WEB_PAGE_2\" \"WEB_PAGE_3\" \"WEB_PAGE_1\" \"WEB_PAGE_2\" The three distinct occurences identified are 'WEB_PAGE_1', 'WEB_PAGE_2', and 'WEB_PAGE_3'. max (Aggregate Function) Returns the maximum value for all the events. Origin: siddhi-core:5.1.8 Syntax INT|LONG|DOUBLE|FLOAT max( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be compared to find the maximum value. INT LONG DOUBLE FLOAT No Yes Examples EXAMPLE 1 from fooStream#window.timeBatch(10 sec) select max(temp) as maxTemp insert into barStream; max(temp) returns the maximum temp value recorded for all the events based on their arrival and expiry. maxForever (Aggregate Function) This is the attribute aggregator to store the maximum value for a given attribute throughout the lifetime of the query regardless of any windows in-front. Origin: siddhi-core:5.1.8 Syntax INT|LONG|DOUBLE|FLOAT maxForever( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be compared to find the maximum value. INT LONG DOUBLE FLOAT No Yes Examples EXAMPLE 1 from inputStream select maxForever(temp) as max insert into outputStream; maxForever(temp) returns the maximum temp value recorded for all the events throughout the lifetime of the query. min (Aggregate Function) Returns the minimum value for all the events. Origin: siddhi-core:5.1.8 Syntax INT|LONG|DOUBLE|FLOAT min( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be compared to find the minimum value. INT LONG DOUBLE FLOAT No Yes Examples EXAMPLE 1 from inputStream select min(temp) as minTemp insert into outputStream; min(temp) returns the minimum temp value recorded for all the events based on their arrival and expiry. minForever (Aggregate Function) This is the attribute aggregator to store the minimum value for a given attribute throughout the lifetime of the query regardless of any windows in-front. Origin: siddhi-core:5.1.8 Syntax INT|LONG|DOUBLE|FLOAT minForever( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be compared to find the minimum value. INT LONG DOUBLE FLOAT No Yes Examples EXAMPLE 1 from inputStream select minForever(temp) as max insert into outputStream; minForever(temp) returns the minimum temp value recorded for all the events throughoutthe lifetime of the query. or (Aggregate Function) Returns the results of OR operation for all the events. Origin: siddhi-core:5.1.8 Syntax BOOL or( BOOL arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be OR operation. BOOL No Yes Examples EXAMPLE 1 from cscStream#window.lengthBatch(10) select or(isFraud) as isFraudTransaction insert into alertStream; This will returns the result for OR operation of isFraud values as a boolean value for event chunk expiry by window length batch. stdDev (Aggregate Function) Returns the calculated standard deviation for all the events. Origin: siddhi-core:5.1.8 Syntax DOUBLE stdDev( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that should be used to calculate the standard deviation. INT LONG DOUBLE FLOAT No Yes Examples EXAMPLE 1 from inputStream select stddev(temp) as stdTemp insert into outputStream; stddev(temp) returns the calculated standard deviation of temp for all the events based on their arrival and expiry. sum (Aggregate Function) Returns the sum for all the events. Origin: siddhi-core:5.1.8 Syntax LONG|DOUBLE sum( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be summed. INT LONG DOUBLE FLOAT No Yes Examples EXAMPLE 1 from inputStream select sum(volume) as sumOfVolume insert into outputStream; This will returns the sum of volume values as a long value for each event arrival and expiry. unionSet (Aggregate Function) Union multiple sets. This attribute aggregator maintains a union of sets. The given input set is put into the union set and the union set is returned. Origin: siddhi-core:5.1.8 Syntax OBJECT unionSet( OBJECT set) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic set The java.util.Set object that needs to be added into the union set. OBJECT No Yes Examples EXAMPLE 1 from stockStream select createSet(symbol) as initialSet insert into initStream from initStream#window.timeBatch(10 sec) select unionSet(initialSet) as distinctSymbols insert into distinctStockStream; distinctStockStream will return the set object which contains the distinct set of stock symbols received during a sliding window of 10 seconds. UUID (Function) Generates a UUID (Universally Unique Identifier). Origin: siddhi-core:5.1.8 Syntax STRING UUID() Examples EXAMPLE 1 from TempStream select convert(roomNo, 'string') as roomNo, temp, UUID() as messageID insert into RoomTempStream; This will converts a room number to string, introducing a message ID to each event asUUID() returns a34eec40-32c2-44fe-8075-7f4fde2e2dd8 from TempStream select convert(roomNo, 'string') as roomNo, temp, UUID() as messageID insert into RoomTempStream; cast (Function) Converts the first parameter according to the cast.to parameter. Incompatible arguments cause Class Cast exceptions if further processed. This function is used with map extension that returns attributes of the object type. You can use this function to cast the object to an accurate and concrete type. Origin: siddhi-core:5.1.8 Syntax INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT cast( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT to.be.caster, STRING cast.to) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic to.be.caster This specifies the attribute to be casted. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No Yes cast.to A string constant parameter expressing the cast to type using one of the following strings values: int, long, float, double, string, bool. STRING No Yes Examples EXAMPLE 1 from fooStream select symbol as name, cast(temp, 'double') as temp insert into barStream; This will cast the fooStream temp field value into 'double' format. coalesce (Function) Returns the value of the first input parameter that is not null, and all input parameters have to be on the same type. Origin: siddhi-core:5.1.8 Syntax INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT coalesce( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg, INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT ...) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg This function accepts one or more parameters. They can belong to any one of the available types. All the specified parameters should be of the same type. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No Yes Examples EXAMPLE 1 from fooStream select coalesce('123', null, '789') as value insert into barStream; This will returns first null value 123. EXAMPLE 2 from fooStream select coalesce(null, 76, 567) as value insert into barStream; This will returns first null value 76. EXAMPLE 3 from fooStream select coalesce(null, null, null) as value insert into barStream; This will returns null as there are no notnull values. convert (Function) Converts the first input parameter according to the convertedTo parameter. Origin: siddhi-core:5.1.8 Syntax INT|LONG|DOUBLE|FLOAT|STRING|BOOL convert( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT to.be.converted, STRING converted.to) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic to.be.converted This specifies the value to be converted. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No Yes converted.to A string constant parameter to which type the attribute need to be converted using one of the following strings values: 'int', 'long', 'float', 'double', 'string', 'bool'. STRING No Yes Examples EXAMPLE 1 from fooStream select convert(temp, 'double') as temp insert into barStream; This will convert fooStream temp value into 'double'. EXAMPLE 2 from fooStream select convert(temp, 'int') as temp insert into barStream; This will convert fooStream temp value into 'int' (value = \"convert(45.9, 'int') returns 46\"). createSet (Function) Includes the given input parameter in a java.util.HashSet and returns the set. Origin: siddhi-core:5.1.8 Syntax OBJECT createSet( INT|LONG|DOUBLE|FLOAT|STRING|BOOL input) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input The input that needs to be added into the set. INT LONG DOUBLE FLOAT STRING BOOL No Yes Examples EXAMPLE 1 from stockStream select createSet(symbol) as initialSet insert into initStream; For every incoming stockStream event, the initStream stream will produce a set object having only one element: the symbol in the incoming stockStream. currentTimeMillis (Function) Returns the current timestamp of siddhi application in milliseconds. Origin: siddhi-core:5.1.8 Syntax LONG currentTimeMillis() Examples EXAMPLE 1 from fooStream select symbol as name, currentTimeMillis() as eventTimestamp insert into barStream; This will extract current siddhi application timestamp. default (Function) Checks if the 'attribute' parameter is null and if so returns the value of the 'default' parameter Origin: siddhi-core:5.1.8 Syntax INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT default( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT attribute, INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT default) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic attribute The attribute that could be null. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No Yes default The default value that will be used when 'attribute' parameter is null INT LONG DOUBLE FLOAT STRING BOOL OBJECT No Yes Examples EXAMPLE 1 from TempStream select default(temp, 0.0) as temp, roomNum insert into StandardTempStream; This will replace TempStream's temp attribute with default value if the temp is null. eventTimestamp (Function) Returns the timestamp of the processed event. Origin: siddhi-core:5.1.8 Syntax LONG eventTimestamp() Examples EXAMPLE 1 from fooStream select symbol as name, eventTimestamp() as eventTimestamp insert into barStream; This will extract current events timestamp. ifThenElse (Function) Evaluates the 'condition' parameter and returns value of the 'if.expression' parameter if the condition is true, or returns value of the 'else.expression' parameter if the condition is false. Here both 'if.expression' and 'else.expression' should be of the same type. Origin: siddhi-core:5.1.8 Syntax INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT ifThenElse( BOOL condition, INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT if.expression, INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT else.expression) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic condition This specifies the if then else condition value. BOOL No Yes if.expression This specifies the value to be returned if the value of the condition parameter is true. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No Yes else.expression This specifies the value to be returned if the value of the condition parameter is false. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No Yes Examples EXAMPLE 1 @info(name = 'query1') from sensorEventStream select sensorValue, ifThenElse(sensorValue 35,'High','Low') as status insert into outputStream; This will returns High if sensorValue = 50. EXAMPLE 2 @info(name = 'query1') from sensorEventStream select sensorValue, ifThenElse(voltage 5, 0, 1) as status insert into outputStream; This will returns 1 if voltage= 12. EXAMPLE 3 @info(name = 'query1') from userEventStream select userName, ifThenElse(password == 'admin', true, false) as passwordState insert into outputStream; This will returns passwordState as true if password = admin. instanceOfBoolean (Function) Checks whether the parameter is an instance of Boolean or not. Origin: siddhi-core:5.1.8 Syntax BOOL instanceOfBoolean( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No Yes Examples EXAMPLE 1 from fooStream select instanceOfBoolean(switchState) as state insert into barStream; This will return true if the value of switchState is true. EXAMPLE 2 from fooStream select instanceOfBoolean(value) as state insert into barStream; if the value = 32 then this will returns false as the value is not an instance of the boolean. instanceOfDouble (Function) Checks whether the parameter is an instance of Double or not. Origin: siddhi-core:5.1.8 Syntax BOOL instanceOfDouble( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No Yes Examples EXAMPLE 1 from fooStream select instanceOfDouble(value) as state insert into barStream; This will return true if the value field format is double ex : 56.45. EXAMPLE 2 from fooStream select instanceOfDouble(switchState) as state insert into barStream; if the switchState = true then this will returns false as the value is not an instance of the double. instanceOfFloat (Function) Checks whether the parameter is an instance of Float or not. Origin: siddhi-core:5.1.8 Syntax BOOL instanceOfFloat( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No Yes Examples EXAMPLE 1 from fooStream select instanceOfFloat(value) as state insert into barStream; This will return true if the value field format is float ex : 56.45f. EXAMPLE 2 from fooStream select instanceOfFloat(switchState) as state insert into barStream; if the switchState = true then this will returns false as the value is an instance of the boolean not a float. instanceOfInteger (Function) Checks whether the parameter is an instance of Integer or not. Origin: siddhi-core:5.1.8 Syntax BOOL instanceOfInteger( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No Yes Examples EXAMPLE 1 from fooStream select instanceOfInteger(value) as state insert into barStream; This will return true if the value field format is integer. EXAMPLE 2 from fooStream select instanceOfInteger(switchState) as state insert into barStream; if the switchState = true then this will returns false as the value is an instance of the boolean not a long. instanceOfLong (Function) Checks whether the parameter is an instance of Long or not. Origin: siddhi-core:5.1.8 Syntax BOOL instanceOfLong( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No Yes Examples EXAMPLE 1 from fooStream select instanceOfLong(value) as state insert into barStream; This will return true if the value field format is long ex : 56456l. EXAMPLE 2 from fooStream select instanceOfLong(switchState) as state insert into barStream; if the switchState = true then this will returns false as the value is an instance of the boolean not a long. instanceOfString (Function) Checks whether the parameter is an instance of String or not. Origin: siddhi-core:5.1.8 Syntax BOOL instanceOfString( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No Yes Examples EXAMPLE 1 from fooStream select instanceOfString(value) as state insert into barStream; This will return true if the value field format is string ex : 'test'. EXAMPLE 2 from fooStream select instanceOfString(switchState) as state insert into barStream; if the switchState = true then this will returns false as the value is an instance of the boolean not a string. maximum (Function) Returns the maximum value of the input parameters. Origin: siddhi-core:5.1.8 Syntax INT|LONG|DOUBLE|FLOAT maximum( INT|LONG|DOUBLE|FLOAT arg, INT|LONG|DOUBLE|FLOAT ...) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg This function accepts one or more parameters. They can belong to any one of the available types. All the specified parameters should be of the same type. INT LONG DOUBLE FLOAT No Yes Examples EXAMPLE 1 @info(name = 'query1') from inputStream select maximum(price1, price2, price3) as max insert into outputStream; This will returns the maximum value of the input parameters price1, price2, price3. minimum (Function) Returns the minimum value of the input parameters. Origin: siddhi-core:5.1.8 Syntax INT|LONG|DOUBLE|FLOAT minimum( INT|LONG|DOUBLE|FLOAT arg, INT|LONG|DOUBLE|FLOAT ...) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg This function accepts one or more parameters. They can belong to any one of the available types. All the specified parameters should be of the same type. INT LONG DOUBLE FLOAT No Yes Examples EXAMPLE 1 @info(name = 'query1') from inputStream select maximum(price1, price2, price3) as max insert into outputStream; This will returns the minimum value of the input parameters price1, price2, price3. sizeOfSet (Function) Returns the size of an object of type java.util.Set. Origin: siddhi-core:5.1.8 Syntax INT sizeOfSet( OBJECT set) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic set The set object. This parameter should be of type java.util.Set. A set object may be created by the 'set' attribute aggregator in Siddhi. OBJECT No Yes Examples EXAMPLE 1 from stockStream select initSet(symbol) as initialSet insert into initStream; ;from initStream#window.timeBatch(10 sec) select union(initialSet) as distinctSymbols insert into distinctStockStream; from distinctStockStream select sizeOfSet(distinctSymbols) sizeOfSymbolSet insert into sizeStream; The sizeStream stream will output the number of distinct stock symbols received during a sliding window of 10 seconds. pol2Cart (Stream Function) The pol2Cart function calculating the cartesian coordinates x y for the given theta, rho coordinates and adding them as new attributes to the existing events. Origin: siddhi-core:5.1.8 Syntax pol2Cart( DOUBLE theta, DOUBLE rho) pol2Cart( DOUBLE theta, DOUBLE rho, DOUBLE z) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic theta The theta value of the coordinates. DOUBLE No Yes rho The rho value of the coordinates. DOUBLE No Yes z z value of the cartesian coordinates. If z value is not given, drop the third parameter of the output. DOUBLE Yes Yes Examples EXAMPLE 1 from PolarStream#pol2Cart(theta, rho) select x, y insert into outputStream ; This will return cartesian coordinates (4.99953024681082, 0.06853693328228748) for theta: 0.7854 and rho: 5. EXAMPLE 2 from PolarStream#pol2Cart(theta, rho, 3.4) select x, y, z insert into outputStream ; This will return cartesian coordinates (4.99953024681082, 0.06853693328228748, 3.4)for theta: 0.7854 and rho: 5 and z: 3.4. log (Stream Processor) Logs the message on the given priority with or without the processed event. Origin: siddhi-core:5.1.8 Syntax log() log( STRING log.message) log( BOOL is.event.logged) log( STRING log.message, BOOL is.event.logged) log( STRING priority, STRING log.message) log( STRING priority, STRING log.message, BOOL is.event.logged) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic priority The priority/type of this log message (INFO, DEBUG, WARN, FATAL, ERROR, OFF, TRACE). INFO STRING Yes No log.message This message will be logged. : STRING Yes Yes is.event.logged To log the processed event. true BOOL Yes No Examples EXAMPLE 1 from FooStream#log() select * insert into BarStream; Logs events with SiddhiApp name message prefix on default log level INFO. EXAMPLE 2 from FooStream#log(\"Sample Event :\") select * insert into BarStream; Logs events with the message prefix \"Sample Event :\" on default log level INFO. EXAMPLE 3 from FooStream#log(\"DEBUG\", \"Sample Event :\", true) select * insert into BarStream; Logs events with the message prefix \"Sample Event :\" on log level DEBUG. EXAMPLE 4 from FooStream#log(\"Event Arrived\", false) select * insert into BarStream; For each event logs a message \"Event Arrived\" on default log level INFO. EXAMPLE 5 from FooStream#log(\"Sample Event :\", true) select * insert into BarStream; Logs events with the message prefix \"Sample Event :\" on default log level INFO. EXAMPLE 6 from FooStream#log(true) select * insert into BarStream; Logs events with on default log level INFO. batch (Window) A window that holds an incoming events batch. When a new set of events arrives, the previously arrived old events will be expired. Batch window can be used to aggregate events that comes in batches. If it has the parameter length specified, then batch window process the batch as several chunks. Origin: siddhi-core:5.1.8 Syntax batch() batch( INT window.length) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.length The length of a chunk If length value was not given it assign 0 as length and process the whole batch as once INT Yes No Examples EXAMPLE 1 define stream consumerItemStream (itemId string, price float) from consumerItemStream#window.batch() select price, str:groupConcat(itemId) as itemIds group by price insert into outputStream; This will output comma separated items IDs that have the same price for each incoming batch of events. cron (Window) This window outputs the arriving events as and when they arrive, and resets (expires) the window periodically based on the given cron expression. Origin: siddhi-core:5.1.8 Syntax cron( STRING cron.expression) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic cron.expression The cron expression that resets the window. STRING No No Examples EXAMPLE 1 define stream InputEventStream (symbol string, price float, volume int); @info(name = 'query1') from InputEventStream#cron('*/5 * * * * ?') select symbol, sum(price) as totalPrice insert into OutputStream; This let the totalPrice to gradually increase and resets to zero as a batch every 5 seconds. EXAMPLE 2 define stream StockEventStream (symbol string, price float, volume int) define window StockEventWindow (symbol string, price float, volume int) cron('*/5 * * * * ?'); @info(name = 'query0') from StockEventStream insert into StockEventWindow; @info(name = 'query1') from StockEventWindow select symbol, sum(price) as totalPrice insert into OutputStream ; The defined window will let the totalPrice to gradually increase and resets to zero as a batch every 5 seconds. delay (Window) A delay window holds events for a specific time period that is regarded as a delay period before processing them. Origin: siddhi-core:5.1.8 Syntax delay( INT|LONG|TIME window.delay) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.delay The time period (specified in sec, min, ms) for which the window should delay the events. INT LONG TIME No No Examples EXAMPLE 1 define window delayWindow(symbol string, volume int) delay(1 hour); define stream PurchaseStream(symbol string, volume int); define stream DeliveryStream(symbol string); define stream OutputStream(symbol string); @info(name='query1') from PurchaseStream select symbol, volume insert into delayWindow; @info(name='query2') from delayWindow join DeliveryStream on delayWindow.symbol == DeliveryStream.symbol select delayWindow.symbol insert into OutputStream; In this example, purchase events that arrive in the 'PurchaseStream' stream are directed to a delay window. At any given time, this delay window holds purchase events that have arrived within the last hour. These purchase events in the window are matched by the 'symbol' attribute, with delivery events that arrive in the 'DeliveryStream' stream. This monitors whether the delivery of products is done with a minimum delay of one hour after the purchase. externalTime (Window) A sliding time window based on external time. It holds events that arrived during the last windowTime period from the external timestamp, and gets updated on every monotonically increasing timestamp. Origin: siddhi-core:5.1.8 Syntax externalTime( LONG timestamp, INT|LONG|TIME window.time) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic timestamp The time which the window determines as current time and will act upon. The value of this parameter should be monotonically increasing. LONG No Yes window.time The sliding time period for which the window should hold events. INT LONG TIME No No Examples EXAMPLE 1 define window cseEventWindow (symbol string, price float, volume int) externalTime(eventTime, 20 sec) output expired events; @info(name = 'query0') from cseEventStream insert into cseEventWindow; @info(name = 'query1') from cseEventWindow select symbol, sum(price) as price insert expired events into outputStream ; processing events arrived within the last 20 seconds from the eventTime and output expired events. externalTimeBatch (Window) A batch (tumbling) time window based on external time, that holds events arrived during windowTime periods, and gets updated for every windowTime. Origin: siddhi-core:5.1.8 Syntax externalTimeBatch( LONG timestamp, INT|LONG|TIME window.time) externalTimeBatch( LONG timestamp, INT|LONG|TIME window.time, INT|LONG|TIME start.time) externalTimeBatch( LONG timestamp, INT|LONG|TIME window.time, INT|LONG|TIME start.time, INT|LONG|TIME timeout) externalTimeBatch( LONG timestamp, INT|LONG|TIME window.time, INT|LONG|TIME start.time, INT|LONG|TIME timeout, BOOL replace.with.batchtime) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic timestamp The time which the window determines as current time and will act upon. The value of this parameter should be monotonically increasing. LONG No Yes window.time The batch time period for which the window should hold events. INT LONG TIME No No start.time User defined start time. This could either be a constant (of type int, long or time) or an attribute of the corresponding stream (of type long). If an attribute is provided, initial value of attribute would be considered as startTime. Timestamp of first event INT LONG TIME Yes Yes timeout Time to wait for arrival of new event, before flushing and giving output for events belonging to a specific batch. System waits till an event from next batch arrives to flush current batch INT LONG TIME Yes No replace.with.batchtime This indicates to replace the expired event timeStamp as the batch end timeStamp System waits till an event from next batch arrives to flush current batch BOOL Yes No Examples EXAMPLE 1 define window cseEventWindow (symbol string, price float, volume int) externalTimeBatch(eventTime, 1 sec) output expired events; @info(name = 'query0') from cseEventStream insert into cseEventWindow; @info(name = 'query1') from cseEventWindow select symbol, sum(price) as price insert expired events into outputStream ; This will processing events that arrive every 1 seconds from the eventTime. EXAMPLE 2 define window cseEventWindow (symbol string, price float, volume int) externalTimeBatch(eventTime, 20 sec, 0) output expired events; This will processing events that arrive every 1 seconds from the eventTime. Starts on 0 th millisecond of an hour. EXAMPLE 3 define window cseEventWindow (symbol string, price float, volume int) externalTimeBatch(eventTime, 2 sec, eventTimestamp, 100) output expired events; This will processing events that arrive every 2 seconds from the eventTim. Considers the first event's eventTimestamp value as startTime. Waits 100 milliseconds for the arrival of a new event before flushing current batch. frequent (Window) Deprecated This window returns the latest events with the most frequently occurred value for a given attribute(s). Frequency calculation for this window processor is based on Misra-Gries counting algorithm. Origin: siddhi-core:5.1.8 Syntax frequent( INT event.count) frequent( INT event.count, STRING attribute) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic event.count The number of most frequent events to be emitted to the stream. INT No No attribute The attributes to group the events. If no attributes are given, the concatenation of all the attributes of the event is considered. The concatenation of all the attributes of the event is considered. STRING Yes Yes Examples EXAMPLE 1 @info(name = 'query1') from purchase[price = 30]#window.frequent(2) select cardNo, price insert all events into PotentialFraud; This will returns the 2 most frequent events. EXAMPLE 2 @info(name = 'query1') from purchase[price = 30]#window.frequent(2, cardNo) select cardNo, price insert all events into PotentialFraud; This will returns the 2 latest events with the most frequently appeared card numbers. length (Window) A sliding length window that holds the last 'window.length' events at a given time, and gets updated for each arrival and expiry. Origin: siddhi-core:5.1.8 Syntax length( INT window.length) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.length The number of events that should be included in a sliding length window. INT No No Examples EXAMPLE 1 define window StockEventWindow (symbol string, price float, volume int) length(10) output all events; @info(name = 'query0') from StockEventStream insert into StockEventWindow; @info(name = 'query1') from StockEventWindow select symbol, sum(price) as price insert all events into outputStream ; This will process last 10 events in a sliding manner. lengthBatch (Window) A batch (tumbling) length window that holds and process a number of events as specified in the window.length. Origin: siddhi-core:5.1.8 Syntax lengthBatch( INT window.length) lengthBatch( INT window.length, BOOL stream.current.event) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.length The number of events the window should tumble. INT No No stream.current.event Let the window stream the current events out as and when they arrive to the window while expiring them in batches. false BOOL Yes No Examples EXAMPLE 1 define stream InputEventStream (symbol string, price float, volume int); @info(name = 'query1') from InputEventStream#lengthBatch(10) select symbol, sum(price) as price insert into OutputStream; This collect and process 10 events as a batch and output them. EXAMPLE 2 define stream InputEventStream (symbol string, price float, volume int); @info(name = 'query1') from InputEventStream#lengthBatch(10, true) select symbol, sum(price) as sumPrice insert into OutputStream; This window sends the arriving events directly to the output letting the sumPrice to increase gradually, after every 10 events it clears the window as a batch and resets the sumPrice to zero. EXAMPLE 3 define stream InputEventStream (symbol string, price float, volume int); define window StockEventWindow (symbol string, price float, volume int) lengthBatch(10) output all events; @info(name = 'query0') from InputEventStream insert into StockEventWindow; @info(name = 'query1') from StockEventWindow select symbol, sum(price) as price insert all events into OutputStream ; This uses an defined window to process 10 events as a batch and output all events. lossyFrequent (Window) Deprecated This window identifies and returns all the events of which the current frequency exceeds the value specified for the supportThreshold parameter. Origin: siddhi-core:5.1.8 Syntax lossyFrequent( DOUBLE support.threshold) lossyFrequent( DOUBLE support.threshold, DOUBLE error.bound) lossyFrequent( DOUBLE support.threshold, DOUBLE error.bound, STRING attribute) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic support.threshold The support threshold value. DOUBLE No No error.bound The error bound value. support.threshold /10 DOUBLE Yes No attribute The attributes to group the events. If no attributes are given, the concatenation of all the attributes of the event is considered. The concatenation of all the attributes of the event is considered. STRING Yes Yes Examples EXAMPLE 1 define stream purchase (cardNo string, price float); define window purchaseWindow (cardNo string, price float) lossyFrequent(0.1, 0.01); @info(name = 'query0') from purchase[price = 30] insert into purchaseWindow; @info(name = 'query1') from purchaseWindow select cardNo, price insert all events into PotentialFraud; lossyFrequent(0.1, 0.01) returns all the events of which the current frequency exceeds 0.1, with an error bound of 0.01. EXAMPLE 2 define stream purchase (cardNo string, price float); define window purchaseWindow (cardNo string, price float) lossyFrequent(0.3, 0.05, cardNo); @info(name = 'query0') from purchase[price = 30] insert into purchaseWindow; @info(name = 'query1') from purchaseWindow select cardNo, price insert all events into PotentialFraud; lossyFrequent(0.3, 0.05, cardNo) returns all the events of which the cardNo attributes frequency exceeds 0.3, with an error bound of 0.05. session (Window) This is a session window that holds events that belong to a specific session. The events that belong to a specific session are identified by a grouping attribute (i.e., a session key). A session gap period is specified to determine the time period after which the session is considered to be expired. A new event that arrives with a specific value for the session key is matched with the session window with the same session key. There can be out of order and late arrival of events, these events can arrive after the session is expired, to include those events to the matching session key specify a latency time period that is less than the session gap period.To have aggregate functions with session windows, the events need to be grouped by the session key via a 'group by' clause. Origin: siddhi-core:5.1.8 Syntax session( INT|LONG|TIME window.session) session( INT|LONG|TIME window.session, STRING window.key) session( INT|LONG|TIME window.session, STRING window.key, INT|LONG|TIME window.allowed.latency) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.session The time period for which the session considered is valid. This is specified in seconds, minutes, or milliseconds (i.e., 'min', 'sec', or 'ms'. INT LONG TIME No No window.key The grouping attribute for events. default-key STRING Yes Yes window.allowed.latency This specifies the time period for which the session window is valid after the expiration of the session. The time period specified here should be less than the session time gap (which is specified via the 'window.session' parameter). 0 INT LONG TIME Yes No Examples EXAMPLE 1 define stream PurchaseEventStream (user string, item_number int, price float, quantity int); @info(name='query0) from PurchaseEventStream#window.session(5 sec, user, 2 sec) select * insert all events into OutputStream; This query processes events that arrive at the PurchaseEvent input stream. The 'user' attribute is the session key, and the session gap is 5 seconds. '2 sec' is specified as the allowed latency. Therefore, events with the matching user name that arrive 2 seconds after the expiration of the session are also considered when performing aggregations for the session identified by the given user name. sort (Window) This window holds a batch of events that equal the number specified as the windowLength and sorts them in the given order. Origin: siddhi-core:5.1.8 Syntax sort( INT window.length, STRING|DOUBLE|INT|LONG|FLOAT|LONG attribute) sort( INT window.length, STRING|DOUBLE|INT|LONG|FLOAT|LONG attribute, STRING order, STRING ...) sort( INT window.length, STRING|DOUBLE|INT|LONG|FLOAT|LONG attribute, STRING order, STRING|DOUBLE|INT|LONG|FLOAT|LONG attribute, STRING|DOUBLE|INT|LONG|FLOAT|LONG ...) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.length The size of the window length. INT No No attribute The attribute that should be checked for the order. The concatenation of all the attributes of the event is considered. STRING DOUBLE INT LONG FLOAT LONG No Yes order The order define as \"asc\" or \"desc\". asc STRING Yes No Examples EXAMPLE 1 define stream cseEventStream (symbol string, price float, volume long); define window cseEventWindow (symbol string, price float, volume long) sort(2,volume, 'asc'); @info(name = 'query0') from cseEventStream insert into cseEventWindow; @info(name = 'query1') from cseEventWindow select volume insert all events into outputStream ; sort(5, price, 'asc') keeps the events sorted by price in the ascending order. Therefore, at any given time, the window contains the 5 lowest prices. time (Window) A sliding time window that holds events that arrived during the last windowTime period at a given time, and gets updated for each event arrival and expiry. Origin: siddhi-core:5.1.8 Syntax time( INT|LONG|TIME window.time) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.time The sliding time period for which the window should hold events. INT LONG TIME No No Examples EXAMPLE 1 define window cseEventWindow (symbol string, price float, volume int) time(20) output all events; @info(name = 'query0') from cseEventStream insert into cseEventWindow; @info(name = 'query1') from cseEventWindow select symbol, sum(price) as price insert all events into outputStream ; This will processing events that arrived within the last 20 milliseconds. timeBatch (Window) A batch (tumbling) time window that holds and process events that arrive during 'window.time' period as a batch. Origin: siddhi-core:5.1.8 Syntax timeBatch( INT|LONG|TIME window.time) timeBatch( INT|LONG|TIME window.time, INT|LONG start.time) timeBatch( INT|LONG|TIME window.time, BOOL stream.current.event) timeBatch( INT|LONG|TIME window.time, INT|LONG start.time, BOOL stream.current.event) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.time The batch time period in which the window process the events. INT LONG TIME No No start.time This specifies an offset in milliseconds in order to start the window at a time different to the standard time. Timestamp of first event INT LONG Yes No stream.current.event Let the window stream the current events out as and when they arrive to the window while expiring them in batches. false BOOL Yes No Examples EXAMPLE 1 define stream InputEventStream (symbol string, price float, volume int); @info(name = 'query1') from InputEventStream#timeBatch(20 sec) select symbol, sum(price) as price insert into OutputStream; This collect and process incoming events as a batch every 20 seconds and output them. EXAMPLE 2 define stream InputEventStream (symbol string, price float, volume int); @info(name = 'query1') from InputEventStream#timeBatch(20 sec, true) select symbol, sum(price) as sumPrice insert into OutputStream; This window sends the arriving events directly to the output letting the sumPrice to increase gradually and on every 20 second interval it clears the window as a batch resetting the sumPrice to zero. EXAMPLE 3 define stream InputEventStream (symbol string, price float, volume int); define window StockEventWindow (symbol string, price float, volume int) timeBatch(20 sec) output all events; @info(name = 'query0') from InputEventStream insert into StockEventWindow; @info(name = 'query1') from StockEventWindow select symbol, sum(price) as price insert all events into OutputStream ; This uses an defined window to process events arrived every 20 seconds as a batch and output all events. timeLength (Window) A sliding time window that, at a given time holds the last window.length events that arrived during last window.time period, and gets updated for every event arrival and expiry. Origin: siddhi-core:5.1.8 Syntax timeLength( INT|LONG|TIME window.time, INT window.length) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.time The sliding time period for which the window should hold events. INT LONG TIME No No window.length The number of events that should be be included in a sliding length window.. INT No No Examples EXAMPLE 1 define stream cseEventStream (symbol string, price float, volume int); define window cseEventWindow (symbol string, price float, volume int) timeLength(2 sec, 10); @info(name = 'query0') from cseEventStream insert into cseEventWindow; @info(name = 'query1') from cseEventWindow select symbol, price, volume insert all events into outputStream; window.timeLength(2 sec, 10) holds the last 10 events that arrived during last 2 seconds and gets updated for every event arrival and expiry. Js eval (Function) This extension evaluates a given string and return the output according to the user specified data type. Origin: siddhi-script-js:5.0.2 Syntax INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT js:eval( STRING expression, STRING return.type) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic expression Any single line js expression or function. STRING No Yes return.type The return type of the evaluated expression. Supported types are int|long|float|double|bool|string. STRING No No Examples EXAMPLE 1 js:eval(\"700 800\", 'bool') In this example, the expression 700 800 will be evaluated and return result as false because user specified return type as bool. Json group (Aggregate Function) This function aggregates the JSON elements and returns a JSON object by adding enclosing.element if it is provided. If enclosing.element is not provided it aggregate the JSON elements returns a JSON array. Origin: siddhi-execution-json:2.0.4 Syntax OBJECT json:group( STRING|OBJECT json) OBJECT json:group( STRING|OBJECT json, BOOL distinct) OBJECT json:group( STRING|OBJECT json, STRING enclosing.element) OBJECT json:group( STRING|OBJECT json, STRING enclosing.element, BOOL distinct) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json The JSON element that needs to be aggregated. STRING OBJECT No Yes enclosing.element The JSON element used to enclose the aggregated JSON elements. EMPTY_STRING STRING Yes Yes distinct This is used to only have distinct JSON elements in the concatenated JSON object/array that is returned. false BOOL Yes Yes Examples EXAMPLE 1 from InputStream#window.length(5) select json:group(\"json\") as groupedJSONArray input OutputStream; When we input events having values for the json as {\"date\":\"2013-11-19\",\"time\":\"10:30\"} and {\"date\":\"2013-11-19\",\"time\":\"12:20\"} , it returns [{\"date\":\"2013-11-19\",\"time\":\"10:30\"}{\"date\":\"2013-11-19\",\"time\":\"12:20\"}] to the 'OutputStream'. EXAMPLE 2 from InputStream#window.length(5) select json:group(\"json\", true) as groupedJSONArray input OutputStream; When we input events having values for the json as {\"date\":\"2013-11-19\",\"time\":\"10:30\"} and {\"date\":\"2013-11-19\",\"time\":\"10:30\"} , it returns [{\"date\":\"2013-11-19\",\"time\":\"10:30\"}] to the 'OutputStream'. EXAMPLE 3 from InputStream#window.length(5) select json:group(\"json\", \"result\") as groupedJSONArray input OutputStream; When we input events having values for the json as {\"date\":\"2013-11-19\",\"time\":\"10:30\"} and {\"date\":\"2013-11-19\",\"time\":\"12:20\"} , it returns {\"result\":[{\"date\":\"2013-11-19\",\"time\":\"10:30\"},{\"date\":\"2013-11-19\",\"time\":\"12:20\"}} to the 'OutputStream'. EXAMPLE 4 from InputStream#window.length(5) select json:group(\"json\", \"result\", true) as groupedJSONArray input OutputStream; When we input events having values for the json as {\"date\":\"2013-11-19\",\"time\":\"10:30\"} and {\"date\":\"2013-11-19\",\"time\":\"10:30\"} , it returns {\"result\":[{\"date\":\"2013-11-19\",\"time\":\"10:30\"}]} to the 'OutputStream'. groupAsObject (Aggregate Function) This function aggregates the JSON elements and returns a JSON object by adding enclosing.element if it is provided. If enclosing.element is not provided it aggregate the JSON elements returns a JSON array. Origin: siddhi-execution-json:2.0.4 Syntax OBJECT json:groupAsObject( STRING|OBJECT json) OBJECT json:groupAsObject( STRING|OBJECT json, BOOL distinct) OBJECT json:groupAsObject( STRING|OBJECT json, STRING enclosing.element) OBJECT json:groupAsObject( STRING|OBJECT json, STRING enclosing.element, BOOL distinct) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json The JSON element that needs to be aggregated. STRING OBJECT No Yes enclosing.element The JSON element used to enclose the aggregated JSON elements. EMPTY_STRING STRING Yes Yes distinct This is used to only have distinct JSON elements in the concatenated JSON object/array that is returned. false BOOL Yes Yes Examples EXAMPLE 1 from InputStream#window.length(5) select json:groupAsObject(\"json\") as groupedJSONArray input OutputStream; When we input events having values for the json as {\"date\":\"2013-11-19\",\"time\":\"10:30\"} and {\"date\":\"2013-11-19\",\"time\":\"12:20\"} , it returns [{\"date\":\"2013-11-19\",\"time\":\"10:30\"}{\"date\":\"2013-11-19\",\"time\":\"12:20\"}] to the 'OutputStream'. EXAMPLE 2 from InputStream#window.length(5) select json:groupAsObject(\"json\", true) as groupedJSONArray input OutputStream; When we input events having values for the json as {\"date\":\"2013-11-19\",\"time\":\"10:30\"} and {\"date\":\"2013-11-19\",\"time\":\"10:30\"} , it returns [{\"date\":\"2013-11-19\",\"time\":\"10:30\"}] to the 'OutputStream'. EXAMPLE 3 from InputStream#window.length(5) select json:groupAsObject(\"json\", \"result\") as groupedJSONArray input OutputStream; When we input events having values for the json as {\"date\":\"2013-11-19\",\"time\":\"10:30\"} and {\"date\":\"2013-11-19\",\"time\":\"12:20\"} , it returns {\"result\":[{\"date\":\"2013-11-19\",\"time\":\"10:30\"},{\"date\":\"2013-11-19\",\"time\":\"12:20\"}} to the 'OutputStream'. EXAMPLE 4 from InputStream#window.length(5) select json:groupAsObject(\"json\", \"result\", true) as groupedJSONArray input OutputStream; When we input events having values for the json as {\"date\":\"2013-11-19\",\"time\":\"10:30\"} and {\"date\":\"2013-11-19\",\"time\":\"10:30\"} , it returns {\"result\":[{\"date\":\"2013-11-19\",\"time\":\"10:30\"}]} to the 'OutputStream'. getBool (Function) Function retrieves the 'boolean' value specified in the given path of the JSON element. Origin: siddhi-execution-json:2.0.4 Syntax BOOL json:getBool( STRING|OBJECT json, STRING path) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json The JSON input containing boolean value. STRING OBJECT No Yes path The JSON path to fetch the boolean value. STRING No Yes Examples EXAMPLE 1 json:getBool(json,'$.married') If the json is the format {'name' : 'John', 'married' : true} , the function returns true as there is a matching boolean at .married /code . /p p /p span id=\"example-2\" class=\"md-typeset\" style=\"display: block; color: rgba(0, 0, 0, 0.54); font-size: 12.8px; font-weight: bold;\" EXAMPLE 2 /span json:getBool(json,'$.name') p /p p style=\"word-wrap: break-word;margin: 0;\" If the code json /code is the format code {'name' : 'John', 'married' : true} /code , the function returns code null /code as there is no matching boolean at code .married</code>.</p> <p></p> <span id=\"example-2\" class=\"md-typeset\" style=\"display: block; color: rgba(0, 0, 0, 0.54); font-size: 12.8px; font-weight: bold;\">EXAMPLE 2</span> <pre class=\"codehilite\"><code>json:getBool(json,'$.name')</code></pre> <p></p> <p style=\"word-wrap: break-word;margin: 0;\">If the <code>json</code> is the format <code>{'name' : 'John', 'married' : true}</code>, the function returns <code>null</code> as there is no matching boolean at <code> .name . EXAMPLE 3 json:getBool(json,'$.foo') If the json is the format {'name' : 'John', 'married' : true} , the function returns null as there is no matching element at $.foo . getDouble (Function) Function retrieves the 'double' value specified in the given path of the JSON element. Origin: siddhi-execution-json:2.0.4 Syntax DOUBLE json:getDouble( STRING|OBJECT json, STRING path) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json The JSON input containing double value. STRING OBJECT No Yes path The JSON path to fetch the double value. STRING No Yes Examples EXAMPLE 1 json:getDouble(json,'$.salary') If the json is the format {'name' : 'John', 'salary' : 12000.0} , the function returns 12000.0 as there is a matching double at .salary /code . /p p /p span id=\"example-2\" class=\"md-typeset\" style=\"display: block; color: rgba(0, 0, 0, 0.54); font-size: 12.8px; font-weight: bold;\" EXAMPLE 2 /span json:getDouble(json,'$.salary') p /p p style=\"word-wrap: break-word;margin: 0;\" If the code json /code is the format code {'name' : 'John', 'age' : 23} /code , the function returns code null /code as there are no matching element at code .salary</code>.</p> <p></p> <span id=\"example-2\" class=\"md-typeset\" style=\"display: block; color: rgba(0, 0, 0, 0.54); font-size: 12.8px; font-weight: bold;\">EXAMPLE 2</span> <pre class=\"codehilite\"><code>json:getDouble(json,'$.salary')</code></pre> <p></p> <p style=\"word-wrap: break-word;margin: 0;\">If the <code>json</code> is the format <code>{'name' : 'John', 'age' : 23}</code>, the function returns <code>null</code> as there are no matching element at <code> .salary . EXAMPLE 3 json:getDouble(json,'$.name') If the json is the format {'name' : 'John', 'age' : 23} , the function returns null as there are no matching double at $.name . getFloat (Function) Function retrieves the 'float' value specified in the given path of the JSON element. Origin: siddhi-execution-json:2.0.4 Syntax FLOAT json:getFloat( STRING|OBJECT json, STRING path) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json The JSON input containing float value. STRING OBJECT No Yes path The JSON path to fetch the float value. STRING No Yes Examples EXAMPLE 1 json:getFloat(json,'$.salary') If the json is the format {'name' : 'John', 'salary' : 12000.0} , the function returns 12000 as there is a matching float at .salary /code . /p p /p span id=\"example-2\" class=\"md-typeset\" style=\"display: block; color: rgba(0, 0, 0, 0.54); font-size: 12.8px; font-weight: bold;\" EXAMPLE 2 /span json:getFloat(json,'$.salary') p /p p style=\"word-wrap: break-word;margin: 0;\" If the code json /code is the format code {'name' : 'John', 'age' : 23} /code , the function returns code null /code as there are no matching element at code .salary</code>.</p> <p></p> <span id=\"example-2\" class=\"md-typeset\" style=\"display: block; color: rgba(0, 0, 0, 0.54); font-size: 12.8px; font-weight: bold;\">EXAMPLE 2</span> <pre class=\"codehilite\"><code>json:getFloat(json,'$.salary')</code></pre> <p></p> <p style=\"word-wrap: break-word;margin: 0;\">If the <code>json</code> is the format <code>{'name' : 'John', 'age' : 23}</code>, the function returns <code>null</code> as there are no matching element at <code> .salary . EXAMPLE 3 json:getFloat(json,'$.name') If the json is the format {'name' : 'John', 'age' : 23} , the function returns null as there are no matching float at $.name . getInt (Function) Function retrieves the 'int' value specified in the given path of the JSON element. Origin: siddhi-execution-json:2.0.4 Syntax INT json:getInt( STRING|OBJECT json, STRING path) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json The JSON input containing int value. STRING OBJECT No Yes path The JSON path to fetch the int value. STRING No Yes Examples EXAMPLE 1 json:getInt(json,'$.age') If the json is the format {'name' : 'John', 'age' : 23} , the function returns 23 as there is a matching int at .age /code . /p p /p span id=\"example-2\" class=\"md-typeset\" style=\"display: block; color: rgba(0, 0, 0, 0.54); font-size: 12.8px; font-weight: bold;\" EXAMPLE 2 /span json:getInt(json,'$.salary') p /p p style=\"word-wrap: break-word;margin: 0;\" If the code json /code is the format code {'name' : 'John', 'age' : 23} /code , the function returns code null /code as there are no matching element at code .age</code>.</p> <p></p> <span id=\"example-2\" class=\"md-typeset\" style=\"display: block; color: rgba(0, 0, 0, 0.54); font-size: 12.8px; font-weight: bold;\">EXAMPLE 2</span> <pre class=\"codehilite\"><code>json:getInt(json,'$.salary')</code></pre> <p></p> <p style=\"word-wrap: break-word;margin: 0;\">If the <code>json</code> is the format <code>{'name' : 'John', 'age' : 23}</code>, the function returns <code>null</code> as there are no matching element at <code> .salary . EXAMPLE 3 json:getInt(json,'$.name') If the json is the format {'name' : 'John', 'age' : 23} , the function returns null as there are no matching int at $.name . getLong (Function) Function retrieves the 'long' value specified in the given path of the JSON element. Origin: siddhi-execution-json:2.0.4 Syntax LONG json:getLong( STRING|OBJECT json, STRING path) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json The JSON input containing long value. STRING OBJECT No Yes path The JSON path to fetch the long value. STRING No Yes Examples EXAMPLE 1 json:getLong(json,'$.age') If the json is the format {'name' : 'John', 'age' : 23} , the function returns 23 as there is a matching long at .age /code . /p p /p span id=\"example-2\" class=\"md-typeset\" style=\"display: block; color: rgba(0, 0, 0, 0.54); font-size: 12.8px; font-weight: bold;\" EXAMPLE 2 /span json:getLong(json,'$.salary') p /p p style=\"word-wrap: break-word;margin: 0;\" If the code json /code is the format code {'name' : 'John', 'age' : 23} /code , the function returns code null /code as there are no matching element at code .age</code>.</p> <p></p> <span id=\"example-2\" class=\"md-typeset\" style=\"display: block; color: rgba(0, 0, 0, 0.54); font-size: 12.8px; font-weight: bold;\">EXAMPLE 2</span> <pre class=\"codehilite\"><code>json:getLong(json,'$.salary')</code></pre> <p></p> <p style=\"word-wrap: break-word;margin: 0;\">If the <code>json</code> is the format <code>{'name' : 'John', 'age' : 23}</code>, the function returns <code>null</code> as there are no matching element at <code> .salary . EXAMPLE 3 json:getLong(json,'$.name') If the json is the format {'name' : 'John', 'age' : 23} , the function returns null as there are no matching long at $.name . getObject (Function) Function retrieves the object specified in the given path of the JSON element. Origin: siddhi-execution-json:2.0.4 Syntax OBJECT json:getObject( STRING|OBJECT json, STRING path) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json The JSON input containing the object. STRING OBJECT No Yes path The JSON path to fetch the object. STRING No Yes Examples EXAMPLE 1 json:getObject(json,'$.address') If the json is the format {'name' : 'John', 'address' : {'city' : 'NY', 'country' : 'USA'}} , the function returns {'city' : 'NY', 'country' : 'USA'} as there is a matching object at .address /code . /p p /p span id=\"example-2\" class=\"md-typeset\" style=\"display: block; color: rgba(0, 0, 0, 0.54); font-size: 12.8px; font-weight: bold;\" EXAMPLE 2 /span json:getObject(json,'$.age') p /p p style=\"word-wrap: break-word;margin: 0;\" If the code json /code is the format code {'name' : 'John', 'age' : 23} /code , the function returns code 23 /code as there is a matching object at code .address</code>.</p> <p></p> <span id=\"example-2\" class=\"md-typeset\" style=\"display: block; color: rgba(0, 0, 0, 0.54); font-size: 12.8px; font-weight: bold;\">EXAMPLE 2</span> <pre class=\"codehilite\"><code>json:getObject(json,'$.age')</code></pre> <p></p> <p style=\"word-wrap: break-word;margin: 0;\">If the <code>json</code> is the format <code>{'name' : 'John', 'age' : 23}</code>, the function returns <code>23</code> as there is a matching object at <code> .age . EXAMPLE 3 json:getObject(json,'$.salary') If the json is the format {'name' : 'John', 'age' : 23} , the function returns null as there are no matching element at $.salary . getString (Function) Function retrieves value specified in the given path of the JSON element as a string. Origin: siddhi-execution-json:2.0.4 Syntax STRING json:getString( STRING|OBJECT json, STRING path) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json The JSON input containing value. STRING OBJECT No Yes path The JSON path to fetch the value. STRING No Yes Examples EXAMPLE 1 json:getString(json,'$.name') If the json is the format {'name' : 'John', 'age' : 23} , the function returns John as there is a matching string at .name /code . /p p /p span id=\"example-2\" class=\"md-typeset\" style=\"display: block; color: rgba(0, 0, 0, 0.54); font-size: 12.8px; font-weight: bold;\" EXAMPLE 2 /span json:getString(json,'$.salary') p /p p style=\"word-wrap: break-word;margin: 0;\" If the code json /code is the format code {'name' : 'John', 'age' : 23} /code , the function returns code null /code as there are no matching element at code .name</code>.</p> <p></p> <span id=\"example-2\" class=\"md-typeset\" style=\"display: block; color: rgba(0, 0, 0, 0.54); font-size: 12.8px; font-weight: bold;\">EXAMPLE 2</span> <pre class=\"codehilite\"><code>json:getString(json,'$.salary')</code></pre> <p></p> <p style=\"word-wrap: break-word;margin: 0;\">If the <code>json</code> is the format <code>{'name' : 'John', 'age' : 23}</code>, the function returns <code>null</code> as there are no matching element at <code> .salary . EXAMPLE 3 json:getString(json,'$.age') If the json is the format {'name' : 'John', 'age' : 23} , the function returns 23 as a string as there is a matching element at .age /code . /p p /p span id=\"example-4\" class=\"md-typeset\" style=\"display: block; color: rgba(0, 0, 0, 0.54); font-size: 12.8px; font-weight: bold;\" EXAMPLE 4 /span json:getString(json,'$.address') p /p p style=\"word-wrap: break-word;margin: 0;\" If the code json /code is the format code {'name' : 'John', 'address' : {'city' : 'NY', 'country' : 'USA'}} /code , the function returns code {'city' : 'NY', 'country' : 'USA'} /code as a string as there is a matching element at code .age</code>.</p> <p></p> <span id=\"example-4\" class=\"md-typeset\" style=\"display: block; color: rgba(0, 0, 0, 0.54); font-size: 12.8px; font-weight: bold;\">EXAMPLE 4</span> <pre class=\"codehilite\"><code>json:getString(json,'$.address')</code></pre> <p></p> <p style=\"word-wrap: break-word;margin: 0;\">If the <code>json</code> is the format <code>{'name' : 'John', 'address' : {'city' : 'NY', 'country' : 'USA'}}</code>, the function returns <code>{'city' : 'NY', 'country' : 'USA'}</code> as a string as there is a matching element at <code> .address . isExists (Function) Function checks whether there is a JSON element present in the given path or not. Origin: siddhi-execution-json:2.0.4 Syntax BOOL json:isExists( STRING|OBJECT json, STRING path) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json The JSON input that needs to be searched for an elements. STRING OBJECT No Yes path The JSON path to check for the element. STRING No Yes Examples EXAMPLE 1 json:isExists(json, '$.name') If the json is the format {'name' : 'John', 'age' : 23} , the function returns true as there is an element in the given path. EXAMPLE 2 json:isExists(json, '$.salary') If the json is the format {'name' : 'John', 'age' : 23} , the function returns false as there is no element in the given path. setElement (Function) Function sets JSON element into a given JSON at the specific path. Origin: siddhi-execution-json:2.0.4 Syntax OBJECT json:setElement( STRING|OBJECT json, STRING path, STRING|BOOL|DOUBLE|FLOAT|INT|LONG|OBJECT json.element) OBJECT json:setElement( STRING|OBJECT json, STRING path, STRING|BOOL|DOUBLE|FLOAT|INT|LONG|OBJECT json.element, STRING key) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json The JSON to which a JSON element needs to be added/replaced. STRING OBJECT No Yes path The JSON path where the JSON element should be added/replaced. STRING No Yes json.element The JSON element being added. STRING BOOL DOUBLE FLOAT INT LONG OBJECT No Yes key The key to be used to refer the newly added element in the input JSON. Assumes the element is added to a JSON array, or the element selected by the JSON path will be updated. STRING Yes Yes Examples EXAMPLE 1 json:setElement(json, '$', \"{'country' : 'USA'}\", 'address') If the json is the format {'name' : 'John', 'married' : true} ,the function updates the json as {'name' : 'John', 'married' : true, 'address' : {'country' : 'USA'}} by adding 'address' element and returns the updated JSON. EXAMPLE 2 json:setElement(json, '$', 40, 'age') If the json is the format {'name' : 'John', 'married' : true} ,the function updates the json as {'name' : 'John', 'married' : true, 'age' : 40} by adding 'age' element and returns the updated JSON. EXAMPLE 3 json:setElement(json, '$', 45, 'age') If the json is the format {'name' : 'John', 'married' : true, 'age' : 40} , the function updates the json as {'name' : 'John', 'married' : true, 'age' : 45} by replacing 'age' element and returns the updated JSON. EXAMPLE 4 json:setElement(json, '$.items', 'book') If the json is the format {'name' : 'Stationary', 'items' : ['pen', 'pencil']} , the function updates the json as {'name' : 'John', 'items' : ['pen', 'pencil', 'book']} by adding 'book' in the items array and returns the updated JSON. EXAMPLE 5 json:setElement(json, '$.item', 'book') If the json is the format {'name' : 'Stationary', 'item' : 'pen'} , the function updates the json as {'name' : 'John', 'item' : 'book'} by replacing 'item' element and returns the updated JSON. EXAMPLE 6 json:setElement(json, '$.address', 'city', 'SF') If the json is the format {'name' : 'John', 'married' : true} ,the function will not update, but returns the original JSON as there are no valid path for $.address . toObject (Function) Function generate JSON object from the given JSON string. Origin: siddhi-execution-json:2.0.4 Syntax OBJECT json:toObject( STRING json) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json A valid JSON string that needs to be converted to a JSON object. STRING No Yes Examples EXAMPLE 1 json:toJson(json) This returns the JSON object corresponding to the given JSON string. toString (Function) Function generates a JSON string corresponding to a given JSON object. Origin: siddhi-execution-json:2.0.4 Syntax STRING json:toString( OBJECT json) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json A valid JSON object to generates a JSON string. OBJECT No Yes Examples EXAMPLE 1 json:toString(json) This returns the JSON string corresponding to a given JSON object. tokenize (Stream Processor) Stream processor tokenizes the given JSON into to multiple JSON string elements and sends them as separate events. Origin: siddhi-execution-json:2.0.4 Syntax json:tokenize( STRING|OBJECT json, STRING path) json:tokenize( STRING|OBJECT json, STRING path, BOOL fail.on.missing.attribute) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json The input JSON that needs to be tokenized. STRING OBJECT No Yes path The path of the set of elements that will be tokenized. STRING No Yes fail.on.missing.attribute If there are no element on the given path, when set to true the system will drop the event, and when set to false the system will pass 'null' value to the jsonElement output attribute. true BOOL Yes No Extra Return Attributes Name Description Possible Types jsonElement The JSON element retrieved based on the given path will be returned as a JSON string. If the 'path' selects a JSON array then the system returns each element in the array as a JSON string via a separate events. STRING Examples EXAMPLE 1 define stream InputStream (json string, path string); @info(name = 'query1') from InputStream#json:tokenizeAsObject(json, path) select path, jsonElement insert into OutputStream; If the input 'json' is {name:'John', enrolledSubjects:['Mathematics', 'Physics']} , and the 'path' is passed as .enrolledSubjects /code then for both the elements in the selected JSON array, it generates it generates events as code (' .enrolledSubjects</code> then for both the elements in the selected JSON array, it generates it generates events as <code>(' .enrolledSubjects', 'Mathematics') , and (' .enrolledSubjects', 'Physics') /code . br For the same input JSON, if the 'path' is passed as code .enrolledSubjects', 'Physics')</code>.<br>For the same input JSON, if the 'path' is passed as <code> .name then it will only produce one event (' .name', 'John') /code as the 'path' provided a single JSON element. /p p /p span id=\"example-2\" class=\"md-typeset\" style=\"display: block; color: rgba(0, 0, 0, 0.54); font-size: 12.8px; font-weight: bold;\" EXAMPLE 2 /span define stream InputStream (json string, path string); @info(name = 'query1') from InputStream#json:tokenizeAsObject(json, path, true) select path, jsonElement insert into OutputStream; p /p p style=\"word-wrap: break-word;margin: 0;\" If the input 'json' is code {name:'John', age:25} /code ,and the 'path' is passed as code .name', 'John')</code> as the 'path' provided a single JSON element.</p> <p></p> <span id=\"example-2\" class=\"md-typeset\" style=\"display: block; color: rgba(0, 0, 0, 0.54); font-size: 12.8px; font-weight: bold;\">EXAMPLE 2</span> <pre class=\"codehilite\"><code>define stream InputStream (json string, path string); @info(name = 'query1') from InputStream#json:tokenizeAsObject(json, path, true) select path, jsonElement insert into OutputStream;</code></pre> <p></p> <p style=\"word-wrap: break-word;margin: 0;\">If the input 'json' is <code>{name:'John', age:25}</code>,and the 'path' is passed as <code> .salary then the system will produce (' .salary', null) /code , as the 'fail.on.missing.attribute' is code true /code and there are no matching element for code .salary', null)</code>, as the 'fail.on.missing.attribute' is <code>true</code> and there are no matching element for <code> .salary . tokenizeAsObject (Stream Processor) Stream processor tokenizes the given JSON into to multiple JSON object elements and sends them as separate events. Origin: siddhi-execution-json:2.0.4 Syntax json:tokenizeAsObject( STRING|OBJECT json, STRING path) json:tokenizeAsObject( STRING|OBJECT json, STRING path, BOOL fail.on.missing.attribute) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json The input JSON that needs to be tokenized. STRING OBJECT No Yes path The path of the set of elements that will be tokenized. STRING No Yes fail.on.missing.attribute If there are no element on the given path, when set to true the system will drop the event, and when set to false the system will pass 'null' value to the jsonElement output attribute. true BOOL Yes No Extra Return Attributes Name Description Possible Types jsonElement The JSON element retrieved based on the given path will be returned as a JSON object. If the 'path' selects a JSON array then the system returns each element in the array as a JSON object via a separate events. OBJECT Examples EXAMPLE 1 define stream InputStream (json string, path string); @info(name = 'query1') from InputStream#json:tokenizeAsObject(json, path) select path, jsonElement insert into OutputStream; If the input 'json' is {name:'John', enrolledSubjects:['Mathematics', 'Physics']} , and the 'path' is passed as .enrolledSubjects /code then for both the elements in the selected JSON array, it generates it generates events as code (' .enrolledSubjects</code> then for both the elements in the selected JSON array, it generates it generates events as <code>(' .enrolledSubjects', 'Mathematics') , and (' .enrolledSubjects', 'Physics') /code . br For the same input JSON, if the 'path' is passed as code .enrolledSubjects', 'Physics')</code>.<br>For the same input JSON, if the 'path' is passed as <code> .name then it will only produce one event (' .name', 'John') /code as the 'path' provided a single JSON element. /p p /p span id=\"example-2\" class=\"md-typeset\" style=\"display: block; color: rgba(0, 0, 0, 0.54); font-size: 12.8px; font-weight: bold;\" EXAMPLE 2 /span define stream InputStream (json string, path string); @info(name = 'query1') from InputStream#json:tokenizeAsObject(json, path, true) select path, jsonElement insert into OutputStream; p /p p style=\"word-wrap: break-word;margin: 0;\" If the input 'json' is code {name:'John', age:25} /code ,and the 'path' is passed as code .name', 'John')</code> as the 'path' provided a single JSON element.</p> <p></p> <span id=\"example-2\" class=\"md-typeset\" style=\"display: block; color: rgba(0, 0, 0, 0.54); font-size: 12.8px; font-weight: bold;\">EXAMPLE 2</span> <pre class=\"codehilite\"><code>define stream InputStream (json string, path string); @info(name = 'query1') from InputStream#json:tokenizeAsObject(json, path, true) select path, jsonElement insert into OutputStream;</code></pre> <p></p> <p style=\"word-wrap: break-word;margin: 0;\">If the input 'json' is <code>{name:'John', age:25}</code>,and the 'path' is passed as <code> .salary then the system will produce (' .salary', null) /code , as the 'fail.on.missing.attribute' is code true /code and there are no matching element for code .salary', null)</code>, as the 'fail.on.missing.attribute' is <code>true</code> and there are no matching element for <code> .salary . List collect (Aggregate Function) Collects multiple values to construct a list. Origin: siddhi-execution-list:1.0.0 Syntax OBJECT list:collect( OBJECT|INT|LONG|FLOAT|DOUBLE|BOOL|STRING value) OBJECT list:collect( OBJECT|INT|LONG|FLOAT|DOUBLE|BOOL|STRING value, BOOL is.distinct) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic value Value of the list element OBJECT INT LONG FLOAT DOUBLE BOOL STRING No Yes is.distinct If true only distinct elements are collected false BOOL Yes Yes Examples EXAMPLE 1 from StockStream#window.lengthBatch(10) select list:collect(symbol) as stockSymbols insert into OutputStream; For the window expiry of 10 events, the collect() function will collect attributes of symbol to a single list and return as stockSymbols. merge (Aggregate Function) Collects multiple lists to merge as a single list. Origin: siddhi-execution-list:1.0.0 Syntax OBJECT list:merge( OBJECT list) OBJECT list:merge( OBJECT list, BOOL is.distinct) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic list List to be merged OBJECT No Yes is.distinct Whether to return list with distinct values false BOOL Yes Yes Examples EXAMPLE 1 from StockStream#window.lengthBatch(2) select list:merge(list) as stockSymbols insert into OutputStream; For the window expiry of 2 events, the merge() function will collect attributes of list and merge them to a single list, returned as stockSymbols. add (Function) Function returns the updated list after adding the given value. Origin: siddhi-execution-list:1.0.0 Syntax OBJECT list:add( OBJECT list, OBJECT|INT|LONG|FLOAT|DOUBLE|BOOL|STRING value) OBJECT list:add( OBJECT list, OBJECT|INT|LONG|FLOAT|DOUBLE|BOOL|STRING value, INT index) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic list The list to which the value should be added. OBJECT No Yes value The value to be added. OBJECT INT LONG FLOAT DOUBLE BOOL STRING No Yes index The index in which the value should to be added. last INT Yes Yes Examples EXAMPLE 1 list:add(stockSymbols, 'IBM') Function returns the updated list after adding the value IBM in the last index. EXAMPLE 2 list:add(stockSymbols, 'IBM', 0) Function returns the updated list after adding the value IBM in the 0 th index`. addAll (Function) Function returns the updated list after adding all the values from the given list. Origin: siddhi-execution-list:1.0.0 Syntax OBJECT list:addAll( OBJECT to.list, OBJECT from.list) OBJECT list:addAll( OBJECT to.list, OBJECT from.list, BOOL is.distinct) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic to.list The list into which the values need to copied. OBJECT No Yes from.list The list from which the values are copied. OBJECT No Yes is.distinct If true returns list with distinct values false BOOL Yes Yes Examples EXAMPLE 1 list:putAll(toList, fromList) If toList contains values ('IBM', 'WSO2), and if fromList contains values ('IBM', 'XYZ') then the function returns updated toList with values ('IBM', 'WSO2', 'IBM', 'XYZ'). EXAMPLE 2 list:putAll(toList, fromList, true) If toList contains values ('IBM', 'WSO2), and if fromList contains values ('IBM', 'XYZ') then the function returns updated toList with values ('IBM', 'WSO2', 'XYZ'). clear (Function) Function returns the cleared list. Origin: siddhi-execution-list:1.0.0 Syntax OBJECT list:clear( OBJECT list) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic list The list which needs to be cleared OBJECT No Yes Examples EXAMPLE 1 list:clear(stockDetails) Returns an empty list. clone (Function) Function returns the cloned list. Origin: siddhi-execution-list:1.0.0 Syntax OBJECT list:clone( OBJECT list) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic list The list to which needs to be cloned. OBJECT No Yes Examples EXAMPLE 1 list:clone(stockSymbols) Function returns cloned list of stockSymbols. contains (Function) Function checks whether the list contains the specific value. Origin: siddhi-execution-list:1.0.0 Syntax BOOL list:contains( OBJECT list, OBJECT|INT|LONG|FLOAT|DOUBLE|BOOL|STRING value) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic list The list that needs to be checked on whether it contains the value or not. OBJECT No Yes value The value that needs to be checked. OBJECT INT LONG FLOAT DOUBLE BOOL STRING No Yes Examples EXAMPLE 1 list:contains(stockSymbols, 'IBM') Returns 'true' if the stockSymbols list contains value IBM else it returns false . containsAll (Function) Function checks whether the list contains all the values in the given list. Origin: siddhi-execution-list:1.0.0 Syntax BOOL list:containsAll( OBJECT list, OBJECT given.list) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic list The list that needs to be checked on whether it contains all the values or not. OBJECT No Yes given.list The list which contains all the values to be checked. OBJECT No Yes Examples EXAMPLE 1 list:containsAll(stockSymbols, latestStockSymbols) Returns 'true' if the stockSymbols list contains values in latestStockSymbols else it returns false . create (Function) Function creates a list containing all values provided. Origin: siddhi-execution-list:1.0.0 Syntax OBJECT list:create() OBJECT list:create( OBJECT|INT|LONG|FLOAT|DOUBLE|BOOL|STRING value1) OBJECT list:create( OBJECT|INT|LONG|FLOAT|DOUBLE|BOOL|STRING value1, OBJECT|INT|LONG|FLOAT|DOUBLE|BOOL|STRING ...) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic value1 Value 1 OBJECT INT LONG FLOAT DOUBLE BOOL STRING Yes Yes Examples EXAMPLE 1 list:create(1, 2, 3, 4, 5, 6) This returns a list with values 1 , 2 , 3 , 4 , 5 and 6 . EXAMPLE 2 list:create() This returns an empty list. get (Function) Function returns the value at the specific index, null if index is out of range. Origin: siddhi-execution-list:1.0.0 Syntax OBJECT|INT|LONG|FLOAT|DOUBLE|BOOL|STRING list:get( OBJECT list, INT index) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic list Attribute containing the list OBJECT No Yes index Index of the element INT No Yes Examples EXAMPLE 1 list:get(stockSymbols, 1) This returns the element in the 1 st index in the stockSymbols list. indexOf (Function) Function returns the last index of the given element. Origin: siddhi-execution-list:1.0.0 Syntax INT list:indexOf( OBJECT list, OBJECT|INT|LONG|FLOAT|DOUBLE|BOOL|STRING value) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic list The list to be checked to get index of an element. OBJECT No Yes value Value for which last index needs to be identified. OBJECT INT LONG FLOAT DOUBLE BOOL STRING No Yes Examples EXAMPLE 1 list:indexOf(stockSymbols. `IBM`) Returns the last index of the element IBM if present else it returns -1. isEmpty (Function) Function checks if the list is empty. Origin: siddhi-execution-list:1.0.0 Syntax BOOL list:isEmpty( OBJECT list) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic list The list that needs to be checked whether it's empty or not. OBJECT No Yes Examples EXAMPLE 1 list:isEmpty(stockSymbols) Returns 'true' if the stockSymbols list is empty else it returns false . isList (Function) Function checks if the object is type of a list. Origin: siddhi-execution-list:1.0.0 Syntax BOOL list:isList( OBJECT|INT|LONG|FLOAT|DOUBLE|BOOL|STRING arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The argument the need to be determined whether it's a list or not. OBJECT INT LONG FLOAT DOUBLE BOOL STRING No Yes Examples EXAMPLE 1 list:isList(stockSymbols) Returns 'true' if the stockSymbols is and an instance of java.util.List else it returns false . lastIndexOf (Function) Function returns the index of the given value. Origin: siddhi-execution-list:1.0.0 Syntax INT list:lastIndexOf( OBJECT list, OBJECT|INT|LONG|FLOAT|DOUBLE|BOOL|STRING value) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic list The list to be checked to get index of an element. OBJECT No Yes value Value for which last index needs to be identified. OBJECT INT LONG FLOAT DOUBLE BOOL STRING No Yes Examples EXAMPLE 1 list:lastIndexOf(stockSymbols. `IBM`) Returns the last index of the element IBM if present else it returns -1. remove (Function) Function returns the updated list after removing the element with the specified value. Origin: siddhi-execution-list:1.0.0 Syntax OBJECT list:remove( OBJECT list, OBJECT|INT|LONG|FLOAT|DOUBLE|BOOL|STRING value) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic list The list that needs to be updated. OBJECT No Yes value The value of the element that needs to removed. OBJECT INT LONG FLOAT DOUBLE BOOL STRING No Yes Examples EXAMPLE 1 list:remove(stockSymbols, 'IBM') This returns the updated list, stockSymbols after stockSymbols the value IBM . removeAll (Function) Function returns the updated list after removing all the element with the specified list. Origin: siddhi-execution-list:1.0.0 Syntax OBJECT list:removeAll( OBJECT list, OBJECT given.list) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic list The list that needs to be updated. OBJECT No Yes given.list The list with all the elements that needs to removed. OBJECT No Yes Examples EXAMPLE 1 list:removeAll(stockSymbols, latestStockSymbols) This returns the updated list, stockSymbols after removing all the values in latestStockSymbols. removeByIndex (Function) Function returns the updated list after removing the element with the specified index. Origin: siddhi-execution-list:1.0.0 Syntax OBJECT list:removeByIndex( OBJECT list, INT index) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic list The list that needs to be updated. OBJECT No Yes index The index of the element that needs to removed. INT No Yes Examples EXAMPLE 1 list:removeByIndex(stockSymbols, 0) This returns the updated list, stockSymbols after removing value at 0 th index. retainAll (Function) Function returns the updated list after retaining all the elements in the specified list. Origin: siddhi-execution-list:1.0.0 Syntax OBJECT list:retainAll( OBJECT list, OBJECT given.list) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic list The list that needs to be updated. OBJECT No Yes given.list The list with all the elements that needs to reatined. OBJECT No Yes Examples EXAMPLE 1 list:retainAll(stockSymbols, latestStockSymbols) This returns the updated list, stockSymbols after retaining all the values in latestStockSymbols. setValue (Function) Function returns the updated list after replacing the element in the given index by the given value. Origin: siddhi-execution-list:1.0.0 Syntax OBJECT list:setValue( OBJECT list, INT index, OBJECT|INT|LONG|FLOAT|DOUBLE|BOOL|STRING value) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic list The list to which the value should be updated. OBJECT No Yes index The index in which the value should to be updated. INT No Yes value The value to be updated with. OBJECT INT LONG FLOAT DOUBLE BOOL STRING No Yes Examples EXAMPLE 1 list:set(stockSymbols, 0, 'IBM') Function returns the updated list after replacing the value at 0 th index with the value IBM size (Function) Function to return the size of the list. Origin: siddhi-execution-list:1.0.0 Syntax INT list:size( OBJECT list) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic list The list for which size should be returned. OBJECT No Yes Examples EXAMPLE 1 list:size(stockSymbols) Returns size of the stockSymbols list. sort (Function) Function returns lists sorted in ascending or descending order. Origin: siddhi-execution-list:1.0.0 Syntax OBJECT list:sort( OBJECT list) OBJECT list:sort( OBJECT list, STRING order) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic list The list which should be sorted. OBJECT No Yes order Order in which the list needs to be sorted (ASC/DESC/REV). REV STRING Yes No Examples EXAMPLE 1 list:sort(stockSymbols) Function returns the sorted list in ascending order. EXAMPLE 2 list:sort(stockSymbols, 'DESC') Function returns the sorted list in descending order. tokenize (Stream Processor) Tokenize the list and return each key, value as new attributes in events Origin: siddhi-execution-list:1.0.0 Syntax list:tokenize( OBJECT list) list:tokenize( OBJECT list, OBJECT ...) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic list Array list which needs to be tokenized OBJECT No Yes Extra Return Attributes Name Description Possible Types index Index of an entry consisted in the list INT value Value of an entry consisted in the list OBJECT Examples EXAMPLE 1 list:tokenize(customList) If custom list contains ('WSO2', 'IBM', 'XYZ') elements, then tokenize function will return 3 events with value attributes WSO2, IBM and XYZ respectively. Map collect (Aggregate Function) Collect multiple key-value pairs to construct a map. Only distinct keys are collected, if a duplicate key arrives, it overrides the old value Origin: siddhi-execution-map:5.0.5 Syntax OBJECT map:collect( INT|LONG|FLOAT|DOUBLE|FLOAT|BOOL|STRING key, OBJECT|INT|LONG|FLOAT|DOUBLE|BOOL|STRING value) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic key Key of the map entry INT LONG FLOAT DOUBLE FLOAT BOOL STRING No Yes value Value of the map entry OBJECT INT LONG FLOAT DOUBLE BOOL STRING No Yes Examples EXAMPLE 1 from StockStream#window.lengthBatch(10) select map:collect(symbol, price) as stockDetails insert into OutputStream; For the window expiry of 10 events, the collect() function will collect attributes of key and value to a single map and return as stockDetails. merge (Aggregate Function) Collect multiple maps to merge as a single map. Only distinct keys are collected, if a duplicate key arrives, it overrides the old value. Origin: siddhi-execution-map:5.0.5 Syntax OBJECT map:merge( OBJECT map) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic map Maps to be collected OBJECT No Yes Examples EXAMPLE 1 from StockStream#window.lengthBatch(2) select map:merge(map) as stockDetails insert into OutputStream; For the window expiry of 2 events, the merge() function will collect attributes of map and merge them to a single map, returned as stockDetails. clear (Function) Function returns the cleared map. Origin: siddhi-execution-map:5.0.5 Syntax OBJECT map:clear( OBJECT map) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic map The map which needs to be cleared OBJECT No Yes Examples EXAMPLE 1 map:clear(stockDetails) Returns an empty map. clone (Function) Function returns the cloned map. Origin: siddhi-execution-map:5.0.5 Syntax OBJECT map:clone( OBJECT map) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic map The map to which needs to be cloned. OBJECT No Yes Examples EXAMPLE 1 map:clone(stockDetails) Function returns cloned map of stockDetails. combineByKey (Function) Function returns the map after combining all the maps given as parameters, such that the keys, of all the maps will be matched with an Array list of values from each map respectively. Origin: siddhi-execution-map:5.0.5 Syntax OBJECT map:combineByKey( OBJECT map, OBJECT map) OBJECT map:combineByKey( OBJECT map, OBJECT map, OBJECT ...) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic map The map into which the key-values need to copied. OBJECT No Yes Examples EXAMPLE 1 map:combineByKey(map1, map2) If map2 contains key-value pairs ('symbol': 'wso2'), ('volume' : 100), and if map2 contains key-value pairs ('symbol': 'IBM'), ('price' : 12), then the function returns the map with key value pairs as follows, (symbol: ArrayList('wso2, 'IBM')), (volume: ArrayList(100, null)) and (price: ArrayList(null, 12)) containsKey (Function) Function checks if the map contains the key. Origin: siddhi-execution-map:5.0.5 Syntax BOOL map:containsKey( OBJECT map, INT|LONG|FLOAT|DOUBLE|BOOL|STRING key) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic map The map the needs to be checked on containing the key or not. OBJECT No Yes key The key to be checked. INT LONG FLOAT DOUBLE BOOL STRING No Yes Examples EXAMPLE 1 map:containsKey(stockDetails, '1234') Returns 'true' if the stockDetails map contains key 1234 else it returns false . containsValue (Function) Function checks if the map contains the value. Origin: siddhi-execution-map:5.0.5 Syntax BOOL map:containsValue( OBJECT map, INT|LONG|FLOAT|DOUBLE|BOOL|STRING value) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic map The map the needs to be checked on containing the value or not. OBJECT No Yes value The value to be checked. INT LONG FLOAT DOUBLE BOOL STRING No Yes Examples EXAMPLE 1 map:containsValue(stockDetails, 'IBM') Returns 'true' if the stockDetails map contains value IBM else it returns false . create (Function) Function creates a map pairing the keys and their corresponding values. Origin: siddhi-execution-map:5.0.5 Syntax OBJECT map:create() OBJECT map:create( OBJECT|INT|LONG|FLOAT|DOUBLE|FLOAT|BOOL|STRING key1, OBJECT|INT|LONG|FLOAT|DOUBLE|FLOAT|BOOL|STRING value1) OBJECT map:create( OBJECT|INT|LONG|FLOAT|DOUBLE|FLOAT|BOOL|STRING key1, OBJECT|INT|LONG|FLOAT|DOUBLE|FLOAT|BOOL|STRING value1, OBJECT|INT|LONG|FLOAT|DOUBLE|FLOAT|BOOL|STRING ...) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic key1 Key 1 - OBJECT INT LONG FLOAT DOUBLE FLOAT BOOL STRING Yes Yes value1 Value 1 - OBJECT INT LONG FLOAT DOUBLE FLOAT BOOL STRING Yes Yes Examples EXAMPLE 1 map:create(1, 'one', 2, 'two', 3, 'three') This returns a map with keys 1 , 2 , 3 mapped with their corresponding values, one , two , three . EXAMPLE 2 map:create() This returns an empty map. createFromJSON (Function) Function returns the map created by pairing the keys with their corresponding values given in the JSON string. Origin: siddhi-execution-map:5.0.5 Syntax OBJECT map:createFromJSON( STRING json.string) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json.string JSON as a string, which is used to create the map. STRING No Yes Examples EXAMPLE 1 map:createFromJSON(\"{\u2018symbol' : 'IBM', 'price' : 200, 'volume' : 100}\") This returns a map with the keys symbol , price , and volume , and their values, IBM , 200 and 100 respectively. createFromXML (Function) Function returns the map created by pairing the keys with their corresponding values,given as an XML string. Origin: siddhi-execution-map:5.0.5 Syntax OBJECT map:createFromXML( STRING xml.string) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic xml.string The XML string, which is used to create the map. STRING No Yes Examples EXAMPLE 1 map:createFromXML(\" stock symbol IBM /symbol price 200 /price volume 100 /volume /stock \") This returns a map with the keys symbol , price , volume , and with their values IBM , 200 and 100 respectively. get (Function) Function returns the value corresponding to the given key from the map. Origin: siddhi-execution-map:5.0.5 Syntax OBJECT|INT|LONG|FLOAT|DOUBLE|FLOAT|BOOL|STRING map:get( OBJECT map, INT|LONG|FLOAT|DOUBLE|FLOAT|BOOL|STRING key) OBJECT|INT|LONG|FLOAT|DOUBLE|FLOAT|BOOL|STRING map:get( OBJECT map, INT|LONG|FLOAT|DOUBLE|FLOAT|BOOL|STRING key, OBJECT|INT|LONG|FLOAT|DOUBLE|FLOAT|BOOL|STRING default.value) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic map The map from where the value should be obtained. OBJECT No Yes key The key to fetch the value. INT LONG FLOAT DOUBLE FLOAT BOOL STRING No Yes default.value The value to be returned if the map does not have the key. OBJECT INT LONG FLOAT DOUBLE FLOAT BOOL STRING Yes Yes Examples EXAMPLE 1 map:get(companyMap, 1) If the companyMap has key 1 and value ABC in it's set of key value pairs. The function returns ABC . EXAMPLE 2 map:get(companyMap, 2) If the companyMap does not have any value for key 2 then the function returns null . EXAMPLE 3 map:get(companyMap, 2, 'two') If the companyMap does not have any value for key 2 then the function returns two . isEmpty (Function) Function checks if the map is empty. Origin: siddhi-execution-map:5.0.5 Syntax BOOL map:isEmpty( OBJECT map) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic map The map the need to be checked whether it's empty or not. OBJECT No Yes Examples EXAMPLE 1 map:isEmpty(stockDetails) Returns 'true' if the stockDetails map is empty else it returns false . isMap (Function) Function checks if the object is type of a map. Origin: siddhi-execution-map:5.0.5 Syntax BOOL map:isMap( OBJECT|INT|LONG|FLOAT|DOUBLE|FLOAT|BOOL|STRING arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The argument the need to be determined whether it's a map or not. OBJECT INT LONG FLOAT DOUBLE FLOAT BOOL STRING No Yes Examples EXAMPLE 1 map:isMap(stockDetails) Returns 'true' if the stockDetails is and an instance of java.util.Map else it returns false . keys (Function) Function to return the keys of the map as a list. Origin: siddhi-execution-map:5.0.5 Syntax OBJECT map:keys( OBJECT map) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic map The map from which list of keys to be returned. OBJECT No Yes Examples EXAMPLE 1 map:keys(stockDetails) Returns keys of the stockDetails map. put (Function) Function returns the updated map after adding the given key-value pair. If the key already exist in the map the key is updated with the new value. Origin: siddhi-execution-map:5.0.5 Syntax OBJECT map:put( OBJECT map, OBJECT|INT|LONG|FLOAT|DOUBLE|FLOAT|BOOL|STRING key, OBJECT|INT|LONG|FLOAT|DOUBLE|FLOAT|BOOL|STRING value) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic map The map to which the value should be added. OBJECT No Yes key The key to be added. OBJECT INT LONG FLOAT DOUBLE FLOAT BOOL STRING No Yes value The value to be added. OBJECT INT LONG FLOAT DOUBLE FLOAT BOOL STRING No Yes Examples EXAMPLE 1 map:put(stockDetails , 'IBM' , '200') Function returns the updated map named stockDetails after adding the value 200 with the key IBM . putAll (Function) Function returns the updated map after adding all the key-value pairs from another map. If there are duplicate keys, the key will be assigned new values from the map that's being copied. Origin: siddhi-execution-map:5.0.5 Syntax OBJECT map:putAll( OBJECT to.map, OBJECT from.map) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic to.map The map into which the key-values need to copied. OBJECT No Yes from.map The map from which the key-values are copied. OBJECT No Yes Examples EXAMPLE 1 map:putAll(toMap, fromMap) If toMap contains key-value pairs ('symbol': 'wso2'), ('volume' : 100), and if fromMap contains key-value pairs ('symbol': 'IBM'), ('price' : 12), then the function returns updated toMap with key-value pairs ('symbol': 'IBM'), ('price' : 12), ('volume' : 100). putIfAbsent (Function) Function returns the updated map after adding the given key-value pair if key is absent. Origin: siddhi-execution-map:5.0.5 Syntax OBJECT map:putIfAbsent( OBJECT map, INT|LONG|FLOAT|DOUBLE|BOOL|STRING key, INT|LONG|FLOAT|DOUBLE|BOOL|STRING value) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic map The map to which the value should be added. OBJECT No Yes key The key to be added. INT LONG FLOAT DOUBLE BOOL STRING No Yes value The value to be added. INT LONG FLOAT DOUBLE BOOL STRING No Yes Examples EXAMPLE 1 map:putIfAbsent(stockDetails , 1234 , 'IBM') Function returns the updated map named stockDetails after adding the value IBM with the key 1234 if key is absent from the original map. remove (Function) Function returns the updated map after removing the element with the specified key. Origin: siddhi-execution-map:5.0.5 Syntax OBJECT map:remove( OBJECT map, OBJECT|INT|LONG|FLOAT|DOUBLE|FLOAT|BOOL|STRING key) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic map The map that needs to be updated. OBJECT No Yes key The key of the element that needs to removed. OBJECT INT LONG FLOAT DOUBLE FLOAT BOOL STRING No Yes Examples EXAMPLE 1 map:remove(stockDetails, 1234) This returns the updated map, stockDetails after removing the key-value pair corresponding to the key 1234 . replace (Function) Function returns the updated map after replacing the given key-value pair only if key is present. Origin: siddhi-execution-map:5.0.5 Syntax OBJECT map:replace( OBJECT map, INT|LONG|FLOAT|DOUBLE|FLOAT|BOOL|STRING key, INT|LONG|FLOAT|DOUBLE|BOOL|STRING value) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic map The map to which the key-value should be replaced. OBJECT No Yes key The key to be replaced. INT LONG FLOAT DOUBLE FLOAT BOOL STRING No Yes value The value to be replaced. INT LONG FLOAT DOUBLE BOOL STRING No Yes Examples EXAMPLE 1 map:replace(stockDetails , 1234 , 'IBM') Function returns the updated map named stockDetails after replacing the value IBM with the key 1234 if present. replaceAll (Function) Function returns the updated map after replacing all the key-value pairs from another map, if keys are present. Origin: siddhi-execution-map:5.0.5 Syntax OBJECT map:replaceAll( OBJECT to.map, OBJECT from.map) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic to.map The map into which the key-values need to copied. OBJECT No Yes from.map The map from which the key-values are copied. OBJECT No Yes Examples EXAMPLE 1 map:replaceAll(toMap, fromMap) If toMap contains key-value pairs ('symbol': 'wso2'), ('volume' : 100), and if fromMap contains key-value pairs ('symbol': 'IBM'), ('price' : 12), then the function returns updated toMap with key-value pairs ('symbol': 'IBM'), ('volume' : 100). size (Function) Function to return the size of the map. Origin: siddhi-execution-map:5.0.5 Syntax INT map:size( OBJECT map) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic map The map for which size should be returned. OBJECT No Yes Examples EXAMPLE 1 map:size(stockDetails) Returns size of the stockDetails map. toJSON (Function) Function converts a map into a JSON object and returns the JSON as a string. Origin: siddhi-execution-map:5.0.5 Syntax STRING map:toJSON( OBJECT map) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic map The map that needs to be converted to JSON OBJECT No Yes Examples EXAMPLE 1 map:toJSON(company) If company is a map with key-value pairs, ('symbol': 'wso2'),('volume' : 100), and ('price', 200), it returns the JSON string {\"symbol\" : \"wso2\", \"volume\" : 100 , \"price\" : 200} . toXML (Function) Function returns the map as an XML string. Origin: siddhi-execution-map:5.0.5 Syntax STRING map:toXML( OBJECT map) STRING map:toXML( OBJECT map, OBJECT|STRING root.element.name) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic map The map that needs to be converted to XML. OBJECT No Yes root.element.name The root element of the map. The XML root element will be ignored OBJECT STRING Yes Yes Examples EXAMPLE 1 toXML(company, 'abcCompany') If company is a map with key-value pairs, ('symbol' : 'wso2'), ('volume' : 100), and ('price' : 200), this function returns XML as a string, abcCompany symbol wso2 /symbol volume 100 /volume price 200 /price /abcCompany . EXAMPLE 2 toXML(company) If company is a map with key-value pairs, ('symbol' : 'wso2'), ('volume' : 100), and ('price' : 200), this function returns XML without root element as a string, symbol wso2 /symbol volume 100 /volume price 200 /price . values (Function) Function to return the values of the map. Origin: siddhi-execution-map:5.0.5 Syntax OBJECT map:values( OBJECT map) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic map The map from which list if values to be returned. OBJECT No Yes Examples EXAMPLE 1 map:values(stockDetails) Returns values of the stockDetails map. tokenize (Stream Processor) Tokenize the map and return each key, value as new attributes in events Origin: siddhi-execution-map:5.0.5 Syntax map:tokenize( OBJECT map) map:tokenize( OBJECT map, OBJECT ...) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic map Hash map containing key value pairs OBJECT No Yes Extra Return Attributes Name Description Possible Types key Key of an entry consisted in the map OBJECT value Value of an entry consisted in the map. If more than one map is given, then an Array List of values from each map is returned for the value attribute. OBJECT Examples EXAMPLE 1 define stream StockStream(symbol string, price float); from StockStream#window.lengthBatch(2) select map:collect(symbol, price) as symbolPriceMap insert into TempStream; from TempStream#map:tokenize(customMap) select key, value insert into SymbolStream; Based on the length batch window, symbolPriceMap will collect two events, and the map will then again tokenized to give 2 events with key and values being symbol name and price respectively. Math percentile (Aggregate Function) This functions returns the pth percentile value of a given argument. Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:percentile( INT|LONG|FLOAT|DOUBLE arg, DOUBLE p) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value of the parameter whose percentile should be found. INT LONG FLOAT DOUBLE No Yes p Estimate of the percentile to be found (pth percentile) where p is any number greater than 0 or lesser than or equal to 100. DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (sensorId int, temperature double); from InValueStream select math:percentile(temperature, 97.0) as percentile insert into OutMediationStream; This function returns the percentile value based on the argument given. For example, math:percentile(temperature, 97.0) returns the 97 th percentile value of all the temperature events. abs (Function) This function returns the absolute value of the given parameter. It wraps the java.lang.Math.abs() function. Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:abs( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The parameter whose absolute value is found. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:abs(inValue) as absValue insert into OutMediationStream; Irrespective of whether the 'invalue' in the input stream holds a value of abs(3) or abs(-3),the function returns 3 since the absolute value of both 3 and -3 is 3. The result directed to OutMediationStream stream. acos (Function) If -1 = p1 = 1, this function returns the arc-cosine (inverse cosine) value of p1.If the domain is invalid, it returns NULL. The value returned is in radian scale. This function wraps the java.lang.Math.acos() function. Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:acos( FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose arc-cosine (inverse cosine) value is found. FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:acos(inValue) as acosValue insert into OutMediationStream; If the 'inValue' in the input stream is given, the function calculates the arc-cosine value of it and returns the arc-cosine value to the output stream, OutMediationStream. For example, acos(0.5) returns 1.0471975511965979. asin (Function) If -1 = p1 = 1, this function returns the arc-sin (inverse sine) value of p1. If the domain is invalid, it returns NULL. The value returned is in radian scale. This function wraps the java.lang.Math.asin() function. Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:asin( FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose arc-sin (inverse sine) value is found. FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:asin(inValue) as asinValue insert into OutMediationStream; If the 'inValue' in the input stream is given, the function calculates the arc-sin value of it and returns the arc-sin value to the output stream, OutMediationStream. For example, asin(0.5) returns 0.5235987755982989. atan (Function) 1. If a single p1 is received, this function returns the arc-tangent (inverse tangent) value of p1 . 2. If p1 is received along with an optional p1 , it considers them as x and y coordinates and returns the arc-tangent (inverse tangent) value. The returned value is in radian scale. This function wraps the java.lang.Math.atan() function. Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:atan( INT|LONG|FLOAT|DOUBLE p1) DOUBLE math:atan( INT|LONG|FLOAT|DOUBLE p1, INT|LONG|FLOAT|DOUBLE p2) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose arc-tangent (inverse tangent) is found. If the optional second parameter is given this represents the x coordinate of the (x,y) coordinate pair. INT LONG FLOAT DOUBLE No Yes p2 This optional parameter represents the y coordinate of the (x,y) coordinate pair. 0D INT LONG FLOAT DOUBLE Yes Yes Examples EXAMPLE 1 define stream InValueStream (inValue1 double, inValue2 double); from InValueStream select math:atan(inValue1, inValue2) as convertedValue insert into OutMediationStream; If the 'inValue1' in the input stream is given, the function calculates the arc-tangent value of it and returns the arc-tangent value to the output stream, OutMediationStream. If both the 'inValue1' and 'inValue2' are given, then the function considers them to be x and y coordinates respectively and returns the calculated arc-tangent value to the output stream, OutMediationStream. For example, atan(12d, 5d) returns 1.1760052070951352. bin (Function) This function returns a string representation of the p1 argument, that is of either 'integer' or 'long' data type, as an unsigned integer in base 2. It wraps the java.lang.Integer.toBinaryString and java.lang.Long.toBinaryString` methods. Origin: siddhi-execution-math:5.0.4 Syntax STRING math:bin( INT|LONG p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value in either 'integer' or 'long', that should be converted into an unsigned integer of base 2. INT LONG No Yes Examples EXAMPLE 1 define stream InValueStream (inValue long); from InValueStream select math:bin(inValue) as binValue insert into OutMediationStream; If the 'inValue' in the input stream is given, the function converts it into an unsigned integer in base 2 and directs the output to the output stream, OutMediationStream. For example, bin(9) returns '1001'. cbrt (Function) This function returns the cube-root of 'p1' which is in radians. It wraps the java.lang.Math.cbrt() function. Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:cbrt( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose cube-root should be found. Input is required to be in radians. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:cbrt(inValue) as cbrtValue insert into OutMediationStream; If the 'inValue' is given, the function calculates the cube-root value for the same and directs the output to the output stream, OutMediationStream. For example, cbrt(17d) returns 2.5712815906582356. ceil (Function) This function returns the smallest double value, i.e., the closest to the negative infinity, that is greater than or equal to the p1 argument, and is equal to a mathematical integer. It wraps the java.lang.Math.ceil() method. Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:ceil( FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose ceiling value is found. FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:ceil(inValue) as ceilingValue insert into OutMediationStream; This function calculates the ceiling value of the given 'inValue' and directs the result to 'OutMediationStream' output stream. For example, ceil(423.187d) returns 424.0. conv (Function) This function converts a from the fromBase base to the toBase base. Origin: siddhi-execution-math:5.0.4 Syntax STRING math:conv( STRING a, INT from.base, INT to.base) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic a The value whose base should be changed. Input should be given as a 'String'. STRING No Yes from.base The source base of the input parameter 'a'. INT No Yes to.base The target base that the input parameter 'a' should be converted into. INT No Yes Examples EXAMPLE 1 define stream InValueStream (inValue string,fromBase int,toBase int); from InValueStream select math:conv(inValue,fromBase,toBase) as convertedValue insert into OutMediationStream; If the 'inValue' in the input stream is given, and the base in which it currently resides in and the base to which it should be converted to is specified, the function converts it into a string in the target base and directs it to the output stream, OutMediationStream. For example, conv(\"7f\", 16, 10) returns \"127\". copySign (Function) This function returns a value of an input with the received magnitude and sign of another input. It wraps the java.lang.Math.copySign() function. Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:copySign( INT|LONG|FLOAT|DOUBLE magnitude, INT|LONG|FLOAT|DOUBLE sign) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic magnitude The magnitude of this parameter is used in the output attribute. INT LONG FLOAT DOUBLE No Yes sign The sign of this parameter is used in the output attribute. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue1 double, inValue2 double); from InValueStream select math:copySign(inValue1,inValue2) as copysignValue insert into OutMediationStream; If two values are provided as 'inValue1' and 'inValue2', the function copies the magnitude and sign of the second argument into the first one and directs the result to the output stream, OutMediatonStream. For example, copySign(5.6d, -3.0d) returns -5.6. cos (Function) This function returns the cosine of p1 which is in radians. It wraps the java.lang.Math.cos() function. Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:cos( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose cosine value should be found.The input is required to be in radians. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:cos(inValue) as cosValue insert into OutMediationStream; If the 'inValue' is given, the function calculates the cosine value for the same and directs the output to the output stream, OutMediationStream. For example, cos(6d) returns 0.9601702866503661. cosh (Function) This function returns the hyperbolic cosine of p1 which is in radians. It wraps the java.lang.Math.cosh() function. Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:cosh( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose hyperbolic cosine should be found. The input is required to be in radians. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:cosh(inValue) as cosValue insert into OutMediationStream; If the 'inValue' is given, the function calculates the hyperbolic cosine value for the same and directs the output to the output stream, OutMediationStream. For example, cosh (6d) returns 201.7156361224559. e (Function) This function returns the java.lang.Math.E constant, which is the closest double value to e, where e is the base of the natural logarithms. Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:e() Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:e() as eValue insert into OutMediationStream; This function returns the constant, 2.7182818284590452354 which is the closest double value to e and directs the output to 'OutMediationStream' output stream. exp (Function) This function returns the Euler's number e raised to the power of p1 . It wraps the java.lang.Math.exp() function. Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:exp( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The power that the Euler's number e is raised to. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:exp(inValue) as expValue insert into OutMediationStream; If the 'inValue' in the inputstream holds a value, this function calculates the corresponding Euler's number 'e' and directs it to the output stream, OutMediationStream. For example, exp(10.23) returns 27722.51006805505. floor (Function) This function wraps the java.lang.Math.floor() function and returns the largest value, i.e., closest to the positive infinity, that is less than or equal to p1 , and is equal to a mathematical integer. Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:floor( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose floor value should be found. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:floor(inValue) as floorValue insert into OutMediationStream; This function calculates the floor value of the given 'inValue' input and directs the output to the 'OutMediationStream' output stream. For example, (10.23) returns 10.0. getExponent (Function) This function returns the unbiased exponent that is used in the representation of p1 . This function wraps the java.lang.Math.getExponent() function. Origin: siddhi-execution-math:5.0.4 Syntax INT math:getExponent( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of whose unbiased exponent representation should be found. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:getExponent(inValue) as expValue insert into OutMediationStream; This function calculates the unbiased exponent of a given input, 'inValue' and directs the result to the 'OutMediationStream' output stream. For example, getExponent(60984.1) returns 15. hex (Function) This function wraps the java.lang.Double.toHexString() function. It returns a hexadecimal string representation of the input, p1`. Origin: siddhi-execution-math:5.0.4 Syntax STRING math:hex( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose hexadecimal value should be found. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue int); from InValueStream select math:hex(inValue) as hexString insert into OutMediationStream; If the 'inValue' in the input stream is provided, the function converts this into its corresponding hexadecimal format and directs the output to the output stream, OutMediationStream. For example, hex(200) returns \"c8\". isInfinite (Function) This function wraps the java.lang.Float.isInfinite() and java.lang.Double.isInfinite() and returns true if p1 is infinitely large in magnitude and false if otherwise. Origin: siddhi-execution-math:5.0.4 Syntax BOOL math:isInfinite( FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 This is the value of the parameter that the function determines to be either infinite or finite. FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue1 double,inValue2 int); from InValueStream select math:isInfinite(inValue1) as isInfinite insert into OutMediationStream; If the value given in the 'inValue' in the input stream is of infinitely large magnitude, the function returns the value, 'true' and directs the result to the output stream, OutMediationStream'. For example, isInfinite(java.lang.Double.POSITIVE_INFINITY) returns true. isNan (Function) This function wraps the java.lang.Float.isNaN() and java.lang.Double.isNaN() functions and returns true if p1 is NaN (Not-a-Number), and returns false if otherwise. Origin: siddhi-execution-math:5.0.4 Syntax BOOL math:isNan( FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter which the function determines to be either NaN or a number. FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue1 double,inValue2 int); from InValueStream select math:isNan(inValue1) as isNaN insert into OutMediationStream; If the 'inValue1' in the input stream has a value that is undefined, then the function considers it as an 'NaN' value and directs 'True' to the output stream, OutMediationStream. For example, isNan(java.lang.Math.log(-12d)) returns true. ln (Function) This function returns the natural logarithm (base e) of p1 . Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:ln( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose natural logarithm (base e) should be found. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:ln(inValue) as lnValue insert into OutMediationStream; If the 'inValue' in the input stream is given, the function calculates its natural logarithm (base e) and directs the results to the output stream, 'OutMeditionStream'. For example, ln(11.453) returns 2.438251704415579. log (Function) This function returns the logarithm of the received number as per the given base . Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:log( INT|LONG|FLOAT|DOUBLE number, INT|LONG|FLOAT|DOUBLE base) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic number The value of the parameter whose base should be changed. INT LONG FLOAT DOUBLE No Yes base The base value of the ouput. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (number double, base double); from InValueStream select math:log(number, base) as logValue insert into OutMediationStream; If the number and the base to which it has to be converted into is given in the input stream, the function calculates the number to the base specified and directs the result to the output stream, OutMediationStream. For example, log(34, 2f) returns 5.08746284125034. log10 (Function) This function returns the base 10 logarithm of p1 . Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:log10( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose base 10 logarithm should be found. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:log10(inValue) as lnValue insert into OutMediationStream; If the 'inValue' in the input stream is given, the function calculates the base 10 logarithm of the same and directs the result to the output stream, OutMediatioStream. For example, log10(19.234) returns 1.2840696117100832. log2 (Function) This function returns the base 2 logarithm of p1 . Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:log2( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose base 2 logarithm should be found. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:log2(inValue) as lnValue insert into OutMediationStream; If the 'inValue' in the input stream is given, the function calculates the base 2 logarithm of the same and returns the value to the output stream, OutMediationStream. For example log2(91d) returns 6.507794640198696. max (Function) This function returns the greater value of p1 and p2 . Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:max( INT|LONG|FLOAT|DOUBLE p1, INT|LONG|FLOAT|DOUBLE p2) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 One of the input values to be compared in order to find the larger value of the two INT LONG FLOAT DOUBLE No Yes p2 The input value to be compared with 'p1' in order to find the larger value of the two. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue1 double,inValue2 int); from InValueStream select math:max(inValue1,inValue2) as maxValue insert into OutMediationStream; If two input values 'inValue1, and 'inValue2' are given, the function compares them and directs the larger value to the output stream, OutMediationStream. For example, max(123.67d, 91) returns 123.67. min (Function) This function returns the smaller value of p1 and p2 . Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:min( INT|LONG|FLOAT|DOUBLE p1, INT|LONG|FLOAT|DOUBLE p2) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 One of the input values that are to be compared in order to find the smaller value. INT LONG FLOAT DOUBLE No Yes p2 The input value that is to be compared with 'p1' in order to find the smaller value. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue1 double,inValue2 int); from InValueStream select math:min(inValue1,inValue2) as minValue insert into OutMediationStream; If two input values, 'inValue1' and 'inValue2' are given, the function compares them and directs the smaller value of the two to the output stream, OutMediationStream. For example, min(123.67d, 91) returns 91. oct (Function) This function converts the input parameter p1 to octal. Origin: siddhi-execution-math:5.0.4 Syntax STRING math:oct( INT|LONG p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose octal representation should be found. INT LONG No Yes Examples EXAMPLE 1 define stream InValueStream (inValue long); from InValueStream select math:oct(inValue) as octValue insert into OutMediationStream; If the 'inValue' in the input stream is given, this function calculates the octal value corresponding to the same and directs it to the output stream, OutMediationStream. For example, oct(99l) returns \"143\". parseDouble (Function) This function returns the double value of the string received. Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:parseDouble( STRING p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that should be converted into a double value. STRING No Yes Examples EXAMPLE 1 define stream InValueStream (inValue string); from InValueStream select math:parseDouble(inValue) as output insert into OutMediationStream; If the 'inValue' in the input stream holds a value, this function converts it into the corresponding double value and directs it to the output stream, OutMediationStream. For example, parseDouble(\"123\") returns 123.0. parseFloat (Function) This function returns the float value of the received string. Origin: siddhi-execution-math:5.0.4 Syntax FLOAT math:parseFloat( STRING p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that should be converted into a float value. STRING No Yes Examples EXAMPLE 1 define stream InValueStream (inValue string); from InValueStream select math:parseFloat(inValue) as output insert into OutMediationStream; The function converts the input value given in 'inValue',into its corresponding float value and directs the result into the output stream, OutMediationStream. For example, parseFloat(\"123\") returns 123.0. parseInt (Function) This function returns the integer value of the received string. Origin: siddhi-execution-math:5.0.4 Syntax INT math:parseInt( STRING p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that should be converted to an integer. STRING No Yes Examples EXAMPLE 1 define stream InValueStream (inValue string); from InValueStream select math:parseInt(inValue) as output insert into OutMediationStream; The function converts the 'inValue' into its corresponding integer value and directs the output to the output stream, OutMediationStream. For example, parseInt(\"123\") returns 123. parseLong (Function) This function returns the long value of the string received. Origin: siddhi-execution-math:5.0.4 Syntax LONG math:parseLong( STRING p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that should be converted to a long value. STRING No Yes Examples EXAMPLE 1 define stream InValueStream (inValue string); from InValueStream select math:parseLong(inValue) as output insert into OutMediationStream; The function converts the 'inValue' to its corresponding long value and directs the result to the output stream, OutMediationStream. For example, parseLong(\"123\") returns 123. pi (Function) This function returns the java.lang.Math.PI constant, which is the closest value to pi, i.e., the ratio of the circumference of a circle to its diameter. Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:pi() Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:pi() as piValue insert into OutMediationStream; pi() always returns 3.141592653589793. power (Function) This function raises the given value to a given power. Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:power( INT|LONG|FLOAT|DOUBLE value, INT|LONG|FLOAT|DOUBLE to.power) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic value The value that should be raised to the power of 'to.power' input parameter. INT LONG FLOAT DOUBLE No Yes to.power The power to which the 'value' input parameter should be raised. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue1 double, inValue2 double); from InValueStream select math:power(inValue1,inValue2) as powerValue insert into OutMediationStream; This function raises the 'inValue1' to the power of 'inValue2' and directs the output to the output stream, 'OutMediationStream. For example, (5.6d, 3.0d) returns 175.61599999999996. rand (Function) This returns a stream of pseudo-random numbers when a sequence of calls are sent to the rand() . Optionally, it is possible to define a seed, i.e., rand(seed) using which the pseudo-random numbers are generated. These functions internally use the java.util.Random class. Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:rand() DOUBLE math:rand( INT|LONG seed) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic seed An optional seed value that will be used to generate the random number sequence. defaultSeed INT LONG Yes Yes Examples EXAMPLE 1 define stream InValueStream (symbol string, price long, volume long); from InValueStream select symbol, math:rand() as randNumber select math:oct(inValue) as octValue insert into OutMediationStream; In the example given above, a random double value between 0 and 1 will be generated using math:rand(). round (Function) This function returns the value of the input argument rounded off to the closest integer/long value. Origin: siddhi-execution-math:5.0.4 Syntax INT|LONG math:round( FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that should be rounded off to the closest integer/long value. FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:round(inValue) as roundValue insert into OutMediationStream; The function rounds off 'inValue1' to the closest int/long value and directs the output to the output stream, 'OutMediationStream'. For example, round(3252.353) returns 3252. signum (Function) This returns +1, 0, or -1 for the given positive, zero and negative values respectively. This function wraps the java.lang.Math.signum() function. Origin: siddhi-execution-math:5.0.4 Syntax INT math:signum( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that should be checked to be positive, negative or zero. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:signum(inValue) as sign insert into OutMediationStream; The function evaluates the 'inValue' given to be positive, negative or zero and directs the result to the output stream, 'OutMediationStream'. For example, signum(-6.32d) returns -1. sin (Function) This returns the sine of the value given in radians. This function wraps the java.lang.Math.sin() function. Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:sin( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose sine value should be found. Input is required to be in radians. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:sin(inValue) as sinValue insert into OutMediationStream; The function calculates the sine value of the given 'inValue' and directs the output to the output stream, 'OutMediationStream. For example, sin(6d) returns -0.27941549819892586. sinh (Function) This returns the hyperbolic sine of the value given in radians. This function wraps the java.lang.Math.sinh() function. Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:sinh( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose hyperbolic sine value should be found. Input is required to be in radians. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:sinh(inValue) as sinhValue insert into OutMediationStream; This function calculates the hyperbolic sine value of 'inValue' and directs the output to the output stream, 'OutMediationStream'. For example, sinh(6d) returns 201.71315737027922. sqrt (Function) This function returns the square-root of the given value. It wraps the java.lang.Math.sqrt() s function. Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:sqrt( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose square-root value should be found. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:sqrt(inValue) as sqrtValue insert into OutMediationStream; The function calculates the square-root value of the 'inValue' and directs the output to the output stream, 'OutMediationStream'. For example, sqrt(4d) returns 2. tan (Function) This function returns the tan of the given value in radians. It wraps the java.lang.Math.tan() function. Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:tan( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose tan value should be found. Input is required to be in radians. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:tan(inValue) as tanValue insert into OutMediationStream; This function calculates the tan value of the 'inValue' given and directs the output to the output stream, 'OutMediationStream'. For example, tan(6d) returns -0.29100619138474915. tanh (Function) This function returns the hyperbolic tangent of the value given in radians. It wraps the java.lang.Math.tanh() function. Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:tanh( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose hyperbolic tangent value should be found. Input is required to be in radians. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:tanh(inValue) as tanhValue insert into OutMediationStream; If the 'inVaue' in the input stream is given, this function calculates the hyperbolic tangent value of the same and directs the output to 'OutMediationStream' stream. For example, tanh(6d) returns 0.9999877116507956. toDegrees (Function) This function converts the value given in radians to degrees. It wraps the java.lang.Math.toDegrees() function. Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:toDegrees( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The input value in radians that should be converted to degrees. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:toDegrees(inValue) as degreesValue insert into OutMediationStream; The function converts the 'inValue' in the input stream from radians to degrees and directs the output to 'OutMediationStream' output stream. For example, toDegrees(6d) returns 343.77467707849394. toRadians (Function) This function converts the value given in degrees to radians. It wraps the java.lang.Math.toRadians() function. Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:toRadians( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The input value in degrees that should be converted to radians. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:toRadians(inValue) as radiansValue insert into OutMediationStream; This function converts the input, from degrees to radians and directs the result to 'OutMediationStream' output stream. For example, toRadians(6d) returns 0.10471975511965977. Rdbms cud (Stream Processor) This function performs SQL CUD (INSERT, UPDATE, DELETE) queries on data sources. Note: This function to work data sources should be set at the Siddhi Manager level. Origin: siddhi-store-rdbms:7.0.2 Syntax rdbms:cud( STRING datasource.name, STRING query) rdbms:cud( STRING datasource.name, STRING query, STRING|BOOL|INT|DOUBLE|FLOAT|LONG parameter) rdbms:cud( STRING datasource.name, STRING query, STRING|BOOL|INT|DOUBLE|FLOAT|LONG parameter, STRING|BOOL|INT|DOUBLE|FLOAT|LONG ...) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic datasource.name The name of the datasource for which the query should be performed. If Siddhi is used as a Java/Python library the datasource should be explicitly set in the siddhi manager in order for the function to work. STRING No No query The update, delete, or insert query(formatted according to the relevant database type) that needs to be performed. STRING No Yes parameter If the second parameter is a parametrised SQL query, then siddhi attributes can be passed to set the values of the parameters STRING BOOL INT DOUBLE FLOAT LONG Yes Yes System Parameters Name Description Default Value Possible Parameters perform.CUD.operations If this parameter is set to 'true', the RDBMS CUD function is enabled to perform CUD operations. false true false Extra Return Attributes Name Description Possible Types numRecords The number of records manipulated by the query. INT Examples EXAMPLE 1 from TriggerStream#rdbms:cud(\"SAMPLE_DB\", \"UPDATE Customers_Table SET customerName='abc' where customerName='xyz'\") select numRecords insert into RecordStream; This query updates the events from the input stream named 'TriggerStream' with an additional attribute named 'numRecords', of which the value indicates the number of records manipulated. The updated events are inserted into an output stream named 'RecordStream'. EXAMPLE 2 from TriggerStream#rdbms:cud(\"SAMPLE_DB\", \"UPDATE Customers_Table SET customerName=? where customerName=?\", changedName, previousName) select numRecords insert into RecordStream; This query updates the events from the input stream named 'TriggerStream' with an additional attribute named 'numRecords', of which the value indicates the number of records manipulated. The updated events are inserted into an output stream named 'RecordStream'. Here the values of attributes changedName and previousName in the event will be set to the query. query (Stream Processor) This function performs SQL retrieval queries on data sources. Note: This function to work data sources should be set at the Siddhi Manager level. Origin: siddhi-store-rdbms:7.0.2 Syntax rdbms:query( STRING datasource.name, STRING attribute.definition.list, STRING query) rdbms:query( STRING datasource.name, STRING attribute.definition.list, STRING query, STRING|BOOL|INT|DOUBLE|FLOAT|LONG parameter) rdbms:query( STRING datasource.name, STRING attribute.definition.list, STRING query, STRING|BOOL|INT|DOUBLE|FLOAT|LONG parameter, STRING|BOOL|INT|DOUBLE|FLOAT|LONG ...) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic datasource.name The name of the datasource for which the query should be performed. If Siddhi is used as a Java/Python library the datasource should be explicitly set in the siddhi manager in order for the function to work. STRING No No attribute.definition.list This is provided as a comma-separated list in the ' AttributeName AttributeType ' format. The SQL query is expected to return the attributes in the given order. e.g., If one attribute is defined here, the SQL query should return one column result set. If more than one column is returned, then the first column is processed. The Siddhi data types supported are 'STRING', 'INT', 'LONG', 'DOUBLE', 'FLOAT', and 'BOOL'. Mapping of the Siddhi data type to the database data type can be done as follows, Siddhi Datatype - Datasource Datatype STRING - CHAR , VARCHAR , LONGVARCHAR INT - INTEGER LONG - BIGINT DOUBLE - DOUBLE FLOAT - REAL BOOL - BIT STRING No No query The select query(formatted according to the relevant database type) that needs to be performed STRING No Yes parameter If the second parameter is a parametrised SQL query, then siddhi attributes can be passed to set the values of the parameters STRING BOOL INT DOUBLE FLOAT LONG Yes Yes Extra Return Attributes Name Description Possible Types attributeName The return attributes will be the ones defined in the parameter attribute.definition.list . STRING INT LONG DOUBLE FLOAT BOOL Examples EXAMPLE 1 from TriggerStream#rdbms:query('SAMPLE_DB', 'creditcardno string, country string, transaction string, amount int', 'select * from Transactions_Table') select creditcardno, country, transaction, amount insert into recordStream; Events inserted into recordStream includes all records matched for the query i.e an event will be generated for each record retrieved from the datasource. The event will include as additional attributes, the attributes defined in the attribute.definition.list (creditcardno, country, transaction, amount). EXAMPLE 2 from TriggerStream#rdbms:query('SAMPLE_DB', 'creditcardno string, country string,transaction string, amount int', 'select * from where country=?', countrySearchWord) select creditcardno, country, transaction, amount insert into recordStream; Events inserted into recordStream includes all records matched for the query i.e an event will be generated for each record retrieved from the datasource. The event will include as additional attributes, the attributes defined in the attribute.definition.list (creditcardno, country, transaction, amount). countrySearchWord value from the event will be set in the query when querying the datasource. Regex find (Function) Finds the subsequence that matches the given regex pattern. Origin: siddhi-execution-regex:5.0.5 Syntax BOOL regex:find( STRING regex, STRING input.sequence) BOOL regex:find( STRING regex, STRING input.sequence, INT starting.index) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic regex A regular expression that is matched to a sequence in order to find the subsequence of the same. For example, \\d\\d(.*)WSO2 . STRING No Yes input.sequence The input sequence to be matched with the regular expression. For example, 21 products are produced by WSO2 . STRING No Yes starting.index The starting index of the input sequence from where the input sequence ismatched with the given regex pattern.For example, 10 . 0 INT Yes Yes Examples EXAMPLE 1 regex:find('\\d\\d(.*)WSO2', '21 products are produced by WSO2 currently') This method attempts to find the subsequence of the input.sequence that matches the regex pattern, \\d\\d(. )WSO2 . It returns true as a subsequence exists. EXAMPLE 2 regex:find('\\d\\d(.*)WSO2', '21 products are produced by WSO2.', 4) This method attempts to find the subsequence of the input.sequence that matches the regex pattern, \\d\\d(. )WSO2 starting from index 4 . It returns 'false' as subsequence does not exists. group (Function) Returns the subsequence captured by the given group during the regex match operation. Origin: siddhi-execution-regex:5.0.5 Syntax STRING regex:group( STRING regex, STRING input.sequence, INT group.id) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic regex A regular expression. For example, \\d\\d(.*)WSO2. STRING No Yes input.sequence The input sequence to be matched with the regular expression. For example, 2 1 products are produced by WSO2 . STRING No Yes group.id The given group id of the regex expression. For example, 2 . INT No Yes Examples EXAMPLE 1 regex:group('\\d\\d(.*)(WSO2.*)(WSO2.*)', '21 products are produced within 10 years by WSO2 currently by WSO2 employees', 3) Function returns 'WSO2 employees', the subsequence captured by the groupID 3 according to the regex pattern, \\d\\d(. )(WSO2. )(WSO2.*) . lookingAt (Function) Matches the input.sequence from the beginning against the regex pattern, and unlike regex:matches() it does not require that the entire input.sequence be matched. Origin: siddhi-execution-regex:5.0.5 Syntax BOOL regex:lookingAt( STRING regex, STRING input.sequence) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic regex A regular expression. For example, \\d\\d(.*)WSO2 . STRING No Yes input.sequence The input sequence to be matched with the regular expression. For example, 21 products are produced by WSO2 . STRING No Yes Examples EXAMPLE 1 regex:lookingAt('\\d\\d(.*)(WSO2.*)', '21 products are produced by WSO2 currently in Sri Lanka') Function matches the input.sequence against the regex pattern, \\d\\d(. )(WSO2. ) from the beginning, and as it matches it returns true . EXAMPLE 2 regex:lookingAt('WSO2(.*)middleware(.*)', 'sample test string and WSO2 is situated in trace and it's a middleware company') Function matches the input.sequence against the regex pattern, WSO2(. )middleware(. ) from the beginning, and as it does not match it returns false . matches (Function) Matches the entire input.sequence against the regex pattern. Origin: siddhi-execution-regex:5.0.5 Syntax BOOL regex:matches( STRING regex, STRING input.sequence) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic regex A regular expression. For example, \\d\\d(.*)WSO2 . STRING No Yes input.sequence The input sequence to be matched with the regular expression. For example, 21 products are produced by WSO2 . STRING No Yes Examples EXAMPLE 1 regex:matches('WSO2(.*)middleware(.*)', 'WSO2 is situated in trace and its a middleware company') Function matches the entire input.sequence against WSO2(. )middleware(. ) regex pattern, and as it matches it returns true . EXAMPLE 2 regex:matches('WSO2(.*)middleware', 'WSO2 is situated in trace and its a middleware company') Function matches the entire input.sequence against WSO2(.*)middleware regex pattern. As it does not match it returns false . Reorder akslack (Stream Processor) Stream processor performs reordering of out-of-order events optimized for a givenparameter using AQ-K-Slack algorithm . This is best for reordering events on attributes those are used for aggregations.data . Origin: siddhi-execution-reorder:5.0.3 Syntax reorder:akslack( LONG timestamp, INT|FLOAT|LONG|DOUBLE correlation.field) reorder:akslack( LONG timestamp, INT|FLOAT|LONG|DOUBLE correlation.field, LONG batch.size) reorder:akslack( LONG timestamp, INT|FLOAT|LONG|DOUBLE correlation.field, LONG batch.size, LONG timeout) reorder:akslack( LONG timestamp, INT|FLOAT|LONG|DOUBLE correlation.field, LONG batch.size, LONG timeout, LONG max.k) reorder:akslack( LONG timestamp, INT|FLOAT|LONG|DOUBLE correlation.field, LONG batch.size, LONG timeout, LONG max.k, BOOL discard.late.arrival) reorder:akslack( LONG timestamp, INT|FLOAT|LONG|DOUBLE correlation.field, LONG batch.size, LONG timeout, LONG max.k, BOOL discard.late.arrival, DOUBLE error.threshold, DOUBLE confidence.level) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic timestamp The event timestamp on which the events should be ordered. LONG No Yes correlation.field By monitoring the changes in this field Alpha K-Slack dynamically optimises its behavior. This field is used to calculate the runtime window coverage threshold, which represents the upper limit set for unsuccessfully handled late arrivals. INT FLOAT LONG DOUBLE No Yes batch.size The parameter 'batch.size' denotes the number of events that should be considered in the calculation of an alpha value. This should be greater than or equal to 15. 10,000 LONG Yes No timeout A timeout value in milliseconds, where the buffered events who are older than the given timeout period get flushed every second. -1 (timeout is infinite) LONG Yes No max.k The maximum K-Slack window threshold ('K' parameter). 9,223,372,036,854,775,807 (The maximum Long value) LONG Yes No discard.late.arrival If set to true the processor would discarded the out-of-order events arriving later than the K-Slack window, and in otherwise it allows the late arrivals to proceed. false BOOL Yes No error.threshold The error threshold to be applied in Alpha K-Slack algorithm. 0.03 (3%) DOUBLE Yes No confidence.level The confidence level to be applied in Alpha K-Slack algorithm. 0.95 (95%) DOUBLE Yes No Examples EXAMPLE 1 define stream StockStream (eventTime long, symbol string, volume long); @info(name = 'query1') from StockStream#reorder:akslack(eventTime, volume, 20)#window.time(5 min) select eventTime, symbol, sum(volume) as total insert into OutputStream; The query reorders events based on the 'eventTime' attribute value and optimises for aggregating 'volume' attribute considering last 20 events. kslack (Stream Processor) Stream processor performs reordering of out-of-order events using K-Slack algorithm . Origin: siddhi-execution-reorder:5.0.3 Syntax reorder:kslack( LONG timestamp) reorder:kslack( LONG timestamp, LONG timeout) reorder:kslack( LONG timestamp, BOOL discard.late.arrival) reorder:kslack( LONG timestamp, LONG timeout, LONG max.k) reorder:kslack( LONG timestamp, LONG timeout, BOOL discard.late.arrival) reorder:kslack( LONG timestamp, LONG timeout, LONG max.k, BOOL discard.late.arrival) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic timestamp The event timestamp on which the events should be ordered. LONG No Yes timeout A timeout value in milliseconds, where the buffered events who are older than the given timeout period get flushed every second. -1 (timeout is infinite) LONG Yes No max.k The maximum K-Slack window threshold ('K' parameter). 9,223,372,036,854,775,807 (The maximum Long value) LONG Yes No discard.late.arrival If set to true the processor would discarded the out-of-order events arriving later than the K-Slack window, and in otherwise it allows the late arrivals to proceed. false BOOL Yes No Examples EXAMPLE 1 define stream StockStream (eventTime long, symbol string, volume long); @info(name = 'query1') from StockStream#reorder:kslack(eventTime, 5000) select eventTime, symbol, volume insert into OutputStream; The query reorders events based on the 'eventTime' attribute value, and it forcefully flushes all the events who have arrived older than the given 'timeout' value ( 5000 milliseconds) every second. Script javascript (Script) This extension allows you to include JavaScript functions within the Siddhi Query Language. Origin: siddhi-script-js:5.0.2 Syntax define function FunctionName [javascript] return type { // Script code }; Examples EXAMPLE 1 define function concatJ[JavaScript] return string {\" var str1 = data[0]; var str2 = data[1]; var str3 = data[2]; var res = str1.concat(str2,str3); return res; }; This JS function will consume 3 var variables, concatenate them and will return as a string Sink email (Sink) The email sink uses the 'smtp' server to publish events via emails. The events can be published in 'text', 'xml' or 'json' formats. The user can define email sink parameters in either the ' SP_HOME /conf/ PROFILE /deployment yaml' file or in the stream definition. The email sink first checks the stream definition for parameters, and if they are no configured there, it checks the 'deployment.yaml' file. If the parameters are not configured in either place, default values are considered for optional parameters. If you need to configure server system parameters that are not provided as options in the stream definition, then those parameters need to be defined them in the 'deployment.yaml' file under 'email sink properties'. For more information about the SMTP server parameters, see https://javaee.github.io/javamail/SMTP-Transport. Further, some email accounts are required to enable the 'access to less secure apps' option. For gmail accounts, you can enable this option via https://myaccount.google.com/lesssecureapps. Origin: siddhi-io-email:2.0.5 Syntax @sink(type=\"email\", username=\" STRING \", address=\" STRING \", password=\" STRING \", host=\" STRING \", port=\" INT \", ssl.enable=\" BOOL \", auth=\" BOOL \", content.type=\" STRING \", subject=\" STRING \", to=\" STRING \", cc=\" STRING \", bcc=\" STRING \", attachments=\" STRING \", connection.pool.size=\" INT \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic username The username of the email account that is used to send emails. e.g., 'abc' is the username of the 'abc@gmail.com' account. STRING No No address The address of the email account that is used to send emails. STRING No No password The password of the email account. STRING No No host The host name of the SMTP server. e.g., 'smtp.gmail.com' is a host name for a gmail account. The default value 'smtp.gmail.com' is only valid if the email account is a gmail account. smtp.gmail.com STRING Yes No port The port that is used to create the connection. '465' the default value is only valid is SSL is enabled. INT Yes No ssl.enable This parameter specifies whether the connection should be established via a secure connection or not. The value can be either 'true' or 'false'. If it is 'true', then the connection is establish via the 493 port which is a secure connection. true BOOL Yes No auth This parameter specifies whether to use the 'AUTH' command when authenticating or not. If the parameter is set to 'true', an attempt is made to authenticate the user using the 'AUTH' command. true BOOL Yes No content.type The content type can be either 'text/plain' or 'text/html'. text/plain STRING Yes No subject The subject of the mail to be send. STRING No Yes to The address of the 'to' recipient. If there are more than one 'to' recipients, then all the required addresses can be given as a comma-separated list. STRING No Yes cc The address of the 'cc' recipient. If there are more than one 'cc' recipients, then all the required addresses can be given as a comma-separated list. None STRING Yes No bcc The address of the 'bcc' recipient. If there are more than one 'bcc' recipients, then all the required addresses can be given as a comma-separated list. None STRING Yes No attachments File paths of the files that need to be attached to the email. These paths should be absolute paths. They can be either directories or files . If the path is to a directory, all the files located at the first level (i.e., not within another sub directory) are attached. None STRING Yes Yes connection.pool.size Number of concurrent Email client connections. 1 INT Yes No System Parameters Name Description Default Value Possible Parameters mail.smtp.ssl.trust If this parameter is se, and a socket factory has not been specified, it enables the use of a MailSSLSocketFactory. If this parameter is set to \" \", all the hosts are trusted. If it is set to a whitespace-separated list of hosts, only those specified hosts are trusted. If not, the hosts trusted depends on the certificate presented by the server. String mail.smtp.connectiontimeout The socket connection timeout value in milliseconds. infinite timeout Any Integer mail.smtp.timeout The socket I/O timeout value in milliseconds. infinite timeout Any Integer mail.smtp.from The email address to use for the SMTP MAIL command. This sets the envelope return address. Defaults to msg.getFrom() or InternetAddress.getLocalAddress(). Any valid email address mail.smtp.localport The local port number to bind to when creating the SMTP socket. Defaults to the port number picked by the Socket class. Any Integer mail.smtp.ehlo If this parameter is set to 'false', you must not attempt to sign in with the EHLO command. true true or false mail.smtp.auth.login.disable If this is set to 'true', it is not allowed to use the 'AUTH LOGIN' command. false true or false mail.smtp.auth.plain.disable If this parameter is set to 'true', it is not allowed to use the 'AUTH PLAIN' command. false true or false mail.smtp.auth.digest-md5.disable If this parameter is set to 'true', it is not allowed to use the 'AUTH DIGEST-MD5' command. false true or false mail.smtp.auth.ntlm.disable If this parameter is set to 'true', it is not allowed to use the 'AUTH NTLM' command false true or false mail.smtp.auth.ntlm.domain The NTLM authentication domain. None The valid NTLM authentication domain name. mail.smtp.auth.ntlm.flags NTLM protocol-specific flags. For more details, see http://curl.haxx.se/rfc/ntlm.html#theNtlmFlags. None Valid NTLM protocol-specific flags. mail.smtp.dsn.notify The NOTIFY option to the RCPT command. None Either 'NEVER', or a combination of 'SUCCESS', 'FAILURE', and 'DELAY' (separated by commas). mail.smtp.dsn.ret The 'RET' option to the 'MAIL' command. None Either 'FULL' or 'HDRS'. mail.smtp.sendpartial If this parameter is set to 'true' and a message is addressed to both valid and invalid addresses, the message is sent with a log that reports the partial failure with a 'SendFailedException' error. If this parameter is set to 'false' (which is default), the message is not sent to any of the recipients when the recipient lists contain one or more invalid addresses. false true or false mail.smtp.sasl.enable If this parameter is set to 'true', the system attempts to use the 'javax.security.sasl' package to choose an authentication mechanism for the login. false true or false mail.smtp.sasl.mechanisms Enter a space or a comma-separated list of SASL mechanism names that the system shouldt try to use. None mail.smtp.sasl.authorizationid The authorization ID to be used in the SASL authentication. If no value is specified, the authentication ID (i.e., username) is used. username Valid ID mail.smtp.sasl.realm The realm to be used with the 'DIGEST-MD5' authentication. None mail.smtp.quitwait If this parameter is set to 'false', the 'QUIT' command is issued and the connection is immediately closed. If this parameter is set to 'true' (which is default), the transport waits for the response to the QUIT command. false true or false mail.smtp.reportsuccess If this parameter is set to 'true', the transport to includes an 'SMTPAddressSucceededException' for each address to which the message is successfully delivered. false true or false mail.smtp.socketFactory If this parameter is set to a class that implements the 'javax.net.SocketFactory' interface, this class is used to create SMTP sockets. None Socket Factory mail.smtp.socketFactory.class If this parameter is set, it specifies the name of a class that implements the 'javax.net.SocketFactory interface'. This class is used to create SMTP sockets. None mail.smtp.socketFactory.fallback If this parameter is set to 'true', the failure to create a socket using the specified socket factory class causes the socket to be created using the 'java.net.Socket' class. true true or false mail.smtp.socketFactory.port This specifies the port to connect to when using the specified socket factory. 25 Valid port number mail.smtp.ssl.protocols This specifies the SSL protocols that need to be enabled for the SSL connections. None This parameter specifies a whitespace separated list of tokens that are acceptable to the 'javax.net.ssl.SSLSocket.setEnabledProtocols' method. mail.smtp.starttls.enable If this parameter is set to 'true', it is possible to issue the 'STARTTLS' command (if supported by the server) to switch the connection to a TLS-protected connection before issuing any login commands. false true or false mail.smtp.starttls.required If this parameter is set to 'true', it is required to use the 'STARTTLS' command. If the server does not support the 'STARTTLS' command, or if the command fails, the connection method will fail. false true or false mail.smtp.socks.host This specifies the host name of a SOCKS5 proxy server to be used for the connections to the mail server. None mail.smtp.socks.port This specifies the port number for the SOCKS5 proxy server. This needs to be used only if the proxy server is not using the standard port number 1080. 1080 valid port number mail.smtp.auth.ntlm.disable If this parameter is set to 'true', the AUTH NTLM command cannot be issued. false true or false mail.smtp.mailextension The extension string to be appended to the MAIL command. None mail.smtp.userset If this parameter is set to 'true', you should use the 'RSET' command instead of the 'NOOP' command in the 'isConnected' method. In some scenarios, 'sendmail' responds slowly after many 'NOOP' commands. This is avoided by using 'RSET' instead. false true or false Examples EXAMPLE 1 @sink(type='email', @map(type ='json'), username='sender.account', address='sender.account@gmail.com',password='account.password',subject='Alerts from Wso2 Stream Processor',to='{{email}}',)define stream FooStream (email string, loginId int, name string); This example illustrates how to publish events via an email sink based on the values provided for the mandatory parameters. As shown in the example, it publishes events from the 'FooStream' in 'json' format as emails to the specified 'to' recipients via the email sink. The email is sent from the 'sender.account@gmail.com' email address via a secure connection. EXAMPLE 2 @sink(type='email', @map(type ='json'), subject='Alerts from Wso2 Stream Processor',to='{{email}}',)define stream FooStream (email string, loginId int, name string); This example illustrates how to configure the query parameters and the system parameters in the 'deployment.yaml' file. Corresponding parameters need to be configured under 'email', and namespace:'sink' as follows: siddhi: extensions: - extension: name:'email' namespace:'sink' properties: username: sender's email username address: sender's email address password: sender's email password As shown in the example, events from the FooStream are published in 'json' format via the email sink as emails to the given 'to' recipients. The email is sent from the 'sender.account@gmail.com' address via a secure connection. EXAMPLE 3 @sink(type='email', @map(type ='json'), username='sender.account', address='sender.account@gmail.com',password='account.password',host='smtp.gmail.com',port='465',ssl.enable='true',auth='true',content.type='text/html',subject='Alerts from Wso2 Stream Processor-{{name}}',to='to1.account@gmail.com, to2.account@gmail.com',cc='cc1.account@gmail.com, cc2.account@gmail.com',bcc='bcc1.account@gmail.com)define stream FooStream (name string, age int, country string); This example illustrates how to publish events via the email sink. Events from the 'FooStream' stream are published in 'xml' format via the email sink as a text/html message and sent to the specified 'to', 'cc', and 'bcc' recipients via a secure connection. The 'name' namespace in the 'subject' attribute is the value of the 'name' parameter in the corresponding output event. EXAMPLE 4 @sink(type='email', @map(type ='json'), username='sender.account', address='sender.account@gmail.com',password='account.password',host='smtp.gmail.com',port='465',ssl.enable='true',auth='true',content.type='text/html',subject='Alerts from Wso2 Stream Processor-{{name}}',to='to1.account@gmail.com, to2.account@gmail.com',cc='cc1.account@gmail.com, cc2.account@gmail.com',bcc='bcc1.account@gmail.comattachments= '{{attachments}}')define stream FooStream (name string, age int, country string, attachments string); This example illustrates how to publish events via the email sink. Here, the email also contains attachments. Events from the FooStream are published in 'xml' format via the email sink as a 'text/html' message to the specified 'to','cc', and 'bcc' recipients via a secure connection. The 'name' namespace in the 'subject' attribute is the value for the 'name' parameter in the corresponding output event. The attachments included in the email message are the local files available in the path specified as the value for the 'attachments' attribute. file (Sink) File Sink can be used to publish (write) event data which is processed within siddhi to files. Siddhi-io-file sink provides support to write both textual and binary data into files Origin: siddhi-io-file:2.0.3 Syntax @sink(type=\"file\", file.uri=\" STRING \", append=\" BOOL \", add.line.separator=\" BOOL \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic file.uri Used to specify the file for data to be written. STRING No Yes append This parameter is used to specify whether the data should be append to the file or not. If append = 'true', data will be write at the end of the file without changing the existing content. If file does not exist, a new fill will be crated and then data will be written. If append append = 'false', If given file exists, existing content will be deleted and then data will be written back to the file. If given file does not exist, a new file will be created and then data will be written on it. true BOOL Yes No add.line.separator This parameter is used to specify whether events added to the file should be separated by a newline. If add.event.separator= 'true',then a newline will be added after data is added to the file. true. (However, if csv mapper is used, it is false) BOOL Yes No Examples EXAMPLE 1 @sink(type='file', @map(type='json'), append='false', file.uri='/abc/{{symbol}}.txt') define stream BarStream (symbol string, price float, volume long); Under above configuration, for each event, a file will be generated if there's no such a file,and then data will be written to that file as json messagesoutput will looks like below. { \"event\":{ \"symbol\":\"WSO2\", \"price\":55.6, \"volume\":100 } } grpc (Sink) This extension publishes event data encoded into GRPC Classes as defined in the user input jar. This extension has a default gRPC service classes added. The default service is called \"EventService\". Please find the protobuf definition here . If we want to use our custom gRPC services, we have to pack auto-generated gRPC service classes and protobuf classes into a jar file and add it into the project classpath (or to the jars folder in the siddhi-tooling folder if we use it with siddhi-tooling ). Please find the custom protobuf definition that uses in examples here . This grpc sink is used for scenarios where we send a request and don't expect a response back. I.e getting a google.protobuf.Empty response back. Origin: siddhi-io-grpc:1.0.5 Syntax @sink(type=\"grpc\", publisher.url=\" STRING \", headers=\" STRING \", idle.timeout=\" LONG \", keep.alive.time=\" LONG \", keep.alive.timeout=\" LONG \", keep.alive.without.calls=\" BOOL \", enable.retry=\" BOOL \", max.retry.attempts=\" INT \", retry.buffer.size=\" LONG \", per.rpc.buffer.size=\" LONG \", channel.termination.waiting.time=\" LONG \", truststore.file=\" STRING \", truststore.password=\" STRING \", truststore.algorithm=\" STRING \", tls.store.type=\" STRING \", keystore.file=\" STRING \", keystore.password=\" STRING \", keystore.algorithm=\" STRING \", enable.ssl=\" BOOL \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic publisher.url The url to which the outgoing events should be published via this extension. This url should consist the host hostPort, port, fully qualified service name, method name in the following format. grpc://0.0.0.0:9763/ serviceName / methodName For example: grpc://0.0.0.0:9763/org.wso2.grpc.EventService/consume STRING No No headers GRPC Request headers in format \"' key : value ',' key : value '\" . If header parameter is not provided just the payload is sent - STRING Yes No idle.timeout Set the duration in seconds without ongoing RPCs before going to idle mode. 1800 LONG Yes No keep.alive.time Sets the time in seconds without read activity before sending a keepalive ping. Keepalives can increase the load on services so must be used with caution. By default set to Long.MAX_VALUE which disables keep alive pinging. Long.MAX_VALUE LONG Yes No keep.alive.timeout Sets the time in seconds waiting for read activity after sending a keepalive ping. 20 LONG Yes No keep.alive.without.calls Sets whether keepalive will be performed when there are no outstanding RPC on a connection. false BOOL Yes No enable.retry Enables the retry mechanism provided by the gRPC library. false BOOL Yes No max.retry.attempts Sets max number of retry attempts. The total number of retry attempts for each RPC will not exceed this number even if service config may allow a higher number. 5 INT Yes No retry.buffer.size Sets the retry buffer size in bytes. If the buffer limit is exceeded, no RPC could retry at the moment, and in hedging case all hedges but one of the same RPC will cancel. 16777216 LONG Yes No per.rpc.buffer.size Sets the per RPC buffer limit in bytes used for retry. The RPC is not retriable if its buffer limit is exceeded. 1048576 LONG Yes No channel.termination.waiting.time The time in seconds to wait for the channel to become terminated, giving up if the timeout is reached. 5 LONG Yes No truststore.file the file path of truststore. If this is provided then server authentication is enabled - STRING Yes No truststore.password the password of truststore. If this is provided then the integrity of the keystore is checked - STRING Yes No truststore.algorithm the encryption algorithm to be used for server authentication - STRING Yes No tls.store.type TLS store type - STRING Yes No keystore.file the file path of keystore. If this is provided then client authentication is enabled - STRING Yes No keystore.password the password of keystore - STRING Yes No keystore.algorithm the encryption algorithm to be used for client authentication - STRING Yes No enable.ssl to enable ssl. If set to true and truststore.file is not given then it will be set to default carbon jks by default FALSE BOOL Yes No Examples EXAMPLE 1 @sink(type='grpc', publisher.url = 'grpc://134.23.43.35:8080/org.wso2.grpc.EventService/consume', @map(type='json')) define stream FooStream (message String); Here a stream named FooStream is defined with grpc sink. A grpc server should be running at 194.23.98.100 listening to port 8080. sink.id is set to 1 here. So we can write a source with sink.id 1 so that it will listen to responses for requests published from this stream. Note that since we are using EventService/consume the sink will be operating in default mode EXAMPLE 2 @sink(type='grpc', publisher.url = 'grpc://134.23.43.35:8080/org.wso2.grpc.EventService/consume', headers='{{headers}}', @map(type='json'), @payload('{{message}}')) define stream FooStream (message String, headers String); A similar example to above but with headers. Headers are also send into the stream as a data. In the sink headers dynamic property reads the value and sends it as MetaData with the request EXAMPLE 3 @sink(type='grpc', publisher.url = 'grpc://134.23.43.35:8080/org.wso2.grpc.MyService/send', @map(type='protobuf'), define stream FooStream (stringValue string, intValue int,longValue long,booleanValue bool,floatValue float,doubleValue double); Here a stream named FooStream is defined with grpc sink. A grpc server should be running at 134.23.43.35 listening to port 8080 since there is no mapper provided, attributes of stream definition should be as same as the attributes of protobuf message definition. EXAMPLE 4 @sink(type='grpc', publisher.url = 'grpc://134.23.43.35:8080/org.wso2.grpc.MyService/testMap', @map(type='protobuf'), define stream FooStream (stringValue string, intValue int,map object); Here a stream named FooStream is defined with grpc sink. A grpc server should be running at 134.23.43.35 listening to port 8080. The 'map object' in the stream definition defines that this stream is going to use Map object with grpc service. We can use any map object that extends 'java.util.AbstractMap' class. EXAMPLE 5 @sink(type='grpc', publisher.url = 'grpc://134.23.43.35:8080/org.wso2.grpc.MyService/testMap', @map(type='protobuf', @payload(stringValue='a',longValue='b',intValue='c',booleanValue='d',floatValue = 'e', doubleValue = 'f'))) define stream FooStream (a string, b long, c int,d bool,e float,f double); Here a stream named FooStream is defined with grpc sink. A grpc server should be running at 194.23.98.100 listening to port 8080. @payload is provided in this stream, therefore we can use any name for the attributes in the stream definition, but we should correctly map those names with protobuf message attributes. If we are planning to send metadata within a stream we should use @payload to map attributes to identify the metadata attribute and the protobuf attributes separately. EXAMPLE 6 @sink(type='grpc', publisher.url = 'grpc://194.23.98.100:8888/org.wso2.grpc.test.StreamService/clientStream', @map(type='protobuf')) define stream FooStream (stringValue string, intValue int,longValue long,booleanValue bool,floatValue float,doubleValue double); Here in the grpc sink, we are sending a stream of requests to the server that runs on 194.23.98.100 and port 8888. When we need to send a stream of requests from the grpc sink we have to define a client stream RPC method.Then the siddhi will identify whether it's a unary method or a stream method and send requests according to the method type. grpc-call (Sink) This extension publishes event data encoded into GRPC Classes as defined in the user input jar. This extension has a default gRPC service classes jar added. The default service is called \"EventService\". Please find the protobuf definition here . If we want to use our custom gRPC services, we have to pack auto-generated gRPC service classes and protobuf classes into a jar file and add it into the project classpath (or to the jars folder in the siddhi-tooling folder if we use it with siddhi-tooling ). Please find the custom protobuf definition that uses in examples here . This grpc-call sink is used for scenarios where we send a request out and expect a response back. In default mode this will use EventService process method. grpc-call-response source is used to receive the responses. A unique sink.id is used to correlate between the sink and its corresponding source. Origin: siddhi-io-grpc:1.0.5 Syntax @sink(type=\"grpc-call\", publisher.url=\" STRING \", sink.id=\" INT \", headers=\" STRING \", idle.timeout=\" LONG \", keep.alive.time=\" LONG \", keep.alive.timeout=\" LONG \", keep.alive.without.calls=\" BOOL \", enable.retry=\" BOOL \", max.retry.attempts=\" INT \", retry.buffer.size=\" LONG \", per.rpc.buffer.size=\" LONG \", channel.termination.waiting.time=\" LONG \", max.inbound.message.size=\" LONG \", max.inbound.metadata.size=\" LONG \", truststore.file=\" STRING \", truststore.password=\" STRING \", truststore.algorithm=\" STRING \", tls.store.type=\" STRING \", keystore.file=\" STRING \", keystore.password=\" STRING \", keystore.algorithm=\" STRING \", enable.ssl=\" BOOL \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic publisher.url The url to which the outgoing events should be published via this extension. This url should consist the host hostPort, port, fully qualified service name, method name in the following format. grpc://0.0.0.0:9763/ serviceName / methodName For example: grpc://0.0.0.0:9763/org.wso2.grpc.EventService/consume STRING No No sink.id a unique ID that should be set for each grpc-call-sink. There is a 1:1 mapping between grpc-call sinks and grpc-call-response sources. Each sink has one particular source listening to the responses to requests published from that sink. So the same sink.id should be given when writing the source also. INT No No headers GRPC Request headers in format \"' key : value ',' key : value '\" . If header parameter is not provided just the payload is sent - STRING Yes No idle.timeout Set the duration in seconds without ongoing RPCs before going to idle mode. 1800 LONG Yes No keep.alive.time Sets the time in seconds without read activity before sending a keepalive ping. Keepalives can increase the load on services so must be used with caution. By default set to Long.MAX_VALUE which disables keep alive pinging. Long.MAX_VALUE LONG Yes No keep.alive.timeout Sets the time in seconds waiting for read activity after sending a keepalive ping. 20 LONG Yes No keep.alive.without.calls Sets whether keepalive will be performed when there are no outstanding RPC on a connection. false BOOL Yes No enable.retry Enables the retry and hedging mechanism provided by the gRPC library. false BOOL Yes No max.retry.attempts Sets max number of retry attempts. The total number of retry attempts for each RPC will not exceed this number even if service config may allow a higher number. 5 INT Yes No retry.buffer.size Sets the retry buffer size in bytes. If the buffer limit is exceeded, no RPC could retry at the moment, and in hedging case all hedges but one of the same RPC will cancel. 16777216 LONG Yes No per.rpc.buffer.size Sets the per RPC buffer limit in bytes used for retry. The RPC is not retriable if its buffer limit is exceeded. 1048576 LONG Yes No channel.termination.waiting.time The time in seconds to wait for the channel to become terminated, giving up if the timeout is reached. 5 LONG Yes No max.inbound.message.size Sets the maximum message size allowed to be received on the channel in bytes 4194304 LONG Yes No max.inbound.metadata.size Sets the maximum size of metadata allowed to be received in bytes 8192 LONG Yes No truststore.file the file path of truststore. If this is provided then server authentication is enabled - STRING Yes No truststore.password the password of truststore. If this is provided then the integrity of the keystore is checked - STRING Yes No truststore.algorithm the encryption algorithm to be used for server authentication - STRING Yes No tls.store.type TLS store type - STRING Yes No keystore.file the file path of keystore. If this is provided then client authentication is enabled - STRING Yes No keystore.password the password of keystore - STRING Yes No keystore.algorithm the encryption algorithm to be used for client authentication - STRING Yes No enable.ssl to enable ssl. If set to true and truststore.file is not given then it will be set to default carbon jks by default FALSE BOOL Yes No Examples EXAMPLE 1 @sink(type='grpc-call', publisher.url = 'grpc://194.23.98.100:8080/EventService/process', sink.id= '1', @map(type='json')) define stream FooStream (message String); @source(type='grpc-call-response', sink.id= '1') define stream BarStream (message String); Here a stream named FooStream is defined with grpc sink. A grpc server should be running at 194.23.98.100 listening to port 8080. sink.id is set to 1 here. So we can write a source with sink.id 1 so that it will listen to responses for requests published from this stream. Note that since we are using EventService/process the sink will be operating in default mode EXAMPLE 2 @sink(type='grpc-call', publisher.url = 'grpc://194.23.98.100:8080/EventService/process', sink.id= '1', @map(type='json')) define stream FooStream (message String); @source(type='grpc-call-response', sink.id= '1') define stream BarStream (message String); Here with the same FooStream definition we have added a BarStream which has a grpc-call-response source with the same sink.id 1. So the responses for calls sent from the FooStream will be added to BarStream. EXAMPLE 3 @sink(type='grpc-call', publisher.url = 'grpc://194.23.98.100:8888/org.wso2.grpc.test.MyService/process', sink.id= '1', @map(type='protobuf')) define stream FooStream (stringValue string, intValue int,longValue long,booleanValue bool,floatValue float,doubleValue double); @source(type='grpc-call-response', receiver.url = 'grpc://localhost:8888/org.wso2.grpc.MyService/process', sink.id= '1', @map(type='protobuf'))define stream FooStream (stringValue string, intValue int,longValue long,booleanValue bool,floatValue float,doubleValue double); Here a stream named FooStream is defined with grpc sink. A grpc server should be running at 194.23.98.100 listening to port 8080. We have added another stream called BarStream which is a grpc-call-response source with the same sink.id 1 and as same as FooStream definition. So the responses for calls sent from the FooStream will be added to BarStream. Since there is no mapping available in the stream definition attributes names should be as same as the attributes of the protobuf message definition. (Here the only reason we provide receiver.url in the grpc-call-response source is for protobuf mapper to map Response into a siddhi event, we can give any address and any port number in the URL, but we should provide the service name and the method name correctly) EXAMPLE 4 @sink(type='grpc-call', publisher.url = 'grpc://194.23.98.100:8888/org.wso2.grpc.test.MyService/process', sink.id= '1', @map(type='protobuf', @payload(stringValue='a',longValue='c',intValue='b',booleanValue='d',floatValue = 'e', doubleValue = 'f')))define stream FooStream (a string, b int,c long,d bool,e float,f double); @source(type='grpc-call-response', receiver.url = 'grpc://localhost:8888/org.wso2.grpc.test.MyService/process', sink.id= '1', @map(type='protobuf',@attributes(a = 'stringValue', b = 'intValue', c = 'longValue',d = 'booleanValue', e ='floatValue', f ='doubleValue')))define stream FooStream (a string, b int,c long,d bool,e float,f double); Here with the same FooStream definition we have added a BarStream which has a grpc-call-response source with the same sink.id 1. So the responses for calls sent from the FooStream will be added to BarStream. In this stream we provided mapping for both the sink and the source. so we can use any name for the attributes in the stream definition, but we have to map those attributes with correct protobuf attributes. As same as the grpc-sink, if we are planning to use metadata we should map the attributes. grpc-service-response (Sink) This extension is used to send responses back to a gRPC client after receiving requests through grpc-service source. This correlates with the particular source using a unique source.id Origin: siddhi-io-grpc:1.0.5 Syntax @sink(type=\"grpc-service-response\", source.id=\" INT \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic source.id A unique id to identify the correct source to which this sink is mapped. There is a 1:1 mapping between source and sink INT No No Examples EXAMPLE 1 @sink(type='grpc-service-response', source.id='1', @map(type='json')) define stream BarStream (messageId String, message String); @source(type='grpc-service', url='grpc://134.23.43.35:8080/org.wso2.grpc.EventService/process', source.id='1', @map(type='json', @attributes(messageId='trp:messageId', message='message'))) define stream FooStream (messageId String, message String); from FooStream select * insert into BarStream; The grpc requests are received through the grpc-service sink. Each received event is sent back through grpc-service-source. This is just a passthrough through Siddhi as we are selecting everything from FooStream and inserting into BarStream. http (Sink) HTTP sink publishes messages via HTTP or HTTPS protocols using methods such as POST, GET, PUT, and DELETE on formats text , XML and JSON . It can also publish to endpoints protected by basic authentication or OAuth 2.0. Origin: siddhi-io-http:2.2.0 Syntax @sink(type=\"http\", publisher.url=\" STRING \", basic.auth.username=\" STRING \", basic.auth.password=\" STRING \", https.truststore.file=\" STRING \", https.truststore.password=\" STRING \", oauth.username=\" STRING \", oauth.password=\" STRING \", consumer.key=\" STRING \", consumer.secret=\" STRING \", token.url=\" STRING \", refresh.token=\" STRING \", headers=\" STRING \", method=\" STRING \", socket.idle.timeout=\" INT \", chunk.disabled=\" BOOL \", ssl.protocol=\" STRING \", ssl.verification.disabled=\" BOOL \", tls.store.type=\" STRING \", ssl.configurations=\" STRING \", proxy.host=\" STRING \", proxy.port=\" STRING \", proxy.username=\" STRING \", proxy.password=\" STRING \", client.bootstrap.configurations=\" STRING \", max.pool.active.connections=\" INT \", min.pool.idle.connections=\" INT \", max.pool.idle.connections=\" INT \", min.evictable.idle.time=\" STRING \", time.between.eviction.runs=\" STRING \", max.wait.time=\" STRING \", test.on.borrow=\" BOOL \", test.while.idle=\" BOOL \", exhausted.action=\" INT \", hostname.verification.enabled=\" BOOL \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic publisher.url The URL to which the outgoing events should be published. Examples: http://localhost:8080/endpoint , https://localhost:8080/endpoint STRING No No basic.auth.username The username to be included in the authentication header when calling endpoints protected by basic authentication. basic.auth.password property should be also set when using this property. - STRING Yes No basic.auth.password The password to be included in the authentication header when calling endpoints protected by basic authentication. basic.auth.username property should be also set when using this property. - STRING Yes No https.truststore.file The file path of the client truststore when sending messages through https protocol. ${carbon.home}/resources/security/client-truststore.jks STRING Yes No https.truststore.password The password for the client-truststore. wso2carbon STRING Yes No oauth.username The username to be included in the authentication header when calling endpoints protected by OAuth 2.0. oauth.password property should be also set when using this property. - STRING Yes No oauth.password The password to be included in the authentication header when calling endpoints protected by OAuth 2.0. oauth.username property should be also set when using this property. - STRING Yes No consumer.key Consumer key used for calling endpoints protected by OAuth 2.0 - STRING Yes No consumer.secret Consumer secret used for calling endpoints protected by OAuth 2.0 - STRING Yes No token.url Token URL to generate a new access tokens when calling endpoints protected by OAuth 2.0 - STRING Yes No refresh.token Refresh token used for generating new access tokens when calling endpoints protected by OAuth 2.0 - STRING Yes No headers HTTP request headers in format \"' key : value ',' key : value '\" . When Content-Type header is not provided the system derives the Content-Type based on the provided sink mapper as following: - @map(type='xml') : application/xml - @map(type='json') : application/json - @map(type='text') : plain/text - @map(type='keyvalue') : application/x-www-form-urlencoded - For all other cases system defaults to plain/text Also the Content-Length header need not to be provided, as the system automatically defines it by calculating the size of the payload. Content-Type and Content-Length headers STRING Yes No method The HTTP method used for calling the endpoint. POST STRING Yes No socket.idle.timeout Socket timeout in millis. 6000 INT Yes No chunk.disabled Disable chunked transfer encoding. false BOOL Yes No ssl.protocol SSL/TLS protocol. TLS STRING Yes No ssl.verification.disabled Disable SSL verification. false BOOL Yes No tls.store.type TLS store type. JKS STRING Yes No ssl.configurations SSL/TSL configurations in format \"' key : value ',' key : value '\" . Some supported parameters: - SSL/TLS protocols: 'sslEnabledProtocols:TLSv1.1,TLSv1.2' - List of ciphers: 'ciphers:TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256' - Enable session creation: 'client.enable.session.creation:true' - Supported server names: 'server.suported.server.names:server' - Add HTTP SNIMatcher: 'server.supported.snimatchers:SNIMatcher' - STRING Yes No proxy.host Proxy server host - STRING Yes No proxy.port Proxy server port - STRING Yes No proxy.username Proxy server username - STRING Yes No proxy.password Proxy server password - STRING Yes No client.bootstrap.configurations Client bootstrap configurations in format \"' key : value ',' key : value '\" . Some supported configurations : - Client connect timeout in millis: 'client.bootstrap.connect.timeout:15000' - Client socket timeout in seconds: 'client.bootstrap.socket.timeout:15' - Client socket reuse: 'client.bootstrap.socket.reuse:true' - Enable TCP no delay: 'client.bootstrap.nodelay:true' - Enable client keep alive: 'client.bootstrap.keepalive:true' - Send buffer size: 'client.bootstrap.sendbuffersize:1048576' - Receive buffer size: 'client.bootstrap.recievebuffersize:1048576' - STRING Yes No max.pool.active.connections Maximum possible number of active connection per client pool. -1 INT Yes No min.pool.idle.connections Minimum number of idle connections that can exist per client pool. 0 INT Yes No max.pool.idle.connections Maximum number of idle connections that can exist per client pool. 100 INT Yes No min.evictable.idle.time Minimum time (in millis) a connection may sit idle in the client pool before it become eligible for eviction. 300000 STRING Yes No time.between.eviction.runs Time between two eviction operations (in millis) on the client pool. 30000 STRING Yes No max.wait.time The maximum time (in millis) the pool will wait (when there are no available connections) for a connection to be returned to the pool. 60000 STRING Yes No test.on.borrow Enable connections to be validated before being borrowed from the client pool. true BOOL Yes No test.while.idle Enable connections to be validated during the eviction operation (if any). true BOOL Yes No exhausted.action Action that should be taken when the maximum number of active connections are being used. This action should be indicated as an int and possible action values are following. 0 - Fail the request. 1 - Block the request, until a connection returns to the pool. 2 - Grow the connection pool size. 1 (Block when exhausted) INT Yes No hostname.verification.enabled Enable hostname verification. true BOOL Yes No System Parameters Name Description Default Value Possible Parameters clientBootstrapClientGroupSize Number of client threads to perform non-blocking read and write to one or more channels. (Number of available processors) * 2 Any positive integer clientBootstrapBossGroupSize Number of boss threads to accept incoming connections. Number of available processors Any positive integer clientBootstrapWorkerGroupSize Number of worker threads to accept the connections from boss threads and perform non-blocking read and write from one or more channels. (Number of available processors) * 2 Any positive integer trustStoreLocation The default truststore file path. ${carbon.home}/resources/security/client-truststore.jks Path to client truststore .jks file trustStorePassword The default truststore password. wso2carbon Truststore password as string Examples EXAMPLE 1 @sink(type = 'http', publisher.url = 'http://stocks.com/stocks', @map(type = 'json')) define stream StockStream (symbol string, price float, volume long); Events arriving on the StockStream will be published to the HTTP endpoint http://stocks.com/stocks using POST method with Content-Type application/json by converting those events to the default JSON format as following: { \"event\": { \"symbol\": \"FB\", \"price\": 24.5, \"volume\": 5000 } } EXAMPLE 2 @sink(type='http', publisher.url = 'http://localhost:8009/foo', client.bootstrap.configurations = \"'client.bootstrap.socket.timeout:20'\", max.pool.active.connections = '1', headers = \"{{headers}}\", @map(type='xml', @payload(\"\"\" stock {{payloadBody}} /stock \"\"\"))) define stream FooStream (payloadBody String, headers string); Events arriving on FooStream will be published to the HTTP endpoint http://localhost:8009/foo using POST method with Content-Type application/xml and setting payloadBody and header attribute values. If the payloadBody contains symbol WSO2 /symbol price 55.6 /price volume 100 /volume and header contains 'topic:foobar' values, then the system will generate an output with the body: stock symbol WSO2 /symbol price 55.6 /price volume 100 /volume /stock and HTTP headers: Content-Length:xxx , Content-Location:'xxx' , Content-Type:'application/xml' , HTTP_METHOD:'POST' http-call (Sink) The http-call sink publishes messages to endpoints via HTTP or HTTPS protocols using methods such as POST, GET, PUT, and DELETE on formats text , XML or JSON and consume responses through its corresponding http-call-response source. It also supports calling endpoints protected with basic authentication or OAuth 2.0. Origin: siddhi-io-http:2.2.0 Syntax @sink(type=\"http-call\", publisher.url=\" STRING \", sink.id=\" STRING \", basic.auth.username=\" STRING \", basic.auth.password=\" STRING \", https.truststore.file=\" STRING \", https.truststore.password=\" STRING \", oauth.username=\" STRING \", oauth.password=\" STRING \", consumer.key=\" STRING \", consumer.secret=\" STRING \", token.url=\" STRING \", refresh.token=\" STRING \", headers=\" STRING \", method=\" STRING \", downloading.enabled=\" BOOL \", download.path=\" STRING \", blocking.io=\" BOOL \", socket.idle.timeout=\" INT \", chunk.disabled=\" BOOL \", ssl.protocol=\" STRING \", ssl.verification.disabled=\" BOOL \", ssl.configurations=\" STRING \", proxy.host=\" STRING \", proxy.port=\" STRING \", proxy.username=\" STRING \", proxy.password=\" STRING \", client.bootstrap.configurations=\" STRING \", max.pool.active.connections=\" INT \", min.pool.idle.connections=\" INT \", max.pool.idle.connections=\" INT \", min.evictable.idle.time=\" STRING \", time.between.eviction.runs=\" STRING \", max.wait.time=\" STRING \", test.on.borrow=\" BOOL \", test.while.idle=\" BOOL \", exhausted.action=\" INT \", hostname.verification.enabled=\" BOOL \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic publisher.url The URL which should be called. Examples: http://localhost:8080/endpoint , https://localhost:8080/endpoint STRING No No sink.id Identifier to correlate the http-call sink to its corresponding http-call-response sources to retrieved the responses. STRING No No basic.auth.username The username to be included in the authentication header when calling endpoints protected by basic authentication. basic.auth.password property should be also set when using this property. - STRING Yes No basic.auth.password The password to be included in the authentication header when calling endpoints protected by basic authentication. basic.auth.username property should be also set when using this property. - STRING Yes No https.truststore.file The file path of the client truststore when sending messages through https protocol. ${carbon.home}/resources/security/client-truststore.jks STRING Yes No https.truststore.password The password for the client-truststore. wso2carbon STRING Yes No oauth.username The username to be included in the authentication header when calling endpoints protected by OAuth 2.0. oauth.password property should be also set when using this property. - STRING Yes No oauth.password The password to be included in the authentication header when calling endpoints protected by OAuth 2.0. oauth.username property should be also set when using this property. - STRING Yes No consumer.key Consumer key used for calling endpoints protected by OAuth 2.0 - STRING Yes No consumer.secret Consumer secret used for calling endpoints protected by OAuth 2.0 - STRING Yes No token.url Token URL to generate a new access tokens when calling endpoints protected by OAuth 2.0 - STRING Yes No refresh.token Refresh token used for generating new access tokens when calling endpoints protected by OAuth 2.0 - STRING Yes No headers HTTP request headers in format \"' key : value ',' key : value '\" . When the Content-Type header is not provided the system decides the Content-Type based on the provided sink mapper as following: - @map(type='xml') : application/xml - @map(type='json') : application/json - @map(type='text') : plain/text - @map(type='keyvalue') : application/x-www-form-urlencoded - For all other cases system defaults to plain/text Also the Content-Length header need not to be provided, as the system automatically defines it by calculating the size of the payload. Content-Type and Content-Length headers STRING Yes No method The HTTP method used for calling the endpoint. POST STRING Yes No downloading.enabled Enable response received by the http-call-response source to be written to a file. When this is enabled the download.path property should be also set. false BOOL Yes No download.path The absolute file path along with the file name where the downloads should be saved. - STRING Yes Yes blocking.io Blocks the request thread until a response it received from HTTP call-response source before sending any other request. false BOOL Yes No socket.idle.timeout Socket timeout in millis. 6000 INT Yes No chunk.disabled Disable chunked transfer encoding. false BOOL Yes No ssl.protocol SSL/TLS protocol. TLS STRING Yes No ssl.verification.disabled Disable SSL verification. false BOOL Yes No ssl.configurations SSL/TSL configurations. Expected format \"' key : value ',' key : value '\" . Some supported parameters: - SSL/TLS protocols: 'sslEnabledProtocols:TLSv1.1,TLSv1.2' - List of ciphers: 'ciphers:TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256' - Enable session creation: 'client.enable.session.creation:true' - Supported server names: 'server.suported.server.names:server' - Add HTTP SNIMatcher: 'server.supported.snimatchers:SNIMatcher' - STRING Yes No proxy.host Proxy server host - STRING Yes No proxy.port Proxy server port - STRING Yes No proxy.username Proxy server username - STRING Yes No proxy.password Proxy server password - STRING Yes No client.bootstrap.configurations Client bootstrap configurations in format \"' key : value ',' key : value '\" . Some supported configurations : - Client connect timeout in millis: 'client.bootstrap.connect.timeout:15000' - Client socket timeout in seconds: 'client.bootstrap.socket.timeout:15' - Client socket reuse: 'client.bootstrap.socket.reuse:true' - Enable TCP no delay: 'client.bootstrap.nodelay:true' - Enable client keep alive: 'client.bootstrap.keepalive:true' - Send buffer size: 'client.bootstrap.sendbuffersize:1048576' - Receive buffer size: 'client.bootstrap.recievebuffersize:1048576' - STRING Yes No max.pool.active.connections Maximum possible number of active connection per client pool. -1 INT Yes No min.pool.idle.connections Minimum number of idle connections that can exist per client pool. 0 INT Yes No max.pool.idle.connections Maximum number of idle connections that can exist per client pool. 100 INT Yes No min.evictable.idle.time Minimum time (in millis) a connection may sit idle in the client pool before it become eligible for eviction. 300000 STRING Yes No time.between.eviction.runs Time between two eviction operations (in millis) on the client pool. 30000 STRING Yes No max.wait.time The maximum time (in millis) the pool will wait (when there are no available connections) for a connection to be returned to the pool. 60000 STRING Yes No test.on.borrow Enable connections to be validated before being borrowed from the client pool. true BOOL Yes No test.while.idle Enable connections to be validated during the eviction operation (if any). true BOOL Yes No exhausted.action Action that should be taken when the maximum number of active connections are being used. This action should be indicated as an int and possible action values are following. 0 - Fail the request. 1 - Block the request, until a connection returns to the pool. 2 - Grow the connection pool size. 1 (Block when exhausted) INT Yes No hostname.verification.enabled Enable hostname verification true BOOL Yes No System Parameters Name Description Default Value Possible Parameters clientBootstrapClientGroupSize Number of client threads to perform non-blocking read and write to one or more channels. (Number of available processors) * 2 Any positive integer clientBootstrapBossGroupSize Number of boss threads to accept incoming connections. Number of available processors Any positive integer clientBootstrapWorkerGroupSize Number of worker threads to accept the connections from boss threads and perform non-blocking read and write from one or more channels. (Number of available processors) * 2 Any positive integer trustStoreLocation The default truststore file path. ${carbon.home}/resources/security/client-truststore.jks Path to client truststore .jks file trustStorePassword The default truststore password. wso2carbon Truststore password as string Examples EXAMPLE 1 @sink(type='http-call', sink.id='foo', publisher.url='http://localhost:8009/foo', @map(type='xml', @payload('{{payloadBody}}'))) define stream FooStream (payloadBody string); @source(type='http-call-response', sink.id='foo', @map(type='text', regex.A='((.|\\n)*)', @attributes(headers='trp:headers', message='A[1]'))) define stream ResponseStream(message string, headers string); When events arrive in FooStream , http-call sink makes calls to endpoint on url http://localhost:8009/foo with POST method and Content-Type application/xml . If the event payloadBody attribute contains following XML: item name apple /name price 55 /price quantity 5 /quantity /item the http-call sink maps that and sends it to the endpoint. When endpoint sends a response it will be consumed by the corresponding http-call-response source correlated via the same sink.id foo and that will map the response message and send it via ResponseStream steam by assigning the message body as message attribute and response headers as headers attribute of the event. EXAMPLE 2 @sink(type='http-call', publisher.url='http://localhost:8005/files/{{name}}' downloading.enabled='true', download.path='{{downloadPath}}{{name}}', method='GET', sink.id='download', @map(type='json')) define stream DownloadRequestStream(name String, id int, downloadPath string); @source(type='http-call-response', sink.id='download', http.status.code='2\\\\d+', @map(type='text', regex.A='((.|\\n)*)', @attributes(name='trp:name', id='trp:id', file='A[1]'))) define stream ResponseStream2xx(name string, id string, file string); @source(type='http-call-response', sink.id='download', http.status.code='4\\\\d+', @map(type='text', regex.A='((.|\\n)*)', @attributes(errorMsg='A[1]'))) define stream ResponseStream4xx(errorMsg string); When events arrive in DownloadRequestStream with name : foo.txt , id : 75 and downloadPath : /user/download/ the http-call sink sends a GET request to the url http://localhost:8005/files/foo.txt to download the file to the given path /user/download/foo.txt and capture the response via its corresponding http-call-response source based on the response status code. If the response status code is in the range of 200 the message will be received by the http-call-response source associated with the ResponseStream2xx stream which expects http.status.code with regex 2\\d+ while downloading the file to the local file system on the path /user/download/foo.txt and mapping the response message having the absolute file path to event's file attribute. If the response status code is in the range of 400 then the message will be received by the http-call-response source associated with the ResponseStream4xx stream which expects http.status.code with regex 4\\d+ while mapping the error response to the errorMsg attribute of the event. http-request (Sink) Deprecated (Use http-call sink instead). The http-request sink publishes messages to endpoints via HTTP or HTTPS protocols using methods such as POST, GET, PUT, and DELETE on formats text , XML or JSON and consume responses through its corresponding http-response source. It also supports calling endpoints protected with basic authentication or OAuth 2.0. Origin: siddhi-io-http:2.2.0 Syntax @sink(type=\"http-request\", publisher.url=\" STRING \", sink.id=\" STRING \", basic.auth.username=\" STRING \", basic.auth.password=\" STRING \", https.truststore.file=\" STRING \", https.truststore.password=\" STRING \", oauth.username=\" STRING \", oauth.password=\" STRING \", consumer.key=\" STRING \", consumer.secret=\" STRING \", token.url=\" STRING \", refresh.token=\" STRING \", headers=\" STRING \", method=\" STRING \", downloading.enabled=\" BOOL \", download.path=\" STRING \", blocking.io=\" BOOL \", socket.idle.timeout=\" INT \", chunk.disabled=\" BOOL \", ssl.protocol=\" STRING \", ssl.verification.disabled=\" BOOL \", ssl.configurations=\" STRING \", proxy.host=\" STRING \", proxy.port=\" STRING \", proxy.username=\" STRING \", proxy.password=\" STRING \", client.bootstrap.configurations=\" STRING \", max.pool.active.connections=\" INT \", min.pool.idle.connections=\" INT \", max.pool.idle.connections=\" INT \", min.evictable.idle.time=\" STRING \", time.between.eviction.runs=\" STRING \", max.wait.time=\" STRING \", test.on.borrow=\" BOOL \", test.while.idle=\" BOOL \", exhausted.action=\" INT \", hostname.verification.enabled=\" BOOL \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic publisher.url The URL which should be called. Examples: http://localhost:8080/endpoint , https://localhost:8080/endpoint STRING No No sink.id Identifier to correlate the http-request sink to its corresponding http-response sources to retrieved the responses. STRING No No basic.auth.username The username to be included in the authentication header when calling endpoints protected by basic authentication. basic.auth.password property should be also set when using this property. - STRING Yes No basic.auth.password The password to be included in the authentication header when calling endpoints protected by basic authentication. basic.auth.username property should be also set when using this property. - STRING Yes No https.truststore.file The file path of the client truststore when sending messages through https protocol. ${carbon.home}/resources/security/client-truststore.jks STRING Yes No https.truststore.password The password for the client-truststore. wso2carbon STRING Yes No oauth.username The username to be included in the authentication header when calling endpoints protected by OAuth 2.0. oauth.password property should be also set when using this property. - STRING Yes No oauth.password The password to be included in the authentication header when calling endpoints protected by OAuth 2.0. oauth.username property should be also set when using this property. - STRING Yes No consumer.key Consumer key used for calling endpoints protected by OAuth 2.0 - STRING Yes No consumer.secret Consumer secret used for calling endpoints protected by OAuth 2.0 - STRING Yes No token.url Token URL to generate a new access tokens when calling endpoints protected by OAuth 2.0 - STRING Yes No refresh.token Refresh token used for generating new access tokens when calling endpoints protected by OAuth 2.0 - STRING Yes No headers HTTP request headers in format \"' key : value ',' key : value '\" . When the Content-Type header is not provided the system decides the Content-Type based on the provided sink mapper as following: - @map(type='xml') : application/xml - @map(type='json') : application/json - @map(type='text') : plain/text - @map(type='keyvalue') : application/x-www-form-urlencoded - For all other cases system defaults to plain/text Also the Content-Length header need not to be provided, as the system automatically defines it by calculating the size of the payload. Content-Type and Content-Length headers STRING Yes No method The HTTP method used for calling the endpoint. POST STRING Yes No downloading.enabled Enable response received by the http-response source to be written to a file. When this is enabled the download.path property should be also set. false BOOL Yes No download.path The absolute file path along with the file name where the downloads should be saved. - STRING Yes Yes blocking.io Blocks the request thread until a response it received from HTTP call-response source before sending any other request. false BOOL Yes No socket.idle.timeout Socket timeout in millis. 6000 INT Yes No chunk.disabled Disable chunked transfer encoding. false BOOL Yes No ssl.protocol SSL/TLS protocol. TLS STRING Yes No ssl.verification.disabled Disable SSL verification. false BOOL Yes No ssl.configurations SSL/TSL configurations in format \"' key : value ',' key : value '\" . Some supported parameters: - SSL/TLS protocols: 'sslEnabledProtocols:TLSv1.1,TLSv1.2' - List of ciphers: 'ciphers:TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256' - Enable session creation: 'client.enable.session.creation:true' - Supported server names: 'server.suported.server.names:server' - Add HTTP SNIMatcher: 'server.supported.snimatchers:SNIMatcher' - STRING Yes No proxy.host Proxy server host - STRING Yes No proxy.port Proxy server port - STRING Yes No proxy.username Proxy server username - STRING Yes No proxy.password Proxy server password - STRING Yes No client.bootstrap.configurations Client bootstrap configurations in format \"' key : value ',' key : value '\" . Some supported configurations : - Client connect timeout in millis: 'client.bootstrap.connect.timeout:15000' - Client socket timeout in seconds: 'client.bootstrap.socket.timeout:15' - Client socket reuse: 'client.bootstrap.socket.reuse:true' - Enable TCP no delay: 'client.bootstrap.nodelay:true' - Enable client keep alive: 'client.bootstrap.keepalive:true' - Send buffer size: 'client.bootstrap.sendbuffersize:1048576' - Receive buffer size: 'client.bootstrap.recievebuffersize:1048576' - STRING Yes No max.pool.active.connections Maximum possible number of active connection per client pool. -1 INT Yes No min.pool.idle.connections Minimum number of idle connections that can exist per client pool. 0 INT Yes No max.pool.idle.connections Maximum number of idle connections that can exist per client pool. 100 INT Yes No min.evictable.idle.time Minimum time (in millis) a connection may sit idle in the client pool before it become eligible for eviction. 300000 STRING Yes No time.between.eviction.runs Time between two eviction operations (in millis) on the client pool. 30000 STRING Yes No max.wait.time The maximum time (in millis) the pool will wait (when there are no available connections) for a connection to be returned to the pool. 60000 STRING Yes No test.on.borrow Enable connections to be validated before being borrowed from the client pool. true BOOL Yes No test.while.idle Enable connections to be validated during the eviction operation (if any). true BOOL Yes No exhausted.action Action that should be taken when the maximum number of active connections are being used. This action should be indicated as an int and possible action values are following. 0 - Fail the request. 1 - Block the request, until a connection returns to the pool. 2 - Grow the connection pool size. 1 (Block when exhausted) INT Yes No hostname.verification.enabled Enable hostname verification true BOOL Yes No System Parameters Name Description Default Value Possible Parameters clientBootstrapClientGroupSize Number of client threads to perform non-blocking read and write to one or more channels. (Number of available processors) * 2 Any positive integer clientBootstrapBossGroupSize Number of boss threads to accept incoming connections. Number of available processors Any positive integer clientBootstrapWorkerGroupSize Number of worker threads to accept the connections from boss threads and perform non-blocking read and write from one or more channels. (Number of available processors) * 2 Any positive integer trustStoreLocation The default truststore file path. ${carbon.home}/resources/security/client-truststore.jks Path to client truststore .jks file trustStorePassword The default truststore password. wso2carbon Truststore password as string Examples EXAMPLE 1 @sink(type='http-request', sink.id='foo', publisher.url='http://localhost:8009/foo', @map(type='xml', @payload('{{payloadBody}}'))) define stream FooStream (payloadBody string); @source(type='http-response', sink.id='foo', @map(type='text', regex.A='((.|\\n)*)', @attributes(headers='trp:headers', message='A[1]'))) define stream ResponseStream(message string, headers string); When events arrive in FooStream , http-request sink makes calls to endpoint on url http://localhost:8009/foo with POST method and Content-Type application/xml . If the event payloadBody attribute contains following XML: item name apple /name price 55 /price quantity 5 /quantity /item the http-request sink maps that and sends it to the endpoint. When endpoint sends a response it will be consumed by the corresponding http-response source correlated via the same sink.id foo and that will map the response message and send it via ResponseStream steam by assigning the message body as message attribute and response headers as headers attribute of the event. EXAMPLE 2 @sink(type='http-request', publisher.url='http://localhost:8005/files/{{name}}' downloading.enabled='true', download.path='{{downloadPath}}{{name}}', method='GET', sink.id='download', @map(type='json')) define stream DownloadRequestStream(name String, id int, downloadPath string); @source(type='http-response', sink.id='download', http.status.code='2\\\\d+', @map(type='text', regex.A='((.|\\n)*)', @attributes(name='trp:name', id='trp:id', file='A[1]'))) define stream ResponseStream2xx(name string, id string, file string); @source(type='http-response', sink.id='download', http.status.code='4\\\\d+', @map(type='text', regex.A='((.|\\n)*)', @attributes(errorMsg='A[1]'))) define stream ResponseStream4xx(errorMsg string); When events arrive in DownloadRequestStream with name : foo.txt , id : 75 and downloadPath : /user/download/ the http-request sink sends a GET request to the url http://localhost:8005/files/foo.txt to download the file to the given path /user/download/foo.txt and capture the response via its corresponding http-response source based on the response status code. If the response status code is in the range of 200 the message will be received by the http-response source associated with the ResponseStream2xx stream which expects http.status.code with regex 2\\d+ while downloading the file to the local file system on the path /user/download/foo.txt and mapping the response message having the absolute file path to event's file attribute. If the response status code is in the range of 400 then the message will be received by the http-response source associated with the ResponseStream4xx stream which expects http.status.code with regex 4\\d+ while mapping the error response to the errorMsg attribute of the event. http-response (Sink) Deprecated (Use http-service-response sink instead). The http-response sink send responses of the requests consumed by its corresponding http-request source, by mapping the response messages to formats such as text , XML and JSON . Origin: siddhi-io-http:2.2.0 Syntax @sink(type=\"http-response\", source.id=\" STRING \", message.id=\" STRING \", headers=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic source.id Identifier to correlate the http-response sink to its corresponding http-request source which consumed the request. STRING No No message.id Identifier to correlate the response with the request received by http-request source. STRING No Yes headers HTTP request headers in format \"' key : value ',' key : value '\" . When the Content-Type header is not provided the system decides the Content-Type based on the provided sink mapper as following: - @map(type='xml') : application/xml - @map(type='json') : application/json - @map(type='text') : plain/text - @map(type='keyvalue') : application/x-www-form-urlencoded - For all other cases system defaults to plain/text Also the Content-Length header need not to be provided, as the system automatically defines it by calculating the size of the payload. Content-Type and Content-Length headers STRING Yes No Examples EXAMPLE 1 @source(type='http-request', receiver.url='http://localhost:5005/add', source.id='adder', @map(type='json, @attributes(messageId='trp:messageId', value1='$.event.value1', value2='$.event.value2'))) define stream AddStream (messageId string, value1 long, value2 long); @sink(type='http-response', source.id='adder', message.id='{{messageId}}', @map(type = 'json')) define stream ResultStream (messageId string, results long); @info(name = 'query1') from AddStream select messageId, value1 + value2 as results insert into ResultStream; The http-request source on stream AddStream listens on url http://localhost:5005/stocks for JSON messages with format: { \"event\": { \"value1\": 3, \"value2\": 4 } } and when events arrive it maps to AddStream events and pass them to query query1 for processing. The query results produced on ResultStream are sent as a response via http-response sink with format: { \"event\": { \"results\": 7 } } Here the request and response are correlated by passing the messageId produced by the http-request to the respective http-response sink. http-service-response (Sink) The http-service-response sink send responses of the requests consumed by its corresponding http-service source, by mapping the response messages to formats such as text , XML and JSON . Origin: siddhi-io-http:2.2.0 Syntax @sink(type=\"http-service-response\", source.id=\" STRING \", message.id=\" STRING \", headers=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic source.id Identifier to correlate the http-service-response sink to its corresponding http-service source which consumed the request. STRING No No message.id Identifier to correlate the response with the request received by http-service source. STRING No Yes headers HTTP request headers in format \"' key : value ',' key : value '\" . When the Content-Type header is not provided the system decides the Content-Type based on the provided sink mapper as following: - @map(type='xml') : application/xml - @map(type='json') : application/json - @map(type='text') : plain/text - @map(type='keyvalue') : application/x-www-form-urlencoded - For all other cases system defaults to plain/text Also the Content-Length header need not to be provided, as the system automatically defines it by calculating the size of the payload. Content-Type and Content-Length headers STRING Yes No Examples EXAMPLE 1 @source(type='http-service', receiver.url='http://localhost:5005/add', source.id='adder', @map(type='json, @attributes(messageId='trp:messageId', value1='$.event.value1', value2='$.event.value2'))) define stream AddStream (messageId string, value1 long, value2 long); @sink(type='http-service-response', source.id='adder', message.id='{{messageId}}', @map(type = 'json')) define stream ResultStream (messageId string, results long); @info(name = 'query1') from AddStream select messageId, value1 + value2 as results insert into ResultStream; The http-service source on stream AddStream listens on url http://localhost:5005/stocks for JSON messages with format: { \"event\": { \"value1\": 3, \"value2\": 4 } } and when events arrive it maps to AddStream events and pass them to query query1 for processing. The query results produced on ResultStream are sent as a response via http-service-response sink with format: { \"event\": { \"results\": 7 } } Here the request and response are correlated by passing the messageId produced by the http-service to the respective http-service-response sink. inMemory (Sink) In-memory sink publishes events to In-memory sources that are subscribe to the same topic to which the sink publishes. This provides a way to connect multiple Siddhi Apps deployed under the same Siddhi Manager (JVM). Here both the publisher and subscriber should have the same event schema (stream definition) for successful data transfer. Origin: siddhi-core:5.1.8 Syntax @sink(type=\"inMemory\", topic=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic topic Event are delivered to allthe subscribers subscribed on this topic. STRING No No Examples EXAMPLE 1 @sink(type='inMemory', topic='Stocks', @map(type='passThrough')) define stream StocksStream (symbol string, price float, volume long); Here the StocksStream uses inMemory sink to emit the Siddhi events to all the inMemory sources deployed in the same JVM and subscribed to the topic Stocks . jms (Sink) JMS Sink allows users to subscribe to a JMS broker and publish JMS messages. Origin: siddhi-io-jms:2.0.3 Syntax @sink(type=\"jms\", destination=\" STRING \", connection.factory.jndi.name=\" STRING \", factory.initial=\" STRING \", provider.url=\" STRING \", connection.factory.type=\" STRING \", connection.username=\" STRING \", connection.password=\" STRING \", connection.factory.nature=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic destination Queue/Topic name which JMS Source should subscribe to STRING No Yes connection.factory.jndi.name JMS Connection Factory JNDI name. This value will be used for the JNDI lookup to find the JMS Connection Factory. QueueConnectionFactory STRING Yes No factory.initial Naming factory initial value STRING No No provider.url Java naming provider URL. Property for specifying configuration information for the service provider to use. The value of the property should contain a URL string (e.g. \"ldap://somehost:389\") STRING No No connection.factory.type Type of the connection connection factory. This can be either queue or topic. queue STRING Yes No connection.username username for the broker. None STRING Yes No connection.password Password for the broker None STRING Yes No connection.factory.nature Connection factory nature for the broker(cached/pooled). default STRING Yes No Examples EXAMPLE 1 @sink(type='jms', @map(type='xml'), factory.initial='org.apache.activemq.jndi.ActiveMQInitialContextFactory', provider.url='vm://localhost',destination='DAS_JMS_OUTPUT_TEST', connection.factory.type='topic',connection.factory.jndi.name='TopicConnectionFactory') define stream inputStream (name string, age int, country string); This example shows how to publish to an ActiveMQ topic. EXAMPLE 2 @sink(type='jms', @map(type='xml'), factory.initial='org.apache.activemq.jndi.ActiveMQInitialContextFactory', provider.url='vm://localhost',destination='DAS_JMS_OUTPUT_TEST') define stream inputStream (name string, age int, country string); This example shows how to publish to an ActiveMQ queue. Note that we are not providing properties like connection factory type kafka (Sink) A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to use the Kafka transport, the type parameter should have kafka as its value. Origin: siddhi-io-kafka:5.0.5 Syntax @sink(type=\"kafka\", bootstrap.servers=\" STRING \", topic=\" STRING \", partition.no=\" INT \", sequence.id=\" STRING \", key=\" STRING \", is.binary.message=\" BOOL \", optional.configuration=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No is.binary.message In order to send the binary events via kafka sink, this parameter is set to 'True'. null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='topic_with_partitions', partition.no='0', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka sink configuration publishes to 0 th partition of the topic named topic_with_partitions . EXAMPLE 2 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='{{symbol}}', partition.no='{{volume}}', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes dynamic topic and partitions that are taken from the Siddhi event. The value for partition.no is taken from the volume attribute, and the topic value is taken from the symbol attribute. kafkaMultiDC (Sink) A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to publish events via the Kafka transport, and using two Kafka brokers to publish events to the same topic, the type parameter must have kafkaMultiDC as its value. Origin: siddhi-io-kafka:5.0.5 Syntax @sink(type=\"kafkaMultiDC\", bootstrap.servers=\" STRING \", topic=\" STRING \", sequence.id=\" STRING \", key=\" STRING \", partition.no=\" INT \", is.binary.message=\" BOOL \", optional.configuration=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma -separated values. There must be at least two servers in this list. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No is.binary.message In order to send the binary events via kafkaMultiDCSink, it is required to set this parameter to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink(type='kafkaMultiDC', topic='myTopic', partition.no='0',bootstrap.servers='host1:9092, host2:9092', @map(type='xml'))Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes to the default (i.e., 0 th ) partition of the brokers in two data centers log (Sink) This is a sink that can be used as a logger. This will log the output events in the output stream with user specified priority and a prefix Origin: siddhi-core:5.1.8 Syntax @sink(type=\"log\", priority=\" STRING \", prefix=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic priority This will set the logger priority i.e log level. Accepted values are INFO, DEBUG, WARN, FATAL, ERROR, OFF, TRACE INFO STRING Yes No prefix This will be the prefix to the output message. If the output stream has event [2,4] and the prefix is given as \"Hello\" then the log will show \"Hello : [2,4]\" default prefix will be : STRING Yes No Examples EXAMPLE 1 @sink(type='log', prefix='My Log', priority='DEBUG') define stream BarStream (symbol string, price float, volume long) In this example BarStream uses log sink and the prefix is given as My Log. Also the priority is set to DEBUG. EXAMPLE 2 @sink(type='log', priority='DEBUG') define stream BarStream (symbol string, price float, volume long) In this example BarStream uses log sink and the priority is set to DEBUG. User has not specified prefix so the default prefix will be in the form Siddhi App Name : Stream Name EXAMPLE 3 @sink(type='log', prefix='My Log') define stream BarStream (symbol string, price float, volume long) In this example BarStream uses log sink and the prefix is given as My Log. User has not given a priority so it will be set to default INFO. EXAMPLE 4 @sink(type='log') define stream BarStream (symbol string, price float, volume long) In this example BarStream uses log sink. The user has not given prefix or priority so they will be set to their default values. nats (Sink) NATS Sink allows users to subscribe to a NATS broker and publish messages. Origin: siddhi-io-nats:2.0.8 Syntax @sink(type=\"nats\", destination=\" STRING \", bootstrap.servers=\" STRING \", client.id=\" STRING \", cluster.id=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic destination Subject name which NATS sink should publish to. STRING No Yes bootstrap.servers The NATS based url of the NATS server. nats://localhost:4222 STRING Yes No client.id The identifier of the client publishing/connecting to the NATS broker. Should be unique for each client connecting to the server/cluster. None STRING Yes No cluster.id The identifier of the NATS server/cluster. test-cluster STRING Yes No Examples EXAMPLE 1 @sink(type='nats', @map(type='xml'), destination='SP_NATS_OUTPUT_TEST', bootstrap.servers='nats://localhost:4222',client.id='nats_client',server.id='test-cluster') define stream outputStream (name string, age int, country string); This example shows how to publish to a NATS subject with all supporting configurations. With the following configuration the sink identified as 'nats-client' will publish to a subject named as 'SP_NATS_OUTPUT_TEST' which resides in a nats instance with a cluster id of 'test-cluster', running in localhost and listening to the port 4222 for client connection. EXAMPLE 2 @sink(type='nats', @map(type='xml'), destination='SP_NATS_OUTPUT_TEST') define stream outputStream (name string, age int, country string); This example shows how to publish to a NATS subject with mandatory configurations. With the following configuration the sink identified with an auto generated client id will publish to a subject named as 'SP_NATS_OUTPUT_TEST' which resides in a nats instance with a cluster id of 'test-cluster', running in localhost and listening to the port 4222 for client connection. prometheus (Sink) This sink publishes events processed by Siddhi into Prometheus metrics and exposes them to the Prometheus server at the specified URL. The created metrics can be published to Prometheus via 'server' or 'pushGateway', depending on your preference. The metric types that are supported by the Prometheus sink are 'counter', 'gauge', 'histogram', and 'summary'. The values and labels of the Prometheus metrics can be updated through the events. Origin: siddhi-io-prometheus:2.1.0 Syntax @sink(type=\"prometheus\", job=\" STRING \", publish.mode=\" STRING \", push.url=\" STRING \", server.url=\" STRING \", metric.type=\" STRING \", metric.help=\" STRING \", metric.name=\" STRING \", buckets=\" STRING \", quantiles=\" STRING \", quantile.error=\" DOUBLE \", value.attribute=\" STRING \", push.operation=\" STRING \", grouping.key=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic job This parameter specifies the job name of the metric. This must be the same job name that is defined in the Prometheus configuration file. siddhiJob STRING Yes No publish.mode The mode in which the metrics need to be exposed to the Prometheus server.The possible publishing modes are 'server' and 'pushgateway'.The server mode exposes the metrics through an HTTP server at the specified URL, and the 'pushGateway' mode pushes the metrics to the pushGateway that needs to be running at the specified URL. server STRING Yes No push.url This parameter specifies the target URL of the Prometheus pushGateway. This is the URL at which the pushGateway must be listening. This URL needs to be defined in the Prometheus configuration file as a target before it can be used here. http://localhost:9091 STRING Yes No server.url This parameter specifies the URL where the HTTP server is initiated to expose metrics in the 'server' publish mode. This URL needs to be defined in the Prometheus configuration file as a target before it can be used here. http://localhost:9080 STRING Yes No metric.type The type of Prometheus metric that needs to be created at the sink. The supported metric types are 'counter', 'gauge',c'histogram' and 'summary'. STRING No No metric.help A brief description of the metric and its purpose. STRING Yes No metric.name This parameter allows you to assign a preferred name for the metric. The metric name must match the regex format, i.e., [a-zA-Z_:][a-zA-Z0-9_:]*. STRING Yes No buckets The bucket values preferred by the user for histogram metrics. The bucket values must be in the 'string' format with each bucket value separated by a comma as shown in the example below. \"2,4,6,8\" null STRING Yes No quantiles This parameter allows you to specify quantile values for summary metrics as preferred. The quantile values must be in the 'string' format with each quantile value separated by a comma as shown in the example below. \"0.5,0.75,0.95\" null STRING Yes No quantile.error The error tolerance value for calculating quantiles in summary metrics. This must be a positive value, but less than 1. 0.001 DOUBLE Yes No value.attribute The name of the attribute in the stream definition that specifies the metric value. The defined 'value' attribute must be included in the stream definition. The system increases the metric value for the counter and gauge metric types by the value of the 'value attribute. The system observes the value of the 'value' attribute for the calculations of 'summary' and 'histogram' metric types. value STRING Yes No push.operation This parameter defines the mode for pushing metrics to the pushGateway. The available push operations are 'push' and 'pushadd'. The operations differ according to the existing metrics in pushGateway where 'push' operation replaces the existing metrics, and 'pushadd' operation only updates the newly created metrics. pushadd STRING Yes No grouping.key This parameter specifies the grouping key of created metrics in key-value pairs. The grouping key is used only in pushGateway mode in order to distinguish the metrics from already existing metrics. The expected format of the grouping key is as follows: \"'key1:value1','key2:value2'\" STRING Yes No System Parameters Name Description Default Value Possible Parameters jobName This property specifies the default job name for the metric. This job name must be the same as the job name defined in the Prometheus configuration file. siddhiJob Any string publishMode The default publish mode for the Prometheus sink for exposing metrics to the Prometheus server. The mode can be either 'server' or 'pushgateway'. server server or pushgateway serverURL This property configures the URL where the HTTP server is initiated to expose metrics. This URL needs to be defined in the Prometheus configuration file as a target to be identified by Prometheus before it can be used here. By default, the HTTP server is initiated at 'http://localhost:9080'. http://localhost:9080 Any valid URL pushURL This property configures the target URL of the Prometheus pushGateway (where the pushGateway needs to listen). This URL needs to be defined in the Prometheus configuration file as a target to be identified by Prometheus before it can be used here. http://localhost:9091 Any valid URL groupingKey This property configures the grouping key of created metrics in key-value pairs. Grouping key is used only in pushGateway mode in order to distinguish these metrics from already existing metrics under the same job. The expected format of the grouping key is as follows: \"'key1:value1','key2:value2'\" . null Any key value pairs in the supported format Examples EXAMPLE 1 @sink(type='prometheus',job='fooOrderCount', server.url ='http://localhost:9080', publish.mode='server', metric.type='counter', metric.help= 'Number of foo orders', @map(type='keyvalue')) define stream FooCountStream (Name String, quantity int, value int); In the above example, the Prometheus-sink creates a counter metric with the stream name and defined attributes as labels. The metric is exposed through an HTTP server at the target URL. EXAMPLE 2 @sink(type='prometheus',job='inventoryLevel', push.url='http://localhost:9080', publish.mode='pushGateway', metric.type='gauge', metric.help= 'Current level of inventory', @map(type='keyvalue')) define stream InventoryLevelStream (Name String, value int); In the above example, the Prometheus-sink creates a gauge metric with the stream name and defined attributes as labels.The metric is pushed to the Prometheus pushGateway at the target URL. rabbitmq (Sink) The rabbitmq sink pushes the events into a rabbitmq broker using the AMQP protocol Origin: siddhi-io-rabbitmq:3.0.2 Syntax @sink(type=\"rabbitmq\", uri=\" STRING \", heartbeat=\" INT \", exchange.name=\" STRING \", exchange.type=\" STRING \", exchange.durable.enabled=\" BOOL \", exchange.autodelete.enabled=\" BOOL \", delivery.mode=\" INT \", content.type=\" STRING \", content.encoding=\" STRING \", priority=\" INT \", correlation.id=\" STRING \", reply.to=\" STRING \", expiration=\" STRING \", message.id=\" STRING \", timestamp=\" STRING \", type=\" STRING \", user.id=\" STRING \", app.id=\" STRING \", routing.key=\" STRING \", headers=\" STRING \", tls.enabled=\" BOOL \", tls.truststore.path=\" STRING \", tls.truststore.password=\" STRING \", tls.truststore.type=\" STRING \", tls.version=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic uri The URI that used to connect to an AMQP server. If no URI is specified, an error is logged in the CLI.e.g., amqp://guest:guest , amqp://guest:guest@localhost:5672 STRING No No heartbeat The period of time (in seconds) after which the peer TCP connection should be considered unreachable (down) by RabbitMQ and client libraries. 60 INT Yes No exchange.name The name of the exchange that decides what to do with a message it sends.If the exchange.name already exists in the RabbitMQ server, then the system uses that exchange.name instead of redeclaring. STRING No Yes exchange.type The type of the exchange.name. The exchange types available are direct , fanout , topic and headers . For a detailed description of each type, see RabbitMQ - AMQP Concepts direct STRING Yes Yes exchange.durable.enabled If this is set to true , the exchange remains declared even if the broker restarts. false BOOL Yes Yes exchange.autodelete.enabled If this is set to true , the exchange is automatically deleted when it is not used anymore. false BOOL Yes Yes delivery.mode This determines whether the connection should be persistent or not. The value must be either 1 or 2 .If the delivery.mode = 1, then the connection is not persistent. If the delivery.mode = 2, then the connection is persistent. 1 INT Yes No content.type The message content type. This should be the MIME content type. null STRING Yes No content.encoding The message content encoding. The value should be MIME content encoding. null STRING Yes No priority Specify a value within the range 0 to 9 in this parameter to indicate the message priority. 0 INT Yes Yes correlation.id The message correlated to the current message. e.g., The request to which this message is a reply. When a request arrives, a message describing the task is pushed to the queue by the front end server. After that the frontend server blocks to wait for a response message with the same correlation ID. A pool of worker machines listen on queue, and one of them picks up the task, performs it, and returns the result as message. Once a message with right correlation ID arrives, thefront end server continues to return the response to the caller. null STRING Yes Yes reply.to This is an anonymous exclusive callback queue. When the RabbitMQ receives a message with the reply.to property, it sends the response to the mentioned queue. This is commonly used to name a reply queue (or any other identifier that helps a consumer application to direct its response). null STRING Yes No expiration The expiration time after which the message is deleted. The value of the expiration field describes the TTL (Time To Live) period in milliseconds. null STRING Yes No message.id The message identifier. If applications need to identify messages, it is recommended that they use this attribute instead of putting it into the message payload. null STRING Yes Yes timestamp Timestamp of the moment when the message was sent. If you do not specify a value for this parameter, the system automatically generates the current date and time as the timestamp value. The format of the timestamp value is dd/mm/yyyy . current timestamp STRING Yes No type The type of the message. e.g., The type of the event or the command represented by the message. null STRING Yes No user.id The user ID specified here is verified by RabbitMQ against theuser name of the actual connection. This is an optional parameter. null STRING Yes No app.id The identifier of the application that produced the message. null STRING Yes No routing.key The key based on which the excahnge determines how to route the message to the queue. The routing key is similar to an address for the message. empty STRING Yes Yes headers The headers of the message. The attributes used for routing are taken from the this paremeter. A message is considered matching if the value of the header equals the value specified upon binding. null STRING Yes Yes tls.enabled This parameter specifies whether an encrypted communication channel should be established or not. When this parameter is set to true , the tls.truststore.path and tls.truststore.password parameters are initialized. false BOOL Yes No tls.truststore.path The file path to the location of the truststore of the client that sends the RabbitMQ events via the AMQP protocol. A custom client-truststore can be specified if required. If a custom truststore is not specified, then the system uses the default client-trustore in the {carbon.home}/resources/security /code directory. /p /td td style=\"vertical-align: top\" {carbon.home}/resources/security</code> directory.</p></td> <td style=\"vertical-align: top\"> /resources/security/client-truststore.jks STRING Yes No tls.truststore.password The password for the client-truststore. A custom password can be specified if required. If no custom password is specified, then the system uses wso2carbon as the default password. wso2carbon STRING Yes No tls.truststore.type The type of the truststore. JKS STRING Yes No tls.version The version of the tls/ssl. SSL STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink(type ='rabbitmq', uri = 'amqp://guest:guest@localhost:5672', exchange.name = 'direct', routing.key= 'direct', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes events to the direct exchange with the direct exchange type and the directTest routing key. s3 (Sink) S3 sink publishes events as Amazon AWS S3 buckets. Origin: siddhi-io-s3:1.0.2 Syntax @sink(type=\"s3\", credential.provider.class=\" STRING \", aws.access.key=\" STRING \", aws.secret.key=\" STRING \", bucket.name=\" STRING \", aws.region=\" STRING \", versioning.enabled=\" BOOL \", object.path=\" STRING \", storage.class=\" STRING \", content.type=\" STRING \", bucket.acl=\" STRING \", node.id=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic credential.provider.class AWS credential provider class to be used. If blank along with the username and the password, default credential provider will be used. EMPTY_STRING STRING Yes No aws.access.key AWS access key. This cannot be used along with the credential.provider.class EMPTY_STRING STRING Yes No aws.secret.key AWS secret key. This cannot be used along with the credential.provider.class EMPTY_STRING STRING Yes No bucket.name Name of the S3 bucket STRING No No aws.region The region to be used to create the bucket EMPTY_STRING STRING Yes No versioning.enabled Flag to enable versioning support in the bucket false BOOL Yes No object.path Path for each S3 object STRING No Yes storage.class AWS storage class standard STRING Yes No content.type Content type of the event application/octet-stream STRING Yes Yes bucket.acl Access control list for the bucket EMPTY_STRING STRING Yes No node.id The node ID of the current publisher. This needs to be unique for each publisher instance as it may cause object overwrites while uploading the objects to same S3 bucket from different publishers. EMPTY_STRING STRING Yes No Examples EXAMPLE 1 @sink(type='s3', bucket.name='user-stream-bucket',object.path='bar/users', credential.provider='software.amazon.awssdk.auth.credentials.ProfileCredentialsProvider', flush.size='3', @map(type='json', enclosing.element='$.user', @payload(\"\"\"{\"name\": \"{{name}}\", \"age\": {{age}}}\"\"\"))) define stream UserStream(name string, age int); This creates a S3 bucket named 'user-stream-bucket'. Then this will collect 3 events together and create a JSON object and save that in S3. tcp (Sink) A Siddhi application can be configured to publish events via the TCP transport by adding the @Sink(type = 'tcp') annotation at the top of an event stream definition. Origin: siddhi-io-tcp:3.0.4 Syntax @sink(type=\"tcp\", url=\" STRING \", sync=\" STRING \", tcp.no.delay=\" BOOL \", keep.alive=\" BOOL \", worker.threads=\" INT|LONG \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic url The URL to which outgoing events should be published via TCP. The URL should adhere to tcp:// host : port / context format. STRING No No sync This parameter defines whether the events should be published in a synchronized manner or not. If sync = 'true', then the worker will wait for the ack after sending the message. Else it will not wait for an ack. false STRING Yes Yes tcp.no.delay This is to specify whether to disable Nagle algorithm during message passing. If tcp.no.delay = 'true', the execution of Nagle algorithm will be disabled in the underlying TCP logic. Hence there will be no delay between two successive writes to the TCP connection. Else there can be a constant ack delay. true BOOL Yes No keep.alive This property defines whether the server should be kept alive when there are no connections available. true BOOL Yes No worker.threads Number of threads to publish events. 10 INT LONG Yes No Examples EXAMPLE 1 @Sink(type = 'tcp', url='tcp://localhost:8080/abc', sync='true' @map(type='binary')) define stream Foo (attribute1 string, attribute2 int); A sink of type 'tcp' has been defined. All events arriving at Foo stream via TCP transport will be sent to the url tcp://localhost:8080/abc in a synchronous manner. Sinkmapper avro (Sink Mapper) This extension is a Siddhi Event to Avro Message output mapper.Transports that publish messages to Avro sink can utilize this extension to convert Siddhi events to Avro messages. You can either specify the Avro schema or provide the schema registry URL and the schema reference ID as parameters in the stream definition. If no Avro schema is specified, a flat Avro schema of the 'record' type is generated with the stream attributes as schema fields. Origin: siddhi-map-avro:2.0.6 Syntax @sink(..., @map(type=\"avro\", schema.def=\" STRING \", schema.registry=\" STRING \", schema.id=\" STRING \") QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic schema.def This specifies the required Avro schema to be used to convert Siddhi events to Avro messages. The schema needs to be specified as a quoted JSON string. STRING No No schema.registry This specifies the URL of the schema registry. STRING No No schema.id This specifies the ID of the avro schema. This ID is the global ID that is returned from the schema registry when posting the schema to the registry. The specified ID is used to retrieve the schema from the schema registry. STRING No No Examples EXAMPLE 1 @sink(type='inMemory', topic='stock', @map(type='avro',schema.def = \"\"\"{\"type\":\"record\",\"name\":\"stock\",\"namespace\":\"stock.example\",\"fields\":[{\"name\":\"symbol\",\"type\":\"string\"},{\"name\":\"price\",\"type\":\"float\"},{\"name\":\"volume\",\"type\":\"long\"}]}\"\"\")) define stream StockStream (symbol string, price float, volume long); The above configuration performs a default Avro mapping that generates an Avro message as an output ByteBuffer. EXAMPLE 2 @sink(type='inMemory', topic='stock', @map(type='avro',schema.registry = 'http://localhost:8081', schema.id ='22',@payload(\"\"\"{\"Symbol\":{{symbol}},\"Price\":{{price}},\"Volume\":{{volume}}}\"\"\" ))) define stream StockStream (symbol string, price float, volume long); The above configuration performs a custom Avro mapping that generates an Avro message as an output ByteBuffer. The Avro schema is retrieved from the given schema registry (localhost:8081) using the schema ID provided. binary (Sink Mapper) This section explains how to map events processed via Siddhi in order to publish them in the binary format. Origin: siddhi-map-binary:2.0.4 Syntax @sink(..., @map(type=\"binary\") Examples EXAMPLE 1 @sink(type='inMemory', topic='WSO2', @map(type='binary')) define stream FooStream (symbol string, price float, volume long); This will publish Siddhi event in binary format. csv (Sink Mapper) This output mapper extension allows you to convert Siddhi events processed by the WSO2 SP to CSV message before publishing them. You can either use custom placeholder to map a custom CSV message or use pre-defined CSV format where event conversion takes place without extra configurations. Origin: siddhi-map-csv:2.0.3 Syntax @sink(..., @map(type=\"csv\", delimiter=\" STRING \", header=\" BOOL \", event.grouping.enabled=\" BOOL \") QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic delimiter This parameter used to separate the output CSV data, when converting a Siddhi event to CSV format, , STRING Yes No header This parameter specifies whether the CSV messages will be generated with header or not. If this parameter is set to true, message will be generated with header false BOOL Yes No event.grouping.enabled If this parameter is set to true , events are grouped via a line.separator when multiple events are received. It is required to specify a value for the System.lineSeparator() when the value for this parameter is true . false BOOL Yes No Examples EXAMPLE 1 @sink(type='inMemory', topic='{{symbol}}', @map(type='csv')) define stream BarStream (symbol string, price float, volume long); Above configuration will perform a default CSV output mapping, which will generate output as follows: WSO2,55.6,100 OS supported line separator If header is true and delimiter is \"-\", then the output will be as follows: symbol-price-volume OS supported line separator WSO2-55.6-100 OS supported line separator EXAMPLE 2 @sink(type='inMemory', topic='{{symbol}}', @map(type='csv',header='true',delimiter='-',@payload(symbol='0',price='2',volume='1')))define stream BarStream (symbol string, price float,volume long); Above configuration will perform a custom CSV mapping. Here, user can add custom place order in the @payload. The place order indicates that where the attribute name's value will be appear in the output message, The output will be produced output as follows: WSO2,100,55.6 If header is true and delimiter is \"-\", then the output will be as follows: symbol-price-volume WSO2-55.6-100 OS supported line separator If event grouping is enabled, then the output is as follows: WSO2-55.6-100 OS supported line separator WSO2-55.6-100 OS supported line separator WSO2-55.6-100 OS supported line separator json (Sink Mapper) This extension is an Event to JSON output mapper. Transports that publish messages can utilize this extension to convert Siddhi events to JSON messages. You can either send a pre-defined JSON format or a custom JSON message. Origin: siddhi-map-json:5.0.5 Syntax @sink(..., @map(type=\"json\", validate.json=\" BOOL \", enclosing.element=\" STRING \") QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic validate.json If this property is set to true , it enables JSON validation for the JSON messages generated. When validation is carried out, messages that do not adhere to proper JSON standards are dropped. This property is set to 'false' by default. false BOOL Yes No enclosing.element This specifies the enclosing element to be used if multiple events are sent in the same JSON message. Siddhi treats the child elements of the given enclosing element as events and executes JSON expressions on them. If an enclosing.element is not provided, the multiple event scenario is disregarded and JSON path is evaluated based on the root element. $ STRING Yes No Examples EXAMPLE 1 @sink(type='inMemory', topic='stock', @map(type='json')) define stream FooStream (symbol string, price float, volume long); Above configuration does a default JSON input mapping that generates the output given below. { \"event\":{ \"symbol\":WSO2, \"price\":55.6, \"volume\":100 } } EXAMPLE 2 @sink(type='inMemory', topic='{{symbol}}', @map(type='json', enclosing.element='$.portfolio', validate.json='true', @payload( \"\"\"{\"StockData\":{\"Symbol\":\"{{symbol}}\",\"Price\":{{price}}}\"\"\"))) define stream BarStream (symbol string, price float, volume long); The above configuration performs a custom JSON mapping that generates the following JSON message as the output. {\"portfolio\":{ \"StockData\":{ \"Symbol\":WSO2, \"Price\":55.6 } } } keyvalue (Sink Mapper) The Event to Key-Value Map output mapper extension allows you to convert Siddhi events processed by WSO2 SP to key-value map events before publishing them. You can either use pre-defined keys where conversion takes place without extra configurations, or use custom keys with which the messages can be published. Origin: siddhi-map-keyvalue:2.0.5 Syntax @sink(..., @map(type=\"keyvalue\") Examples EXAMPLE 1 @sink(type='inMemory', topic='stock', @map(type='keyvalue')) define stream FooStream (symbol string, price float, volume long); This query performs a default Key-Value output mapping. The expected output is something similar to the following: symbol:'WSO2' price : 55.6f volume: 100L EXAMPLE 2 @sink(type='inMemory', topic='stock', @map(type='keyvalue', @payload(a='symbol',b='price',c='volume'))) define stream FooStream (symbol string, price float, volume long); This query performs a custom Key-Value output mapping where values are passed as objects. Values for symbol , price , and volume attributes are published with the keys a , b and c respectively. The expected output is a map similar to the following: a:'WSO2' b : 55.6f c: 100L EXAMPLE 3 @sink(type='inMemory', topic='stock', @map(type='keyvalue', @payload(a='{{symbol}} is here',b='`price`',c='volume'))) define stream FooStream (symbol string, price float, volume long); This query performs a custom Key-Value output mapping where the values of the a and b attributes are strings and c is object. The expected output should be a Map similar to the following: a:'WSO2 is here' b : 'price' c: 100L passThrough (Sink Mapper) Pass-through mapper passed events (Event[]) through without any mapping or modifications. Origin: siddhi-core:5.1.8 Syntax @sink(..., @map(type=\"passThrough\") Examples EXAMPLE 1 @sink(type='inMemory', @map(type='passThrough')) define stream BarStream (symbol string, price float, volume long); In the following example BarStream uses passThrough outputmapper which emit Siddhi event directly without any transformation into sink. protobuf (Sink Mapper) This output mapper allows you to convert Events to protobuf messages before publishing them. To work with this mapper you have to add auto-generated protobuf classes to the project classpath. When you use this output mapper, you can either define stream attributes as the same names as the protobuf message attributes or you can use custom mapping to map stream definition attributes with the protobuf attributes..Please find the sample proto definition here Origin: siddhi-map-protobuf:1.0.2 Syntax @sink(..., @map(type=\"protobuf\", class=\" STRING \") QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic class This specifies the class name of the protobuf message class, If sink type is grpc then it's not necessary to provide this parameter. - STRING Yes No Examples EXAMPLE 1 @sink(type='grpc', url = 'grpc://localhost:2000/org.wso2.grpc.test.MyService/process @map(type='protobuf')) define stream BarStream (stringValue string, intValue int,longValue long,booleanValue bool,floatValue float,doubleValue double) Above definition will map BarStream values into the protobuf message type of the 'process' method in 'MyService' service EXAMPLE 2 @sink(type='grpc', url = 'grpc://localhost:2000/org.wso2.grpc.test.MyService/process @map(type='protobuf'), @payload(stringValue='a',longValue='b',intValue='c',booleanValue='d',floatValue = 'e', doubleValue = 'f'))) define stream BarStream (a string, b long, c int,d bool,e float,f double); The above definition will map BarStream values to request message type of the 'process' method in 'MyService' service. and stream values will map like this, - value of 'a' will be assign 'stringValue' variable in the message class - value of 'b' will be assign 'longValue' variable in the message class - value of 'c' will be assign 'intValue' variable in the message class - value of 'd' will be assign 'booleanValue' variable in the message class - value of 'e' will be assign 'floatValue' variable in the message class - value of 'f' will be assign 'doubleValue' variable in the message class EXAMPLE 3 @sink(type='grpc', url = 'grpc://localhost:2000/org.wso2.grpc.test.MyService/testMap' @map(type='protobuf')) define stream BarStream (stringValue string,intValue int,map object); The above definition will map BarStream values to request message type of the 'testMap' method in 'MyService' service and since there is an object data type is inthe stream(map object) , mapper will assume that 'map' is an instance of 'java.util.Map' class, otherwise it will throws and error. EXAMPLE 4 @sink(type='inMemory', topic='test01', @map(type='protobuf', class='org.wso2.grpc.test.Request')) define stream BarStream (stringValue string, intValue int,longValue long,booleanValue bool,floatValue float,doubleValue double); The above definition will map BarStream values to 'org.wso2.grpc.test.Request'protobuf class type. If sink type is not a grpc, sink is expecting to get the mapping protobuf class from the 'class' parameter in the @map extension text (Sink Mapper) This extension is a Event to Text output mapper. Transports that publish text messages can utilize this extension to convert the Siddhi events to text messages. Users can use a pre-defined text format where event conversion is carried out without any additional configurations, or use custom placeholder(using {{ and }} ) to map custom text messages. Again, you can also enable mustache based custom mapping. In mustache based custom mapping you can use custom placeholder (using {{ and }} or {{{ and }}} ) to map custom text. In mustache based custom mapping, all variables are HTML escaped by default. For example: is replaced with amp; \" is replaced with quot; = is replaced with #61; If you want to return unescaped HTML, use the triple mustache {{{ instead of double {{ . Origin: siddhi-map-text:2.0.4 Syntax @sink(..., @map(type=\"text\", event.grouping.enabled=\" BOOL \", delimiter=\" STRING \", new.line.character=\" STRING \", mustache.enabled=\" BOOL \") QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic event.grouping.enabled If this parameter is set to true , events are grouped via a delimiter when multiple events are received. It is required to specify a value for the delimiter parameter when the value for this parameter is true . false BOOL Yes No delimiter This parameter specifies how events are separated when a grouped event is received. This must be a whole line and not a single character. ~ ~ ~ ~ STRING Yes No new.line.character This attribute indicates the new line character of the event that is expected to be received. This is used mostly when communication between 2 types of operating systems is expected. For example, Linux uses \\n whereas Windows uses \\r\\n as the end of line character. \\n STRING Yes No mustache.enabled If this parameter is set to true , then mustache mapping gets enabled forcustom text mapping. false BOOL Yes No Examples EXAMPLE 1 @sink(type='inMemory', topic='stock', @map(type='text')) define stream FooStream (symbol string, price float, volume long); This query performs a default text input mapping. The expected output is as follows: symbol:\"WSO2\", price:55.6, volume:100 EXAMPLE 2 @sink(type='inMemory', topic='stock', @map(type='text', event.grouping.enabled='true')) define stream FooStream (symbol string, price float, volume long); This query performs a default text input mapping with event grouping. The expected output is as follows: symbol:\"WSO2\", price:55.6, volume:100 ~ ~ ~ ~ symbol:\"WSO2\", price:55.6, volume:100 EXAMPLE 3 @sink(type='inMemory', topic='stock', @map(type='text', @payload(\"SensorID : {{symbol}}/{{volume}}, SensorPrice : Rs{{price}}/=, Value : {{volume}}ml\"))) define stream FooStream (symbol string, price float, volume long); This query performs a custom text mapping. The expected output is as follows: SensorID : wso2/100, SensorPrice : Rs1000/=, Value : 100ml for the following siddhi event. {wso2,1000,100} EXAMPLE 4 @sink(type='inMemory', topic='stock', @map(type='text', event.grouping.enabled='true', @payload(\"Stock price of {{symbol}} is {{price}}\"))) define stream FooStream (symbol string, price float, volume long); This query performs a custom text mapping with event grouping. The expected output is as follows: Stock price of WSO2 is 55.6 ~ ~ ~ ~ Stock price of WSO2 is 55.6 ~ ~ ~ ~ Stock price of WSO2 is 55.6 for the following siddhi event. {WSO2,55.6,10} EXAMPLE 5 @sink(type='inMemory', topic='stock', @map(type='text', mustache.enabled='true', @payload(\"SensorID : {{{symbol}}}/{{{volume}}}, SensorPrice : Rs{{{price}}}/=, Value : {{{volume}}}ml\"))) define stream FooStream (symbol string, price float, volume long); This query performs a custom text mapping to return unescaped HTML. The expected output is as follows: SensorID : a b/100, SensorPrice : Rs1000/=, Value : 100ml for the following siddhi event. {a b,1000,100} xml (Sink Mapper) This mapper converts Siddhi output events to XML before they are published via transports that publish in XML format. Users can either send a pre-defined XML format or a custom XML message containing event data. Origin: siddhi-map-xml:5.0.3 Syntax @sink(..., @map(type=\"xml\", validate.xml=\" BOOL \", enclosing.element=\" STRING \") QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic validate.xml This parameter specifies whether the XML messages generated should be validated or not. If this parameter is set to true, messages that do not adhere to proper XML standards are dropped. false BOOL Yes No enclosing.element When an enclosing element is specified, the child elements (e.g., the immediate child elements) of that element are considered as events. This is useful when you need to send multiple events in a single XML message. When an enclosing element is not specified, one XML message per every event will be emitted without enclosing. None in custom mapping and events in default mapping STRING Yes No Examples EXAMPLE 1 @sink(type='inMemory', topic='stock', @map(type='xml')) define stream FooStream (symbol string, price float, volume long); Above configuration will do a default XML input mapping which will generate below output events event symbol WSO2 /symbol price 55.6 /price volume 100 /volume /event /events EXAMPLE 2 @sink(type='inMemory', topic='{{symbol}}', @map(type='xml', enclosing.element=' portfolio ', validate.xml='true', @payload( \" StockData Symbol {{symbol}} /Symbol Price {{price}} /Price /StockData \"))) define stream BarStream (symbol string, price float, volume long); Above configuration will perform a custom XML mapping. Inside @payload you can specify the custom template that you want to send the messages out and addd placeholders to places where you need to add event attributes.Above config will produce below output XML message portfolio StockData Symbol WSO2 /Symbol Price 55.6 /Price /StockData /portfolio Source cdc (Source) The CDC source receives events when change events (i.e., INSERT, UPDATE, DELETE) are triggered for a database table. Events are received in the 'key-value' format. There are two modes you could perform CDC: Listening mode and Polling mode. In polling mode, the datasource is periodically polled for capturing the changes. The polling period can be configured. In polling mode, you can only capture INSERT and UPDATE changes. On listening mode, the Source will keep listening to the Change Log of the database and notify in case a change has taken place. Here, you are immediately notified about the change, compared to polling mode. The key values of the map of a CDC change event are as follows. For 'listening' mode: For insert: Keys are specified as columns of the table. For delete: Keys are followed by the specified table columns. This is achieved via 'before_'. e.g., specifying 'before_X' results in the key being added before the column named 'X'. For update: Keys are followed followed by the specified table columns. This is achieved via 'before_'. e.g., specifying 'before_X' results in the key being added before the column named 'X'. For 'polling' mode: Keys are specified as the columns of the table.#### Preparations required for working with Oracle Databases in listening mode Using the extension in Windows, Mac OSX and AIX are pretty straight forward inorder to achieve the required behaviour please follow the steps given below - Download the compatible version of oracle instantclient for the database version from here and extract - Extract and set the environment variable LD_LIBRARY_PATH to the location of instantclient which was exstracted as shown below export LD_LIBRARY_PATH= path to the instant client location - Inside the instantclient folder which was download there are two jars xstreams.jar and ojdbc version .jar convert them to OSGi bundles using the tools which were provided in the distribution /bin for converting the ojdbc.jar use the tool spi-provider.sh|bat and for the conversion of xstreams.jar use the jni-provider.sh as shown below(Note: this way of converting Xstreams jar is applicable only for Linux environments for other OSs this step is not required and converting it through the jartobundle.sh tool is enough) ./jni-provider.sh input-jar destination comma seperated native library names once ojdbc and xstreams jars are converted to OSGi copy the generated jars to the distribution /lib . Currently siddhi-io-cdc only supports the oracle database distributions 12 and above See parameter: mode for supported databases and change events. Origin: siddhi-io-cdc:2.0.4 Syntax @source(type=\"cdc\", url=\" STRING \", mode=\" STRING \", jdbc.driver.name=\" STRING \", username=\" STRING \", password=\" STRING \", pool.properties=\" STRING \", datasource.name=\" STRING \", table.name=\" STRING \", polling.column=\" STRING \", polling.interval=\" INT \", operation=\" STRING \", connector.properties=\" STRING \", database.server.id=\" STRING \", database.server.name=\" STRING \", wait.on.missed.record=\" BOOL \", missed.record.waiting.timeout=\" INT \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic url The connection URL to the database. F=The format used is: 'jdbc:mysql:// host : port / database_name ' STRING No No mode Mode to capture the change data. The type of events that can be received, and the required parameters differ based on the mode. The mode can be one of the following: 'polling': This mode uses a column named 'polling.column' to monitor the given table. It captures change events of the 'RDBMS', 'INSERT, and 'UPDATE' types. 'listening': This mode uses logs to monitor the given table. It currently supports change events only of the 'MySQL', 'INSERT', 'UPDATE', and 'DELETE' types. listening STRING Yes No jdbc.driver.name The driver class name for connecting the database. It is required to specify a value for this parameter when the mode is 'polling'. STRING Yes No username The username to be used for accessing the database. This user needs to have the 'SELECT', 'RELOAD', 'SHOW DATABASES', 'REPLICATION SLAVE', and 'REPLICATION CLIENT'privileges for the change data capturing table (specified via the 'table.name' parameter). To operate in the polling mode, the user needs 'SELECT' privileges. STRING No No password The password of the username you specified for accessing the database. STRING No No pool.properties The pool parameters for the database connection can be specified as key-value pairs. STRING Yes No datasource.name Name of the wso2 datasource to connect to the database. When datasource name is provided, the URL, username and password are not needed. A datasource based connection is given more priority over the URL based connection. This parameter is applicable only when the mode is set to 'polling', and it can be applied only when you use this extension with WSO2 Stream Processor. STRING Yes No table.name The name of the table that needs to be monitored for data changes. STRING No No polling.column The column name that is polled to capture the change data. It is recommended to have a TIMESTAMP field as the 'polling.column' in order to capture the inserts and updates. Numeric auto-incremental fields and char fields can also be used as 'polling.column'. However, note that fields of these types only support insert change capturing, and the possibility of using a char field also depends on how the data is input. It is required to enter a value for this parameter only when the mode is 'polling'. STRING Yes No polling.interval The time interval (specified in seconds) to poll the given table for changes. This parameter is applicable only when the mode is set to 'polling'. 1 INT Yes No operation The change event operation you want to carry out. Possible values are 'insert', 'update' or 'delete'. This parameter is not case sensitive. It is required to specify a value only when the mode is 'listening'. STRING No No connector.properties Here, you can specify Debezium connector properties as a comma-separated string. The properties specified here are given more priority over the parameters. This parameter is applicable only for the 'listening' mode. Empty_String STRING Yes No database.server.id An ID to be used when joining MySQL database cluster to read the bin log. This should be a unique integer between 1 to 2^32. This parameter is applicable only when the mode is 'listening'. Random integer between 5400 and 6400 STRING Yes No database.server.name A logical name that identifies and provides a namespace for the database server. This parameter is applicable only when the mode is 'listening'. {host}_{port} STRING Yes No wait.on.missed.record Indicates whether the process needs to wait on missing/out-of-order records. When this flag is set to 'true' the process will be held once it identifies a missing record. The missing recrod is identified by the sequence of the polling.column value. This can be used only with number fields and not recommended to use with time values as it will not be sequential. This should be enabled ONLY where the records can be written out-of-order, (eg. concurrent writers) as this degrades the performance. false BOOL Yes No missed.record.waiting.timeout The timeout (specified in seconds) to retry for missing/out-of-order record. This should be used along with the wait.on.missed.record parameter. If the parameter is not set, the process will indefinitely wait for the missing record. -1 INT Yes No Examples EXAMPLE 1 @source(type = 'cdc' , url = 'jdbc:mysql://localhost:3306/SimpleDB', username = 'cdcuser', password = 'pswd4cdc', table.name = 'students', operation = 'insert', @map(type='keyvalue', @attributes(id = 'id', name = 'name'))) define stream inputStream (id string, name string); In this example, the CDC source listens to the row insertions that are made in the 'students' table with the column name, and the ID. This table belongs to the 'SimpleDB' MySQL database that can be accessed via the given URL. EXAMPLE 2 @source(type = 'cdc' , url = 'jdbc:mysql://localhost:3306/SimpleDB', username = 'cdcuser', password = 'pswd4cdc', table.name = 'students', operation = 'update', @map(type='keyvalue', @attributes(id = 'id', name = 'name', before_id = 'before_id', before_name = 'before_name'))) define stream inputStream (before_id string, id string, before_name string , name string); In this example, the CDC source listens to the row updates that are made in the 'students' table. This table belongs to the 'SimpleDB' MySQL database that can be accessed via the given URL. EXAMPLE 3 @source(type = 'cdc' , url = 'jdbc:mysql://localhost:3306/SimpleDB', username = 'cdcuser', password = 'pswd4cdc', table.name = 'students', operation = 'delete', @map(type='keyvalue', @attributes(before_id = 'before_id', before_name = 'before_name'))) define stream inputStream (before_id string, before_name string); In this example, the CDC source listens to the row deletions made in the 'students' table. This table belongs to the 'SimpleDB' database that can be accessed via the given URL. EXAMPLE 4 @source(type = 'cdc', mode='polling', polling.column = 'id', jdbc.driver.name = 'com.mysql.jdbc.Driver', url = 'jdbc:mysql://localhost:3306/SimpleDB', username = 'cdcuser', password = 'pswd4cdc', table.name = 'students', @map(type='keyvalue'), @attributes(id = 'id', name = 'name')) define stream inputStream (id int, name string); In this example, the CDC source polls the 'students' table for inserts. 'id' that is specified as the polling colum' is an auto incremental field. The connection to the database is made via the URL, username, password, and the JDBC driver name. EXAMPLE 5 @source(type = 'cdc', mode='polling', polling.column = 'id', datasource.name = 'SimpleDB', table.name = 'students', @map(type='keyvalue'), @attributes(id = 'id', name = 'name')) define stream inputStream (id int, name string); In this example, the CDC source polls the 'students' table for inserts. The given polling column is a char column with the 'S001, S002, ... .' pattern. The connection to the database is made via a data source named 'SimpleDB'. Note that the 'datasource.name' parameter works only with the Stream Processor. EXAMPLE 6 @source(type = 'cdc', mode='polling', polling.column = 'last_updated', datasource.name = 'SimpleDB', table.name = 'students', @map(type='keyvalue')) define stream inputStream (name string); In this example, the CDC source polls the 'students' table for inserts and updates. The polling column is a timestamp field. EXAMPLE 7 @source(type='cdc', jdbc.driver.name='com.mysql.jdbc.Driver', url='jdbc:mysql://localhost:3306/SimpleDB', username='cdcuser', password='pswd4cdc', table.name='students', mode='polling', polling.column='id', operation='insert', wait.on.missed.record='true', missed.record.waiting.timeout='10', @map(type='keyvalue'), @attributes(batch_no='batch_no', item='item', qty='qty')) define stream inputStream (id int, name string); In this example, the CDC source polls the 'students' table for inserts. The polling column is a numeric field. This source expects the records in the database to be written concurrently/out-of-order so it waits if it encounters a missing record. If the record doesn't appear within 10 seconds it resumes the process. EXAMPLE 8 @source(type = 'cdc', url = 'jdbc:oracle:thin://localhost:1521/ORCLCDB', username='c##xstrm', password='xs', table.name='DEBEZIUM.sweetproductiontable', operation = 'insert', connector.properties='oracle.outserver.name=DBZXOUT,oracle.pdb=ORCLPDB1' @map(type = 'keyvalue')) define stream insertSweetProductionStream (ID int, NAME string, WEIGHT int); In this example, the CDC source connect to an Oracle database and listens for insert queries of sweetproduction table email (Source) The 'Email' source allows you to receive events via emails. An 'Email' source can be configured using the 'imap' or 'pop3' server to receive events. This allows you to filter the messages that satisfy the criteria specified under the 'search term' option. The email source parameters can be defined in either the ' SP_HOME /conf/ PROFILE /deployment yaml' file or the stream definition. If the parameter configurations are not available in either place, the default values are considered (i.e., if default values are available). If you need to configure server system parameters that are not provided as options in the stream definition, they need to be defined in the 'deployment yaml' file under 'email source properties'. For more information about 'imap' and 'pop3' server system parameters, see the following. JavaMail Reference Implementation - IMAP Store JavaMail Reference Implementation - POP3 Store Store Origin: siddhi-io-email:2.0.5 Syntax @source(type=\"email\", username=\" STRING \", password=\" STRING \", store=\" STRING \", host=\" STRING \", port=\" INT \", folder=\" STRING \", search.term=\" STRING \", polling.interval=\" LONG \", action.after.processed=\" STRING \", folder.to.move=\" STRING \", content.type=\" STRING \", ssl.enable=\" BOOL \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic username The user name of the email account. e.g., 'wso2mail' is the username of the 'wso2mail@gmail.com' mail account. STRING No No password The password of the email account STRING No No store The store type that used to receive emails. Possible values are 'imap' and 'pop3'. imap STRING Yes No host The host name of the server (e.g., 'imap.gmail.com' is the host name for a gmail account with an IMAP store.). The default value 'imap.gmail.com' is only valid if the email account is a gmail account with IMAP enabled. If store type is 'imap', then the default value is 'imap.gmail.com'. If the store type is 'pop3', then thedefault value is 'pop3.gmail.com'. STRING Yes No port The port that is used to create the connection. '993', the default value is valid only if the store is 'imap' and ssl-enabled. INT Yes No folder The name of the folder to which the emails should be fetched. INBOX STRING Yes No search.term The option that includes conditions such as key-value pairs to search for emails. In a string search term, the key and the value should be separated by a semicolon (';'). Each key-value pair must be within inverted commas (' '). The string search term can define multiple comma-separated key-value pairs. This string search term currently supports only the 'subject', 'from', 'to', 'bcc', and 'cc' keys. e.g., if you enter 'subject:DAS, from:carbon, bcc:wso2', the search term creates a search term instance that filters emails that contain 'DAS' in the subject, 'carbon' in the 'from' address, and 'wso2' in one of the 'bcc' addresses. The string search term carries out sub string matching that is case-sensitive. If '@' in included in the value for any key other than the 'subject' key, it checks for an address that is equal to the value given. e.g., If you search for 'abc@', the string search terms looks for an address that contains 'abc' before the '@' symbol. None STRING Yes No polling.interval This defines the time interval in seconds at which th email source should poll the account to check for new mail arrivals.in seconds. 600 LONG Yes No action.after.processed The action to be performed by the email source for the processed mail. Possible values are as follows: 'FLAGGED': Sets the flag as 'flagged'. 'SEEN': Sets the flag as 'read'. 'ANSWERED': Sets the flag as 'answered'. 'DELETE': Deletes tha mail after the polling cycle. 'MOVE': Moves the mail to the folder specified in the 'folder.to.move' parameter. If the folder specified is 'pop3', then the only option available is 'DELETE'. NONE STRING Yes No folder.to.move The name of the folder to which the mail must be moved once it is processed. If the action after processing is 'MOVE', it is required to specify a value for this parameter. STRING No No content.type The content type of the email. It can be either 'text/plain' or 'text/html.' text/plain STRING Yes No ssl.enable If this is set to 'true', a secure port is used to establish the connection. The possible values are 'true' and 'false'. true BOOL Yes No System Parameters Name Description Default Value Possible Parameters mail.imap.partialfetch This determines whether the IMAP partial-fetch capability should be used. true true or false mail.imap.fetchsize The partial fetch size in bytes. 16K value in bytes mail.imap.peek If this is set to 'true', the IMAP PEEK option should be used when fetching body parts to avoid setting the 'SEEN' flag on messages. The default value is 'false'. This can be overridden on a per-message basis by the 'setPeek method' in 'IMAPMessage'. false true or false mail.imap.connectiontimeout The socket connection timeout value in milliseconds. This timeout is implemented by 'java.net.Socket'. infinity timeout Any Integer value mail.imap.timeout The socket read timeout value in milliseconds. This timeout is implemented by 'java.net.Socket'. infinity timeout Any Integer value mail.imap.writetimeout The socket write timeout value in milliseconds. This timeout is implemented by using a 'java.util.concurrent.ScheduledExecutorService' per connection that schedules a thread to close the socket if the timeout period elapses. Therefore, the overhead of using this timeout is one thread per connection. infinity timeout Any Integer value mail.imap.statuscachetimeout The timeout value in milliseconds for the cache of 'STATUS' command response. 1000ms Time out in miliseconds mail.imap.appendbuffersize The maximum size of a message to buffer in memory when appending to an IMAP folder. None Any Integer value mail.imap.connectionpoolsize The maximum number of available connections in the connection pool. 1 Any Integer value mail.imap.connectionpooltimeout The timeout value in milliseconds for connection pool connections. 45000ms Any Integer mail.imap.separatestoreconnection If this parameter is set to 'true', it indicates that a dedicated store connection needs to be used for store commands. true true or false mail.imap.auth.login.disable If this is set to 'true', it is not possible to use the non-standard 'AUTHENTICATE LOGIN' command instead of the plain 'LOGIN' command. false true or false mail.imap.auth.plain.disable If this is set to 'true', the 'AUTHENTICATE PLAIN' command cannot be used. false true or false mail.imap.auth.ntlm.disable If true, prevents use of the AUTHENTICATE NTLM command. false true or false mail.imap.proxyauth.user If the server supports the PROXYAUTH extension, this property specifies the name of the user to act as. Authentication to log in to the server is carried out using the administrator's credentials. After authentication, the IMAP provider issues the 'PROXYAUTH' command with the user name specified in this property. None Valid string value mail.imap.localaddress The local address (host name) to bind to when creating the IMAP socket. Defaults to the address picked by the Socket class. Valid string value mail.imap.localport The local port number to bind to when creating the IMAP socket. Defaults to the port number picked by the Socket class. Valid String value mail.imap.sasl.enable If this parameter is set to 'true', the system attempts to use the 'javax.security.sasl' package to choose an authentication mechanism for the login. false true or false mail.imap.sasl.mechanisms A list of SASL mechanism names that the system should to try to use. The names can be separated by spaces or commas. None Valid string value mail.imap.sasl.authorizationid The authorization ID to use in the SASL authentication. If this parameter is not set, the authentication ID (username) is used. Valid string value mail.imap.sasl.realm The realm to use with SASL authentication mechanisms that require a realm, such as 'DIGEST-MD5'. None Valid string value mail.imap.auth.ntlm.domain The NTLM authentication domain. None Valid string value The NTLM authentication domain. NTLM protocol-specific flags. None Valid integer value mail.imap.socketFactory If this parameter is set to a class that implements the 'javax.net.SocketFactory' interface, this class is used to create IMAP sockets. None Valid SocketFactory mail.imap.socketFactory.class If this parameter is set, it specifies the name of a class that implements the 'javax.net.SocketFactory' interface. This class is used to create IMAP sockets. None Valid string mail.imap.socketFactory.fallback If this parameter is set to 'true', failure to create a socket using the specified socket factory class results in the socket being created using the 'java.net.Socket' class. true true or false mail.imap.socketFactory.port This specifies the port to connect to when using the specified socket factory. If this parameter is not set, the default port is used. 143 Valid Integer mail.imap.ssl.checkserveridentity If this parameter is set to 'true', the system checks the server identity as specified by RFC 2595. false true or false mail.imap.ssl.trust If this parameter is set and a socket factory has not been specified, it enables the use of a 'MailSSLSocketFactory'. If this parameter is set to ' ', all the hosts are trusted. If this parameter specifies list of hosts separated by white spaces, only those hosts are trusted. If the parameter is not set to any of the values mentioned above, trust depends on the certificate presented by the server. Valid String mail.imap.ssl.socketFactory If this parameter is set to a class that extends the 'javax.net.ssl.SSLSocketFactory' class this class is used to create IMAP SSL sockets. None SSL Socket Factory mail.imap.ssl.socketFactory.class If this parameter is set, it specifies the name of a class that extends the 'javax.net.ssl.SSLSocketFactory' class. This class is used to create IMAP SSL sockets. None Valid String mail.imap.ssl.socketFactory.port This specifies the port to connect to when using the specified socket factory. the default port 993 is used. valid port number mail.imap.ssl.protocols This specifies the SSL protocols that are enabled for SSL connections. The property value is a whitespace-separated list of tokens acceptable to the 'javax.net.ssl.SSLSocket.setEnabledProtocols' method. None Valid string mail.imap.starttls.enable If this parameter is set to 'true', it is possible to use the 'STARTTLS' command (if supported by the server) to switch the connection to a TLS-protected connection before issuing any login commands. false true or false mail.imap.socks.host This specifies the host name of a 'SOCKS5' proxy server that is used to connect to the mail server. None Valid String mail.imap.socks.port This specifies the port number for the 'SOCKS5' proxy server. This is needed if the proxy server is not using the standard port number 1080. 1080 Valid String mail.imap.minidletime This property sets the delay in milliseconds. 10 milliseconds time in seconds (Integer) mail.imap.enableimapevents If this property is set to 'true', it enables special IMAP-specific events to be delivered to the 'ConnectionListener' of the store. The unsolicited responses received during the idle method of the store are sent as connection events with 'IMAPStore.RESPONSE' as the type. The event's message is the raw IMAP response string. false true or false mail.imap.folder.class The class name of a subclass of 'com.sun.mail.imap.IMAPFolder'. The subclass can be used to provide support for additional IMAP commands. The subclass must have public constructors of the form 'public MyIMAPFolder'(String fullName, char separator, IMAPStore store, Boolean isNamespace) and public 'MyIMAPFolder'(ListInfo li, IMAPStore store) None Valid String mail.pop3.connectiontimeout The socket connection timeout value in milliseconds. Infinite timeout Integer value mail.pop3.timeout The socket I/O timeout value in milliseconds. Infinite timeout Integer value mail.pop3.message.class The class name of a subclass of 'com.sun.mail.pop3.POP3Message'. None Valid String mail.pop3.localaddress The local address (host name) to bind to when creating the POP3 socket. Defaults to the address picked by the Socket class. Valid String mail.pop3.localport The local port number to bind to when creating the POP3 socket. Defaults to the port number picked by the Socket class. Valid port number mail.pop3.apop.enable If this parameter is set to 'true', use 'APOP' instead of 'USER/PASS' to log in to the 'POP3' server (if the 'POP3' server supports 'APOP'). APOP sends a digest of the password instead of clearing the text password. false true or false mail.pop3.socketFactory If this parameter is set to a class that implements the 'javax.net.SocketFactory' interface, this class is used to create 'POP3' sockets. None Socket Factory mail.pop3.socketFactory.class If this parameter is set, it specifies the name of a class that implements the 'javax.net.SocketFactory' interface. This class is used to create 'POP3' sockets. None Valid String mail.pop3.socketFactory.fallback If this parameter is set to 'true', failure to create a socket using the specified socket factory class results in the socket being created using the 'java.net.Socket' class. false true or false mail.pop3.socketFactory.port This specifies the port to connect to when using the specified socket factory. Default port Valid port number mail.pop3.ssl.checkserveridentity If this parameter is set to 'true', check the server identity as specified by RFC 2595. false true or false mail.pop3.ssl.trust If this parameter is set and a socket factory has not been specified, it is possible to use a 'MailSSLSocketFactory'. If this parameter is set to ' ', all the hosts are trusted. If the parameter is set to a whitespace-separated list of hosts, only those hosts are trusted. If the parameter is not set to any of the values mentioned above, trust depends on the certificate presented by the server. Valid String mail.pop3.ssl.socketFactory If this parameter is set to a class that extends the 'javax.net.ssl.SSLSocketFactory' class, this class is used to create 'POP3' SSL sockets. None SSL Socket Factory mail.pop3.ssl.checkserveridentity If this parameter is set to 'true', the system checks the server identity as specified by 'RFC 2595'. false true or false mail.pop3.ssl.trust If this parameter is set and a socket factory has not been specified, it is possible to use a 'MailSSLSocketFactory'. If this parameter is set to '*', all the hosts are trusted. If the parameter is set to a whitespace-separated list of hosts, only those hosts are trusted. Trust depends on the certificate presented by the server. Valid String mail.pop3.ssl.socketFactory If this parameter is set to a class that extends the 'javax.net.ssl.SSLSocketFactory' class, this class is used to create 'POP3 SSL' sockets. None SSL Socket Factory mail.pop3.ssl.socketFactory.class If this parameter is set, it specifies the name of a class that extends the 'javax.net.ssl.SSLSocketFactory' class. This class is used to create 'POP3 SSL' sockets. None Valid String mail.pop3.ssl.socketFactory.p This parameter pecifies the port to connect to when using the specified socket factory. 995 Valid Integer mail.pop3.ssl.protocols This parameter specifies the SSL protocols that are enabled for SSL connections. The property value is a whitespace-separated list of tokens acceptable to the 'javax.net.ssl.SSLSocket.setEnabledProtocols' method. None Valid String mail.pop3.starttls.enable If this parameter is set to 'true', it is possible to use the 'STLS' command (if supported by the server) to switch the connection to a TLS-protected connection before issuing any login commands. false true or false mail.pop3.starttls.required If this parameter is set to 'true', it is required to use the 'STLS' command. The connect method fails if the server does not support the 'STLS' command or if the command fails. false true or false mail.pop3.socks.host This parameter specifies the host name of a 'SOCKS5' proxy server that can be used to connect to the mail server. None Valid String mail.pop3.socks.port This parameter specifies the port number for the 'SOCKS5' proxy server. None Valid String mail.pop3.disabletop If this parameter is set to 'true', the 'POP3 TOP' command is not used to fetch message headers. false true or false mail.pop3.forgettopheaders If this parameter is set to 'true', the headers that might have been retrieved using the 'POP3 TOP' command is forgotten and replaced by the headers retrieved when the 'POP3 RETR' command is executed. false true or false mail.pop3.filecache.enable If this parameter is set to 'true', the 'POP3' provider caches message data in a temporary file instead of caching them in memory. Messages are only added to the cache when accessing the message content. Message headers are always cached in memory (on demand). The file cache is removed when the folder is closed or the JVM terminates. false true or false mail.pop3.filecache.dir If the file cache is enabled, this property is used to override the default directory used by the JDK for temporary files. None Valid String mail.pop3.cachewriteto This parameter controls the behavior of the 'writeTo' method on a 'POP3' message object. If the parameter is set to 'true', the message content has not been cached yet, and the 'ignoreList' is null, the message is cached before being written. If not, the message is streamed directly to the output stream without being cached. false true or false mail.pop3.keepmessagecontent If this property is set to 'true', a hard reference to the cached content is retained, preventing the memory from being reused until the folder is closed, or until the cached content is explicitly invalidated (using the 'invalidate' method). false true or false Examples EXAMPLE 1 @source(type='email', @map(type='xml'), username='receiver.account', password='account.password',)define stream inputStream (name string, age int, country string); This example illustrates how to receive events in 'xml' format via the email source. In this example, only the required parameters are defined in the stream definition. The default values are taken for the other parameters. The search term is not defined, and therefore, all the new messages in the inbox folder are polled and taken. EXAMPLE 2 @source(type='email', @map(type='xml'), username='receiver.account', password='account.password',store = 'imap',host = 'imap.gmail.com',port = '993',searchTerm = 'subject:Stream Processor, from: from.account@ , cc: cc.account',polling.interval='500',action.after.processed='DELETE',content.type='text/html,)define stream inputStream (name string, age int, country string); This example illustrates how to receive events in 'xml' format via the email source. The email source polls the mail account every 500 seconds to check whether any new mails have arrived. It processes new mails only if they satisfy the conditions specified for the email search term (the value for 'from' of the email message should be 'from.account@. host name ', and the message should contain 'cc.account' in the cc receipient list and the word 'Stream Processor' in the mail subject). in this example, the action after processing is 'DELETE'. Therefore,after processing the event, corresponding mail is deleted from the mail folder. file (Source) File Source provides the functionality for user to feed data to siddhi from files. Both text and binary files are supported by file source. Origin: siddhi-io-file:2.0.3 Syntax @source(type=\"file\", dir.uri=\" STRING \", file.uri=\" STRING \", mode=\" STRING \", tailing=\" BOOL \", action.after.process=\" STRING \", action.after.failure=\" STRING \", move.after.process=\" STRING \", move.after.failure=\" STRING \", begin.regex=\" STRING \", end.regex=\" STRING \", file.polling.interval=\" STRING \", dir.polling.interval=\" STRING \", timeout=\" STRING \", file.read.wait.timeout=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic dir.uri Used to specify a directory to be processed. All the files inside this directory will be processed. Only one of 'dir.uri' and 'file.uri' should be provided. This uri MUST have the respective protocol specified. STRING No No file.uri Used to specify a file to be processed. Only one of 'dir.uri' and 'file.uri' should be provided. This uri MUST have the respective protocol specified. STRING No No mode This parameter is used to specify how files in given directory should.Possible values for this parameter are, 1. TEXT.FULL : to read a text file completely at once. 2. BINARY.FULL : to read a binary file completely at once. 3. LINE : to read a text file line by line. 4. REGEX : to read a text file and extract data using a regex. line STRING Yes No tailing This can either have value true or false. By default it will be true. This attribute allows user to specify whether the file should be tailed or not. If tailing is enabled, the first file of the directory will be tailed. Also tailing should not be enabled in 'binary.full' or 'text.full' modes. true BOOL Yes No action.after.process This parameter is used to specify the action which should be carried out after processing a file in the given directory. It can be either DELETE or MOVE and default value will be 'DELETE'. If the action.after.process is MOVE, user must specify the location to move consumed files using 'move.after.process' parameter. delete STRING Yes No action.after.failure This parameter is used to specify the action which should be carried out if a failure occurred during the process. It can be either DELETE or MOVE and default value will be 'DELETE'. If the action.after.failure is MOVE, user must specify the location to move consumed files using 'move.after.failure' parameter. delete STRING Yes No move.after.process If action.after.process is MOVE, user must specify the location to move consumed files using 'move.after.process' parameter. This should be the absolute path of the file that going to be created after moving is done. This uri MUST have the respective protocol specified. STRING No No move.after.failure If action.after.failure is MOVE, user must specify the location to move consumed files using 'move.after.failure' parameter. This should be the absolute path of the file that going to be created after moving is done. This uri MUST have the respective protocol specified. STRING No No begin.regex This will define the regex to be matched at the beginning of the retrieved content. None STRING Yes No end.regex This will define the regex to be matched at the end of the retrieved content. None STRING Yes No file.polling.interval This parameter is used to specify the time period (in milliseconds) of a polling cycle for a file. 1000 STRING Yes No dir.polling.interval This parameter is used to specify the time period (in milliseconds) of a polling cycle for a directory. 1000 STRING Yes No timeout This parameter is used to specify the maximum time period (in milliseconds) for waiting until a file is processed. 5000 STRING Yes No file.read.wait.timeout This parameter is used to specify the maximum time period (in milliseconds) till it waits before retrying to read the full file content. 1000 STRING Yes No Examples EXAMPLE 1 @source(type='file', mode='text.full', tailing='false' dir.uri='file://abc/xyz', action.after.process='delete', @map(type='json')) define stream FooStream (symbol string, price float, volume long); Under above configuration, all the files in directory will be picked and read one by one. In this case, it's assumed that all the files contains json valid json strings with keys 'symbol','price' and 'volume'. Once a file is read, its content will be converted to an event using siddhi-map-json extension and then, that event will be received to the FooStream. Finally, after reading is finished, the file will be deleted. EXAMPLE 2 @source(type='file', mode='files.repo.line', tailing='true', dir.uri='file://abc/xyz', @map(type='json')) define stream FooStream (symbol string, price float, volume long); Under above configuration, the first file in directory '/abc/xyz' will be picked and read line by line. In this case, it is assumed that the file contains lines json strings. For each line, line content will be converted to an event using siddhi-map-json extension and then, that event will be received to the FooStream. Once file content is completely read, it will keep checking whether a new entry is added to the file or not. If such entry is added, it will be immediately picked up and processed. grpc (Source) This extension starts a grpc server during initialization time. The server listens to requests from grpc stubs. This source has a default mode of operation and custom user defined grpc service mode. By default this uses EventService. Please find the proto definition here . In the default mode this source will use EventService consume method. If we want to use our custom gRPC services, we have to pack auto-generated gRPC service classes and protobuf classes into a jar file and add it into the project classpath (or to the jars folder in the siddhi-tooling folder if we use it with siddhi-tooling ). Please find the custom protobuf definition that uses in examples here . This method will receive requests and injects them into stream through a mapper. Origin: siddhi-io-grpc:1.0.5 Syntax @source(type=\"grpc\", receiver.url=\" STRING \", max.inbound.message.size=\" INT \", max.inbound.metadata.size=\" INT \", server.shutdown.waiting.time=\" LONG \", truststore.file=\" STRING \", truststore.password=\" STRING \", truststore.algorithm=\" STRING \", tls.store.type=\" STRING \", keystore.file=\" STRING \", keystore.password=\" STRING \", keystore.algorithm=\" STRING \", enable.ssl=\" BOOL \", threadpool.size=\" INT \", threadpool.buffer.size=\" INT \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic receiver.url The url which can be used by a client to access the grpc server in this extension. This url should consist the host hostPort, port, fully qualified service name, method name in the following format. grpc://0.0.0.0:9763/ serviceName / methodName For example: grpc://0.0.0.0:9763/org.wso2.grpc.EventService/consume STRING No No max.inbound.message.size Sets the maximum message size in bytes allowed to be received on the server. 4194304 INT Yes No max.inbound.metadata.size Sets the maximum size of metadata in bytes allowed to be received. 8192 INT Yes No server.shutdown.waiting.time The time in seconds to wait for the server to shutdown, giving up if the timeout is reached. 5 LONG Yes No truststore.file the file path of truststore. If this is provided then server authentication is enabled - STRING Yes No truststore.password the password of truststore. If this is provided then the integrity of the keystore is checked - STRING Yes No truststore.algorithm the encryption algorithm to be used for server authentication - STRING Yes No tls.store.type TLS store type - STRING Yes No keystore.file the file path of keystore. If this is provided then client authentication is enabled - STRING Yes No keystore.password the password of keystore - STRING Yes No keystore.algorithm the encryption algorithm to be used for client authentication - STRING Yes No enable.ssl to enable ssl. If set to true and truststore.file is not given then it will be set to default carbon jks by default FALSE BOOL Yes No threadpool.size Sets the maximum size of threadpool dedicated to serve requests at the gRPC server 100 INT Yes No threadpool.buffer.size Sets the maximum size of threadpool buffer server 100 INT Yes No System Parameters Name Description Default Value Possible Parameters keyStoreFile Path of the key store file src/main/resources/security/wso2carbon.jks valid path for a key store file keyStorePassword This is the password used with key store file wso2carbon valid password for the key store file keyStoreAlgorithm The encryption algorithm to be used for client authentication SunX509 - trustStoreFile This is the trust store file with the path src/main/resources/security/client-truststore.jks - trustStorePassword This is the password used with trust store file wso2carbon valid password for the trust store file trustStoreAlgorithm the encryption algorithm to be used for server authentication SunX509 - Examples EXAMPLE 1 @source(type='grpc', receiver.url='grpc://localhost:8888/org.wso2.grpc.EventService/consume', @map(type='json')) define stream BarStream (message String); Here the port is given as 8888. So a grpc server will be started on port 8888 and the server will expose EventService. This is the default service packed with the source. In EventService the consume method is EXAMPLE 2 @source(type='grpc', receiver.url='grpc://localhost:8888/org.wso2.grpc.EventService/consume', @map(type='json', @attributes(name='trp:name', age='trp:age', message='message'))) define stream BarStream (message String, name String, age int); Here we are getting headers sent with the request as transport properties and injecting them into the stream. With each request a header will be sent in MetaData in the following format: 'Name:John', 'Age:23' EXAMPLE 3 @source(type='grpc', receiver.url='grpc://localhost:8888/org.wso2.grpc.MyService/send', @map(type='protobuf')) define stream BarStream (stringValue string, intValue int,longValue long,booleanValue bool,floatValue float,doubleValue double); Here the port is given as 8888. So a grpc server will be started on port 8888 and sever will keep listening to the 'send' RPC method in the 'MyService' service. EXAMPLE 4 @source(type='grpc', receiver.url='grpc://localhost:8888/org.wso2.grpc.MyService/send', @map(type='protobuf', @attributes(a = 'stringValue', b = 'intValue', c = 'longValue',d = 'booleanValue', e ='floatValue', f ='doubleValue'))) define stream BarStream (a string ,c long,b int, d bool,e float,f double); Here the port is given as 8888. So a grpc server will be started on port 8888 and sever will keep listening to the 'send' method in the 'MyService' service. Since we provide mapping in the stream we can use any names for stream attributes, but we have to map those names with correct protobuf message attributes' names. If we want to send metadata, we should map the attributes. EXAMPLE 5 @source(type='grpc', receiver.url='grpc://localhost:8888/org.wso2.grpc.StreamService/clientStream', @map(type='protobuf')) define stream BarStream (stringValue string, intValue int,longValue long,booleanValue bool,floatValue float,doubleValue double); Here we receive a stream of requests to the grpc source. Whenever we want to use streaming with grpc source, we have to define the RPC method as client streaming method (look at the sample proto file provided in the resource folder here ), when we define a stream method siddhi will identify it as a stream RPC method and ready to accept stream of request from the client. grpc-call-response (Source) This grpc source receives responses received from gRPC server for requests sent from a grpc-call sink. The source will receive responses for sink with the same sink.id. For example if you have a gRPC sink with sink.id 15 then we need to set the sink.id as 15 in the source to receives responses. Sinks and sources have 1:1 mapping Origin: siddhi-io-grpc:1.0.5 Syntax @source(type=\"grpc-call-response\", sink.id=\" INT \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic sink.id a unique ID that should be set for each grpc-call source. There is a 1:1 mapping between grpc-call sinks and grpc-call-response sources. Each sink has one particular source listening to the responses to requests published from that sink. So the same sink.id should be given when writing the sink also. INT No No Examples EXAMPLE 1 @source(type='grpc-call-response', sink.id= '1') define stream BarStream (message String);@sink(type='grpc-call', publisher.url = 'grpc://194.23.98.100:8080/EventService/process', sink.id= '1', @map(type='json')) define stream FooStream (message String); Here we are listening to responses for requests sent from the sink with sink.id 1 will be received here. The results will be injected into BarStream grpc-service (Source) This extension implements a grpc server for receiving and responding to requests. During initialization time a grpc server is started on the user specified port exposing the required service as given in the url. This source also has a default mode and a user defined grpc service mode. By default this uses EventService. Please find the proto definition here In the default mode this will use the EventService process method. If we want to use our custom gRPC services, we have to pack auto-generated gRPC service classes and protobuf classes into a jar file and add it into the project classpath (or to the jars folder in the siddhi-tooling folder if we use it with siddhi-tooling ). Please find the custom protobuf definition that uses in examples here . This accepts grpc message class Event as defined in the EventService proto. This uses GrpcServiceResponse sink to send reponses back in the same Event message format. Origin: siddhi-io-grpc:1.0.5 Syntax @source(type=\"grpc-service\", receiver.url=\" STRING \", max.inbound.message.size=\" INT \", max.inbound.metadata.size=\" INT \", service.timeout=\" INT \", server.shutdown.waiting.time=\" LONG \", truststore.file=\" STRING \", truststore.password=\" STRING \", truststore.algorithm=\" STRING \", tls.store.type=\" STRING \", keystore.file=\" STRING \", keystore.password=\" STRING \", keystore.algorithm=\" STRING \", enable.ssl=\" BOOL \", threadpool.size=\" INT \", threadpool.buffer.size=\" INT \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic receiver.url The url which can be used by a client to access the grpc server in this extension. This url should consist the host hostPort, port, fully qualified service name, method name in the following format. grpc://0.0.0.0:9763/ serviceName / methodName For example: grpc://0.0.0.0:9763/org.wso2.grpc.EventService/consume STRING No No max.inbound.message.size Sets the maximum message size in bytes allowed to be received on the server. 4194304 INT Yes No max.inbound.metadata.size Sets the maximum size of metadata in bytes allowed to be received. 8192 INT Yes No service.timeout The period of time in milliseconds to wait for siddhi to respond to a request received. After this time period of receiving a request it will be closed with an error message. 10000 INT Yes No server.shutdown.waiting.time The time in seconds to wait for the server to shutdown, giving up if the timeout is reached. 5 LONG Yes No truststore.file the file path of truststore. If this is provided then server authentication is enabled - STRING Yes No truststore.password the password of truststore. If this is provided then the integrity of the keystore is checked - STRING Yes No truststore.algorithm the encryption algorithm to be used for server authentication - STRING Yes No tls.store.type TLS store type - STRING Yes No keystore.file the file path of keystore. If this is provided then client authentication is enabled - STRING Yes No keystore.password the password of keystore - STRING Yes No keystore.algorithm the encryption algorithm to be used for client authentication - STRING Yes No enable.ssl to enable ssl. If set to true and truststore.file is not given then it will be set to default carbon jks by default FALSE BOOL Yes No threadpool.size Sets the maximum size of threadpool dedicated to serve requests at the gRPC server 100 INT Yes No threadpool.buffer.size Sets the maximum size of threadpool buffer server 100 INT Yes No System Parameters Name Description Default Value Possible Parameters keyStoreFile This is the key store file with the path src/main/resources/security/wso2carbon.jks valid path for a key store file keyStorePassword This is the password used with key store file wso2carbon valid password for the key store file keyStoreAlgorithm The encryption algorithm to be used for client authentication SunX509 - trustStoreFile This is the trust store file with the path src/main/resources/security/client-truststore.jks - trustStorePassword This is the password used with trust store file wso2carbon valid password for the trust store file trustStoreAlgorithm the encryption algorithm to be used for server authentication SunX509 - Examples EXAMPLE 1 @source(type='grpc-service', receiver.url='grpc://localhost:8888/org.wso2.grpc.EventService/process', source.id='1', @map(type='json', @attributes(messageId='trp:messageId', message='message'))) define stream FooStream (messageId String, message String); Here a grpc server will be started at port 8888. The process method of EventService will be exposed for clients. source.id is set as 1. So a grpc-service-response sink with source.id = 1 will send responses back for requests received to this source. Note that it is required to specify the transport property messageId since we need to correlate the request message with the response. EXAMPLE 2 @sink(type='grpc-service-response', source.id='1', @map(type='json')) define stream BarStream (messageId String, message String); @source(type='grpc-service', receiver.url='grpc://134.23.43.35:8080/org.wso2.grpc.EventService/process', source.id='1', @map(type='json', @attributes(messageId='trp:messageId', message='message'))) define stream FooStream (messageId String, message String); from FooStream select * insert into BarStream; The grpc requests are received through the grpc-service sink. Each received event is sent back through grpc-service-source. This is just a passthrough through Siddhi as we are selecting everything from FooStream and inserting into BarStream. EXAMPLE 3 @source(type='grpc-service', source.id='1' receiver.url='grpc://locanhost:8888/org.wso2.grpc.EventService/consume', @map(type='json', @attributes(name='trp:name', age='trp:age', message='message'))) define stream BarStream (message String, name String, age int); Here we are getting headers sent with the request as transport properties and injecting them into the stream. With each request a header will be sent in MetaData in the following format: 'Name:John', 'Age:23' EXAMPLE 4 @sink(type='grpc-service-response', source.id='1', message.id='{{messageId}}', @map(type='protobuf', @payload(stringValue='a',intValue='b',longValue='c',booleanValue='d',floatValue = 'e', doubleValue ='f'))) define stream BarStream (a string,messageId string, b int,c long,d bool,e float,f double); @source(type='grpc-service', receiver.url='grpc://134.23.43.35:8888/org.wso2.grpc.test.MyService/process', source.id='1', @map(type='protobuf', @attributes(messageId='trp:message.id', a = 'stringValue', b = 'intValue', c = 'longValue',d = 'booleanValue', e = 'floatValue', f ='doubleValue'))) define stream FooStream (a string,messageId string, b int,c long,d bool,e float,f double); from FooStream select * insert into BarStream; Here a grpc server will be started at port 8888. The process method of the MyService will be exposed to the clients. 'source.id' is set as 1. So a grpc-service-response sink with source.id = 1 will send responses back for requests received to this source. Note that it is required to specify the transport property messageId since we need to correlate the request message with the response and also we should map stream attributes with correct protobuf message attributes even they define using the same name as protobuf message attributes. http (Source) HTTP source receives POST requests via HTTP and HTTPS protocols in format such as text , XML and JSON . It also supports basic authentication to ensure events are received from authorized users/systems. The request headers and properties can be accessed via transport properties in the format trp: header . Origin: siddhi-io-http:2.2.0 Syntax @source(type=\"http\", receiver.url=\" STRING \", basic.auth.enabled=\" STRING \", worker.count=\" INT \", socket.idle.timeout=\" INT \", ssl.verify.client=\" STRING \", ssl.protocol=\" STRING \", tls.store.type=\" STRING \", ssl.configurations=\" STRING \", request.size.validation.configurations=\" STRING \", header.validation.configurations=\" STRING \", server.bootstrap.configurations=\" STRING \", trace.log.enabled=\" BOOL \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic receiver.url The URL on which events should be received. To enable SSL use https protocol in the url. http://0.0.0.0:9763/ appNAme / streamName STRING Yes No basic.auth.enabled This only works in VM, Docker and Kubernetes. Where when enabled it authenticates each request using the Authorization:'Basic encodeBase64(username:Password)' header. false STRING Yes No worker.count The number of active worker threads to serve the incoming events. By default the value is set to 1 to ensure events are processed in the same order they arrived. By increasing this value, higher performance can be achieved in the expense of loosing event ordering. 1 INT Yes No socket.idle.timeout Idle timeout for HTTP connection in millis. 120000 INT Yes No ssl.verify.client The type of client certificate verification. Supported values are require , optional . - STRING Yes No ssl.protocol SSL/TLS protocol. TLS STRING Yes No tls.store.type TLS store type. JKS STRING Yes No ssl.configurations SSL/TSL configurations in format \"' key : value ',' key : value '\" . Some supported parameters: - SSL/TLS protocols: 'sslEnabledProtocols:TLSv1.1,TLSv1.2' - List of ciphers: 'ciphers:TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256' - Enable session creation: 'client.enable.session.creation:true' - Supported server names: 'server.suported.server.names:server' - Add HTTP SNIMatcher: 'server.supported.snimatchers:SNIMatcher' - STRING Yes No request.size.validation.configurations Configurations to validate the HTTP request size. Expected format \"' key : value ',' key : value '\" . Some supported configurations : - Enable request size validation: 'request.size.validation:true' If request size is validated - Maximum request size: 'request.size.validation.maximum.value:2048' - Response status code when request size validation fails: 'request.size.validation.reject.status.code:401' - Response message when request size validation fails: 'request.size.validation.reject.message:Message is bigger than the valid size' - Response Content-Type when request size validation fails: 'request.size.validation.reject.message.content.type:plain/text' - STRING Yes No header.validation.configurations Configurations to validate HTTP headers. Expected format \"' key : value ',' key : value '\" . Some supported configurations : - Enable header size validation: 'header.size.validation:true' If header size is validated - Maximum length of initial line: 'header.validation.maximum.request.line:4096' - Maximum length of all headers: 'header.validation.maximum.size:8192' - Maximum length of the content or each chunk: 'header.validation.maximum.chunk.size:8192' - Response status code when header validation fails: 'header.validation.reject.status.code:401' - Response message when header validation fails: 'header.validation.reject.message:Message header is bigger than the valid size' - Response Content-Type when header validation fails: 'header.validation.reject.message.content.type:plain/text' - STRING Yes No server.bootstrap.configurations Server bootstrap configurations in format \"' key : value ',' key : value '\" . Some supported configurations : - Server connect timeout in millis: 'server.bootstrap.connect.timeout:15000' - Server socket timeout in seconds: 'server.bootstrap.socket.timeout:15' - Enable TCP no delay: 'server.bootstrap.nodelay:true' - Enable server keep alive: 'server.bootstrap.keepalive:true' - Send buffer size: 'server.bootstrap.sendbuffersize:1048576' - Receive buffer size: 'server.bootstrap.recievebuffersize:1048576' - Number of connections queued: 'server.bootstrap.socket.backlog:100' - STRING Yes No trace.log.enabled Enable trace log for traffic monitoring. false BOOL Yes No System Parameters Name Description Default Value Possible Parameters serverBootstrapBossGroupSize Number of boss threads to accept incoming connections. Number of available processors Any positive integer serverBootstrapWorkerGroupSize Number of worker threads to accept the connections from boss threads and perform non-blocking read and write from one or more channels. (Number of available processors) * 2 Any positive integer serverBootstrapClientGroupSize Number of client threads to perform non-blocking read and write to one or more channels. (Number of available processors) * 2 Any positive integer defaultHost The default host of the transport. 0.0.0.0 Any valid host defaultScheme The default protocol. http http https defaultHttpPort The default HTTP port when default scheme is http . 8280 Any valid port defaultHttpsPort The default HTTPS port when default scheme is https . 8243 Any valid port keyStoreLocation The default keystore file path. ${carbon.home}/resources/security/wso2carbon.jks Path to .jks file keyStorePassword The default keystore password. wso2carbon Keystore password as string Examples EXAMPLE 1 @app.name('StockProcessor') @source(type='http', @map(type = 'json')) define stream StockStream (symbol string, price float, volume long); Above HTTP source listeners on url http://0.0.0.0:9763/StockProcessor/StockStream for JSON messages on the format: { \"event\": { \"symbol\": \"FB\", \"price\": 24.5, \"volume\": 5000 } } It maps the incoming messages and sends them to StockStream for processing. EXAMPLE 2 @source(type='http', receiver.url='http://localhost:5005/stocks', @map(type = 'xml')) define stream StockStream (symbol string, price float, volume long); Above HTTP source listeners on url http://localhost:5005/stocks for JSON messages on the format: events event symbol Fb /symbol price 55.6 /price volume 100 /volume /event /events It maps the incoming messages and sends them to StockStream for processing. http-call-response (Source) The http-call-response source receives the responses for the calls made by its corresponding http-call sink, and maps them from formats such as text , XML and JSON . To handle messages with different http status codes having different formats, multiple http-call-response sources are allowed to associate with a single http-call sink. It allows accessing the attributes of the event that initiated the call, and the response headers and properties via transport properties in the format trp: attribute name and trp: header/property respectively. Origin: siddhi-io-http:2.2.0 Syntax @source(type=\"http-call-response\", sink.id=\" STRING \", http.status.code=\" STRING \", allow.streaming.responses=\" BOOL \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic sink.id Identifier to correlate the http-call-response source with its corresponding http-call sink that published the messages. STRING No No http.status.code The matching http responses status code regex, that is used to filter the the messages which will be processed by the source.Eg: http.status.code = '200' , http.status.code = '4\\d+' 200 STRING Yes No allow.streaming.responses Enable consuming responses on a streaming manner. false BOOL Yes No Examples EXAMPLE 1 @sink(type='http-call', method='POST', publisher.url='http://localhost:8005/registry/employee', sink.id='employee-info', @map(type='json')) define stream EmployeeRequestStream (name string, id int); @source(type='http-call-response', sink.id='employee-info', http.status.code='2\\\\d+', @map(type='json', @attributes(name='trp:name', id='trp:id', location='$.town', age='$.age'))) define stream EmployeeResponseStream(name string, id int, location string, age int); @source(type='http-call-response', sink.id='employee-info', http.status.code='4\\\\d+', @map(type='text', regex.A='((.|\\n)*)', @attributes(error='A[1]'))) define stream EmployeeErrorStream(error string); When events arrive in EmployeeRequestStream , http-call sink makes calls to endpoint on url http://localhost:8005/registry/employee with POST method and Content-Type application/json . If the arriving event has attributes name : John and id : 1423 it will send a message with default JSON mapping as follows: { \"event\": { \"name\": \"John\", \"id\": 1423 } } When the endpoint responds with status code in the range of 200 the message will be received by the http-call-response source associated with the EmployeeResponseStream stream, because it is correlated with the sink by the same sink.id employee-info and as that expects messages with http.status.code in regex format 2\\d+ . If the response message is in the format { \"town\": \"NY\", \"age\": 24 } the source maps the location and age attributes by executing JSON path on the message and maps the name and id attributes by extracting them from the request event via as transport properties. If the response status code is in the range of 400 then the message will be received by the http-call-response source associated with the EmployeeErrorStream stream, because it is correlated with the sink by the same sink.id employee-info and it expects messages with http.status.code in regex format 4\\d+ , and maps the error response to the error attribute of the event. http-request (Source) Deprecated (Use http-service source instead). The http-request source receives POST requests via HTTP and HTTPS protocols in format such as text , XML and JSON and sends responses via its corresponding http-response sink correlated through a unique source.id . For request and response correlation, it generates a messageId upon each incoming request and expose it via transport properties in the format trp:messageId to correlate them with the responses at the http-response sink. The request headers and properties can be accessed via transport properties in the format trp: header . It also supports basic authentication to ensure events are received from authorized users/systems. Origin: siddhi-io-http:2.2.0 Syntax @source(type=\"http-request\", receiver.url=\" STRING \", source.id=\" STRING \", connection.timeout=\" INT \", basic.auth.enabled=\" STRING \", worker.count=\" INT \", socket.idle.timeout=\" INT \", ssl.verify.client=\" STRING \", ssl.protocol=\" STRING \", tls.store.type=\" STRING \", ssl.configurations=\" STRING \", request.size.validation.configurations=\" STRING \", header.validation.configurations=\" STRING \", server.bootstrap.configurations=\" STRING \", trace.log.enabled=\" BOOL \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic receiver.url The URL on which events should be received. To enable SSL use https protocol in the url. http://0.0.0.0:9763/ appNAme / streamName STRING Yes No source.id Identifier to correlate the http-request source to its corresponding http-response sinks to send responses. STRING No No connection.timeout Connection timeout in millis. The system will send a timeout, if a corresponding response is not sent by an associated http-response sink within the given time. 120000 INT Yes No basic.auth.enabled This only works in VM, Docker and Kubernetes. Where when enabled it authenticates each request using the Authorization:'Basic encodeBase64(username:Password)' header. false STRING Yes No worker.count The number of active worker threads to serve the incoming events. By default the value is set to 1 to ensure events are processed in the same order they arrived. By increasing this value, higher performance can be achieved in the expense of loosing event ordering. 1 INT Yes No socket.idle.timeout Idle timeout for HTTP connection in millis. 120000 INT Yes No ssl.verify.client The type of client certificate verification. Supported values are require , optional . - STRING Yes No ssl.protocol SSL/TLS protocol. TLS STRING Yes No tls.store.type TLS store type. JKS STRING Yes No ssl.configurations SSL/TSL configurations in format \"' key : value ',' key : value '\" . Some supported parameters: - SSL/TLS protocols: 'sslEnabledProtocols:TLSv1.1,TLSv1.2' - List of ciphers: 'ciphers:TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256' - Enable session creation: 'client.enable.session.creation:true' - Supported server names: 'server.suported.server.names:server' - Add HTTP SNIMatcher: 'server.supported.snimatchers:SNIMatcher' - STRING Yes No request.size.validation.configurations Configurations to validate the HTTP request size. Expected format \"' key : value ',' key : value '\" . Some supported configurations : - Enable request size validation: 'request.size.validation:true' If request size is validated - Maximum request size: 'request.size.validation.maximum.value:2048' - Response status code when request size validation fails: 'request.size.validation.reject.status.code:401' - Response message when request size validation fails: 'request.size.validation.reject.message:Message is bigger than the valid size' - Response Content-Type when request size validation fails: 'request.size.validation.reject.message.content.type:plain/text' - STRING Yes No header.validation.configurations Configurations to validate HTTP headers. Expected format \"' key : value ',' key : value '\" . Some supported configurations : - Enable header size validation: 'header.size.validation:true' If header size is validated - Maximum length of initial line: 'header.validation.maximum.request.line:4096' - Maximum length of all headers: 'header.validation.maximum.size:8192' - Maximum length of the content or each chunk: 'header.validation.maximum.chunk.size:8192' - Response status code when header validation fails: 'header.validation.reject.status.code:401' - Response message when header validation fails: 'header.validation.reject.message:Message header is bigger than the valid size' - Response Content-Type when header validation fails: 'header.validation.reject.message.content.type:plain/text' - STRING Yes No server.bootstrap.configurations Server bootstrap configurations in format \"' key : value ',' key : value '\" . Some supported configurations : - Server connect timeout in millis: 'server.bootstrap.connect.timeout:15000' - Server socket timeout in seconds: 'server.bootstrap.socket.timeout:15' - Enable TCP no delay: 'server.bootstrap.nodelay:true' - Enable server keep alive: 'server.bootstrap.keepalive:true' - Send buffer size: 'server.bootstrap.sendbuffersize:1048576' - Receive buffer size: 'server.bootstrap.recievebuffersize:1048576' - Number of connections queued: 'server.bootstrap.socket.backlog:100' - STRING Yes No trace.log.enabled Enable trace log for traffic monitoring. false BOOL Yes No System Parameters Name Description Default Value Possible Parameters serverBootstrapBossGroupSize Number of boss threads to accept incoming connections. Number of available processors Any positive integer serverBootstrapWorkerGroupSize Number of worker threads to accept the connections from boss threads and perform non-blocking read and write from one or more channels. (Number of available processors) * 2 Any positive integer serverBootstrapClientGroupSize Number of client threads to perform non-blocking read and write to one or more channels. (Number of available processors) * 2 Any positive integer defaultHost The default host of the transport. 0.0.0.0 Any valid host defaultScheme The default protocol. http http https defaultHttpPort The default HTTP port when default scheme is http . 8280 Any valid port defaultHttpsPort The default HTTPS port when default scheme is https . 8243 Any valid port keyStoreLocation The default keystore file path. ${carbon.home}/resources/security/wso2carbon.jks Path to .jks file keyStorePassword The default keystore password. wso2carbon Keystore password as string Examples EXAMPLE 1 @source(type='http-request', receiver.url='http://localhost:5005/add', source.id='adder', @map(type='json, @attributes(messageId='trp:messageId', value1='$.event.value1', value2='$.event.value2'))) define stream AddStream (messageId string, value1 long, value2 long); @sink(type='http-response', source.id='adder', message.id='{{messageId}}', @map(type = 'json')) define stream ResultStream (messageId string, results long); @info(name = 'query1') from AddStream select messageId, value1 + value2 as results insert into ResultStream; Above sample listens events on http://localhost:5005/stocks url for JSON messages on the format: { \"event\": { \"value1\": 3, \"value2\": 4 } } Map the vents into AddStream, process the events through query query1 , and sends the results produced on ResultStream via http-response sink on the message format: { \"event\": { \"results\": 7 } } http-response (Source) Deprecated (Use http-call-response source instead). The http-response source receives the responses for the calls made by its corresponding http-request sink, and maps them from formats such as text , XML and JSON . To handle messages with different http status codes having different formats, multiple http-response sources are allowed to associate with a single http-request sink. It allows accessing the attributes of the event that initiated the call, and the response headers and properties via transport properties in the format trp: attribute name and trp: header/property respectively. Origin: siddhi-io-http:2.2.0 Syntax @source(type=\"http-response\", sink.id=\" STRING \", http.status.code=\" STRING \", allow.streaming.responses=\" BOOL \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic sink.id Identifier to correlate the http-response source with its corresponding http-request sink that published the messages. STRING No No http.status.code The matching http responses status code regex, that is used to filter the the messages which will be processed by the source.Eg: http.status.code = '200' , http.status.code = '4\\d+' 200 STRING Yes No allow.streaming.responses Enable consuming responses on a streaming manner. false BOOL Yes No Examples EXAMPLE 1 @sink(type='http-request', method='POST', publisher.url='http://localhost:8005/registry/employee', sink.id='employee-info', @map(type='json')) define stream EmployeeRequestStream (name string, id int); @source(type='http-response', sink.id='employee-info', http.status.code='2\\\\d+', @map(type='json', @attributes(name='trp:name', id='trp:id', location='$.town', age='$.age'))) define stream EmployeeResponseStream(name string, id int, location string, age int); @source(type='http-response', sink.id='employee-info', http.status.code='4\\\\d+', @map(type='text', regex.A='((.|\\n)*)', @attributes(error='A[1]'))) define stream EmployeeErrorStream(error string); When events arrive in EmployeeRequestStream , http-request sink makes calls to endpoint on url http://localhost:8005/registry/employee with POST method and Content-Type application/json . If the arriving event has attributes name : John and id : 1423 it will send a message with default JSON mapping as follows: { \"event\": { \"name\": \"John\", \"id\": 1423 } } When the endpoint responds with status code in the range of 200 the message will be received by the http-response source associated with the EmployeeResponseStream stream, because it is correlated with the sink by the same sink.id employee-info and as that expects messages with http.status.code in regex format 2\\d+ . If the response message is in the format { \"town\": \"NY\", \"age\": 24 } the source maps the location and age attributes by executing JSON path on the message and maps the name and id attributes by extracting them from the request event via as transport properties. If the response status code is in the range of 400 then the message will be received by the http-response source associated with the EmployeeErrorStream stream, because it is correlated with the sink by the same sink.id employee-info and it expects messages with http.status.code in regex format 4\\d+ , and maps the error response to the error attribute of the event. http-service (Source) The http-service source receives POST requests via HTTP and HTTPS protocols in format such as text , XML and JSON and sends responses via its corresponding http-service-response sink correlated through a unique source.id . For request and response correlation, it generates a messageId upon each incoming request and expose it via transport properties in the format trp:messageId to correlate them with the responses at the http-service-response sink. The request headers and properties can be accessed via transport properties in the format trp: header . It also supports basic authentication to ensure events are received from authorized users/systems. Origin: siddhi-io-http:2.2.0 Syntax @source(type=\"http-service\", receiver.url=\" STRING \", source.id=\" STRING \", connection.timeout=\" INT \", basic.auth.enabled=\" STRING \", worker.count=\" INT \", socket.idle.timeout=\" INT \", ssl.verify.client=\" STRING \", ssl.protocol=\" STRING \", tls.store.type=\" STRING \", ssl.configurations=\" STRING \", request.size.validation.configurations=\" STRING \", header.validation.configurations=\" STRING \", server.bootstrap.configurations=\" STRING \", trace.log.enabled=\" BOOL \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic receiver.url The URL on which events should be received. To enable SSL use https protocol in the url. http://0.0.0.0:9763/ appNAme / streamName STRING Yes No source.id Identifier to correlate the http-service source to its corresponding http-service-response sinks to send responses. STRING No No connection.timeout Connection timeout in millis. The system will send a timeout, if a corresponding response is not sent by an associated http-service-response sink within the given time. 120000 INT Yes No basic.auth.enabled This only works in VM, Docker and Kubernetes. Where when enabled it authenticates each request using the Authorization:'Basic encodeBase64(username:Password)' header. false STRING Yes No worker.count The number of active worker threads to serve the incoming events. By default the value is set to 1 to ensure events are processed in the same order they arrived. By increasing this value, higher performance can be achieved in the expense of loosing event ordering. 1 INT Yes No socket.idle.timeout Idle timeout for HTTP connection in millis. 120000 INT Yes No ssl.verify.client The type of client certificate verification. Supported values are require , optional . - STRING Yes No ssl.protocol SSL/TLS protocol. TLS STRING Yes No tls.store.type TLS store type. JKS STRING Yes No ssl.configurations SSL/TSL configurations in format \"' key : value ',' key : value '\" . Some supported parameters: - SSL/TLS protocols: 'sslEnabledProtocols:TLSv1.1,TLSv1.2' - List of ciphers: 'ciphers:TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256' - Enable session creation: 'client.enable.session.creation:true' - Supported server names: 'server.suported.server.names:server' - Add HTTP SNIMatcher: 'server.supported.snimatchers:SNIMatcher' - STRING Yes No request.size.validation.configurations Configurations to validate the HTTP request size. Expected format \"' key : value ',' key : value '\" . Some supported configurations : - Enable request size validation: 'request.size.validation:true' If request size is validated - Maximum request size: 'request.size.validation.maximum.value:2048' - Response status code when request size validation fails: 'request.size.validation.reject.status.code:401' - Response message when request size validation fails: 'request.size.validation.reject.message:Message is bigger than the valid size' - Response Content-Type when request size validation fails: 'request.size.validation.reject.message.content.type:plain/text' - STRING Yes No header.validation.configurations Configurations to validate HTTP headers. Expected format \"' key : value ',' key : value '\" . Some supported configurations : - Enable header size validation: 'header.size.validation:true' If header size is validated - Maximum length of initial line: 'header.validation.maximum.request.line:4096' - Maximum length of all headers: 'header.validation.maximum.size:8192' - Maximum length of the content or each chunk: 'header.validation.maximum.chunk.size:8192' - Response status code when header validation fails: 'header.validation.reject.status.code:401' - Response message when header validation fails: 'header.validation.reject.message:Message header is bigger than the valid size' - Response Content-Type when header validation fails: 'header.validation.reject.message.content.type:plain/text' - STRING Yes No server.bootstrap.configurations Server bootstrap configurations in format \"' key : value ',' key : value '\" . Some supported configurations : - Server connect timeout in millis: 'server.bootstrap.connect.timeout:15000' - Server socket timeout in seconds: 'server.bootstrap.socket.timeout:15' - Enable TCP no delay: 'server.bootstrap.nodelay:true' - Enable server keep alive: 'server.bootstrap.keepalive:true' - Send buffer size: 'server.bootstrap.sendbuffersize:1048576' - Receive buffer size: 'server.bootstrap.recievebuffersize:1048576' - Number of connections queued: 'server.bootstrap.socket.backlog:100' - STRING Yes No trace.log.enabled Enable trace log for traffic monitoring. false BOOL Yes No System Parameters Name Description Default Value Possible Parameters serverBootstrapBossGroupSize Number of boss threads to accept incoming connections. Number of available processors Any positive integer serverBootstrapWorkerGroupSize Number of worker threads to accept the connections from boss threads and perform non-blocking read and write from one or more channels. (Number of available processors) * 2 Any positive integer serverBootstrapClientGroupSize Number of client threads to perform non-blocking read and write to one or more channels. (Number of available processors) * 2 Any positive integer defaultHost The default host of the transport. 0.0.0.0 Any valid host defaultScheme The default protocol. http http https defaultHttpPort The default HTTP port when default scheme is http . 8280 Any valid port defaultHttpsPort The default HTTPS port when default scheme is https . 8243 Any valid port keyStoreLocation The default keystore file path. ${carbon.home}/resources/security/wso2carbon.jks Path to .jks file keyStorePassword The default keystore password. wso2carbon Keystore password as string Examples EXAMPLE 1 @source(type='http-service', receiver.url='http://localhost:5005/add', source.id='adder', @map(type='json, @attributes(messageId='trp:messageId', value1='$.event.value1', value2='$.event.value2'))) define stream AddStream (messageId string, value1 long, value2 long); @sink(type='http-service-response', source.id='adder', message.id='{{messageId}}', @map(type = 'json')) define stream ResultStream (messageId string, results long); @info(name = 'query1') from AddStream select messageId, value1 + value2 as results insert into ResultStream; Above sample listens events on http://localhost:5005/stocks url for JSON messages on the format: { \"event\": { \"value1\": 3, \"value2\": 4 } } Map the vents into AddStream, process the events through query query1 , and sends the results produced on ResultStream via http-service-response sink on the message format: { \"event\": { \"results\": 7 } } inMemory (Source) In-memory source subscribes to a topic to consume events which are published on the same topic by In-memory sinks. This provides a way to connect multiple Siddhi Apps deployed under the same Siddhi Manager (JVM). Here both the publisher and subscriber should have the same event schema (stream definition) for successful data transfer. Origin: siddhi-core:5.1.8 Syntax @source(type=\"inMemory\", topic=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic topic Subscribes to the events sent on the given topic. STRING No No Examples EXAMPLE 1 @source(type='inMemory', topic='Stocks', @map(type='passThrough')) define stream StocksStream (symbol string, price float, volume long); Here the StocksStream uses inMemory source to consume events published on the topic Stocks by the inMemory sinks deployed in the same JVM. jms (Source) JMS Source allows users to subscribe to a JMS broker and receive JMS messages. It has the ability to receive Map messages and Text messages. Origin: siddhi-io-jms:2.0.3 Syntax @source(type=\"jms\", destination=\" STRING \", connection.factory.jndi.name=\" STRING \", factory.initial=\" STRING \", provider.url=\" STRING \", connection.factory.type=\" STRING \", worker.count=\" INT \", connection.username=\" STRING \", connection.password=\" STRING \", retry.interval=\" INT \", retry.count=\" INT \", use.receiver=\" BOOL \", subscription.durable=\" BOOL \", connection.factory.nature=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic destination Queue/Topic name which JMS Source should subscribe to STRING No No connection.factory.jndi.name JMS Connection Factory JNDI name. This value will be used for the JNDI lookup to find the JMS Connection Factory. QueueConnectionFactory STRING Yes No factory.initial Naming factory initial value STRING No No provider.url Java naming provider URL. Property for specifying configuration information for the service provider to use. The value of the property should contain a URL string (e.g. \"ldap://somehost:389\") STRING No No connection.factory.type Type of the connection connection factory. This can be either queue or topic. queue STRING Yes No worker.count Number of worker threads listening on the given queue/topic. 1 INT Yes No connection.username username for the broker. None STRING Yes No connection.password Password for the broker None STRING Yes No retry.interval Interval between each retry attempt in case of connection failure in milliseconds. 10000 INT Yes No retry.count Number of maximum reties that will be attempted in case of connection failure with broker. 5 INT Yes No use.receiver Implementation to be used when consuming JMS messages. By default transport will use MessageListener and tweaking this property will make make use of MessageReceiver false BOOL Yes No subscription.durable Property to enable durable subscription. false BOOL Yes No connection.factory.nature Connection factory nature for the broker. default STRING Yes No Examples EXAMPLE 1 @source(type='jms', @map(type='json'), factory.initial='org.apache.activemq.jndi.ActiveMQInitialContextFactory', provider.url='tcp://localhost:61616',destination='DAS_JMS_TEST', connection.factory.type='topic',connection.factory.jndi.name='TopicConnectionFactory') define stream inputStream (name string, age int, country string); This example shows how to connect to an ActiveMQ topic and receive messages. EXAMPLE 2 @source(type='jms', @map(type='json'), factory.initial='org.apache.activemq.jndi.ActiveMQInitialContextFactory', provider.url='tcp://localhost:61616',destination='DAS_JMS_TEST' ) define stream inputStream (name string, age int, country string); This example shows how to connect to an ActiveMQ queue and receive messages. Note that we are not providing properties like connection factory type kafka (Source) A Kafka source receives events to be processed by WSO2 SP from a topic with a partition for a Kafka cluster. The events received can be in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. Origin: siddhi-io-kafka:5.0.5 Syntax @source(type=\"kafka\", bootstrap.servers=\" STRING \", topic.list=\" STRING \", group.id=\" STRING \", threading.option=\" STRING \", partition.no.list=\" STRING \", seq.enabled=\" BOOL \", is.binary.message=\" BOOL \", topic.offsets.map=\" STRING \", enable.auto.commit=\" BOOL \", optional.configuration=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This specifies the list of Kafka servers to which the Kafka source must listen. This list can be provided as a set of comma-separated values. e.g., localhost:9092,localhost:9093 STRING No No topic.list This specifies the list of topics to which the source must listen. This list can be provided as a set of comma-separated values. e.g., topic_one,topic_two STRING No No group.id This is an ID to identify the Kafka source group. The group ID ensures that sources with the same topic and partition that are in the same group do not receive the same event. STRING No No threading.option This specifies whether the Kafka source is to be run on a single thread, or in multiple threads based on a condition. Possible values are as follows: single.thread : To run the Kafka source on a single thread. topic.wise : To use a separate thread per topic. partition.wise : To use a separate thread per partition. STRING No No partition.no.list The partition number list for the given topic. This is provided as a list of comma-separated values. e.g., 0,1,2, . 0 STRING Yes No seq.enabled If this parameter is set to true , the sequence of the events received via the source is taken into account. Therefore, each event should contain a sequence number as an attribute value to indicate the sequence. false BOOL Yes No is.binary.message In order to receive binary events via the Kafka source,it is required to setthis parameter to 'True'. false BOOL Yes No topic.offsets.map This parameter specifies reading offsets for each topic and partition. The value for this parameter is specified in the following format: topic = offset , topic = offset , When an offset is defined for a topic, the Kafka source skips reading the message with the number specified as the offset as well as all the messages sent previous to that message. If the offset is not defined for a specific topic it reads messages from the beginning. e.g., stocks=100,trades=50 reads from the 101th message of the stocks topic, and from the 51 st message of the trades topic. null STRING Yes No enable.auto.commit This parameter specifies whether to commit offsets automatically. By default, as the Siddhi Kafka source reads messages from Kafka, it will periodically(Default value is set to 1000ms. You can configure it with auto.commit.interval.ms property as an optional.configuration ) commit its current offset (defined as the offset of the next message to be read) for the partitions it is reading from back to Kafka. To guarantee at-least-once processing, we recommend you to enable Siddhi Periodic State Persistence when enable.auto.commit property is set to true . When you would like more control over exactly when offsets are committed, you can set enable.auto.commit to false and Siddhi will commit the offset once the records are successfully processed at the Source. When enable.auto.commit is set to false , manual committing would introduce a latency during consumption. true BOOL Yes No optional.configuration This parameter contains all the other possible configurations that the consumer is created with. e.g., ssl.keystore.type:JKS,batch.size:200 . null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic,kafka_topic2', group.id='test', threading.option='partition.wise', bootstrap.servers='localhost:9092', partition.no.list='0,1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This kafka source configuration listens to the kafka_topic and kafka_topic2 topics with 0 and 1 partitions. A thread is created for each topic and partition combination. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . EXAMPLE 2 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic', group.id='test', threading.option='single.thread', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka source configuration listens to the kafka_topic topic for the default partition because no partition.no.list is defined. Only one thread is created for the topic. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . kafkaMultiDC (Source) The Kafka Multi-Datacenter(DC) source receives records from the same topic in brokers deployed in two different kafka clusters. It filters out all the duplicate messages and ensuresthat the events are received in the correct order using sequential numbering. It receives events in formats such as TEXT , XML JSON and Binary`.The Kafka Source creates the default partition '0' for a given topic, if the topic has not yet been created in the Kafka cluster. Origin: siddhi-io-kafka:5.0.5 Syntax @source(type=\"kafkaMultiDC\", bootstrap.servers=\" STRING \", topic=\" STRING \", partition.no=\" INT \", is.binary.message=\" BOOL \", optional.configuration=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This contains the kafka server list which the kafka source listens to. This is given using comma-separated values. eg: 'localhost:9092,localhost:9093' STRING No No topic This is the topic that the source listens to. eg: 'topic_one' STRING No No partition.no This is the partition number of the given topic. 0 INT Yes No is.binary.message In order to receive the binary events via the Kafka Multi-DC source, the value of this parameter needs to be set to 'True'. false BOOL Yes No optional.configuration This contains all the other possible configurations with which the consumer can be created.eg: producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source(type='kafkaMultiDC', topic='kafka_topic', bootstrap.servers='host1:9092,host1:9093', partition.no='1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; The following query listens to 'kafka_topic' topic, deployed in the broker host1:9092 and host1:9093, with partition 1. A thread is created for each broker. The receiving xml events are mapped to a siddhi event and sent to the FooStream. nats (Source) NATS Source allows users to subscribe to a NATS broker and receive messages. It has the ability to receive all the message types supported by NATS. Origin: siddhi-io-nats:2.0.8 Syntax @source(type=\"nats\", destination=\" STRING \", bootstrap.servers=\" STRING \", client.id=\" STRING \", cluster.id=\" STRING \", queue.group.name=\" STRING \", durable.name=\" STRING \", subscription.sequence=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic destination Subject name which NATS Source should subscribe to. STRING No No bootstrap.servers The NATS based url of the NATS server. nats://localhost:4222 STRING Yes No client.id The identifier of the client subscribing/connecting to the NATS broker. None STRING Yes No cluster.id The identifier of the NATS server/cluster. test-cluster STRING Yes No queue.group.name This can be used when there is a requirement to share the load of a NATS subject. Clients belongs to the same queue group share the subscription load. None STRING Yes No durable.name This can be used to subscribe to a subject from the last acknowledged message when a client or connection failure happens. The client can be uniquely identified using the tuple (client.id, durable.name). None STRING Yes No subscription.sequence This can be used to subscribe to a subject from a given number of message sequence. All the messages from the given point of sequence number will be passed to the client. If not provided then the either the persisted value or 0 will be used. None STRING Yes No Examples EXAMPLE 1 @source(type='nats', @map(type='text'), destination='SP_NATS_INPUT_TEST', bootstrap.servers='nats://localhost:4222',client.id='nats_client',server.id='test-cluster',queue.group.name = 'group_nats',durable.name = 'nats-durable',subscription.sequence = '100') define stream inputStream (name string, age int, country string); This example shows how to subscribe to a NATS subject with all supporting configurations.With the following configuration the source identified as 'nats-client' will subscribes to a subject named as 'SP_NATS_INPUT_TEST' which resides in a nats instance with a cluster id of 'test-cluster', running in localhost and listening to the port 4222 for client connection. This subscription will receive all the messages from 100 th in the subject. EXAMPLE 2 @source(type='nats', @map(type='text'), destination='SP_NATS_INPUT_TEST', ) define stream inputStream (name string, age int, country string); This example shows how to subscribe to a NATS subject with mandatory configurations.With the following configuration the source identified with an auto generated client id will subscribes to a subject named as 'SP_NATS_INTPUT_TEST' which resides in a nats instance with a cluster id of 'test-cluster', running in localhost and listening to the port 4222 for client connection. This will receive all available messages in the subject EXAMPLE 3 @source(type='nats', @map(type='json', @attributes(name='$.name', age='$.age', country='$.country', sequenceNum='trp:sequenceNumber')), destination='SIDDHI_NATS_SOURCE_TEST_DEST', client.id='nats_client', bootstrap.servers='nats://localhost:4222', cluster.id='test-cluster') define stream inputStream (name string, age int, country string, sequenceNum string); This example shows how to pass NATS Streaming sequence number to the event. prometheus (Source) This source consumes Prometheus metrics that are exported from a specified URL as Siddhi events by sending HTTP requests to the URL. Based on the source configuration, it analyzes metrics from the text response and sends them as Siddhi events through key-value mapping.The user can retrieve metrics of the 'including', 'counter', 'gauge', 'histogram', and 'summary' types. The source retrieves the metrics from a text response of the target. Therefore, it is you need to use 'string' as the attribute type for the attributes that correspond with the Prometheus metric labels. Further, the Prometheus metric value is passed through the event as 'value'. This requires you to include an attribute named 'value' in the stream definition. The supported types for the 'value' attribute are 'INT', 'LONG', 'FLOAT', and 'DOUBLE'. Origin: siddhi-io-prometheus:2.1.0 Syntax @source(type=\"prometheus\", target.url=\" STRING \", scrape.interval=\" INT \", scrape.timeout=\" INT \", scheme=\" STRING \", metric.name=\" STRING \", metric.type=\" STRING \", username=\" STRING \", password=\" STRING \", client.truststore.file=\" STRING \", client.truststore.password=\" STRING \", headers=\" STRING \", job=\" STRING \", instance=\" STRING \", grouping.key=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic target.url This property specifies the target URL to which the Prometheus metrics are exported in the 'TEXT' format. STRING No No scrape.interval This property specifies the time interval in seconds within which the source should send an HTTP request to the specified target URL. 60 INT Yes No scrape.timeout This property is the time duration in seconds for a scrape request to get timed-out if the server at the URL does not respond. 10 INT Yes No scheme This property specifies the scheme of the target URL. The supported schemes are 'HTTP' and 'HTTPS'. HTTP STRING Yes No metric.name This property specifies the name of the metrics that are to be fetched. The metric name must match the regex format, i.e., '[a-zA-Z_:][a-zA-Z0-9_:]* '. Stream name STRING Yes No metric.type This property specifies the type of the Prometheus metric that is required to be fetched. The supported metric types are 'counter', 'gauge',\" 'histogram', and 'summary'. STRING No No username This property specifies the username that needs to be added in the authorization header of the HTTP request if basic authentication is enabled at the target. It is required to specify both the username and the password to enable basic authentication. If you do not provide a value for one or both of these parameters, an error is logged in the console. STRING Yes No password This property specifies the password that needs to be added in the authorization header of the HTTP request if basic authentication is enabled at the target. It is required to specify both the username and the password to enable basic authentication. If you do not provide a value for one or both of these parameters, an error is logged in the console. STRING Yes No client.truststore.file The file path to the location of the truststore to which the client needs to send HTTPS requests via the 'HTTPS' protocol. STRING Yes No client.truststore.password The password for the client-truststore. This is required to send HTTPS requests. A custom password can be specified if required. STRING Yes No headers Headers that need to be included as HTTP request headers in the request. The format of the supported input is as follows, \"'header1:value1','header2:value2'\" STRING Yes No job This property defines the job name of the exported Prometheus metrics that needs to be fetched. STRING Yes No instance This property defines the instance of the exported Prometheus metrics that needs to be fetched. STRING Yes No grouping.key This parameter specifies the grouping key of the required metrics in key-value pairs. The grouping key is used if the metrics are exported by Prometheus 'pushGateway' in order to distinguish those metrics from already existing metrics. The expected format of the grouping key is as follows: \"'key1:value1','key2:value2'\" STRING Yes No System Parameters Name Description Default Value Possible Parameters scrapeInterval The default time interval in seconds for the Prometheus source to send HTTP requests to the target URL. 60 Any integer value scrapeTimeout The default time duration (in seconds) for an HTTP request to time-out if the server at the URL does not respond. 10 Any integer value scheme The scheme of the target for the Prometheus source to send HTTP requests. The supported schemes are 'HTTP' and 'HTTPS'. HTTP HTTP or HTTPS username The username that needs to be added in the authorization header of the HTTP request if basic authentication is enabled at the target. It is required to specify both the username and password to enable basic authentication. If you do not specify a value for one or both of these parameters, an error is logged in the console. Any string password The password that needs to be added in the authorization header of the HTTP request if basic authentication is enabled at the target. It is required to specify both the username and password to enable basic authentication. If you do not specify a value for one or both of these parameters, an error is logged in the console. Any string trustStoreFile The default file path to the location of truststore that the client needs to access in order to send HTTPS requests through 'HTTPS' protocol. ${carbon.home}/resources/security/client-truststore.jks Any valid path for the truststore file trustStorePassword The default password for the client-truststore that the client needs to access in order to send HTTPS requests through 'HTTPS' protocol. wso2carbon Any string headers The headers that need to be included as HTTP request headers in the scrape request. The format of the supported input is as follows, \"'header1:value1','header2:value2'\" Any valid http headers job The default job name of the exported Prometheus metrics that needs to be fetched. Any valid job name instance The default instance of the exported Prometheus metrics that needs to be fetched. Any valid instance name groupingKey The default grouping key of the required Prometheus metrics in key-value pairs. The grouping key is used if the metrics are exported by the Prometheus pushGateway in order to distinguish these metrics from already existing metrics. The expected format of the grouping key is as follows: \"'key1:value1','key2:value2'\" Any valid grouping key pairs Examples EXAMPLE 1 @source(type= 'prometheus', target.url= 'http://localhost:9080/metrics', metric.type= 'counter', metric.name= 'sweet_production_counter', @map(type= 'keyvalue')) define stream FooStream1(metric_name string, metric_type string, help string, subtype string, name string, quantity string, value double); In this example, the Prometheus source sends an HTTP request to the 'target.url' and analyzes the response. From the analyzed response, the source retrieves the Prometheus counter metrics with the 'sweet_production_counter' nameand converts the filtered metrics into Siddhi events using the key-value mapper. The generated maps have keys and values as follows: metric_name - sweet_production_counter metric_type - counter help - help_string_of_metric subtype - null name - value_of_label_name quantity - value_of_label_quantity value - value_of_metric EXAMPLE 2 @source(type= 'prometheus', target.url= 'http://localhost:9080/metrics', metric.type= 'summary', metric.name= 'sweet_production_summary', @map(type= 'keyvalue')) define stream FooStream2(metric_name string, metric_type string, help string, subtype string, name string, quantity string, quantile string, value double); In this example, the Prometheus source sends an HTTP request to the 'target.url' and analyzes the response. From the analysed response, the source retrieves the Prometheus summary metrics with the 'sweet_production_summary' nameand converts the filtered metrics into Siddhi events using the key-value mapper. The generated maps have keys and values as follows: metric_name - sweet_production_summary metric_type - summary help - help_string_of_metric subtype - 'sum'/'count'/'null' name - value_of_label_name quantity - value_of_label_quantity quantile - value of the quantile value - value_of_metric EXAMPLE 3 @source(type= 'prometheus', target.url= 'http://localhost:9080/metrics', metric.type= 'histogram', metric.name= 'sweet_production_histogram', @map(type= 'keyvalue')) define stream FooStream3(metric_name string, metric_type string, help string, subtype string, name string, quantity string, le string, value double); In this example, the prometheus source sends an HTTP request to the 'target.url' and analyzes the response. From the analyzed response, the source retrieves the Prometheus histogram metrics with the 'sweet_production_histogram' name and converts the filtered metrics into Siddhi events using the key-value mapper. The generated maps have keys and values as follows, metric_name - sweet_production_histogram metric_type - histogram help - help_string_of_metric subtype - 'sum'/'count'/'bucket' name - value_of_label_name quantity - value_of_label_quantity le - value of the bucket value - value_of_metric rabbitmq (Source) The rabbitmq source receives the events from the rabbitmq broker via the AMQP protocol. Origin: siddhi-io-rabbitmq:3.0.2 Syntax @source(type=\"rabbitmq\", uri=\" STRING \", heartbeat=\" INT \", exchange.name=\" STRING \", exchange.type=\" STRING \", exchange.durable.enabled=\" BOOL \", exchange.autodelete.enabled=\" BOOL \", routing.key=\" STRING \", headers=\" STRING \", queue.name=\" STRING \", queue.durable.enabled=\" BOOL \", queue.exclusive.enabled=\" BOOL \", queue.autodelete.enabled=\" BOOL \", tls.enabled=\" BOOL \", tls.truststore.path=\" STRING \", tls.truststore.password=\" STRING \", tls.truststore.type=\" STRING \", tls.version=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic uri The URI that is used to connect to an AMQP server. If no URI is specified,an error is logged in the CLI.e.g., amqp://guest:guest , amqp://guest:guest@localhost:5672 STRING No No heartbeat The period of time (in seconds) after which the peer TCP connection should be considered unreachable (down) by RabbitMQ and client libraries. 60 INT Yes No exchange.name The name of the exchange that decides what to do with a message it receives.If the exchange.name already exists in the RabbitMQ server, then the system uses that exchange.name instead of redeclaring. STRING No No exchange.type The type of the exchange name. The exchange types available are direct , fanout , topic and headers . For a detailed description of each type, see RabbitMQ - AMQP Concepts . direct STRING Yes No exchange.durable.enabled If this is set to true , the exchange remains declared even if the broker restarts. false BOOL Yes No exchange.autodelete.enabled If this is set to true , the exchange is automatically deleted when it is not used anymore. false BOOL Yes No routing.key The key based on which the exchange determines how to route the message to queues. The routing key is like an address for the message. The routing.key must be initialized when the value for the exchange.type parameter is direct or topic . empty STRING Yes No headers The headers of the message. The attributes used for routing are taken from the this paremeter. A message is considered matching if the value of the header equals the value specified upon binding. null STRING Yes No queue.name A queue is a buffer that stores messages. If the queue name already exists in the RabbitMQ server, then the system usees that queue name instead of redeclaring it. If no value is specified for this parameter, the system uses the unique queue name that is automatically generated by the RabbitMQ server. system generated queue name STRING Yes No queue.durable.enabled If this parameter is set to true , the queue remains declared even if the broker restarts false BOOL Yes No queue.exclusive.enabled If this parameter is set to true , the queue is exclusive for the current connection. If it is set to false , it is also consumable by other connections. false BOOL Yes No queue.autodelete.enabled If this parameter is set to true , the queue is automatically deleted when it is not used anymore. false BOOL Yes No tls.enabled This parameter specifies whether an encrypted communication channel should be established or not. When this parameter is set to true , the tls.truststore.path and tls.truststore.password parameters are initialized. false BOOL Yes No tls.truststore.path The file path to the location of the truststore of the client that receives the RabbitMQ events via the AMQP protocol. A custom client-truststore can be specified if required. If a custom truststore is not specified, then the system uses the default client-trustore in the {carbon.home}/resources/security /code directory. /p /td td style=\"vertical-align: top\" {carbon.home}/resources/security</code> directory.</p></td> <td style=\"vertical-align: top\"> /resources/security/client-truststore.jks STRING Yes No tls.truststore.password The password for the client-truststore. A custom password can be specified if required. If no custom password is specified, then the system uses wso2carbon as the default password. wso2carbon STRING Yes No tls.truststore.type The type of the truststore. JKS STRING Yes No tls.version The version of the tls/ssl. SSL STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @source(type ='rabbitmq', uri = 'amqp://guest:guest@localhost:5672', exchange.name = 'direct', routing.key= 'direct', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query receives events from the direct exchange with the direct exchange type, and the directTest routing key. tcp (Source) A Siddhi application can be configured to receive events via the TCP transport by adding the @Source(type = 'tcp') annotation at the top of an event stream definition. When this is defined the associated stream will receive events from the TCP transport on the host and port defined in the system. Origin: siddhi-io-tcp:3.0.4 Syntax @source(type=\"tcp\", context=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic context The URL 'context' that should be used to receive the events. / STRING Yes No System Parameters Name Description Default Value Possible Parameters host Tcp server host. 0.0.0.0 Any valid host or IP port Tcp server port. 9892 Any integer representing valid port receiver.threads Number of threads to receive connections. 10 Any positive integer worker.threads Number of threads to serve events. 10 Any positive integer tcp.no.delay This is to specify whether to disable Nagle algorithm during message passing. If tcp.no.delay = 'true', the execution of Nagle algorithm will be disabled in the underlying TCP logic. Hence there will be no delay between two successive writes to the TCP connection. Else there can be a constant ack delay. true true false keep.alive This property defines whether the server should be kept alive when there are no connections available. true true false Examples EXAMPLE 1 @Source(type = 'tcp', context='abc', @map(type='binary')) define stream Foo (attribute1 string, attribute2 int ); Under this configuration, events are received via the TCP transport on default host,port, abc context, and they are passed to Foo stream for processing. Sourcemapper avro (Source Mapper) This extension is an Avro to Event input mapper. Transports that accept Avro messages can utilize this extension to convert the incoming Avro messages to Siddhi events. The Avro schema to be used for creating Avro messages can be specified as a parameter in the stream definition. If no Avro schema is specified, a flat avro schema of the 'record' type is generated with the stream attributes as schema fields. The generated/specified Avro schema is used to convert Avro messages to Siddhi events. Origin: siddhi-map-avro:2.0.6 Syntax @source(..., @map(type=\"avro\", schema.def=\" STRING \", schema.registry=\" STRING \", schema.id=\" STRING \", fail.on.missing.attribute=\" BOOL \") QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic schema.def This specifies the schema of the Avro message. The full schema used to create the Avro message needs to be specified as a quoted JSON string. STRING No No schema.registry This specifies the URL of the schema registry. STRING No No schema.id This specifies the ID of the Avro schema. This ID is the global ID that is returned from the schema registry when posting the schema to the registry. The schema is retrieved from the schema registry via the specified ID. STRING No No fail.on.missing.attribute If this parameter is set to 'true', a JSON execution failing or returning a null value results in that message being dropped by the system. If this parameter is set to 'false', a JSON execution failing or returning a null value results in the system being prompted to send the event with a null value to Siddhi so that the user can handle it as required (i.e., by assigning a default value. true BOOL Yes No Examples EXAMPLE 1 @source(type='inMemory', topic='user', @map(type='avro', schema .def = \"\"\"{\"type\":\"record\",\"name\":\"userInfo\",\"namespace\":\"user.example\",\"fields\":[{\"name\":\"name\",\"type\":\"string\"}, {\"name\":\"age\",\"type\":\"int\"}]}\"\"\")) define stream UserStream (name string, age int ); The above Siddhi query performs a default Avro input mapping. The input Avro message that contains user information is converted to a Siddhi event. The expected input is a byte array or ByteBuffer. EXAMPLE 2 @source(type='inMemory', topic='user', @map(type='avro', schema .def = \"\"\"{\"type\":\"record\",\"name\":\"userInfo\",\"namespace\":\"avro.userInfo\",\"fields\":[{\"name\":\"username\",\"type\":\"string\"}, {\"name\":\"age\",\"type\":\"int\"}]}\"\"\",@attributes(name=\"username\",age=\"age\"))) define stream userStream (name string, age int ); The above Siddhi query performs a custom Avro input mapping. The input Avro message that contains user information is converted to a Siddhi event. The expected input is a byte array or ByteBuffer. EXAMPLE 3 @source(type='inMemory', topic='user', @map(type='avro',schema.registry='http://192.168.2.5:9090', schema.id='1',@attributes(name=\"username\",age=\"age\"))) define stream UserStream (name string, age int ); The above Siddhi query performs a custom Avro input mapping. The input Avro message that contains user information is converted to a Siddhi event via the schema retrieved from the given schema registry(localhost:8081). The expected input is a byte array or ByteBuffer. binary (Source Mapper) This extension is a binary input mapper that converts events received in binary format to Siddhi events before they are processed. Origin: siddhi-map-binary:2.0.4 Syntax @source(..., @map(type=\"binary\") Examples EXAMPLE 1 @source(type='inMemory', topic='WSO2', @map(type='binary'))define stream FooStream (symbol string, price float, volume long); This query performs a mapping to convert an event of the binary format to a Siddhi event. csv (Source Mapper) This extension is used to convert CSV message to Siddhi event input mapper. You can either receive pre-defined CSV message where event conversion takes place without extra configurations,or receive custom CSV message where a custom place order to map from custom CSV message. Origin: siddhi-map-csv:2.0.3 Syntax @source(..., @map(type=\"csv\", delimiter=\" STRING \", header.present=\" BOOL \", fail.on.unknown.attribute=\" BOOL \", event.grouping.enabled=\" BOOL \") QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic delimiter When converting a CSV format message to Siddhi event, this parameter indicatesinput CSV message's data should be split by this parameter , STRING Yes No header.present When converting a CSV format message to Siddhi event, this parameter indicates whether CSV message has header or not. This can either have value true or false.If it's set to false then it indicates that CSV message has't header. false BOOL Yes No fail.on.unknown.attribute This parameter specifies how unknown attributes should be handled. If it's set to true and one or more attributes don't havevalues, then SP will drop that message. If this parameter is set to false , the Stream Processor adds the required attribute's values to such events with a null value and the event is converted to a Siddhi event. true BOOL Yes No event.grouping.enabled This parameter specifies whether event grouping is enabled or not. To receive a group of events together and generate multiple events, this parameter must be set to true . false BOOL Yes No Examples EXAMPLE 1 @source(type='inMemory', topic='stock', @map(type='csv')) define stream FooStream (symbol string, price float, volume int); Above configuration will do a default CSV input mapping. Expected input will look like below: WSO2 ,55.6 , 100OR \"WSO2,No10,Palam Groove Rd,Col-03\" ,55.6 , 100If header.present is true and delimiter is \"-\", then the input is as follows: symbol-price-volumeWSO2-55.6-100 EXAMPLE 2 @source(type='inMemory', topic='stock', @map(type='csv',header='true', @attributes(symbol = \"2\", price = \"0\", volume = \"1\"))) define stream FooStream (symbol string, price float, volume long); Above configuration will perform a custom CSV mapping. Here, user can add place order of each attribute in the @attribute. The place order indicates where the attribute name's value has appeared in the input.Expected input will look like below: 55.6,100,WSO2 OR55.6,100,\"WSO2,No10,Palm Groove Rd,Col-03\" If header is true and delimiter is \"-\", then the output is as follows: price-volume-symbol 55.6-100-WSO2 If group events is enabled then input should be as follows: price-volume-symbol 55.6-100-WSO2System.lineSeparator() 55.6-100-IBMSystem.lineSeparator() 55.6-100-IFSSystem.lineSeparator() json (Source Mapper) This extension is a JSON-to-Event input mapper. Transports that accept JSON messages can utilize this extension to convert an incoming JSON message into a Siddhi event. Users can either send a pre-defined JSON format, where event conversion happens without any configurations, or use the JSON path to map from a custom JSON message. In default mapping, the JSON string of the event can be enclosed by the element \"event\", though optional. Origin: siddhi-map-json:5.0.5 Syntax @source(..., @map(type=\"json\", enclosing.element=\" STRING \", fail.on.missing.attribute=\" BOOL \") QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic enclosing.element This is used to specify the enclosing element when sending multiple events in the same JSON message. Mapper treats the child elements of a given enclosing element as events and executes the JSON path expressions on these child elements. If the enclosing.element is not provided then the multiple-event scenario is disregarded and the JSON path is evaluated based on the root element. $ STRING Yes No fail.on.missing.attribute This parameter allows users to handle unknown attributes.The value of this can either be true or false. By default it is true. If a JSON execution fails or returns null, mapper drops that message. However, setting this property to false prompts mapper to send an event with a null value to Siddhi, where users can handle it as required, ie., assign a default value.) true BOOL Yes No Examples EXAMPLE 1 @source(type='inMemory', topic='stock', @map(type='json')) define stream FooStream (symbol string, price float, volume long); This configuration performs a default JSON input mapping. For a single event, the input is required to be in one of the following formats: { \"event\":{ \"symbol\":\"WSO2\", \"price\":55.6, \"volume\":100 } } or { \"symbol\":\"WSO2\", \"price\":55.6, \"volume\":100 } EXAMPLE 2 @source(type='inMemory', topic='stock', @map(type='json')) define stream FooStream (symbol string, price float, volume long); This configuration performs a default JSON input mapping. For multiple events, the input is required to be in one of the following formats: [ {\"event\":{\"symbol\":\"WSO2\",\"price\":55.6,\"volume\":100}}, {\"event\":{\"symbol\":\"WSO2\",\"price\":56.6,\"volume\":99}}, {\"event\":{\"symbol\":\"WSO2\",\"price\":57.6,\"volume\":80}} ] or [ {\"symbol\":\"WSO2\",\"price\":55.6,\"volume\":100}, {\"symbol\":\"WSO2\",\"price\":56.6,\"volume\":99}, {\"symbol\":\"WSO2\",\"price\":57.6,\"volume\":80} ] EXAMPLE 3 @source(type='inMemory', topic='stock', @map(type='json', enclosing.element=\"$.portfolio\", @attributes(symbol = \"company.symbol\", price = \"price\", volume = \"volume\"))) This configuration performs a custom JSON mapping. For a single event, the expected input is similar to the one shown below: { \"portfolio\":{ \"stock\":{ \"volume\":100, \"company\":{ \"symbol\":\"WSO2\" }, \"price\":55.6 } } } EXAMPLE 4 @source(type='inMemory', topic='stock', @map(type='json', enclosing.element=\"$.portfolio\", @attributes(symbol = \"stock.company.symbol\", price = \"stock.price\", volume = \"stock.volume\"))) define stream FooStream (symbol string, price float, volume long); The configuration performs a custom JSON mapping. For multiple events, expected input looks as follows. .{\"portfolio\": [ {\"stock\":{\"volume\":100,\"company\":{\"symbol\":\"wso2\"},\"price\":56.6}}, {\"stock\":{\"volume\":200,\"company\":{\"symbol\":\"wso2\"},\"price\":57.6}} ] } keyvalue (Source Mapper) Key-Value Map to Event input mapper extension allows transports that accept events as key value maps to convert those events to Siddhi events. You can either receive pre-defined keys where conversion takes place without extra configurations, or use custom keys to map from the message. Origin: siddhi-map-keyvalue:2.0.5 Syntax @source(..., @map(type=\"keyvalue\", fail.on.missing.attribute=\" BOOL \") QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic fail.on.missing.attribute If this parameter is set to true , if an event arrives without a matching key for a specific attribute in the connected stream, it is dropped and not processed by the Stream Processor. If this parameter is set to false the Stream Processor adds the required key to such events with a null value, and the event is converted to a Siddhi event so that you could handle them as required before they are further processed. true BOOL Yes No Examples EXAMPLE 1 @source(type='inMemory', topic='stock', @map(type='keyvalue')) define stream FooStream (symbol string, price float, volume long); This query performs a default key value input mapping. The expected input is a map similar to the following: symbol: 'WSO2' price: 55.6f volume: 100 EXAMPLE 2 @source(type='inMemory', topic='stock', @map(type='keyvalue', fail.on.missing.attribute='true', @attributes(symbol = 's', price = 'p', volume = 'v')))define stream FooStream (symbol string, price float, volume long); This query performs a custom key value input mapping. The matching keys for the symbol , price and volume attributes are be s , p , and v respectively. The expected input is a map similar to the following: s: 'WSO2' p: 55.6 v: 100 passThrough (Source Mapper) Pass-through mapper passed events (Event[]) through without any mapping or modifications. Origin: siddhi-core:5.1.8 Syntax @source(..., @map(type=\"passThrough\") Examples EXAMPLE 1 @source(type='tcp', @map(type='passThrough')) define stream BarStream (symbol string, price float, volume long); In this example BarStream uses passThrough inputmapper which passes the received Siddhi event directly without any transformation into source. protobuf (Source Mapper) This input mapper allows you to convert protobuf messages into Events. To work with this input mapper you have to add auto-generated protobuf classes to the project classpath. When you use this input mapper, you can either define stream attributes as the same names as the protobuf message attributes or you can use custom mapping to map stream definition attributes with the protobuf attributes..Please find the sample proto definition here Origin: siddhi-map-protobuf:1.0.2 Syntax @source(..., @map(type=\"protobuf\", class=\" STRING \") QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic class This specifies the class name of the protobuf message class, If sink type is grpc then it's not necessary to provide this field. - STRING Yes No Examples EXAMPLE 1 source(type='grpc', receiver.url = 'grpc://localhost: 2000/org.wso2.grpc.test.MyService/process', @map(type='protobuf')) define stream FooStream (stringValue string, intValue int,longValue long,booleanValue bool,floatValue float,doubleValue double); Above definition will convert the protobuf messages that are received to this source into siddhi events. EXAMPLE 2 source(type='grpc', receiver.url = 'grpc://localhost: 2000/org.wso2.grpc.test.MyService/process', @map(type='protobuf', @attributes(a = 'stringValue', b = 'intValue', c = 'longValue',d = 'booleanValue',' e = floatValue', f ='doubleValue'))) define stream FooStream (a string ,c long,b int, d bool,e float,f double); Above definition will convert the protobuf messages that are received to this source into siddhi events. since there's a mapping available for the stream, protobuf message object will be map like this, -'stringValue' of the protobuf message will be assign to the 'a' attribute of the stream - 'intValue' of the protobuf message will be assign to the 'b' attribute of the stream - 'longValue' of the protobuf message will be assign to the 'c' attribute of the stream - 'booleanValue' of the protobuf message will be assign to the 'd' attribute of the stream - 'floatValue' of the protobuf message will be assign to the 'e' attribute of the stream - 'doubleValue' of the protobuf message will be assign to the 'f' attribute of the stream EXAMPLE 3 source(type='grpc', receiver.url = 'grpc://localhost: 2000/org.wso2.grpc.test.MyService/testMap', @map(type='protobuf')) define stream FooStream (stringValue string ,intValue int,map object); Above definition will convert the protobuf messages that are received to this source into siddhi events. since there's an object type attribute available in the stream (map object), mapper will assume that object is an instance of 'java.util.Map' class. otherwise mapper will throws an exception EXAMPLE 4 @source(type='inMemory', topic='test01', @map(type='protobuf', class='org.wso2.grpc.test.Request')) define stream FooStream (stringValue string, intValue int,longValue long,booleanValue bool,floatValue float,doubleValue double); The above definition will convert the 'org.wso2.grpc.test.Request' type protobuf messages into siddhi events. If we did not provide the 'receiver.url' in the stream definition we have to provide the protobuf class name in the 'class' parameter inside @map. text (Source Mapper) This extension is a text to Siddhi event input mapper. Transports that accept text messages can utilize this extension to convert the incoming text message to Siddhi event. Users can either use a pre-defined text format where event conversion happens without any additional configurations, or specify a regex to map a text message using custom configurations. Origin: siddhi-map-text:2.0.4 Syntax @source(..., @map(type=\"text\", regex.groupid=\" STRING \", fail.on.missing.attribute=\" BOOL \", event.grouping.enabled=\" BOOL \", delimiter=\" STRING \", new.line.character=\" STRING \") QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic regex.groupid This parameter specifies a regular expression group. The groupid can be any capital letter (e.g., regex.A,regex.B .. etc). You can specify any number of regular expression groups. In the attribute annotation, you need to map all attributes to the regular expression group with the matching group index. If you need to to enable custom mapping, it is required to specifythe matching group for each and every attribute. STRING No No fail.on.missing.attribute This parameter specifies how unknown attributes should be handled. If it is set to true a message is dropped if its execution fails, or if one or more attributes do not have values. If this parameter is set to false , null values are assigned to attributes with missing values, and messages with such attributes are not dropped. true BOOL Yes No event.grouping.enabled This parameter specifies whether event grouping is enabled or not. To receive a group of events together and generate multiple events, this parameter must be set to true . false BOOL Yes No delimiter This parameter specifies how events must be separated when multiple events are received. This must be whole line and not a single character. ~ ~ ~ ~ STRING Yes No new.line.character This attribute indicates the new line character of the event that is expected to be received. This is used mostly when communication between 2 types of operating systems is expected. For example, Linux uses \\n as the end of line character whereas windows uses \\r\\n . \\n STRING Yes No Examples EXAMPLE 1 @source(type='inMemory', topic='stock', @map(type='text')) define stream FooStream (symbol string, price float, volume long); This query performs a default text input mapping. The expected input is as follows: symbol:\"WSO2\", price:55.6, volume:100 OR symbol:'WSO2', price:55.6, volume:100 If group events is enabled then input should be as follows: symbol:\"WSO2\", price:55.6, volume:100 ~ ~ ~ ~ symbol:\"WSO2\", price:55.6, volume:100 EXAMPLE 2 @source(type='inMemory', topic='stock', @map(type='text', fail.on.missing.attribute = 'true', regex.A='(\\w+)\\s([-0-9]+)',regex.B='volume\\s([-0-9]+)', @attributes(symbol = 'A[1]',price = 'A[2]',volume = 'B'))) define stream FooStream (symbol string, price float, volume long); This query performs a custom text mapping. The expected input is as follows: wos2 550 volume 100 If group events is enabled then input should be as follows: wos2 550 volume 100 ~ ~ ~ ~ wos2 550 volume 100 ~ ~ ~ ~ wos2 550 volume 100 xml (Source Mapper) This mapper converts XML input to Siddhi event. Transports which accepts XML messages can utilize this extension to convert the incoming XML message to Siddhi event. Users can either send a pre-defined XML format where event conversion will happen without any configs or can use xpath to map from a custom XML message. Origin: siddhi-map-xml:5.0.3 Syntax @source(..., @map(type=\"xml\", namespaces=\" STRING \", enclosing.element=\" STRING \", fail.on.missing.attribute=\" BOOL \") QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic namespaces Used to provide namespaces used in the incoming XML message beforehand to configure xpath expressions. User can provide a comma separated list. If these are not provided xpath evaluations will fail None STRING Yes No enclosing.element Used to specify the enclosing element in case of sending multiple events in same XML message. WSO2 DAS will treat the child element of given enclosing element as events and execute xpath expressions on child elements. If enclosing.element is not provided multiple event scenario is disregarded and xpaths will be evaluated with respect to root element. Root element STRING Yes No fail.on.missing.attribute This can either have value true or false. By default it will be true. This attribute allows user to handle unknown attributes. By default if an xpath execution fails or returns null DAS will drop that message. However setting this property to false will prompt DAS to send and event with null value to Siddhi where user can handle it accordingly(ie. Assign a default value) True BOOL Yes No Examples EXAMPLE 1 @source(type='inMemory', topic='stock', @map(type='xml')) define stream FooStream (symbol string, price float, volume long); Above configuration will do a default XML input mapping. Expected input will look like below. events event symbol WSO2 /symbol price 55.6 /price volume 100 /volume /event /events EXAMPLE 2 @source(type='inMemory', topic='stock', @map(type='xml', namespaces = \"dt=urn:schemas-microsoft-com:datatypes\", enclosing.element=\"//portfolio\", @attributes(symbol = \"company/symbol\", price = \"price\", volume = \"volume\"))) define stream FooStream (symbol string, price float, volume long); Above configuration will perform a custom XML mapping. In the custom mapping user can add xpath expressions representing each event attribute using @attribute annotation. Expected input will look like below. portfolio xmlns:dt=\"urn:schemas-microsoft-com:datatypes\" stock exchange=\"nasdaq\" volume 100 /volume company symbol WSO2 /symbol /company price dt:type=\"number\" 55.6 /price /stock /portfolio Store mongodb (Store) Using this extension a MongoDB Event Table can be configured to persist events in a MongoDB of user's choice. Origin: siddhi-store-mongodb:2.0.3 Syntax @Store(type=\"mongodb\", mongodb.uri=\" STRING \", collection.name=\" STRING \", secure.connection=\" STRING \", trust.store=\" STRING \", trust.store.password=\" STRING \", key.store=\" STRING \", key.store.password=\" STRING \") @PrimaryKey(\"PRIMARY_KEY\") @Index(\"INDEX\") QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic mongodb.uri The MongoDB URI for the MongoDB data store. The uri must be of the format mongodb://[username:password@]host1[:port1][,hostN[:portN]][/[database][?options]] The options specified in the uri will override any connection options specified in the deployment yaml file. Note: The user should have read permissions to the admindb as well as read/write permissions to the database accessed. STRING No No collection.name The name of the collection in the store this Event Table should be persisted as. Name of the siddhi event table. STRING Yes No secure.connection Describes enabling the SSL for the mongodb connection false STRING Yes No trust.store File path to the trust store. {carbon.home}/resources/security/client-truststore.jks /td td style=\"vertical-align: top\" STRING /td td style=\"vertical-align: top\" Yes /td td style=\"vertical-align: top\" No /td /tr tr td style=\"vertical-align: top\" trust.store.password /td td style=\"vertical-align: top; word-wrap: break-word\" p style=\"word-wrap: break-word;margin: 0;\" Password to access the trust store /p /td td style=\"vertical-align: top\" wso2carbon /td td style=\"vertical-align: top\" STRING /td td style=\"vertical-align: top\" Yes /td td style=\"vertical-align: top\" No /td /tr tr td style=\"vertical-align: top\" key.store /td td style=\"vertical-align: top; word-wrap: break-word\" p style=\"word-wrap: break-word;margin: 0;\" File path to the keystore. /p /td td style=\"vertical-align: top\" {carbon.home}/resources/security/client-truststore.jks</td> <td style=\"vertical-align: top\">STRING</td> <td style=\"vertical-align: top\">Yes</td> <td style=\"vertical-align: top\">No</td> </tr> <tr> <td style=\"vertical-align: top\">trust.store.password</td> <td style=\"vertical-align: top; word-wrap: break-word\"><p style=\"word-wrap: break-word;margin: 0;\">Password to access the trust store</p></td> <td style=\"vertical-align: top\">wso2carbon</td> <td style=\"vertical-align: top\">STRING</td> <td style=\"vertical-align: top\">Yes</td> <td style=\"vertical-align: top\">No</td> </tr> <tr> <td style=\"vertical-align: top\">key.store</td> <td style=\"vertical-align: top; word-wrap: break-word\"><p style=\"word-wrap: break-word;margin: 0;\">File path to the keystore.</p></td> <td style=\"vertical-align: top\"> /resources/security/client-truststore.jks STRING Yes No key.store.password Password to access the keystore wso2carbon STRING Yes No System Parameters Name Description Default Value Possible Parameters applicationName Sets the logical name of the application using this MongoClient. The application name may be used by the client to identify the application to the server, for use in server logs, slow query logs, and profile collection. null the logical name of the application using this MongoClient. The UTF-8 encoding may not exceed 128 bytes. cursorFinalizerEnabled Sets whether cursor finalizers are enabled. true true false requiredReplicaSetName The name of the replica set null the logical name of the replica set sslEnabled Sets whether to initiate connection with TSL/SSL enabled. true: Initiate the connection with TLS/SSL. false: Initiate the connection without TLS/SSL. false true false trustStore File path to the trust store. {carbon.home}/resources/security/client-truststore.jks /td td style=\"vertical-align: top\" Any valid file path. /td /tr tr td style=\"vertical-align: top\" trustStorePassword /td td style=\"vertical-align: top;\" p style=\"word-wrap: break-word;margin: 0;\" Password to access the trust store /p /td td style=\"vertical-align: top\" wso2carbon /td td style=\"vertical-align: top\" Any valid password. /td /tr tr td style=\"vertical-align: top\" keyStore /td td style=\"vertical-align: top;\" p style=\"word-wrap: break-word;margin: 0;\" File path to the keystore. /p /td td style=\"vertical-align: top\" {carbon.home}/resources/security/client-truststore.jks</td> <td style=\"vertical-align: top\">Any valid file path.</td> </tr> <tr> <td style=\"vertical-align: top\">trustStorePassword</td> <td style=\"vertical-align: top;\"><p style=\"word-wrap: break-word;margin: 0;\">Password to access the trust store</p></td> <td style=\"vertical-align: top\">wso2carbon</td> <td style=\"vertical-align: top\">Any valid password.</td> </tr> <tr> <td style=\"vertical-align: top\">keyStore</td> <td style=\"vertical-align: top;\"><p style=\"word-wrap: break-word;margin: 0;\">File path to the keystore.</p></td> <td style=\"vertical-align: top\"> /resources/security/client-truststore.jks Any valid file path. keyStorePassword Password to access the keystore wso2carbon Any valid password. connectTimeout The time in milliseconds to attempt a connection before timing out. 10000 Any positive integer connectionsPerHost The maximum number of connections in the connection pool. 100 Any positive integer minConnectionsPerHost The minimum number of connections in the connection pool. 0 Any natural number maxConnectionIdleTime The maximum number of milliseconds that a connection can remain idle in the pool before being removed and closed. A zero value indicates no limit to the idle time. A pooled connection that has exceeded its idle time will be closed and replaced when necessary by a new connection. 0 Any positive integer maxWaitTime The maximum wait time in milliseconds that a thread may wait for a connection to become available. A value of 0 means that it will not wait. A negative value means to wait indefinitely 120000 Any integer threadsAllowedToBlockForConnectionMultiplier The maximum number of connections allowed per host for this MongoClient instance. Those connections will be kept in a pool when idle. Once the pool is exhausted, any operation requiring a connection will block waiting for an available connection. 100 Any natural number maxConnectionLifeTime The maximum life time of a pooled connection. A zero value indicates no limit to the life time. A pooled connection that has exceeded its life time will be closed and replaced when necessary by a new connection. 0 Any positive integer socketKeepAlive Sets whether to keep a connection alive through firewalls false true false socketTimeout The time in milliseconds to attempt a send or receive on a socket before the attempt times out. Default 0 means never to timeout. 0 Any natural integer writeConcern The write concern to use. acknowledged acknowledged w1 w2 w3 unacknowledged fsynced journaled replica_acknowledged normal safe majority fsync_safe journal_safe replicas_safe readConcern The level of isolation for the reads from replica sets. default local majority linearizable readPreference Specifies the replica set read preference for the connection. primary primary secondary secondarypreferred primarypreferred nearest localThreshold The size (in milliseconds) of the latency window for selecting among multiple suitable MongoDB instances. 15 Any natural number serverSelectionTimeout Specifies how long (in milliseconds) to block for server selection before throwing an exception. A value of 0 means that it will timeout immediately if no server is available. A negative value means to wait indefinitely. 30000 Any integer heartbeatSocketTimeout The socket timeout for connections used for the cluster heartbeat. A value of 0 means that it will timeout immediately if no cluster member is available. A negative value means to wait indefinitely. 20000 Any integer heartbeatConnectTimeout The connect timeout for connections used for the cluster heartbeat. A value of 0 means that it will timeout immediately if no cluster member is available. A negative value means to wait indefinitely. 20000 Any integer heartbeatFrequency Specify the interval (in milliseconds) between checks, counted from the end of the previous check until the beginning of the next one. 10000 Any positive integer minHeartbeatFrequency Sets the minimum heartbeat frequency. In the event that the driver has to frequently re-check a server's availability, it will wait at least this long since the previous check to avoid wasted effort. 500 Any positive integer Examples EXAMPLE 1 @Store(type=\"mongodb\",mongodb.uri=\"mongodb://admin:admin@localhost/Foo\") @PrimaryKey(\"symbol\") @Index(\"volume:1\", {background:true,unique:true}\") define table FooTable (symbol string, price float, volume long); This will create a collection called FooTable for the events to be saved with symbol as Primary Key(unique index at mongoDB level) and index for the field volume will be created in ascending order with the index option to create the index in the background. Note: @PrimaryKey: This specifies a list of comma-separated values to be treated as unique fields in the table. Each record in the table must have a unique combination of values for the fields specified here. @Index: This specifies the fields that must be indexed at the database level. You can specify multiple values as a come-separated list. A single value to be in the format, FieldName : SortOrder . The last element is optional through which a valid index options can be passed. SortOrder : 1 for Ascending -1 for Descending. Optional, with default value as 1. IndexOptions : Index Options must be defined inside curly brackets. Options must follow the standard mongodb index options format. https://docs.mongodb.com/manual/reference/method/db.collection.createIndex/ Example 1: @Index( 'symbol:1' , '{\"unique\":true}' ) Example 2: @Index( 'symbol' , '{\"unique\":true}' ) Example 3: @Index( 'symbol:1' , 'volume:-1' , '{\"unique\":true}' ) rdbms (Store) This extension assigns data sources and connection instructions to event tables. It also implements read-write operations on connected data sources. Origin: siddhi-store-rdbms:7.0.2 Syntax @Store(type=\"rdbms\", jdbc.url=\" STRING \", username=\" STRING \", password=\" STRING \", jdbc.driver.name=\" STRING \", pool.properties=\" STRING \", jndi.resource=\" STRING \", datasource=\" STRING \", table.name=\" STRING \", field.length=\" STRING \", table.check.query=\" STRING \", use.collation=\" BOOL \") @PrimaryKey(\"PRIMARY_KEY\") @Index(\"INDEX\") QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic jdbc.url The JDBC URL via which the RDBMS data store is accessed. STRING No No username The username to be used to access the RDBMS data store. STRING No No password The password to be used to access the RDBMS data store. STRING No No jdbc.driver.name The driver class name for connecting the RDBMS data store. STRING No No pool.properties Any pool parameters for the database connection must be specified as key-value pairs. null STRING Yes No jndi.resource The name of the JNDI resource through which the connection is attempted. If this is found, the pool properties described above are not taken into account and the connection is attempted via JNDI lookup instead. null STRING Yes No datasource The name of the Carbon datasource that should be used for creating the connection with the database. If this is found, neither the pool properties nor the JNDI resource name described above are taken into account and the connection is attempted via Carbon datasources instead. Only works in Siddhi Distribution null STRING Yes No table.name The name with which the event table should be persisted in the store. If no name is specified via this parameter, the event table is persisted with the same name as the Siddhi table. The table name defined in the Siddhi App query. STRING Yes No field.length The number of characters that the values for fields of the 'STRING' type in the table definition must contain. Each required field must be provided as a comma-separated list of key-value pairs in the ' field.name : length ' format. If this is not specified, the default number of characters specific to the database type is considered. null STRING Yes No table.check.query This query will be used to check whether the table is exist in the given database. But the provided query should return an SQLException if the table does not exist in the database. Furthermore if the provided table is a database view, and it is not exists in the database a table from given name will be created in the database The tableCheckQuery which define in store rdbms configs STRING Yes No use.collation This property allows users to use collation for string attirbutes. By default it's false and binary collation is not used. Currently 'latin1_bin' and 'SQL_Latin1_General_CP1_CS_AS' are used as collations for MySQL and Microsoft SQL database types respectively. false BOOL Yes No System Parameters Name Description Default Value Possible Parameters {{RDBMS-Name}}.maxVersion The latest version supported for {{RDBMS-Name}}. 0 N/A {{RDBMS-Name}}.minVersion The earliest version supported for {{RDBMS-Name}}. 0 N/A {{RDBMS-Name}}.tableCheckQuery The template query for the 'check table' operation in {{RDBMS-Name}}. H2 : CREATE TABLE {{TABLE_NAME}} ({{COLUMNS, PRIMARY_KEYS}}) MySQL : CREATE TABLE {{TABLE_NAME}} ({{COLUMNS, PRIMARY_KEYS}}) Oracle : CREATE TABLE {{TABLE_NAME}} ({{COLUMNS, PRIMARY_KEYS}}) Microsoft SQL Server : CREATE TABLE {{TABLE_NAME}} ({{COLUMNS, PRIMARY_KEYS}}) PostgreSQL : CREATE TABLE {{TABLE_NAME}} ({{COLUMNS, PRIMARY_KEYS}}) DB2. : CREATE TABLE {{TABLE_NAME}} ({{COLUMNS, PRIMARY_KEYS}}) N/A {{RDBMS-Name}}.tableCreateQuery The template query for the 'create table' operation in {{RDBMS-Name}}. H2 : SELECT 1 FROM {{TABLE_NAME}} LIMIT 1 MySQL : SELECT 1 FROM {{TABLE_NAME}} LIMIT 1 Oracle : SELECT 1 FROM {{TABLE_NAME}} WHERE rownum=1 Microsoft SQL Server : SELECT TOP 1 1 from {{TABLE_NAME}} PostgreSQL : SELECT 1 FROM {{TABLE_NAME}} LIMIT 1 DB2. : SELECT 1 FROM {{TABLE_NAME}} FETCH FIRST 1 ROWS ONLY N/A {{RDBMS-Name}}.indexCreateQuery The template query for the 'create index' operation in {{RDBMS-Name}}. H2 : CREATE INDEX {{TABLE_NAME}}_INDEX ON {{TABLE_NAME}} ({{INDEX_COLUMNS}}) MySQL : CREATE INDEX {{TABLE_NAME}}_INDEX ON {{TABLE_NAME}} ({{INDEX_COLUMNS}}) Oracle : CREATE INDEX {{TABLE_NAME}}_INDEX ON {{TABLE_NAME}} ({{INDEX_COLUMNS}}) Microsoft SQL Server : CREATE INDEX {{TABLE_NAME}}_INDEX ON {{TABLE_NAME}} ({{INDEX_COLUMNS}}) {{TABLE_NAME}} ({{INDEX_COLUMNS}}) PostgreSQL : CREATE INDEX {{TABLE_NAME}}_INDEX ON {{TABLE_NAME}} ({{INDEX_COLUMNS}}) DB2. : CREATE INDEX {{TABLE_NAME}}_INDEX ON {{TABLE_NAME}} ({{INDEX_COLUMNS}}) N/A {{RDBMS-Name}}.recordInsertQuery The template query for the 'insert record' operation in {{RDBMS-Name}}. H2 : INSERT INTO {{TABLE_NAME}} ({{COLUMNS}}) VALUES ({{Q}}) MySQL : INSERT INTO {{TABLE_NAME}} ({{COLUMNS}}) VALUES ({{Q}}) Oracle : INSERT INTO {{TABLE_NAME}} ({{COLUMNS}}) VALUES ({{Q}}) Microsoft SQL Server : INSERT INTO {{TABLE_NAME}} ({{COLUMNS}}) VALUES ({{Q}}) PostgreSQL : INSERT INTO {{TABLE_NAME}} ({{COLUMNS}}) VALUES ({{Q}}) DB2. : INSERT INTO {{TABLE_NAME}} ({{COLUMNS}}) VALUES ({{Q}}) N/A {{RDBMS-Name}}.recordUpdateQuery The template query for the 'update record' operation in {{RDBMS-Name}}. H2 : UPDATE {{TABLE_NAME}} SET {{COLUMNS_AND_VALUES}} {{CONDITION}} MySQL : UPDATE {{TABLE_NAME}} SET {{COLUMNS_AND_VALUES}} {{CONDITION}} Oracle : UPDATE {{TABLE_NAME}} SET {{COLUMNS_AND_VALUES}} {{CONDITION}} Microsoft SQL Server : UPDATE {{TABLE_NAME}} SET {{COLUMNS_AND_VALUES}} {{CONDITION}} PostgreSQL : UPDATE {{TABLE_NAME}} SET {{COLUMNS_AND_VALUES}} {{CONDITION}} DB2. : UPDATE {{TABLE_NAME}} SET {{COLUMNS_AND_VALUES}} {{CONDITION}} N/A {{RDBMS-Name}}.recordSelectQuery The template query for the 'select record' operation in {{RDBMS-Name}}. H2 : SELECT * FROM {{TABLE_NAME}} {{CONDITION}} MySQL : SELECT * FROM {{TABLE_NAME}} {{CONDITION}} Oracle : SELECT * FROM {{TABLE_NAME}} {{CONDITION}} Microsoft SQL Server : SELECT * FROM {{TABLE_NAME}} {{CONDITION}} PostgreSQL : SELECT * FROM {{TABLE_NAME}} {{CONDITION}} DB2. : SELECT * FROM {{TABLE_NAME}} {{CONDITION}} N/A {{RDBMS-Name}}.recordExistsQuery The template query for the 'check record existence' operation in {{RDBMS-Name}}. H2 : SELECT TOP 1 1 FROM {{TABLE_NAME}} {{CONDITION}} MySQL : SELECT 1 FROM {{TABLE_NAME}} {{CONDITION}} Oracle : SELECT COUNT(1) INTO existence FROM {{TABLE_NAME}} {{CONDITION}} Microsoft SQL Server : SELECT TOP 1 FROM {{TABLE_NAME}} {{CONDITION}} PostgreSQL : SELECT 1 FROM {{TABLE_NAME}} {{CONDITION}} LIMIT 1 DB2. : SELECT 1 FROM {{TABLE_NAME}} {{CONDITION}} FETCH FIRST 1 ROWS ONLY N/A {{RDBMS-Name}}.recordDeleteQuery The query for the 'delete record' operation in {{RDBMS-Name}}. H2 : DELETE FROM {{TABLE_NAME}} {{CONDITION}} MySQL : DELETE FROM {{TABLE_NAME}} {{CONDITION}} Oracle : DELETE FROM {{TABLE_NAME}} {{CONDITION}} Microsoft SQL Server : DELETE FROM {{TABLE_NAME}} {{CONDITION}} PostgreSQL : DELETE FROM {{TABLE_NAME}} {{CONDITION}} DB2. : DELETE FROM {{TABLE_NAME}} {{CONDITION}} N/A {{RDBMS-Name}}.stringSize This defines the length for the string fields in {{RDBMS-Name}}. H2 : 254 MySQL : 254 Oracle : 254 Microsoft SQL Server : 254 PostgreSQL : 254 DB2. : 254 N/A {{RDBMS-Name}}.fieldSizeLimit This defines the field size limit for select/switch to big string type from the default string type if the 'bigStringType' is available in field type list. H2 : N/A MySQL : N/A Oracle : 2000 Microsoft SQL Server : N/A PostgreSQL : N/A DB2. : N/A 0 = n = INT_MAX {{RDBMS-Name}}.batchSize This defines the batch size when operations are performed for batches of events. H2 : 1000 MySQL : 1000 Oracle : 1000 Microsoft SQL Server : 1000 PostgreSQL : 1000 DB2. : 1000 N/A {{RDBMS-Name}}.batchEnable This specifies whether 'Update' and 'Insert' operations can be performed for batches of events or not. H2 : true MySQL : true Oracle (versions 12.0 and less) : false Oracle (versions 12.1 and above) : true Microsoft SQL Server : true PostgreSQL : true DB2. : true N/A {{RDBMS-Name}}.transactionSupported This is used to specify whether the JDBC connection that is used supports JDBC transactions or not. H2 : true MySQL : true Oracle : true Microsoft SQL Server : true PostgreSQL : true DB2. : true N/A {{RDBMS-Name}}.typeMapping.binaryType This is used to specify the binary data type. An attribute defines as 'object' type in Siddhi stream will be stored into RDBMS with this type. H2 : BLOB MySQL : BLOB Oracle : BLOB Microsoft SQL Server : VARBINARY(max) PostgreSQL : BYTEA DB2. : BLOB(64000) N/A {{RDBMS-Name}}.typeMapping.booleanType This is used to specify the boolean data type. An attribute defines as 'bool' type in Siddhi stream will be stored into RDBMS with this type. H2 : TINYINT(1) MySQL : TINYINT(1) Oracle : NUMBER(1) Microsoft SQL Server : BIT PostgreSQL : BOOLEAN DB2. : SMALLINT N/A {{RDBMS-Name}}.typeMapping.doubleType This is used to specify the double data type. An attribute defines as 'double' type in Siddhi stream will be stored into RDBMS with this type. H2 : DOUBLE MySQL : DOUBLE Oracle : NUMBER(19,4) Microsoft SQL Server : FLOAT(32) PostgreSQL : DOUBLE PRECISION DB2. : DOUBLE N/A {{RDBMS-Name}}.typeMapping.floatType This is used to specify the float data type. An attribute defines as 'float' type in Siddhi stream will be stored into RDBMS with this type. H2 : FLOAT MySQL : FLOAT Oracle : NUMBER(19,4) Microsoft SQL Server : REAL PostgreSQL : REAL DB2. : REAL N/A {{RDBMS-Name}}.typeMapping.integerType This is used to specify the integer data type. An attribute defines as 'int' type in Siddhi stream will be stored into RDBMS with this type. H2 : INTEGER MySQL : INTEGER Oracle : NUMBER(10) Microsoft SQL Server : INTEGER PostgreSQL : INTEGER DB2. : INTEGER N/A {{RDBMS-Name}}.typeMapping.longType This is used to specify the long data type. An attribute defines as 'long' type in Siddhi stream will be stored into RDBMS with this type. H2 : BIGINT MySQL : BIGINT Oracle : NUMBER(19) Microsoft SQL Server : BIGINT PostgreSQL : BIGINT DB2. : BIGINT N/A {{RDBMS-Name}}.typeMapping.stringType This is used to specify the string data type. An attribute defines as 'string' type in Siddhi stream will be stored into RDBMS with this type. H2 : VARCHAR(stringSize) MySQL : VARCHAR(stringSize) Oracle : VARCHAR(stringSize) Microsoft SQL Server : VARCHAR(stringSize) PostgreSQL : VARCHAR(stringSize) DB2. : VARCHAR(stringSize) N/A {{RDBMS-Name}}.typeMapping.bigStringType This is used to specify the big string data type. An attribute defines as 'string' type in Siddhi stream and field.length define in the annotation is greater than the fieldSizeLimit, will be stored into RDBMS with this type. H2 : N/A MySQL : N/A Oracle : CLOB Microsoft SQL Server : N/A PostgreSQL : N/A DB2.* : N/A N/A Examples EXAMPLE 1 @Store(type=\"rdbms\", jdbc.url=\"jdbc:mysql://localhost:3306/stocks\", username=\"root\", password=\"root\", jdbc.driver.name=\"com.mysql.jdbc.Driver\",field.length=\"symbol:100\") @PrimaryKey(\"id\", \"symbol\") @Index(\"volume\") define table StockTable (id string, symbol string, price float, volume long); The above example creates an event table named 'StockTable' in the database if it does not already exist (with four attributes named id , symbol , price , and volume of the types 'string', 'string', 'float', and 'long' respectively). The connection is made as specified by the parameters configured for the '@Store' annotation. The @PrimaryKey() and @Index() annotations can be used to define primary keys or indexes for the table and they follow Siddhi query syntax. RDBMS store supports having more than one attributes in the @PrimaryKey or @Index annotations. In this example a composite Primary key of both attributes id and symbol will be created. EXAMPLE 2 @Store(type=\"rdbms\", jdbc.url=\"jdbc:mysql://localhost:3306/das\", username=\"root\", password=\"root\" , jdbc.driver.name=\"org.h2.Driver\",field.length=\"symbol:100\") @PrimaryKey(\"symbol\") @Index(\"symbol\") define table StockTable (symbol string, price float, volume long); define stream InputStream (symbol string, volume long); from InputStream as a join StockTable as b on str:contains(b.symbol, a.symbol) select a.symbol as symbol, b.volume as volume insert into FooStream; The above example creates an event table named 'StockTable' in the database if it does not already exist (with three attributes named 'symbol', 'price', and 'volume' of the types 'string', 'float' and 'long' respectively). Then the table is joined with a stream named 'InputStream' based on a condition. The following operations are included in the condition: [ AND, OR, Comparisons( = = == !=), IS NULL, NOT, str:contains(Table Column , Stream Attribute or Search.String)] EXAMPLE 3 @Store(type=\"rdbms\", jdbc.url=\"jdbc:mysql://localhost:3306/das\", table.name=\"StockTable\", username=\"root\", password=\"root\" , jdbc.driver.name=\"org.h2.Driver\", field.length=\"symbol:100\", table.check.query=\"SELECT 1 FROM StockTable LIMIT 1\") @PrimaryKey(\"symbol\") @Index(\"symbol\") define table StockTable (symbol string, price float, volume long); define stream InputStream (symbol string, volume long); from InputStream as a join StockTable as b on str:contains(b.symbol, a.symbol) select a.symbol as symbol, b.volume as volume insert into FooStream; The above example creates an event table named 'StockTable' in the database if it does not already exist (with three attributes named 'symbol', 'price', and 'volume' of the types 'string', 'float' and 'long' respectively). Then the table is joined with a stream named 'InputStream' based on a condition. The following operations are included in the condition: [ AND, OR, Comparisons( = = == !=), IS NULL, NOT, str:contains(Table Column , Stream Attribute or Search.String)] redis (Store) This extension assigns data source and connection instructions to event tables. It also implements read write operations on connected datasource. This extension only can be used to read the data which persisted using the same extension since unique implementation has been used to map the relational data in to redis's key and value representation Origin: siddhi-store-redis:3.1.1 Syntax @Store(type=\"redis\", table.name=\" STRING \", cluster.mode=\" BOOL \", nodes=\" STRING \", ttl.seconds=\" LONG \", ttl.on.update=\" BOOL \", ttl.on.read=\" BOOL \") @PrimaryKey(\"PRIMARY_KEY\") @Index(\"INDEX\") QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic table.name The name with which the event table should be persisted in the store. If noname is specified via this parameter, the event table is persisted with the same name as the Siddhi table. The tale name defined in the siddhi app STRING Yes No cluster.mode This will decide the redis mode. if this is false, client will connect to a single redis node. false BOOL No No nodes host, port and the password of the node(s).In single node mode node details can be provided as follows- \"node='hosts:port@password'\" In clustered mode host and port of all the master nodes should be provided separated by a comma(,). As an example \"nodes = 'localhost:30001,localhost:30002'\". localhost:6379@root STRING Yes No ttl.seconds Time to live in seconds for each record -1 LONG Yes No ttl.on.update Set ttl on row update false BOOL Yes No ttl.on.read Set ttl on read rows false BOOL Yes No Examples EXAMPLE 1 @store(type='redis',nodes='localhost:6379@root',table.name='fooTable',cluster.mode=false)define table fooTable(time long, date String) Above example will create a redis table with the name fooTable and work on asingle redis node. EXAMPLE 2 @Store(type='redis', table.name='SweetProductionTable', nodes='localhost:30001,localhost:30002,localhost:30003', cluster.mode='true') @primaryKey('symbol') @index('price') define table SweetProductionTable (symbol string, price float, volume long); Above example demonstrate how to use the redis extension to connect in to redis cluster. Please note that, as nodes all the master node's host and port should be provided in order to work correctly. In clustered node password will not besupported EXAMPLE 3 @store(type='redis',nodes='localhost:6379@root',table.name='fooTable', ttl.seconds='30', ttl.onUpdate='true', ttl.onRead='true')define table fooTable(time long, date String) Above example will create a redis table with the name fooTable and work on asingle redis node. All rows inserted, updated or read will have its ttl set to 30 seconds Str groupConcat (Aggregate Function) This function aggregates the received events by concatenating the keys in those events using a separator, e.g.,a comma (,) or a hyphen (-), and returns the concatenated key string. Origin: siddhi-execution-string:5.0.7 Syntax STRING str:groupConcat( STRING key) STRING str:groupConcat( STRING key, STRING ...) STRING str:groupConcat( STRING key, STRING separator, BOOL distinct) STRING str:groupConcat( STRING key, STRING separator, BOOL distinct, STRING order) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic key The string that needs to be aggregated. STRING No Yes separator The separator that separates each string key after concatenating the keys. , STRING Yes Yes distinct This is used to only have distinct values in the concatenated string that is returned. false BOOL Yes Yes order This parameter accepts 'ASC' or 'DESC' strings to sort the string keys in either ascending or descending order respectively. No order STRING Yes Yes Examples EXAMPLE 1 from InputStream#window.time(5 min) select str:groupConcat(\"key\") as groupedKeys input OutputStream; When we input events having values for the key as 'A' , 'B' , 'S' , 'C' , 'A' , it returns \"A,B,S,C,A\" to the 'OutputStream'. EXAMPLE 2 from InputStream#window.time(5 min) select groupConcat(\"key\",\"-\",true,\"ASC\") as groupedKeys input OutputStream; When we input events having values for the key as 'A' , 'B' , 'S' , 'C' , 'A' , specify the seperator as hyphen and choose the order to be ascending, the function returns \"A-B-C-S\" to the 'OutputStream'. charAt (Function) This function returns the 'char' value that is present at the given index position. of the input string. Origin: siddhi-execution-string:5.0.7 Syntax STRING str:charAt( STRING input.value, INT index) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.value The input string of which the char value at the given position needs to be returned. STRING No Yes index The variable that specifies the index of the char value that needs to be returned. INT No Yes Examples EXAMPLE 1 charAt(\"WSO2\", 1) In this case, the functiion returns the character that exists at index 1. Hence, it returns 'S'. coalesce (Function) This returns the first input parameter value of the given argument, that is not null. Origin: siddhi-execution-string:5.0.7 Syntax INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT str:coalesce( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg, INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT ...) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg It can have one or more input parameters in any data type. However, all the specified parameters are required to be of the same type. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No Yes Examples EXAMPLE 1 coalesce(null, \"BBB\", \"CCC\") This returns the first input parameter that is not null. In this example, it returns \"BBB\". concat (Function) This function returns a string value that is obtained as a result of concatenating two or more input string values. Origin: siddhi-execution-string:5.0.7 Syntax STRING str:concat( STRING arg, STRING ...) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg This can have two or more string type input parameters. STRING No Yes Examples EXAMPLE 1 concat(\"D533\", \"8JU^\", \"XYZ\") This returns a string value by concatenating two or more given arguments. In the example shown above, it returns \"D5338JU^XYZ\". contains (Function) This function returns true if the input.string contains the specified sequence of char values in the search.string . Origin: siddhi-execution-string:5.0.7 Syntax BOOL str:contains( STRING input.string, STRING search.string) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string Input string value. STRING No Yes search.string The string value to be searched for in the input.string . STRING No Yes Examples EXAMPLE 1 contains(\"21 products are produced by WSO2 currently\", \"WSO2\") This returns a boolean value as the output. In this case, it returns true . equalsIgnoreCase (Function) This returns a boolean value by comparing two strings lexicographically without considering the letter case. Origin: siddhi-execution-string:5.0.7 Syntax BOOL str:equalsIgnoreCase( STRING arg1, STRING arg2) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg1 The first input string argument. STRING No Yes arg2 The second input string argument. This is compared with the first argument. STRING No Yes Examples EXAMPLE 1 equalsIgnoreCase(\"WSO2\", \"wso2\") This returns a boolean value as the output. In this scenario, it returns \"true\". fillTemplate (Function) fillTemplate(string, map) will replace all the keys in the string using values in the map. fillTemplate(string, r1, r2 ..) replace all the entries {{1}}, {{2}}, {{3}} with r1 , r2, r3. Origin: siddhi-execution-string:5.0.7 Syntax STRING str:fillTemplate( STRING template, STRING|INT|LONG|DOUBLE|FLOAT|BOOL replacement.type, STRING|INT|LONG|DOUBLE|FLOAT|BOOL ...) STRING str:fillTemplate( STRING template, OBJECT map) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic template The string with templated fields that needs to be filled with the given strings. The format of the templated fields should be as follows: {{KEY}} where 'KEY' is a STRING if you are using fillTemplate(string, map) {{KEY}} where 'KEY' is an INT if you are using fillTemplate(string, r1, r2 ..) This KEY is used to map the values STRING No Yes replacement.type A set of arguments with any type string|int|long|double|float|bool. - STRING INT LONG DOUBLE FLOAT BOOL Yes Yes map A map with key-value pairs to be replaced. - OBJECT Yes Yes Examples EXAMPLE 1 str:fillTemplate(\"{{prize}} 100 {{salary}} 10000\", map:create('prize', 300, 'salary', 10000)) In this example, the template is '{{prize}} 100 {{salary}} 10000'.Here, the templated string {{prize}} is replaced with the value corresponding to the 'prize' key in the given map. Likewise salary replace with the salary value of the map EXAMPLE 2 str:fillTemplate(\"{{1}} 100 {{2}} 10000\", 200, 300) In this example, the template is '{{1}} 100 {{2}} 10000'.Here, the templated string {{1}} is replaced with the corresponding 1 st value 200. Likewise {{2}} replace with the 300 hex (Function) This function returns a hexadecimal string by converting each byte of each character in the input string to two hexadecimal digits. Origin: siddhi-execution-string:5.0.7 Syntax STRING str:hex( STRING input.string) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string The input string to derive the hexadecimal value. STRING No Yes Examples EXAMPLE 1 hex(\"MySQL\") This returns the hexadecimal value of the input.string. In this scenario, the output is \"4d7953514c\". length (Function) Returns the length of the input string. Origin: siddhi-execution-string:5.0.7 Syntax INT str:length( STRING input.string) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string The input string to derive the length. STRING No Yes Examples EXAMPLE 1 length(\"Hello World\") This outputs the length of the provided string. In this scenario, the, output is 11 . lower (Function) Converts the capital letters in the input string to the equivalent simple letters. Origin: siddhi-execution-string:5.0.7 Syntax STRING str:lower( STRING input.string) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string The input string to convert to the lower case (i.e., equivalent simple letters). STRING No Yes Examples EXAMPLE 1 lower(\"WSO2 cep \") This converts the capital letters in the input.string to the equivalent simple letters. In this scenario, the output is \"wso2 cep \". regexp (Function) Returns a boolean value based on the matchability of the input string and the given regular expression. Origin: siddhi-execution-string:5.0.7 Syntax BOOL str:regexp( STRING input.string, STRING regex) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string The input string to match with the given regular expression. STRING No Yes regex The regular expression to be matched with the input string. STRING No Yes Examples EXAMPLE 1 regexp(\"WSO2 abcdh\", \"WSO(.*h)\") This returns a boolean value after matching regular expression with the given string. In this scenario, it returns \"true\" as the output. repeat (Function) Repeats the input string for a specified number of times. Origin: siddhi-execution-string:5.0.7 Syntax STRING str:repeat( STRING input.string, INT times) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string The input string that is repeated the number of times as defined by the user. STRING No Yes times The number of times the input.string needs to be repeated . INT No Yes Examples EXAMPLE 1 repeat(\"StRing 1\", 3) This returns a string value by repeating the string for a specified number of times. In this scenario, the output is \"StRing 1StRing 1StRing 1\". replaceAll (Function) Finds all the substrings of the input string that matches with the given expression, and replaces them with the given replacement string. Origin: siddhi-execution-string:5.0.7 Syntax STRING str:replaceAll( STRING input.string, STRING regex, STRING replacement.string) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string The input string to be replaced. STRING No Yes regex The regular expression to be matched with the input string. STRING No Yes replacement.string The string with which each substring that matches the given expression should be replaced. STRING No Yes Examples EXAMPLE 1 replaceAll(\"hello hi hello\", 'hello', 'test') This returns a string after replacing the substrings of the input string with the replacement string. In this scenario, the output is \"test hi test\" . replaceFirst (Function) Finds the first substring of the input string that matches with the given regular expression, and replaces itwith the given replacement string. Origin: siddhi-execution-string:5.0.7 Syntax STRING str:replaceFirst( STRING input.string, STRING regex, STRING replacement.string) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string The input string that should be replaced. STRING No Yes regex The regular expression with which the input string should be matched. STRING No Yes replacement.string The string with which the first substring of input string that matches the regular expression should be replaced. STRING No Yes Examples EXAMPLE 1 replaceFirst(\"hello WSO2 A hello\", 'WSO2(.*)A', 'XXXX') This returns a string after replacing the first substring with the given replacement string. In this scenario, the output is \"hello XXXX hello\". reverse (Function) Returns the input string in the reverse order character-wise and string-wise. Origin: siddhi-execution-string:5.0.7 Syntax STRING str:reverse( STRING input.string) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string The input string to be reversed. STRING No Yes Examples EXAMPLE 1 reverse(\"Hello World\") This outputs a string value by reversing the incoming input.string . In this scenario, the output is \"dlroW olleH\". split (Function) Splits the input.string into substrings using the value parsed in the split.string and returns the substring at the position specified in the group.number . Origin: siddhi-execution-string:5.0.7 Syntax STRING str:split( STRING input.string, STRING split.string, INT group.number) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string The input string to be replaced. STRING No Yes split.string The string value to be used to split the input.string . STRING No Yes group.number The index of the split group INT No Yes Examples EXAMPLE 1 split(\"WSO2,ABM,NSFT\", \",\", 0) This splits the given input.string by given split.string and returns the string in the index given by group.number. In this scenario, the output will is \"WSO2\". strcmp (Function) Compares two strings lexicographically and returns an integer value. If both strings are equal, 0 is returned. If the first string is lexicographically greater than the second string, a positive value is returned. If the first string is lexicographically greater than the second string, a negative value is returned. Origin: siddhi-execution-string:5.0.7 Syntax INT str:strcmp( STRING arg1, STRING arg2) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg1 The first input string argument. STRING No Yes arg2 The second input string argument that should be compared with the first argument lexicographically. STRING No Yes Examples EXAMPLE 1 strcmp(\"AbCDefghiJ KLMN\", 'Hello') This compares two strings lexicographically and outputs an integer value. substr (Function) Returns a substring of the input string by considering a subset or all of the following factors: starting index, length, regular expression, and regex group number. Origin: siddhi-execution-string:5.0.7 Syntax STRING str:substr( STRING input.string, INT begin.index) STRING str:substr( STRING input.string, INT begin.index, INT length) STRING str:substr( STRING input.string, STRING regex) STRING str:substr( STRING input.string, STRING regex, INT group.number) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string The input string to be processed. STRING No Yes begin.index Starting index to consider for the substring. - INT Yes Yes length The length of the substring. input.string .length - begin.index INT Yes Yes regex The regular expression that should be matched with the input string. - STRING Yes Yes group.number The regex group number 0 INT Yes Yes Examples EXAMPLE 1 substr(\"AbCDefghiJ KLMN\", 4) This outputs the substring based on the given begin.index . In this scenario, the output is \"efghiJ KLMN\". EXAMPLE 2 substr(\"AbCDefghiJ KLMN\", 2, 4) This outputs the substring based on the given begin.index and length. In this scenario, the output is \"CDef\". EXAMPLE 3 substr(\"WSO2D efghiJ KLMN\", '^WSO2(.*)') This outputs the substring by applying the regex. In this scenario, the output is \"WSO2D efghiJ KLMN\". EXAMPLE 4 substr(\"WSO2 cep WSO2 XX E hi hA WSO2 heAllo\", 'WSO2(.*)A(.*)', 2) This outputs the substring by applying the regex and considering the group.number . In this scenario, the output is \" ello\". trim (Function) Returns a copy of the input string without the leading and trailing whitespace (if any). Origin: siddhi-execution-string:5.0.7 Syntax STRING str:trim( STRING input.string) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string The input string that needs to be trimmed. STRING No Yes Examples EXAMPLE 1 trim(\" AbCDefghiJ KLMN \") This returns a copy of the input.string with the leading and/or trailing white-spaces omitted. In this scenario, the output is \"AbCDefghiJ KLMN\". unhex (Function) Returns a string by converting the hexadecimal characters in the input string. Origin: siddhi-execution-string:5.0.7 Syntax STRING str:unhex( STRING input.string) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string The hexadecimal input string that needs to be converted to string. STRING No Yes Examples EXAMPLE 1 unhex(\"4d7953514c\") This converts the hexadecimal value to string. upper (Function) Converts the simple letters in the input string to the equivalent capital/block letters. Origin: siddhi-execution-string:5.0.7 Syntax STRING str:upper( STRING input.string) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string The input string that should be converted to the upper case (equivalent capital/block letters). STRING No Yes Examples EXAMPLE 1 upper(\"Hello World\") This converts the simple letters in the input.string to theequivalent capital letters. In this scenario, the output is \"HELLO WORLD\". tokenize (Stream Processor) This function splits the input string into tokens using a given regular expression and returns the split tokens. Origin: siddhi-execution-string:5.0.7 Syntax str:tokenize( STRING input.string, STRING regex) str:tokenize( STRING input.string, STRING regex, BOOL distinct) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string The input string which needs to be split. STRING No Yes regex The string value which is used to tokenize the 'input.string'. STRING No Yes distinct This flag is used to return only distinct values. false BOOL Yes Yes Extra Return Attributes Name Description Possible Types token The attribute which contains a single token. STRING Examples EXAMPLE 1 define stream inputStream (str string); @info(name = 'query1') from inputStream#str:tokenize(str , ',') select token insert into outputStream; This query performs tokenization on the given string. If the str is \"Android,Windows8,iOS\", then the string is split into 3 events containing the token attribute values, i.e., Android , Windows8 and iOS . Time currentDate (Function) Function returns the system time in yyyy-MM-dd format. Origin: siddhi-execution-time:5.0.4 Syntax STRING time:currentDate() Examples EXAMPLE 1 time:currentDate() Returns the current date in the yyyy-MM-dd format, such as 2019-06-21 . currentTime (Function) Function returns system time in the HH ss format. Origin: siddhi-execution-time:5.0.4 Syntax STRING time:currentTime() Examples EXAMPLE 1 time:currentTime() Returns the current date in the HH ss format, such as 15:23:24 . currentTimestamp (Function) When no argument is provided, function returns the system current timestamp in yyyy-MM-dd HH ss format, and when a timezone is provided as an argument, it converts and return the current system time to the given timezone format. Origin: siddhi-execution-time:5.0.4 Syntax STRING time:currentTimestamp() STRING time:currentTimestamp( STRING timezone) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic timezone The timezone to which the current time need to be converted. For example, Asia/Kolkata , PST . Get the supported timezone IDs from here System timezone STRING Yes No Examples EXAMPLE 1 time:currentTimestamp() Returns current system time in yyyy-MM-dd HH ss format, such as 2019-03-31 14:07:00 . EXAMPLE 2 time:currentTimestamp('Asia/Kolkata') Returns current system time converted to 'Asia/Kolkata' timezone yyyy-MM-dd HH ss format, such as 2019-03-31 19:07:00 . Get the supported timezone IDs from here EXAMPLE 3 time:currentTimestamp('CST') Returns current system time converted to 'CST' timezone yyyy-MM-dd HH ss format, such as 2019-03-31 02:07:00 . Get the supported timezone IDs from here date (Function) Extracts the date part of a date or date-time and return it in yyyy-MM-dd format. Origin: siddhi-execution-time:5.0.4 Syntax STRING time:date( STRING date.value, STRING date.format) STRING time:date( STRING date.value) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic date.value The value of the date. For example, 2014-11-11 13:23:44.657 , 2014-11-11 , 13:23:44.657 . STRING No Yes date.format The format of the date value provided. For example, yyyy/MM/dd HH ss.SSS . yyyy-MM-dd HH:mm:ss.SSS STRING Yes Yes Examples EXAMPLE 1 time:date('2014/11/11 13:23:44', 'yyyy/MM/dd HH:mm:ss') Extracts the date and returns 2014-11-11 . EXAMPLE 2 time:date('2014-11-23 13:23:44.345') Extracts the date and returns 2014-11-13 . EXAMPLE 3 time:date('13:23:44', 'HH:mm:ss') Extracts the date and returns 1970-01-01 . dateAdd (Function) Adds the specified time interval to a date. Origin: siddhi-execution-time:5.0.4 Syntax STRING time:dateAdd( STRING date.value, INT expr, STRING unit) STRING time:dateAdd( LONG timestamp.in.milliseconds, INT expr, STRING unit) STRING time:dateAdd( STRING date.value, INT expr, STRING unit, STRING date.format) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic date.value The value of the date. For example, 2014-11-11 13:23:44.657 , 2014-11-11 , 13:23:44.657 . - STRING Yes Yes expr The amount by which the selected part of the date should be incremented. For example 2 , 5 , 10 , etc. INT No Yes unit This is the part of the date that needs to be modified. For example, MINUTE , HOUR , MONTH , YEAR , QUARTER , WEEK , DAY , SECOND . STRING No No date.format The format of the date value provided. For example, yyyy-MM-dd HH ss.SSS . yyyy-MM-dd HH:mm:ss.SSS STRING Yes Yes timestamp.in.milliseconds The date value in milliseconds. For example, 1415712224000L . - LONG Yes Yes Examples EXAMPLE 1 time:dateAdd('2014-11-11 13:23:44.657', 5, 'YEAR', 'yyyy-MM-dd HH:mm:ss.SSS') Adds five years to the given date value and returns 2019-11-11 13:23:44.657 . EXAMPLE 2 time:dateAdd('2014-11-11 13:23:44.657', 5, 'YEAR') Adds five years to the given date value and returns 2019-11-11 13:23:44.657 using the default date.format yyyy-MM-dd HH ss.SSS . EXAMPLE 3 time:dateAdd( 1415712224000L, 1, 'HOUR') Adds one hour and 1415715824000 as a string . dateDiff (Function) Returns difference between two dates in days. Origin: siddhi-execution-time:5.0.4 Syntax INT time:dateDiff( STRING date.value1, STRING date.value2, STRING date.format1, STRING date.format2) INT time:dateDiff( STRING date.value1, STRING date.value2) INT time:dateDiff( LONG timestamp.in.milliseconds1, LONG timestamp.in.milliseconds2) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic date.value1 The value of the first date parameter. For example, 2014-11-11 13:23:44.657 , 2014-11-11 , 13:23:44.657 . - STRING Yes Yes date.value2 The value of the second date parameter. For example, 2014-11-11 13:23:44.657 , 2014-11-11 , 13:23:44.657 . - STRING Yes Yes date.format1 The format of the first date value provided. For example, yyyy-MM-dd HH ss.SSS . yyyy-MM-dd HH:mm:ss.SSS STRING Yes Yes date.format2 The format of the second date value provided. For example, yyyy-MM-dd HH ss.SSS . yyyy-MM-dd HH:mm:ss.SSS STRING Yes Yes timestamp.in.milliseconds1 The first date value in milliseconds from the epoch. For example, 1415712224000L . - LONG Yes Yes timestamp.in.milliseconds2 The second date value in milliseconds from the epoch. For example, 1415712224000L . - LONG Yes Yes Examples EXAMPLE 1 time:dateDiff('2014-11-11 13:23:44', '2014-11-9 13:23:44', 'yyyy-MM-dd HH:mm:ss', 'yyyy-MM-dd HH:mm:ss') Returns the date difference between the two given dates as 2 . EXAMPLE 2 time:dateDiff('2014-11-13 13:23:44', '2014-11-9 13:23:44') Returns the date difference between the two given dates as 4 . EXAMPLE 3 time:dateDiff(1415692424000L, 1412841224000L) Returns the date difference between the two given dates as 33 . dateFormat (Function) Formats the data in string or milliseconds format to the given date format. Origin: siddhi-execution-time:5.0.4 Syntax STRING time:dateFormat( STRING date.value, STRING date.target.format, STRING date.source.format) STRING time:dateFormat( STRING date.value, STRING date.target.format) STRING time:dateFormat( LONG timestamp.in.milliseconds, STRING date.target.format) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic date.value The value of the date. For example, 2014-11-11 13:23:44.657 , 2014-11-11 , 13:23:44.657 . - STRING Yes Yes date.target.format The format of the date into which the date value needs to be converted. For example, yyyy/MM/dd HH ss . STRING No Yes date.source.format The format input date.value.For example, yyyy-MM-dd HH ss.SSS . yyyy-MM-dd HH:mm:ss.SSS STRING Yes Yes timestamp.in.milliseconds The date value in milliseconds from the epoch. For example, 1415712224000L . - LONG Yes Yes Examples EXAMPLE 1 time:dateFormat('2014/11/11 13:23:44', 'mm:ss', 'yyyy/MM/dd HH:mm:ss') Converts date based on the target date format mm:ss and returns 23:44 . EXAMPLE 2 time:dateFormat('2014-11-11 13:23:44', 'HH:mm:ss') Converts date based on the target date format HH ss and returns 13:23:44 . EXAMPLE 3 time:dateFormat(1415692424000L, 'yyyy-MM-dd') Converts date in millisecond based on the target date format yyyy-MM-dd and returns 2014-11-11 . dateSub (Function) Subtracts the specified time interval from the given date. Origin: siddhi-execution-time:5.0.4 Syntax STRING time:dateSub( STRING date.value, INT expr, STRING unit) STRING time:dateSub( STRING date.value, INT expr, STRING unit, STRING date.format) STRING time:dateSub( LONG timestamp.in.milliseconds, INT expr, STRING unit) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic date.value The value of the date. For example, 2014-11-11 13:23:44.657 , 2014-11-11 , 13:23:44.657 . - STRING Yes Yes expr The amount by which the selected part of the date should be decremented. For example 2 , 5 , 10 , etc. INT No Yes unit This is the part of the date that needs to be modified. For example, MINUTE , HOUR , MONTH , YEAR , QUARTER , WEEK , DAY , SECOND . STRING No No date.format The format of the date value provided. For example, yyyy-MM-dd HH ss.SSS . yyyy-MM-dd HH:mm:ss.SSS STRING Yes Yes timestamp.in.milliseconds The date value in milliseconds. For example, 1415712224000L . - LONG Yes Yes Examples EXAMPLE 1 time:dateSub('2019-11-11 13:23:44.657', 5, 'YEAR', 'yyyy-MM-dd HH:mm:ss.SSS') Subtracts five years to the given date value and returns 2014-11-11 13:23:44.657 . EXAMPLE 2 time:dateSub('2019-11-11 13:23:44.657', 5, 'YEAR') Subtracts five years to the given date value and returns 2014-11-11 13:23:44.657 using the default date.format yyyy-MM-dd HH ss.SSS . EXAMPLE 3 time:dateSub( 1415715824000L, 1, 'HOUR') Subtracts one hour and 1415712224000 as a string . dayOfWeek (Function) Extracts the day on which a given date falls. Origin: siddhi-execution-time:5.0.4 Syntax STRING time:dayOfWeek( STRING date.value, STRING date.format) STRING time:dayOfWeek( STRING date.value) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic date.value The value of the date. For example, 2014-11-11 13:23:44.657 , 2014-11-11 , 13:23:44.657 . STRING No Yes date.format The format of the date value provided. For example, yyyy/MM/dd HH ss.SSS . yyyy-MM-dd HH:mm:ss.SSS STRING Yes Yes Examples EXAMPLE 1 time:date('2014/12/11 13:23:44', 'yyyy/MM/dd HH:mm:ss') Extracts the date and returns Thursday . EXAMPLE 2 time:date('2014-11-11 13:23:44.345') Extracts the date and returns Tuesday . extract (Function) Function extracts a date unit from the date. Origin: siddhi-execution-time:5.0.4 Syntax INT time:extract( STRING unit, STRING date.value) INT time:extract( STRING unit, STRING date.value, STRING date.format) INT time:extract( STRING unit, STRING date.value, STRING date.format, STRING locale) INT time:extract( LONG timestamp.in.milliseconds, STRING unit) INT time:extract( LONG timestamp.in.milliseconds, STRING unit, STRING locale) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic unit This is the part of the date that needs to be modified. For example, MINUTE , HOUR , MONTH , YEAR , QUARTER , WEEK , DAY , SECOND . STRING No No date.value The value of the date. For example, 2014-11-11 13:23:44.657 , 2014-11-11 , 13:23:44.657 . - STRING Yes Yes date.format The format of the date value provided. For example, yyyy-MM-dd HH ss.SSS . yyyy-MM-dd HH:mm:ss.SSS STRING Yes Yes timestamp.in.milliseconds The date value in milliseconds. For example, 1415712224000L . - LONG Yes Yes locale Represents a specific geographical, political or cultural region. For example en_US and fr_FR Current default locale set in the Java Virtual Machine. STRING Yes No Examples EXAMPLE 1 time:extract('YEAR', '2019/11/11 13:23:44.657', 'yyyy/MM/dd HH:mm:ss.SSS') Extracts the year amount and returns 2019 . EXAMPLE 2 time:extract('DAY', '2019-11-12 13:23:44.657') Extracts the day amount and returns 12 . EXAMPLE 3 time:extract(1394556804000L, 'HOUR') Extracts the hour amount and returns 22 . timestampInMilliseconds (Function) Returns the system time or the given time in milliseconds. Origin: siddhi-execution-time:5.0.4 Syntax LONG time:timestampInMilliseconds() LONG time:timestampInMilliseconds( STRING date.value, STRING date.format) LONG time:timestampInMilliseconds( STRING date.value) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic date.value The value of the date. For example, 2014-11-11 13:23:44.657 , 2014-11-11 , 13:23:44.657 . Current system time STRING Yes Yes date.format The format of the date value provided. For example, yyyy/MM/dd HH ss.SSS . yyyy-MM-dd HH:mm:ss.SSS STRING Yes Yes Examples EXAMPLE 1 time:timestampInMilliseconds() Returns the system current time in milliseconds. EXAMPLE 2 time:timestampInMilliseconds('2007-11-30 10:30:19', 'yyyy-MM-DD HH:MM:SS') Converts 2007-11-30 10:30:19 in yyyy-MM-DD HH:MM:SS format to milliseconds as 1170131400019 . EXAMPLE 3 time:timestampInMilliseconds('2007-11-30 10:30:19.000') Converts 2007-11-30 10:30:19 in yyyy-MM-DD HH:MM:ss.SSS format to milliseconds as 1196398819000 . utcTimestamp (Function) Function returns the system current time in UTC timezone with yyyy-MM-dd HH ss format. Origin: siddhi-execution-time:5.0.4 Syntax STRING time:utcTimestamp() Examples EXAMPLE 1 time:utcTimestamp() Returns the system current time in UTC timezone with yyyy-MM-dd HH ss format, and a sample output will be like 2019-07-03 09:58:34 . Unique deduplicate (Stream Processor) Removes duplicate events based on the unique.key parameter that arrive within the time.interval gap from one another. Origin: siddhi-execution-unique:5.0.5 Syntax unique:deduplicate( INT|LONG|FLOAT|BOOL|DOUBLE|STRING unique.key, INT|LONG time.interval) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic unique.key Parameter to uniquely identify events. INT LONG FLOAT BOOL DOUBLE STRING No Yes time.interval The sliding time period within which the duplicate events are dropped. INT LONG No No Examples EXAMPLE 1 define stream TemperatureStream (sensorId string, temperature double) from TemperatureStream#unique:deduplicate(sensorId, 30 sec) select * insert into UniqueTemperatureStream; Query that removes duplicate events of TemperatureStream stream based on sensorId attribute when they arrive within 30 seconds. ever (Window) Window that retains the latest events based on a given unique keys. When a new event arrives with the same key it replaces the one that exist in the window. b This function is not recommended to be used when the maximum number of unique attributes are undefined, as there is a risk of system going out to memory /b . Origin: siddhi-execution-unique:5.0.5 Syntax unique:ever( INT|LONG|FLOAT|BOOL|DOUBLE|STRING unique.key) unique:ever( INT|LONG|FLOAT|BOOL|DOUBLE|STRING unique.key, INT|LONG|FLOAT|BOOL|DOUBLE|STRING ...) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic unique.key The attribute used to checked for uniqueness. INT LONG FLOAT BOOL DOUBLE STRING No Yes Examples EXAMPLE 1 define stream LoginEvents (timestamp long, ip string); from LoginEvents#window.unique:ever(ip) select count(ip) as ipCount insert events into UniqueIps; Query collects all unique events based on the ip attribute by retaining the latest unique events from the LoginEvents stream. Then the query counts the unique ip s arrived so far and outputs the ipCount via the UniqueIps stream. EXAMPLE 2 define stream DriverChangeStream (trainID string, driver string); from DriverChangeStream#window.unique:ever(trainID) select trainID, driver insert expired events into PreviousDriverChangeStream; Query collects all unique events based on the trainID attribute by retaining the latest unique events from the DriverChangeStream stream. The query outputs the previous unique event stored in the window as the expired events are emitted via PreviousDriverChangeStream stream. EXAMPLE 3 define stream StockStream (symbol string, price float); define stream PriceRequestStream(symbol string); from StockStream#window.unique:ever(symbol) as s join PriceRequestStream as p on s.symbol == p.symbol select s.symbol as symbol, s.price as price insert events into PriceResponseStream; Query stores the last unique event for each symbol attribute of StockStream stream, and joins them with events arriving on the PriceRequestStream for equal symbol attributes to fetch the latest price for each requested symbol and output via PriceResponseStream stream. externalTimeBatch (Window) This is a batch (tumbling) time window that is determined based on an external time, i.e., time stamps that are specified via an attribute in the events. It holds the latest unique events that arrived during the last window time period. The unique events are determined based on the value for a specified unique key parameter. When a new event arrives within the time window with a value for the unique key parameter that is the same as that of an existing event in the window, the existing event expires and it is replaced by the new event. Origin: siddhi-execution-unique:5.0.5 Syntax unique:externalTimeBatch( INT|LONG|FLOAT|BOOL|DOUBLE|STRING unique.key, LONG time.stamp, INT|LONG window.time) unique:externalTimeBatch( INT|LONG|FLOAT|BOOL|DOUBLE|STRING unique.key, LONG time.stamp, INT|LONG window.time, INT start.time) unique:externalTimeBatch( INT|LONG|FLOAT|BOOL|DOUBLE|STRING unique.key, LONG time.stamp, INT|LONG window.time, INT start.time, INT|LONG time.out) unique:externalTimeBatch( INT|LONG|FLOAT|BOOL|DOUBLE|STRING unique.key, LONG time.stamp, INT|LONG window.time, INT start.time, INT|LONG time.out, BOOL replace.time.stamp.with.batch.end.time) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic unique.key The attribute that should be checked for uniqueness. INT LONG FLOAT BOOL DOUBLE STRING No Yes time.stamp The time which the window determines as the current time and acts upon. The value of this parameter should be monotonically increasing. LONG No Yes window.time The sliding time period for which the window should hold events. INT LONG No No start.time This specifies an offset in milliseconds in order to start the window at a time different to the standard time. Timestamp of first event INT Yes No time.out Time to wait for arrival of a new event, before flushing and returning the output for events belonging to a specific batch. The system waits till an event from the next batch arrives to flush the current batch INT LONG Yes No replace.time.stamp.with.batch.end.time Replaces the 'timestamp' value with the corresponding batch end time stamp. false BOOL Yes No Examples EXAMPLE 1 define stream LoginEvents (timestamp long, ip string); from LoginEvents#window.unique:externalTimeBatch(ip, timestamp, 1 sec, 0, 2 sec) select timestamp, ip, count() as total insert into UniqueIps ; In this query, the window holds the latest unique events that arrive from the 'LoginEvent' stream during each second. The latest events are determined based on the external time stamp. At a given time, all the events held in the window have unique values for the 'ip' and monotonically increasing values for 'timestamp' attributes. The events in the window are inserted into the 'UniqueIps' output stream. The system waits for 2 seconds for the arrival of a new event before flushing the current batch. first (Window) This is a window that holds only the first set of unique events according to the unique key parameter. When a new event arrives with a key that is already in the window, that event is not processed by the window. Origin: siddhi-execution-unique:5.0.5 Syntax unique:first( INT|LONG|FLOAT|BOOL|DOUBLE|STRING unique.key) unique:first( INT|LONG|FLOAT|BOOL|DOUBLE|STRING unique.key, INT|LONG|FLOAT|BOOL|DOUBLE|STRING ...) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic unique.key The attribute that should be checked for uniqueness. If there is more than one parameter to check for uniqueness, it can be specified as an array separated by commas. INT LONG FLOAT BOOL DOUBLE STRING No Yes Examples EXAMPLE 1 define stream LoginEvents (timeStamp long, ip string); from LoginEvents#window.unique:first(ip) insert into UniqueIps ; This returns the first set of unique items that arrive from the 'LoginEvents' stream, and returns them to the 'UniqueIps' stream. The unique events are only those with a unique value for the 'ip' attribute. firstLengthBatch (Window) This is a batch (tumbling) window that holds a specific number of unique events (depending on which events arrive first). The unique events are selected based on a specific parameter that is considered as the unique key. When a new event arrives with a value for the unique key parameter that matches the same of an existing event in the window, that event is not processed by the window. Origin: siddhi-execution-unique:5.0.5 Syntax unique:firstLengthBatch( INT|LONG|FLOAT|BOOL|DOUBLE|STRING unique.key, INT window.length) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic unique.key The attribute that should be checked for uniqueness. INT LONG FLOAT BOOL DOUBLE STRING No Yes window.length The number of events the window should tumble. INT No No Examples EXAMPLE 1 define window CseEventWindow (symbol string, price float, volume int) from CseEventStream#window.unique:firstLengthBatch(symbol, 10) select symbol, price, volume insert all events into OutputStream ; The window in this configuration holds the first unique events from the 'CseEventStream' stream every second, and outputs them all into the the 'OutputStream' stream. All the events in a window during a given second should have a unique value for the 'symbol' attribute. firstTimeBatch (Window) A batch-time or tumbling window that holds the unique events according to the unique key parameters that have arrived within the time period of that window and gets updated for each such time window. When a new event arrives with a key which is already in the window, that event is not processed by the window. Origin: siddhi-execution-unique:5.0.5 Syntax unique:firstTimeBatch( INT|LONG|FLOAT|BOOL|DOUBLE|STRING unique.key, INT|LONG window.time) unique:firstTimeBatch( INT|LONG|FLOAT|BOOL|DOUBLE|STRING unique.key, INT|LONG window.time, INT|LONG start.time) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic unique.key The attribute that should be checked for uniqueness. INT LONG FLOAT BOOL DOUBLE STRING No Yes window.time The sliding time period for which the window should hold events. INT LONG No No start.time This specifies an offset in milliseconds in order to start the window at a time different to the standard time. Timestamp of the first event. INT LONG Yes No Examples EXAMPLE 1 define stream CseEventStream (symbol string, price float, volume int) from CseEventStream#window.unique:firstTimeBatch(symbol,1 sec) select symbol, price, volume insert all events into OutputStream ; This holds the first unique events that arrive from the 'cseEventStream' input stream during each second, based on the symbol,as a batch, and returns all the events to the 'OutputStream'. length (Window) This is a sliding length window that holds the events of the latest window length with the unique key and gets updated for the expiry and arrival of each event. When a new event arrives with the key that is already there in the window, then the previous event expires and new event is kept within the window. Origin: siddhi-execution-unique:5.0.5 Syntax unique:length( INT|LONG|FLOAT|BOOL|DOUBLE|STRING unique.key, INT window.length) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic unique.key The attribute that should be checked for uniqueness. INT LONG FLOAT BOOL DOUBLE STRING No Yes window.length The number of events that should be included in a sliding length window. INT No No Examples EXAMPLE 1 define stream CseEventStream (symbol string, price float, volume int) from CseEventStream#window.unique:length(symbol,10) select symbol, price, volume insert all events into OutputStream; In this configuration, the window holds the latest 10 unique events. The latest events are selected based on the symbol attribute. If the 'CseEventStream' receives an event for which the value for the symbol attribute is the same as that of an existing event in the window, the existing event is replaced by the new event. All the events are returned to the 'OutputStream' event stream once an event expires or is added to the window. lengthBatch (Window) This is a batch (tumbling) window that holds a specified number of latest unique events. The unique events are determined based on the value for a specified unique key parameter. The window is updated for every window length, i.e., for the last set of events of the specified number in a tumbling manner. When a new event arrives within the window length having the same value for the unique key parameter as an existing event in the window, the previous event is replaced by the new event. Origin: siddhi-execution-unique:5.0.5 Syntax unique:lengthBatch( INT|LONG|FLOAT|BOOL|DOUBLE|STRING unique.key, INT window.length) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic unique.key The attribute that should be checked for uniqueness. INT LONG FLOAT BOOL DOUBLE STRING No Yes window.length The number of events the window should tumble. INT No No Examples EXAMPLE 1 define window CseEventWindow (symbol string, price float, volume int) from CseEventStream#window.unique:lengthBatch(symbol, 10) select symbol, price, volume insert expired events into OutputStream ; In this query, the window at any give time holds the last 10 unique events from the 'CseEventStream' stream. Each of the 10 events within the window at a given time has a unique value for the symbol attribute. If a new event has the same value for the symbol attribute as an existing event within the window length, the existing event expires and it is replaced by the new event. The query returns expired individual events as well as expired batches of events to the 'OutputStream' stream. time (Window) This is a sliding time window that holds the latest unique events that arrived during the previous time window. The unique events are determined based on the value for a specified unique key parameter. The window is updated with the arrival and expiry of each event. When a new event that arrives within a window time period has the same value for the unique key parameter as an existing event in the window, the previous event is replaced by the new event. Origin: siddhi-execution-unique:5.0.5 Syntax unique:time( INT|LONG|FLOAT|BOOL|DOUBLE|STRING unique.key, INT|LONG window.time) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic unique.key The attribute that should be checked for uniqueness. INT LONG FLOAT BOOL DOUBLE STRING No Yes window.time The sliding time period for which the window should hold events. INT LONG No No Examples EXAMPLE 1 define stream CseEventStream (symbol string, price float, volume int) from CseEventStream#window.unique:time(symbol, 1 sec) select symbol, price, volume insert expired events into OutputStream ; In this query, the window holds the latest unique events that arrived within the last second from the 'CseEventStream', and returns the expired events to the 'OutputStream' stream. During any given second, each event in the window should have a unique value for the 'symbol' attribute. If a new event that arrives within the same second has the same value for the symbol attribute as an existing event in the window, the existing event expires. timeBatch (Window) This is a batch (tumbling) time window that is updated with the latest events based on a unique key parameter. If a new event that arrives within the time period of a windowhas a value for the key parameter which matches that of an existing event, the existing event expires and it is replaced by the latest event. Origin: siddhi-execution-unique:5.0.5 Syntax unique:timeBatch( INT|LONG|FLOAT|BOOL|DOUBLE|STRING unique.key, INT|LONG window.time) unique:timeBatch( INT|LONG|FLOAT|BOOL|DOUBLE|STRING unique.key, INT|LONG window.time, INT|LONG start.time) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic unique.key The attribute that should be checked for uniqueness. INT LONG FLOAT BOOL DOUBLE STRING No Yes window.time The tumbling time period for which the window should hold events. INT LONG No No start.time This specifies an offset in milliseconds in order to start the window at a time different to the standard time. Timestamp of first event INT LONG Yes No Examples EXAMPLE 1 define stream CseEventStream (symbol string, price float, volume int) from CseEventStream#window.unique:timeBatch(symbol, 1 sec) select symbol, price, volume insert all events into OutputStream ; This window holds the latest unique events that arrive from the 'CseEventStream' at a given time, and returns all the events to the 'OutputStream' stream. It is updated every second based on the latest values for the 'symbol' attribute. timeLengthBatch (Window) This is a batch or tumbling time length window that is updated with the latest events based on a unique key parameter. The window tumbles upon the elapse of the time window, or when a number of unique events have arrived. If a new event that arrives within the period of the window has a value for the key parameter which matches the value of an existing event, the existing event expires and it is replaced by the new event. Origin: siddhi-execution-unique:5.0.5 Syntax unique:timeLengthBatch( INT|LONG|FLOAT|BOOL|DOUBLE|STRING unique.key, INT|LONG window.time, INT window.length) unique:timeLengthBatch( INT|LONG|FLOAT|BOOL|DOUBLE|STRING unique.key, INT|LONG window.time, INT|LONG start.time, INT window.length) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic unique.key The attribute that should be checked for uniqueness. INT LONG FLOAT BOOL DOUBLE STRING No Yes window.time The sliding time period for which the window should hold the events. INT LONG No No start.time This specifies an offset in milliseconds in order to start the window at a time different to the standard time. Timestamp of first event INT LONG Yes No window.length The number of events the window should tumble. INT No No Examples EXAMPLE 1 define stream CseEventStream (symbol string, price float, volume int) from CseEventStream#window.unique:timeLengthBatch(symbol, 1 sec, 20) select symbol, price, volume insert all events into OutputStream; This window holds the latest unique events that arrive from the 'CseEventStream' at a given time, and returns all the events to the 'OutputStream' stream. It is updated every second based on the latest values for the 'symbol' attribute. Unitconversion MmTokm (Function) This converts the input given in megameters into kilometers. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:MmTokm( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from megameters into kilometers. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:MmTokm(1) The megameter value '1' is converted into kilometers as '1000.0' . cmToft (Function) This converts the input given in centimeters into feet. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:cmToft( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from centimeters into feet. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:cmToft(100) The centimeters value '100' is converted into feet as '3.280' . cmToin (Function) This converts the input given in centimeters into inches. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:cmToin( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from centimeters into inches. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:cmToin(100) Input centimeters value '100' is converted into inches as '39.37'. cmTokm (Function) This converts the input value given in centimeters into kilometers. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:cmTokm( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from centimeters into kilometers. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:cmTokm(100) The centimeters value '100' is converted into kilometers as '0.001'. cmTom (Function) This converts the input given in centimeters into meters. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:cmTom( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from centimeters into meters. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:cmTom(100) The centimeters value '100' is converted into meters as '1.0' . cmTomi (Function) This converts the input given in centimeters into miles. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:cmTomi( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from centimeters into miles. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:cmTomi(10000) The centimeters value '10000' is converted into miles as '0.062' . cmTomm (Function) This converts the input given in centimeters into millimeters. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:cmTomm( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from centimeters into millimeters. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:cmTomm(1) The centimeter value '1' is converted into millimeters as '10.0' . cmTonm (Function) This converts the input given in centimeters into nanometers. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:cmTonm( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from centimeters into nanometers. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:cmTonm(1) The centimeter value '1' is converted into nanometers as '10000000' . cmToum (Function) This converts the input in centimeters into micrometers. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:cmToum( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from centimeters into micrometers. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:cmToum(100) The centimeters value '100' is converted into micrometers as '1000000.0' . cmToyd (Function) This converts the input given in centimeters into yards. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:cmToyd( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from centimeters into yards. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:cmToyd(1) The centimeter value '1' is converted into yards as '0.01' . dToh (Function) This converts the input given in days into hours. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:dToh( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from days into hours. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:dToh(1) The day value '1' is converted into hours as '24.0'. gTokg (Function) This converts the input given in grams into kilograms. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:gTokg( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from grams into kilograms. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:gTokg(1000) The grams value '1000' is converted into kilogram as '1.0' . gTomg (Function) This converts the input given in grams into milligrams. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:gTomg( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from grams into milligrams. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:gTomg(1) The gram value '1' is converted into milligrams as '1000.0' . gToug (Function) This converts the input given in grams into micrograms. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:gToug( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from grams into micrograms. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:gToug(1) The gram value '1' is converted into micrograms as '1000000.0' . hTom (Function) This converts the input given in hours into minutes. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:hTom( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from hours into minutes. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:hTom(1) The hour value '1' is converted into minutes as '60.0' . hTos (Function) This converts the input given in hours into seconds. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:hTos( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from hours into seconds. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:hTos(1) The hour value '1' is converted into seconds as '3600.0'. kgToLT (Function) This converts the input given in kilograms into imperial tons. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:kgToLT( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from kilograms into imperial tons. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:kgToLT(1000) The kilograms value '1000' is converted into imperial tons as '0.9842' . kgToST (Function) This converts the input given in kilograms into US tons. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:kgToST( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from kilograms into US tons. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:kgToST(1000) The kilograms value '1000 is converted into US tons as '1.10' . kgTog (Function) This converts the input given in kilograms into grams. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:kgTog( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from kilograms into grams. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:kgTog(1) The kilogram value '1' is converted into grams as '1000'. kgTolb (Function) This converts the input given in kilograms into pounds. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:kgTolb( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from kilograms into pounds. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:kgTolb(1) The kilogram value '1' is converted into pounds as '2.2' . kgTooz (Function) This converts the input given in kilograms into ounces. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:kgTooz( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from kilograms into ounces. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:kgTooz(1) The kilogram value '1' is converted into ounces as ' 35.274' . kgTost (Function) This converts the input given in kilograms into imperial stones. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:kgTost( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from kilograms into imperial stones. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:kgTost(1) The kilogram value '1' is converted into imperial stones as '0.157' . kgTot (Function) This converts the input given in kilograms into tonnes. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:kgTot( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from kilograms into tonnes. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:kgTot(1) The kilogram value '1' is converted into tonnes as '0.001' . kmTocm (Function) This converts the input given in kilometers into centimeters. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:kmTocm( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from kilometers into centimeters. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:kmTocm(1) The kilometer value '1' is converted into centimeters as '100000.0' . kmToft (Function) This converts the input given in kilometers into feet. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:kmToft( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from kilometers into feet. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:kmToft(1) The kilometer value '1' is converted into feet as '3280.8' . kmToin (Function) This converts the input given in kilometers into inches. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:kmToin( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from kilometers into inches. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:kmToin(1) The kilometer value '1' is converted into inches as '39370.08' . kmTom (Function) This converts the input given in kilometers into meters. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:kmTom( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from kilometers into meters. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:kmTom(1) The kilometer value '1' is converted into meters as '1000.0' . kmTomi (Function) This converts the input given in kilometers into miles. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:kmTomi( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from kilometers into miles. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:kmTomi(1) The kilometer value '1' is converted into miles as '0.621' . kmTomm (Function) This converts the input given in kilometers into millimeters. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:kmTomm( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from kilometers into millimeters. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:kmTomm(1) The kilometer value '1' is converted into millimeters as '1000000.0' . kmTonm (Function) This converts the input given in kilometers into nanometers. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:kmTonm( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from kilometers into nanometers. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:kmTonm(1) The kilometer value '1' is converted into nanometers as '1000000000000.0' . kmToum (Function) This converts the input given in kilometers into micrometers. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:kmToum( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from kilometers into micrometers. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:kmToum(1) The kilometer value '1' is converted into micrometers as '1000000000.0' . kmToyd (Function) This converts the input given in kilometers into yards. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:kmToyd( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from kilometers into yards. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:kmToyd(1) The kilometer value '1' is converted into yards as '1093.6' . lTom3 (Function) This converts the input given in liters into cubic meters. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:lTom3( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from liters into cubic meters. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:lTom3(1000) The liters value '1000' is converted into cubic meters as '1' . lToml (Function) This converts the input given in liters into milliliters. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:lToml( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from liters into milliliters. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:lToml(1) The liter value '1' is converted into milliliters as '1000.0' . m3Tol (Function) This converts the input given in cubic meters into liters. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:m3Tol( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from meters into liters. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:m3Tol(1) The cubic meter value '1' is converted into liters as '1000.0' . mTocm (Function) This converts the input given in meters into centimeters. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:mTocm( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from meters into centimeters. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:mTocm(1) The meter value '1' is converted to centimeters as '100.0' . mToft (Function) This converts the input given in meters into feet. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:mToft( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from meters into feet. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:mToft(1) The meter value '1' is converted into feet as '3.280' . mTomm (Function) This converts the input given in meters into millimeters. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:mTomm( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from meters into millimeters. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:mTomm(1) The meter value '1' is converted into millimeters as '1000.0' . mTos (Function) This converts the input given in minutes into seconds. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:mTos( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from minutes into seconds. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:mTos(1) The minute value '1' is converted into seconds as '60.0' . mToyd (Function) This converts the input given in meters into yards. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:mToyd( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from meters into yards. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:mToyd(1) The meter value '1' is converted into yards as '1.093' . miTokm (Function) This converts the input given in miles into kilometers. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:miTokm( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from miles into kilometers. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:miTokm(1) The mile value '1' is converted into kilometers as '1.6' . mlTol (Function) This converts the input given in milliliters into liters. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:mlTol( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from milliliters into liters. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:mlTol(1000) The milliliters value '1000' is converted into liters as '1'. sToms (Function) This converts the input given in seconds into milliseconds. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:sToms( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from seconds into milliseconds. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:sToms(1) The second value '1' is converted into milliseconds as '1000.0' . sTons (Function) This converts the input given in seconds into nanoseconds. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:sTons( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from seconds into nanoseconds. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:sTons(1) The second value '1' is converted into nanoseconds as '1000000000.0' . sTous (Function) This converts the input given in seconds into microseconds. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:sTous( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from seconds into microseconds. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:sTous(1) The second value '1' is converted into microseconds as '1000000.0' . tTog (Function) This converts the input given in tonnes into grams. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:tTog( INT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from Tonnes into grams. INT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:tTog(1) The tonne value '1' is converted into grams as '1000000.0' . tTokg (Function) This converts the input given in tonnes into kilograms. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:tTokg( INT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from tonnes into kilograms. INT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:tTokg(inValue) The tonne value is converted into kilograms as '1000.0' . yTod (Function) This converts the given input in years into days. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:yTod( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from years into days. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:yTod(1) The year value '1' is converted into days as '365.2525' .","title":"latest"},{"location":"docs/api/latest/#api-docs-v511","text":"","title":"API Docs - v5.1.1"},{"location":"docs/api/latest/#core","text":"","title":"Core"},{"location":"docs/api/latest/#and-aggregate-function","text":"Returns the results of AND operation for all the events. Origin: siddhi-core:5.1.8 Syntax BOOL and( BOOL arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be AND operation. BOOL No Yes Examples EXAMPLE 1 from cscStream#window.lengthBatch(10) select and(isFraud) as isFraudTransaction insert into alertStream; This will returns the result for AND operation of isFraud values as a boolean value for event chunk expiry by window length batch.","title":"and (Aggregate Function)"},{"location":"docs/api/latest/#avg-aggregate-function","text":"Calculates the average for all the events. Origin: siddhi-core:5.1.8 Syntax DOUBLE avg( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that need to be averaged. INT LONG DOUBLE FLOAT No Yes Examples EXAMPLE 1 from fooStream#window.timeBatch select avg(temp) as avgTemp insert into barStream; avg(temp) returns the average temp value for all the events based on their arrival and expiry.","title":"avg (Aggregate Function)"},{"location":"docs/api/latest/#count-aggregate-function","text":"Returns the count of all the events. Origin: siddhi-core:5.1.8 Syntax LONG count() LONG count( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg This function accepts one parameter. It can belong to any one of the available types. INT LONG DOUBLE FLOAT STRING BOOL OBJECT Yes Yes Examples EXAMPLE 1 from fooStream#window.timeBatch(10 sec) select count() as count insert into barStream; This will return the count of all the events for time batch in 10 seconds.","title":"count (Aggregate Function)"},{"location":"docs/api/latest/#distinctcount-aggregate-function","text":"This returns the count of distinct occurrences for a given arg. Origin: siddhi-core:5.1.8 Syntax LONG distinctCount( INT|LONG|DOUBLE|FLOAT|STRING arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The object for which the number of distinct occurences needs to be counted. INT LONG DOUBLE FLOAT STRING No Yes Examples EXAMPLE 1 from fooStream select distinctcount(pageID) as count insert into barStream; distinctcount(pageID) for the following output returns '3' when the available values are as follows. \"WEB_PAGE_1\" \"WEB_PAGE_1\" \"WEB_PAGE_2\" \"WEB_PAGE_3\" \"WEB_PAGE_1\" \"WEB_PAGE_2\" The three distinct occurences identified are 'WEB_PAGE_1', 'WEB_PAGE_2', and 'WEB_PAGE_3'.","title":"distinctCount (Aggregate Function)"},{"location":"docs/api/latest/#max-aggregate-function","text":"Returns the maximum value for all the events. Origin: siddhi-core:5.1.8 Syntax INT|LONG|DOUBLE|FLOAT max( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be compared to find the maximum value. INT LONG DOUBLE FLOAT No Yes Examples EXAMPLE 1 from fooStream#window.timeBatch(10 sec) select max(temp) as maxTemp insert into barStream; max(temp) returns the maximum temp value recorded for all the events based on their arrival and expiry.","title":"max (Aggregate Function)"},{"location":"docs/api/latest/#maxforever-aggregate-function","text":"This is the attribute aggregator to store the maximum value for a given attribute throughout the lifetime of the query regardless of any windows in-front. Origin: siddhi-core:5.1.8 Syntax INT|LONG|DOUBLE|FLOAT maxForever( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be compared to find the maximum value. INT LONG DOUBLE FLOAT No Yes Examples EXAMPLE 1 from inputStream select maxForever(temp) as max insert into outputStream; maxForever(temp) returns the maximum temp value recorded for all the events throughout the lifetime of the query.","title":"maxForever (Aggregate Function)"},{"location":"docs/api/latest/#min-aggregate-function","text":"Returns the minimum value for all the events. Origin: siddhi-core:5.1.8 Syntax INT|LONG|DOUBLE|FLOAT min( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be compared to find the minimum value. INT LONG DOUBLE FLOAT No Yes Examples EXAMPLE 1 from inputStream select min(temp) as minTemp insert into outputStream; min(temp) returns the minimum temp value recorded for all the events based on their arrival and expiry.","title":"min (Aggregate Function)"},{"location":"docs/api/latest/#minforever-aggregate-function","text":"This is the attribute aggregator to store the minimum value for a given attribute throughout the lifetime of the query regardless of any windows in-front. Origin: siddhi-core:5.1.8 Syntax INT|LONG|DOUBLE|FLOAT minForever( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be compared to find the minimum value. INT LONG DOUBLE FLOAT No Yes Examples EXAMPLE 1 from inputStream select minForever(temp) as max insert into outputStream; minForever(temp) returns the minimum temp value recorded for all the events throughoutthe lifetime of the query.","title":"minForever (Aggregate Function)"},{"location":"docs/api/latest/#or-aggregate-function","text":"Returns the results of OR operation for all the events. Origin: siddhi-core:5.1.8 Syntax BOOL or( BOOL arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be OR operation. BOOL No Yes Examples EXAMPLE 1 from cscStream#window.lengthBatch(10) select or(isFraud) as isFraudTransaction insert into alertStream; This will returns the result for OR operation of isFraud values as a boolean value for event chunk expiry by window length batch.","title":"or (Aggregate Function)"},{"location":"docs/api/latest/#stddev-aggregate-function","text":"Returns the calculated standard deviation for all the events. Origin: siddhi-core:5.1.8 Syntax DOUBLE stdDev( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that should be used to calculate the standard deviation. INT LONG DOUBLE FLOAT No Yes Examples EXAMPLE 1 from inputStream select stddev(temp) as stdTemp insert into outputStream; stddev(temp) returns the calculated standard deviation of temp for all the events based on their arrival and expiry.","title":"stdDev (Aggregate Function)"},{"location":"docs/api/latest/#sum-aggregate-function","text":"Returns the sum for all the events. Origin: siddhi-core:5.1.8 Syntax LONG|DOUBLE sum( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be summed. INT LONG DOUBLE FLOAT No Yes Examples EXAMPLE 1 from inputStream select sum(volume) as sumOfVolume insert into outputStream; This will returns the sum of volume values as a long value for each event arrival and expiry.","title":"sum (Aggregate Function)"},{"location":"docs/api/latest/#unionset-aggregate-function","text":"Union multiple sets. This attribute aggregator maintains a union of sets. The given input set is put into the union set and the union set is returned. Origin: siddhi-core:5.1.8 Syntax OBJECT unionSet( OBJECT set) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic set The java.util.Set object that needs to be added into the union set. OBJECT No Yes Examples EXAMPLE 1 from stockStream select createSet(symbol) as initialSet insert into initStream from initStream#window.timeBatch(10 sec) select unionSet(initialSet) as distinctSymbols insert into distinctStockStream; distinctStockStream will return the set object which contains the distinct set of stock symbols received during a sliding window of 10 seconds.","title":"unionSet (Aggregate Function)"},{"location":"docs/api/latest/#uuid-function","text":"Generates a UUID (Universally Unique Identifier). Origin: siddhi-core:5.1.8 Syntax STRING UUID() Examples EXAMPLE 1 from TempStream select convert(roomNo, 'string') as roomNo, temp, UUID() as messageID insert into RoomTempStream; This will converts a room number to string, introducing a message ID to each event asUUID() returns a34eec40-32c2-44fe-8075-7f4fde2e2dd8 from TempStream select convert(roomNo, 'string') as roomNo, temp, UUID() as messageID insert into RoomTempStream;","title":"UUID (Function)"},{"location":"docs/api/latest/#cast-function","text":"Converts the first parameter according to the cast.to parameter. Incompatible arguments cause Class Cast exceptions if further processed. This function is used with map extension that returns attributes of the object type. You can use this function to cast the object to an accurate and concrete type. Origin: siddhi-core:5.1.8 Syntax INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT cast( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT to.be.caster, STRING cast.to) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic to.be.caster This specifies the attribute to be casted. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No Yes cast.to A string constant parameter expressing the cast to type using one of the following strings values: int, long, float, double, string, bool. STRING No Yes Examples EXAMPLE 1 from fooStream select symbol as name, cast(temp, 'double') as temp insert into barStream; This will cast the fooStream temp field value into 'double' format.","title":"cast (Function)"},{"location":"docs/api/latest/#coalesce-function","text":"Returns the value of the first input parameter that is not null, and all input parameters have to be on the same type. Origin: siddhi-core:5.1.8 Syntax INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT coalesce( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg, INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT ...) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg This function accepts one or more parameters. They can belong to any one of the available types. All the specified parameters should be of the same type. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No Yes Examples EXAMPLE 1 from fooStream select coalesce('123', null, '789') as value insert into barStream; This will returns first null value 123. EXAMPLE 2 from fooStream select coalesce(null, 76, 567) as value insert into barStream; This will returns first null value 76. EXAMPLE 3 from fooStream select coalesce(null, null, null) as value insert into barStream; This will returns null as there are no notnull values.","title":"coalesce (Function)"},{"location":"docs/api/latest/#convert-function","text":"Converts the first input parameter according to the convertedTo parameter. Origin: siddhi-core:5.1.8 Syntax INT|LONG|DOUBLE|FLOAT|STRING|BOOL convert( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT to.be.converted, STRING converted.to) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic to.be.converted This specifies the value to be converted. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No Yes converted.to A string constant parameter to which type the attribute need to be converted using one of the following strings values: 'int', 'long', 'float', 'double', 'string', 'bool'. STRING No Yes Examples EXAMPLE 1 from fooStream select convert(temp, 'double') as temp insert into barStream; This will convert fooStream temp value into 'double'. EXAMPLE 2 from fooStream select convert(temp, 'int') as temp insert into barStream; This will convert fooStream temp value into 'int' (value = \"convert(45.9, 'int') returns 46\").","title":"convert (Function)"},{"location":"docs/api/latest/#createset-function","text":"Includes the given input parameter in a java.util.HashSet and returns the set. Origin: siddhi-core:5.1.8 Syntax OBJECT createSet( INT|LONG|DOUBLE|FLOAT|STRING|BOOL input) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input The input that needs to be added into the set. INT LONG DOUBLE FLOAT STRING BOOL No Yes Examples EXAMPLE 1 from stockStream select createSet(symbol) as initialSet insert into initStream; For every incoming stockStream event, the initStream stream will produce a set object having only one element: the symbol in the incoming stockStream.","title":"createSet (Function)"},{"location":"docs/api/latest/#currenttimemillis-function","text":"Returns the current timestamp of siddhi application in milliseconds. Origin: siddhi-core:5.1.8 Syntax LONG currentTimeMillis() Examples EXAMPLE 1 from fooStream select symbol as name, currentTimeMillis() as eventTimestamp insert into barStream; This will extract current siddhi application timestamp.","title":"currentTimeMillis (Function)"},{"location":"docs/api/latest/#default-function","text":"Checks if the 'attribute' parameter is null and if so returns the value of the 'default' parameter Origin: siddhi-core:5.1.8 Syntax INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT default( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT attribute, INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT default) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic attribute The attribute that could be null. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No Yes default The default value that will be used when 'attribute' parameter is null INT LONG DOUBLE FLOAT STRING BOOL OBJECT No Yes Examples EXAMPLE 1 from TempStream select default(temp, 0.0) as temp, roomNum insert into StandardTempStream; This will replace TempStream's temp attribute with default value if the temp is null.","title":"default (Function)"},{"location":"docs/api/latest/#eventtimestamp-function","text":"Returns the timestamp of the processed event. Origin: siddhi-core:5.1.8 Syntax LONG eventTimestamp() Examples EXAMPLE 1 from fooStream select symbol as name, eventTimestamp() as eventTimestamp insert into barStream; This will extract current events timestamp.","title":"eventTimestamp (Function)"},{"location":"docs/api/latest/#ifthenelse-function","text":"Evaluates the 'condition' parameter and returns value of the 'if.expression' parameter if the condition is true, or returns value of the 'else.expression' parameter if the condition is false. Here both 'if.expression' and 'else.expression' should be of the same type. Origin: siddhi-core:5.1.8 Syntax INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT ifThenElse( BOOL condition, INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT if.expression, INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT else.expression) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic condition This specifies the if then else condition value. BOOL No Yes if.expression This specifies the value to be returned if the value of the condition parameter is true. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No Yes else.expression This specifies the value to be returned if the value of the condition parameter is false. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No Yes Examples EXAMPLE 1 @info(name = 'query1') from sensorEventStream select sensorValue, ifThenElse(sensorValue 35,'High','Low') as status insert into outputStream; This will returns High if sensorValue = 50. EXAMPLE 2 @info(name = 'query1') from sensorEventStream select sensorValue, ifThenElse(voltage 5, 0, 1) as status insert into outputStream; This will returns 1 if voltage= 12. EXAMPLE 3 @info(name = 'query1') from userEventStream select userName, ifThenElse(password == 'admin', true, false) as passwordState insert into outputStream; This will returns passwordState as true if password = admin.","title":"ifThenElse (Function)"},{"location":"docs/api/latest/#instanceofboolean-function","text":"Checks whether the parameter is an instance of Boolean or not. Origin: siddhi-core:5.1.8 Syntax BOOL instanceOfBoolean( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No Yes Examples EXAMPLE 1 from fooStream select instanceOfBoolean(switchState) as state insert into barStream; This will return true if the value of switchState is true. EXAMPLE 2 from fooStream select instanceOfBoolean(value) as state insert into barStream; if the value = 32 then this will returns false as the value is not an instance of the boolean.","title":"instanceOfBoolean (Function)"},{"location":"docs/api/latest/#instanceofdouble-function","text":"Checks whether the parameter is an instance of Double or not. Origin: siddhi-core:5.1.8 Syntax BOOL instanceOfDouble( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No Yes Examples EXAMPLE 1 from fooStream select instanceOfDouble(value) as state insert into barStream; This will return true if the value field format is double ex : 56.45. EXAMPLE 2 from fooStream select instanceOfDouble(switchState) as state insert into barStream; if the switchState = true then this will returns false as the value is not an instance of the double.","title":"instanceOfDouble (Function)"},{"location":"docs/api/latest/#instanceoffloat-function","text":"Checks whether the parameter is an instance of Float or not. Origin: siddhi-core:5.1.8 Syntax BOOL instanceOfFloat( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No Yes Examples EXAMPLE 1 from fooStream select instanceOfFloat(value) as state insert into barStream; This will return true if the value field format is float ex : 56.45f. EXAMPLE 2 from fooStream select instanceOfFloat(switchState) as state insert into barStream; if the switchState = true then this will returns false as the value is an instance of the boolean not a float.","title":"instanceOfFloat (Function)"},{"location":"docs/api/latest/#instanceofinteger-function","text":"Checks whether the parameter is an instance of Integer or not. Origin: siddhi-core:5.1.8 Syntax BOOL instanceOfInteger( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No Yes Examples EXAMPLE 1 from fooStream select instanceOfInteger(value) as state insert into barStream; This will return true if the value field format is integer. EXAMPLE 2 from fooStream select instanceOfInteger(switchState) as state insert into barStream; if the switchState = true then this will returns false as the value is an instance of the boolean not a long.","title":"instanceOfInteger (Function)"},{"location":"docs/api/latest/#instanceoflong-function","text":"Checks whether the parameter is an instance of Long or not. Origin: siddhi-core:5.1.8 Syntax BOOL instanceOfLong( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No Yes Examples EXAMPLE 1 from fooStream select instanceOfLong(value) as state insert into barStream; This will return true if the value field format is long ex : 56456l. EXAMPLE 2 from fooStream select instanceOfLong(switchState) as state insert into barStream; if the switchState = true then this will returns false as the value is an instance of the boolean not a long.","title":"instanceOfLong (Function)"},{"location":"docs/api/latest/#instanceofstring-function","text":"Checks whether the parameter is an instance of String or not. Origin: siddhi-core:5.1.8 Syntax BOOL instanceOfString( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No Yes Examples EXAMPLE 1 from fooStream select instanceOfString(value) as state insert into barStream; This will return true if the value field format is string ex : 'test'. EXAMPLE 2 from fooStream select instanceOfString(switchState) as state insert into barStream; if the switchState = true then this will returns false as the value is an instance of the boolean not a string.","title":"instanceOfString (Function)"},{"location":"docs/api/latest/#maximum-function","text":"Returns the maximum value of the input parameters. Origin: siddhi-core:5.1.8 Syntax INT|LONG|DOUBLE|FLOAT maximum( INT|LONG|DOUBLE|FLOAT arg, INT|LONG|DOUBLE|FLOAT ...) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg This function accepts one or more parameters. They can belong to any one of the available types. All the specified parameters should be of the same type. INT LONG DOUBLE FLOAT No Yes Examples EXAMPLE 1 @info(name = 'query1') from inputStream select maximum(price1, price2, price3) as max insert into outputStream; This will returns the maximum value of the input parameters price1, price2, price3.","title":"maximum (Function)"},{"location":"docs/api/latest/#minimum-function","text":"Returns the minimum value of the input parameters. Origin: siddhi-core:5.1.8 Syntax INT|LONG|DOUBLE|FLOAT minimum( INT|LONG|DOUBLE|FLOAT arg, INT|LONG|DOUBLE|FLOAT ...) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg This function accepts one or more parameters. They can belong to any one of the available types. All the specified parameters should be of the same type. INT LONG DOUBLE FLOAT No Yes Examples EXAMPLE 1 @info(name = 'query1') from inputStream select maximum(price1, price2, price3) as max insert into outputStream; This will returns the minimum value of the input parameters price1, price2, price3.","title":"minimum (Function)"},{"location":"docs/api/latest/#sizeofset-function","text":"Returns the size of an object of type java.util.Set. Origin: siddhi-core:5.1.8 Syntax INT sizeOfSet( OBJECT set) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic set The set object. This parameter should be of type java.util.Set. A set object may be created by the 'set' attribute aggregator in Siddhi. OBJECT No Yes Examples EXAMPLE 1 from stockStream select initSet(symbol) as initialSet insert into initStream; ;from initStream#window.timeBatch(10 sec) select union(initialSet) as distinctSymbols insert into distinctStockStream; from distinctStockStream select sizeOfSet(distinctSymbols) sizeOfSymbolSet insert into sizeStream; The sizeStream stream will output the number of distinct stock symbols received during a sliding window of 10 seconds.","title":"sizeOfSet (Function)"},{"location":"docs/api/latest/#pol2cart-stream-function","text":"The pol2Cart function calculating the cartesian coordinates x y for the given theta, rho coordinates and adding them as new attributes to the existing events. Origin: siddhi-core:5.1.8 Syntax pol2Cart( DOUBLE theta, DOUBLE rho) pol2Cart( DOUBLE theta, DOUBLE rho, DOUBLE z) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic theta The theta value of the coordinates. DOUBLE No Yes rho The rho value of the coordinates. DOUBLE No Yes z z value of the cartesian coordinates. If z value is not given, drop the third parameter of the output. DOUBLE Yes Yes Examples EXAMPLE 1 from PolarStream#pol2Cart(theta, rho) select x, y insert into outputStream ; This will return cartesian coordinates (4.99953024681082, 0.06853693328228748) for theta: 0.7854 and rho: 5. EXAMPLE 2 from PolarStream#pol2Cart(theta, rho, 3.4) select x, y, z insert into outputStream ; This will return cartesian coordinates (4.99953024681082, 0.06853693328228748, 3.4)for theta: 0.7854 and rho: 5 and z: 3.4.","title":"pol2Cart (Stream Function)"},{"location":"docs/api/latest/#log-stream-processor","text":"Logs the message on the given priority with or without the processed event. Origin: siddhi-core:5.1.8 Syntax log() log( STRING log.message) log( BOOL is.event.logged) log( STRING log.message, BOOL is.event.logged) log( STRING priority, STRING log.message) log( STRING priority, STRING log.message, BOOL is.event.logged) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic priority The priority/type of this log message (INFO, DEBUG, WARN, FATAL, ERROR, OFF, TRACE). INFO STRING Yes No log.message This message will be logged. : STRING Yes Yes is.event.logged To log the processed event. true BOOL Yes No Examples EXAMPLE 1 from FooStream#log() select * insert into BarStream; Logs events with SiddhiApp name message prefix on default log level INFO. EXAMPLE 2 from FooStream#log(\"Sample Event :\") select * insert into BarStream; Logs events with the message prefix \"Sample Event :\" on default log level INFO. EXAMPLE 3 from FooStream#log(\"DEBUG\", \"Sample Event :\", true) select * insert into BarStream; Logs events with the message prefix \"Sample Event :\" on log level DEBUG. EXAMPLE 4 from FooStream#log(\"Event Arrived\", false) select * insert into BarStream; For each event logs a message \"Event Arrived\" on default log level INFO. EXAMPLE 5 from FooStream#log(\"Sample Event :\", true) select * insert into BarStream; Logs events with the message prefix \"Sample Event :\" on default log level INFO. EXAMPLE 6 from FooStream#log(true) select * insert into BarStream; Logs events with on default log level INFO.","title":"log (Stream Processor)"},{"location":"docs/api/latest/#batch-window","text":"A window that holds an incoming events batch. When a new set of events arrives, the previously arrived old events will be expired. Batch window can be used to aggregate events that comes in batches. If it has the parameter length specified, then batch window process the batch as several chunks. Origin: siddhi-core:5.1.8 Syntax batch() batch( INT window.length) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.length The length of a chunk If length value was not given it assign 0 as length and process the whole batch as once INT Yes No Examples EXAMPLE 1 define stream consumerItemStream (itemId string, price float) from consumerItemStream#window.batch() select price, str:groupConcat(itemId) as itemIds group by price insert into outputStream; This will output comma separated items IDs that have the same price for each incoming batch of events.","title":"batch (Window)"},{"location":"docs/api/latest/#cron-window","text":"This window outputs the arriving events as and when they arrive, and resets (expires) the window periodically based on the given cron expression. Origin: siddhi-core:5.1.8 Syntax cron( STRING cron.expression) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic cron.expression The cron expression that resets the window. STRING No No Examples EXAMPLE 1 define stream InputEventStream (symbol string, price float, volume int); @info(name = 'query1') from InputEventStream#cron('*/5 * * * * ?') select symbol, sum(price) as totalPrice insert into OutputStream; This let the totalPrice to gradually increase and resets to zero as a batch every 5 seconds. EXAMPLE 2 define stream StockEventStream (symbol string, price float, volume int) define window StockEventWindow (symbol string, price float, volume int) cron('*/5 * * * * ?'); @info(name = 'query0') from StockEventStream insert into StockEventWindow; @info(name = 'query1') from StockEventWindow select symbol, sum(price) as totalPrice insert into OutputStream ; The defined window will let the totalPrice to gradually increase and resets to zero as a batch every 5 seconds.","title":"cron (Window)"},{"location":"docs/api/latest/#delay-window","text":"A delay window holds events for a specific time period that is regarded as a delay period before processing them. Origin: siddhi-core:5.1.8 Syntax delay( INT|LONG|TIME window.delay) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.delay The time period (specified in sec, min, ms) for which the window should delay the events. INT LONG TIME No No Examples EXAMPLE 1 define window delayWindow(symbol string, volume int) delay(1 hour); define stream PurchaseStream(symbol string, volume int); define stream DeliveryStream(symbol string); define stream OutputStream(symbol string); @info(name='query1') from PurchaseStream select symbol, volume insert into delayWindow; @info(name='query2') from delayWindow join DeliveryStream on delayWindow.symbol == DeliveryStream.symbol select delayWindow.symbol insert into OutputStream; In this example, purchase events that arrive in the 'PurchaseStream' stream are directed to a delay window. At any given time, this delay window holds purchase events that have arrived within the last hour. These purchase events in the window are matched by the 'symbol' attribute, with delivery events that arrive in the 'DeliveryStream' stream. This monitors whether the delivery of products is done with a minimum delay of one hour after the purchase.","title":"delay (Window)"},{"location":"docs/api/latest/#externaltime-window","text":"A sliding time window based on external time. It holds events that arrived during the last windowTime period from the external timestamp, and gets updated on every monotonically increasing timestamp. Origin: siddhi-core:5.1.8 Syntax externalTime( LONG timestamp, INT|LONG|TIME window.time) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic timestamp The time which the window determines as current time and will act upon. The value of this parameter should be monotonically increasing. LONG No Yes window.time The sliding time period for which the window should hold events. INT LONG TIME No No Examples EXAMPLE 1 define window cseEventWindow (symbol string, price float, volume int) externalTime(eventTime, 20 sec) output expired events; @info(name = 'query0') from cseEventStream insert into cseEventWindow; @info(name = 'query1') from cseEventWindow select symbol, sum(price) as price insert expired events into outputStream ; processing events arrived within the last 20 seconds from the eventTime and output expired events.","title":"externalTime (Window)"},{"location":"docs/api/latest/#externaltimebatch-window","text":"A batch (tumbling) time window based on external time, that holds events arrived during windowTime periods, and gets updated for every windowTime. Origin: siddhi-core:5.1.8 Syntax externalTimeBatch( LONG timestamp, INT|LONG|TIME window.time) externalTimeBatch( LONG timestamp, INT|LONG|TIME window.time, INT|LONG|TIME start.time) externalTimeBatch( LONG timestamp, INT|LONG|TIME window.time, INT|LONG|TIME start.time, INT|LONG|TIME timeout) externalTimeBatch( LONG timestamp, INT|LONG|TIME window.time, INT|LONG|TIME start.time, INT|LONG|TIME timeout, BOOL replace.with.batchtime) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic timestamp The time which the window determines as current time and will act upon. The value of this parameter should be monotonically increasing. LONG No Yes window.time The batch time period for which the window should hold events. INT LONG TIME No No start.time User defined start time. This could either be a constant (of type int, long or time) or an attribute of the corresponding stream (of type long). If an attribute is provided, initial value of attribute would be considered as startTime. Timestamp of first event INT LONG TIME Yes Yes timeout Time to wait for arrival of new event, before flushing and giving output for events belonging to a specific batch. System waits till an event from next batch arrives to flush current batch INT LONG TIME Yes No replace.with.batchtime This indicates to replace the expired event timeStamp as the batch end timeStamp System waits till an event from next batch arrives to flush current batch BOOL Yes No Examples EXAMPLE 1 define window cseEventWindow (symbol string, price float, volume int) externalTimeBatch(eventTime, 1 sec) output expired events; @info(name = 'query0') from cseEventStream insert into cseEventWindow; @info(name = 'query1') from cseEventWindow select symbol, sum(price) as price insert expired events into outputStream ; This will processing events that arrive every 1 seconds from the eventTime. EXAMPLE 2 define window cseEventWindow (symbol string, price float, volume int) externalTimeBatch(eventTime, 20 sec, 0) output expired events; This will processing events that arrive every 1 seconds from the eventTime. Starts on 0 th millisecond of an hour. EXAMPLE 3 define window cseEventWindow (symbol string, price float, volume int) externalTimeBatch(eventTime, 2 sec, eventTimestamp, 100) output expired events; This will processing events that arrive every 2 seconds from the eventTim. Considers the first event's eventTimestamp value as startTime. Waits 100 milliseconds for the arrival of a new event before flushing current batch.","title":"externalTimeBatch (Window)"},{"location":"docs/api/latest/#frequent-window","text":"Deprecated This window returns the latest events with the most frequently occurred value for a given attribute(s). Frequency calculation for this window processor is based on Misra-Gries counting algorithm. Origin: siddhi-core:5.1.8 Syntax frequent( INT event.count) frequent( INT event.count, STRING attribute) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic event.count The number of most frequent events to be emitted to the stream. INT No No attribute The attributes to group the events. If no attributes are given, the concatenation of all the attributes of the event is considered. The concatenation of all the attributes of the event is considered. STRING Yes Yes Examples EXAMPLE 1 @info(name = 'query1') from purchase[price = 30]#window.frequent(2) select cardNo, price insert all events into PotentialFraud; This will returns the 2 most frequent events. EXAMPLE 2 @info(name = 'query1') from purchase[price = 30]#window.frequent(2, cardNo) select cardNo, price insert all events into PotentialFraud; This will returns the 2 latest events with the most frequently appeared card numbers.","title":"frequent (Window)"},{"location":"docs/api/latest/#length-window","text":"A sliding length window that holds the last 'window.length' events at a given time, and gets updated for each arrival and expiry. Origin: siddhi-core:5.1.8 Syntax length( INT window.length) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.length The number of events that should be included in a sliding length window. INT No No Examples EXAMPLE 1 define window StockEventWindow (symbol string, price float, volume int) length(10) output all events; @info(name = 'query0') from StockEventStream insert into StockEventWindow; @info(name = 'query1') from StockEventWindow select symbol, sum(price) as price insert all events into outputStream ; This will process last 10 events in a sliding manner.","title":"length (Window)"},{"location":"docs/api/latest/#lengthbatch-window","text":"A batch (tumbling) length window that holds and process a number of events as specified in the window.length. Origin: siddhi-core:5.1.8 Syntax lengthBatch( INT window.length) lengthBatch( INT window.length, BOOL stream.current.event) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.length The number of events the window should tumble. INT No No stream.current.event Let the window stream the current events out as and when they arrive to the window while expiring them in batches. false BOOL Yes No Examples EXAMPLE 1 define stream InputEventStream (symbol string, price float, volume int); @info(name = 'query1') from InputEventStream#lengthBatch(10) select symbol, sum(price) as price insert into OutputStream; This collect and process 10 events as a batch and output them. EXAMPLE 2 define stream InputEventStream (symbol string, price float, volume int); @info(name = 'query1') from InputEventStream#lengthBatch(10, true) select symbol, sum(price) as sumPrice insert into OutputStream; This window sends the arriving events directly to the output letting the sumPrice to increase gradually, after every 10 events it clears the window as a batch and resets the sumPrice to zero. EXAMPLE 3 define stream InputEventStream (symbol string, price float, volume int); define window StockEventWindow (symbol string, price float, volume int) lengthBatch(10) output all events; @info(name = 'query0') from InputEventStream insert into StockEventWindow; @info(name = 'query1') from StockEventWindow select symbol, sum(price) as price insert all events into OutputStream ; This uses an defined window to process 10 events as a batch and output all events.","title":"lengthBatch (Window)"},{"location":"docs/api/latest/#lossyfrequent-window","text":"Deprecated This window identifies and returns all the events of which the current frequency exceeds the value specified for the supportThreshold parameter. Origin: siddhi-core:5.1.8 Syntax lossyFrequent( DOUBLE support.threshold) lossyFrequent( DOUBLE support.threshold, DOUBLE error.bound) lossyFrequent( DOUBLE support.threshold, DOUBLE error.bound, STRING attribute) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic support.threshold The support threshold value. DOUBLE No No error.bound The error bound value. support.threshold /10 DOUBLE Yes No attribute The attributes to group the events. If no attributes are given, the concatenation of all the attributes of the event is considered. The concatenation of all the attributes of the event is considered. STRING Yes Yes Examples EXAMPLE 1 define stream purchase (cardNo string, price float); define window purchaseWindow (cardNo string, price float) lossyFrequent(0.1, 0.01); @info(name = 'query0') from purchase[price = 30] insert into purchaseWindow; @info(name = 'query1') from purchaseWindow select cardNo, price insert all events into PotentialFraud; lossyFrequent(0.1, 0.01) returns all the events of which the current frequency exceeds 0.1, with an error bound of 0.01. EXAMPLE 2 define stream purchase (cardNo string, price float); define window purchaseWindow (cardNo string, price float) lossyFrequent(0.3, 0.05, cardNo); @info(name = 'query0') from purchase[price = 30] insert into purchaseWindow; @info(name = 'query1') from purchaseWindow select cardNo, price insert all events into PotentialFraud; lossyFrequent(0.3, 0.05, cardNo) returns all the events of which the cardNo attributes frequency exceeds 0.3, with an error bound of 0.05.","title":"lossyFrequent (Window)"},{"location":"docs/api/latest/#session-window","text":"This is a session window that holds events that belong to a specific session. The events that belong to a specific session are identified by a grouping attribute (i.e., a session key). A session gap period is specified to determine the time period after which the session is considered to be expired. A new event that arrives with a specific value for the session key is matched with the session window with the same session key. There can be out of order and late arrival of events, these events can arrive after the session is expired, to include those events to the matching session key specify a latency time period that is less than the session gap period.To have aggregate functions with session windows, the events need to be grouped by the session key via a 'group by' clause. Origin: siddhi-core:5.1.8 Syntax session( INT|LONG|TIME window.session) session( INT|LONG|TIME window.session, STRING window.key) session( INT|LONG|TIME window.session, STRING window.key, INT|LONG|TIME window.allowed.latency) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.session The time period for which the session considered is valid. This is specified in seconds, minutes, or milliseconds (i.e., 'min', 'sec', or 'ms'. INT LONG TIME No No window.key The grouping attribute for events. default-key STRING Yes Yes window.allowed.latency This specifies the time period for which the session window is valid after the expiration of the session. The time period specified here should be less than the session time gap (which is specified via the 'window.session' parameter). 0 INT LONG TIME Yes No Examples EXAMPLE 1 define stream PurchaseEventStream (user string, item_number int, price float, quantity int); @info(name='query0) from PurchaseEventStream#window.session(5 sec, user, 2 sec) select * insert all events into OutputStream; This query processes events that arrive at the PurchaseEvent input stream. The 'user' attribute is the session key, and the session gap is 5 seconds. '2 sec' is specified as the allowed latency. Therefore, events with the matching user name that arrive 2 seconds after the expiration of the session are also considered when performing aggregations for the session identified by the given user name.","title":"session (Window)"},{"location":"docs/api/latest/#sort-window","text":"This window holds a batch of events that equal the number specified as the windowLength and sorts them in the given order. Origin: siddhi-core:5.1.8 Syntax sort( INT window.length, STRING|DOUBLE|INT|LONG|FLOAT|LONG attribute) sort( INT window.length, STRING|DOUBLE|INT|LONG|FLOAT|LONG attribute, STRING order, STRING ...) sort( INT window.length, STRING|DOUBLE|INT|LONG|FLOAT|LONG attribute, STRING order, STRING|DOUBLE|INT|LONG|FLOAT|LONG attribute, STRING|DOUBLE|INT|LONG|FLOAT|LONG ...) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.length The size of the window length. INT No No attribute The attribute that should be checked for the order. The concatenation of all the attributes of the event is considered. STRING DOUBLE INT LONG FLOAT LONG No Yes order The order define as \"asc\" or \"desc\". asc STRING Yes No Examples EXAMPLE 1 define stream cseEventStream (symbol string, price float, volume long); define window cseEventWindow (symbol string, price float, volume long) sort(2,volume, 'asc'); @info(name = 'query0') from cseEventStream insert into cseEventWindow; @info(name = 'query1') from cseEventWindow select volume insert all events into outputStream ; sort(5, price, 'asc') keeps the events sorted by price in the ascending order. Therefore, at any given time, the window contains the 5 lowest prices.","title":"sort (Window)"},{"location":"docs/api/latest/#time-window","text":"A sliding time window that holds events that arrived during the last windowTime period at a given time, and gets updated for each event arrival and expiry. Origin: siddhi-core:5.1.8 Syntax time( INT|LONG|TIME window.time) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.time The sliding time period for which the window should hold events. INT LONG TIME No No Examples EXAMPLE 1 define window cseEventWindow (symbol string, price float, volume int) time(20) output all events; @info(name = 'query0') from cseEventStream insert into cseEventWindow; @info(name = 'query1') from cseEventWindow select symbol, sum(price) as price insert all events into outputStream ; This will processing events that arrived within the last 20 milliseconds.","title":"time (Window)"},{"location":"docs/api/latest/#timebatch-window","text":"A batch (tumbling) time window that holds and process events that arrive during 'window.time' period as a batch. Origin: siddhi-core:5.1.8 Syntax timeBatch( INT|LONG|TIME window.time) timeBatch( INT|LONG|TIME window.time, INT|LONG start.time) timeBatch( INT|LONG|TIME window.time, BOOL stream.current.event) timeBatch( INT|LONG|TIME window.time, INT|LONG start.time, BOOL stream.current.event) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.time The batch time period in which the window process the events. INT LONG TIME No No start.time This specifies an offset in milliseconds in order to start the window at a time different to the standard time. Timestamp of first event INT LONG Yes No stream.current.event Let the window stream the current events out as and when they arrive to the window while expiring them in batches. false BOOL Yes No Examples EXAMPLE 1 define stream InputEventStream (symbol string, price float, volume int); @info(name = 'query1') from InputEventStream#timeBatch(20 sec) select symbol, sum(price) as price insert into OutputStream; This collect and process incoming events as a batch every 20 seconds and output them. EXAMPLE 2 define stream InputEventStream (symbol string, price float, volume int); @info(name = 'query1') from InputEventStream#timeBatch(20 sec, true) select symbol, sum(price) as sumPrice insert into OutputStream; This window sends the arriving events directly to the output letting the sumPrice to increase gradually and on every 20 second interval it clears the window as a batch resetting the sumPrice to zero. EXAMPLE 3 define stream InputEventStream (symbol string, price float, volume int); define window StockEventWindow (symbol string, price float, volume int) timeBatch(20 sec) output all events; @info(name = 'query0') from InputEventStream insert into StockEventWindow; @info(name = 'query1') from StockEventWindow select symbol, sum(price) as price insert all events into OutputStream ; This uses an defined window to process events arrived every 20 seconds as a batch and output all events.","title":"timeBatch (Window)"},{"location":"docs/api/latest/#timelength-window","text":"A sliding time window that, at a given time holds the last window.length events that arrived during last window.time period, and gets updated for every event arrival and expiry. Origin: siddhi-core:5.1.8 Syntax timeLength( INT|LONG|TIME window.time, INT window.length) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.time The sliding time period for which the window should hold events. INT LONG TIME No No window.length The number of events that should be be included in a sliding length window.. INT No No Examples EXAMPLE 1 define stream cseEventStream (symbol string, price float, volume int); define window cseEventWindow (symbol string, price float, volume int) timeLength(2 sec, 10); @info(name = 'query0') from cseEventStream insert into cseEventWindow; @info(name = 'query1') from cseEventWindow select symbol, price, volume insert all events into outputStream; window.timeLength(2 sec, 10) holds the last 10 events that arrived during last 2 seconds and gets updated for every event arrival and expiry.","title":"timeLength (Window)"},{"location":"docs/api/latest/#js","text":"","title":"Js"},{"location":"docs/api/latest/#eval-function","text":"This extension evaluates a given string and return the output according to the user specified data type. Origin: siddhi-script-js:5.0.2 Syntax INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT js:eval( STRING expression, STRING return.type) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic expression Any single line js expression or function. STRING No Yes return.type The return type of the evaluated expression. Supported types are int|long|float|double|bool|string. STRING No No Examples EXAMPLE 1 js:eval(\"700 800\", 'bool') In this example, the expression 700 800 will be evaluated and return result as false because user specified return type as bool.","title":"eval (Function)"},{"location":"docs/api/latest/#json","text":"","title":"Json"},{"location":"docs/api/latest/#group-aggregate-function","text":"This function aggregates the JSON elements and returns a JSON object by adding enclosing.element if it is provided. If enclosing.element is not provided it aggregate the JSON elements returns a JSON array. Origin: siddhi-execution-json:2.0.4 Syntax OBJECT json:group( STRING|OBJECT json) OBJECT json:group( STRING|OBJECT json, BOOL distinct) OBJECT json:group( STRING|OBJECT json, STRING enclosing.element) OBJECT json:group( STRING|OBJECT json, STRING enclosing.element, BOOL distinct) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json The JSON element that needs to be aggregated. STRING OBJECT No Yes enclosing.element The JSON element used to enclose the aggregated JSON elements. EMPTY_STRING STRING Yes Yes distinct This is used to only have distinct JSON elements in the concatenated JSON object/array that is returned. false BOOL Yes Yes Examples EXAMPLE 1 from InputStream#window.length(5) select json:group(\"json\") as groupedJSONArray input OutputStream; When we input events having values for the json as {\"date\":\"2013-11-19\",\"time\":\"10:30\"} and {\"date\":\"2013-11-19\",\"time\":\"12:20\"} , it returns [{\"date\":\"2013-11-19\",\"time\":\"10:30\"}{\"date\":\"2013-11-19\",\"time\":\"12:20\"}] to the 'OutputStream'. EXAMPLE 2 from InputStream#window.length(5) select json:group(\"json\", true) as groupedJSONArray input OutputStream; When we input events having values for the json as {\"date\":\"2013-11-19\",\"time\":\"10:30\"} and {\"date\":\"2013-11-19\",\"time\":\"10:30\"} , it returns [{\"date\":\"2013-11-19\",\"time\":\"10:30\"}] to the 'OutputStream'. EXAMPLE 3 from InputStream#window.length(5) select json:group(\"json\", \"result\") as groupedJSONArray input OutputStream; When we input events having values for the json as {\"date\":\"2013-11-19\",\"time\":\"10:30\"} and {\"date\":\"2013-11-19\",\"time\":\"12:20\"} , it returns {\"result\":[{\"date\":\"2013-11-19\",\"time\":\"10:30\"},{\"date\":\"2013-11-19\",\"time\":\"12:20\"}} to the 'OutputStream'. EXAMPLE 4 from InputStream#window.length(5) select json:group(\"json\", \"result\", true) as groupedJSONArray input OutputStream; When we input events having values for the json as {\"date\":\"2013-11-19\",\"time\":\"10:30\"} and {\"date\":\"2013-11-19\",\"time\":\"10:30\"} , it returns {\"result\":[{\"date\":\"2013-11-19\",\"time\":\"10:30\"}]} to the 'OutputStream'.","title":"group (Aggregate Function)"},{"location":"docs/api/latest/#groupasobject-aggregate-function","text":"This function aggregates the JSON elements and returns a JSON object by adding enclosing.element if it is provided. If enclosing.element is not provided it aggregate the JSON elements returns a JSON array. Origin: siddhi-execution-json:2.0.4 Syntax OBJECT json:groupAsObject( STRING|OBJECT json) OBJECT json:groupAsObject( STRING|OBJECT json, BOOL distinct) OBJECT json:groupAsObject( STRING|OBJECT json, STRING enclosing.element) OBJECT json:groupAsObject( STRING|OBJECT json, STRING enclosing.element, BOOL distinct) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json The JSON element that needs to be aggregated. STRING OBJECT No Yes enclosing.element The JSON element used to enclose the aggregated JSON elements. EMPTY_STRING STRING Yes Yes distinct This is used to only have distinct JSON elements in the concatenated JSON object/array that is returned. false BOOL Yes Yes Examples EXAMPLE 1 from InputStream#window.length(5) select json:groupAsObject(\"json\") as groupedJSONArray input OutputStream; When we input events having values for the json as {\"date\":\"2013-11-19\",\"time\":\"10:30\"} and {\"date\":\"2013-11-19\",\"time\":\"12:20\"} , it returns [{\"date\":\"2013-11-19\",\"time\":\"10:30\"}{\"date\":\"2013-11-19\",\"time\":\"12:20\"}] to the 'OutputStream'. EXAMPLE 2 from InputStream#window.length(5) select json:groupAsObject(\"json\", true) as groupedJSONArray input OutputStream; When we input events having values for the json as {\"date\":\"2013-11-19\",\"time\":\"10:30\"} and {\"date\":\"2013-11-19\",\"time\":\"10:30\"} , it returns [{\"date\":\"2013-11-19\",\"time\":\"10:30\"}] to the 'OutputStream'. EXAMPLE 3 from InputStream#window.length(5) select json:groupAsObject(\"json\", \"result\") as groupedJSONArray input OutputStream; When we input events having values for the json as {\"date\":\"2013-11-19\",\"time\":\"10:30\"} and {\"date\":\"2013-11-19\",\"time\":\"12:20\"} , it returns {\"result\":[{\"date\":\"2013-11-19\",\"time\":\"10:30\"},{\"date\":\"2013-11-19\",\"time\":\"12:20\"}} to the 'OutputStream'. EXAMPLE 4 from InputStream#window.length(5) select json:groupAsObject(\"json\", \"result\", true) as groupedJSONArray input OutputStream; When we input events having values for the json as {\"date\":\"2013-11-19\",\"time\":\"10:30\"} and {\"date\":\"2013-11-19\",\"time\":\"10:30\"} , it returns {\"result\":[{\"date\":\"2013-11-19\",\"time\":\"10:30\"}]} to the 'OutputStream'.","title":"groupAsObject (Aggregate Function)"},{"location":"docs/api/latest/#getbool-function","text":"Function retrieves the 'boolean' value specified in the given path of the JSON element. Origin: siddhi-execution-json:2.0.4 Syntax BOOL json:getBool( STRING|OBJECT json, STRING path) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json The JSON input containing boolean value. STRING OBJECT No Yes path The JSON path to fetch the boolean value. STRING No Yes Examples EXAMPLE 1 json:getBool(json,'$.married') If the json is the format {'name' : 'John', 'married' : true} , the function returns true as there is a matching boolean at .married /code . /p p /p span id=\"example-2\" class=\"md-typeset\" style=\"display: block; color: rgba(0, 0, 0, 0.54); font-size: 12.8px; font-weight: bold;\" EXAMPLE 2 /span json:getBool(json,'$.name') p /p p style=\"word-wrap: break-word;margin: 0;\" If the code json /code is the format code {'name' : 'John', 'married' : true} /code , the function returns code null /code as there is no matching boolean at code .married</code>.</p> <p></p> <span id=\"example-2\" class=\"md-typeset\" style=\"display: block; color: rgba(0, 0, 0, 0.54); font-size: 12.8px; font-weight: bold;\">EXAMPLE 2</span> <pre class=\"codehilite\"><code>json:getBool(json,'$.name')</code></pre> <p></p> <p style=\"word-wrap: break-word;margin: 0;\">If the <code>json</code> is the format <code>{'name' : 'John', 'married' : true}</code>, the function returns <code>null</code> as there is no matching boolean at <code> .name . EXAMPLE 3 json:getBool(json,'$.foo') If the json is the format {'name' : 'John', 'married' : true} , the function returns null as there is no matching element at $.foo .","title":"getBool (Function)"},{"location":"docs/api/latest/#getdouble-function","text":"Function retrieves the 'double' value specified in the given path of the JSON element. Origin: siddhi-execution-json:2.0.4 Syntax DOUBLE json:getDouble( STRING|OBJECT json, STRING path) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json The JSON input containing double value. STRING OBJECT No Yes path The JSON path to fetch the double value. STRING No Yes Examples EXAMPLE 1 json:getDouble(json,'$.salary') If the json is the format {'name' : 'John', 'salary' : 12000.0} , the function returns 12000.0 as there is a matching double at .salary /code . /p p /p span id=\"example-2\" class=\"md-typeset\" style=\"display: block; color: rgba(0, 0, 0, 0.54); font-size: 12.8px; font-weight: bold;\" EXAMPLE 2 /span json:getDouble(json,'$.salary') p /p p style=\"word-wrap: break-word;margin: 0;\" If the code json /code is the format code {'name' : 'John', 'age' : 23} /code , the function returns code null /code as there are no matching element at code .salary</code>.</p> <p></p> <span id=\"example-2\" class=\"md-typeset\" style=\"display: block; color: rgba(0, 0, 0, 0.54); font-size: 12.8px; font-weight: bold;\">EXAMPLE 2</span> <pre class=\"codehilite\"><code>json:getDouble(json,'$.salary')</code></pre> <p></p> <p style=\"word-wrap: break-word;margin: 0;\">If the <code>json</code> is the format <code>{'name' : 'John', 'age' : 23}</code>, the function returns <code>null</code> as there are no matching element at <code> .salary . EXAMPLE 3 json:getDouble(json,'$.name') If the json is the format {'name' : 'John', 'age' : 23} , the function returns null as there are no matching double at $.name .","title":"getDouble (Function)"},{"location":"docs/api/latest/#getfloat-function","text":"Function retrieves the 'float' value specified in the given path of the JSON element. Origin: siddhi-execution-json:2.0.4 Syntax FLOAT json:getFloat( STRING|OBJECT json, STRING path) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json The JSON input containing float value. STRING OBJECT No Yes path The JSON path to fetch the float value. STRING No Yes Examples EXAMPLE 1 json:getFloat(json,'$.salary') If the json is the format {'name' : 'John', 'salary' : 12000.0} , the function returns 12000 as there is a matching float at .salary /code . /p p /p span id=\"example-2\" class=\"md-typeset\" style=\"display: block; color: rgba(0, 0, 0, 0.54); font-size: 12.8px; font-weight: bold;\" EXAMPLE 2 /span json:getFloat(json,'$.salary') p /p p style=\"word-wrap: break-word;margin: 0;\" If the code json /code is the format code {'name' : 'John', 'age' : 23} /code , the function returns code null /code as there are no matching element at code .salary</code>.</p> <p></p> <span id=\"example-2\" class=\"md-typeset\" style=\"display: block; color: rgba(0, 0, 0, 0.54); font-size: 12.8px; font-weight: bold;\">EXAMPLE 2</span> <pre class=\"codehilite\"><code>json:getFloat(json,'$.salary')</code></pre> <p></p> <p style=\"word-wrap: break-word;margin: 0;\">If the <code>json</code> is the format <code>{'name' : 'John', 'age' : 23}</code>, the function returns <code>null</code> as there are no matching element at <code> .salary . EXAMPLE 3 json:getFloat(json,'$.name') If the json is the format {'name' : 'John', 'age' : 23} , the function returns null as there are no matching float at $.name .","title":"getFloat (Function)"},{"location":"docs/api/latest/#getint-function","text":"Function retrieves the 'int' value specified in the given path of the JSON element. Origin: siddhi-execution-json:2.0.4 Syntax INT json:getInt( STRING|OBJECT json, STRING path) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json The JSON input containing int value. STRING OBJECT No Yes path The JSON path to fetch the int value. STRING No Yes Examples EXAMPLE 1 json:getInt(json,'$.age') If the json is the format {'name' : 'John', 'age' : 23} , the function returns 23 as there is a matching int at .age /code . /p p /p span id=\"example-2\" class=\"md-typeset\" style=\"display: block; color: rgba(0, 0, 0, 0.54); font-size: 12.8px; font-weight: bold;\" EXAMPLE 2 /span json:getInt(json,'$.salary') p /p p style=\"word-wrap: break-word;margin: 0;\" If the code json /code is the format code {'name' : 'John', 'age' : 23} /code , the function returns code null /code as there are no matching element at code .age</code>.</p> <p></p> <span id=\"example-2\" class=\"md-typeset\" style=\"display: block; color: rgba(0, 0, 0, 0.54); font-size: 12.8px; font-weight: bold;\">EXAMPLE 2</span> <pre class=\"codehilite\"><code>json:getInt(json,'$.salary')</code></pre> <p></p> <p style=\"word-wrap: break-word;margin: 0;\">If the <code>json</code> is the format <code>{'name' : 'John', 'age' : 23}</code>, the function returns <code>null</code> as there are no matching element at <code> .salary . EXAMPLE 3 json:getInt(json,'$.name') If the json is the format {'name' : 'John', 'age' : 23} , the function returns null as there are no matching int at $.name .","title":"getInt (Function)"},{"location":"docs/api/latest/#getlong-function","text":"Function retrieves the 'long' value specified in the given path of the JSON element. Origin: siddhi-execution-json:2.0.4 Syntax LONG json:getLong( STRING|OBJECT json, STRING path) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json The JSON input containing long value. STRING OBJECT No Yes path The JSON path to fetch the long value. STRING No Yes Examples EXAMPLE 1 json:getLong(json,'$.age') If the json is the format {'name' : 'John', 'age' : 23} , the function returns 23 as there is a matching long at .age /code . /p p /p span id=\"example-2\" class=\"md-typeset\" style=\"display: block; color: rgba(0, 0, 0, 0.54); font-size: 12.8px; font-weight: bold;\" EXAMPLE 2 /span json:getLong(json,'$.salary') p /p p style=\"word-wrap: break-word;margin: 0;\" If the code json /code is the format code {'name' : 'John', 'age' : 23} /code , the function returns code null /code as there are no matching element at code .age</code>.</p> <p></p> <span id=\"example-2\" class=\"md-typeset\" style=\"display: block; color: rgba(0, 0, 0, 0.54); font-size: 12.8px; font-weight: bold;\">EXAMPLE 2</span> <pre class=\"codehilite\"><code>json:getLong(json,'$.salary')</code></pre> <p></p> <p style=\"word-wrap: break-word;margin: 0;\">If the <code>json</code> is the format <code>{'name' : 'John', 'age' : 23}</code>, the function returns <code>null</code> as there are no matching element at <code> .salary . EXAMPLE 3 json:getLong(json,'$.name') If the json is the format {'name' : 'John', 'age' : 23} , the function returns null as there are no matching long at $.name .","title":"getLong (Function)"},{"location":"docs/api/latest/#getobject-function","text":"Function retrieves the object specified in the given path of the JSON element. Origin: siddhi-execution-json:2.0.4 Syntax OBJECT json:getObject( STRING|OBJECT json, STRING path) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json The JSON input containing the object. STRING OBJECT No Yes path The JSON path to fetch the object. STRING No Yes Examples EXAMPLE 1 json:getObject(json,'$.address') If the json is the format {'name' : 'John', 'address' : {'city' : 'NY', 'country' : 'USA'}} , the function returns {'city' : 'NY', 'country' : 'USA'} as there is a matching object at .address /code . /p p /p span id=\"example-2\" class=\"md-typeset\" style=\"display: block; color: rgba(0, 0, 0, 0.54); font-size: 12.8px; font-weight: bold;\" EXAMPLE 2 /span json:getObject(json,'$.age') p /p p style=\"word-wrap: break-word;margin: 0;\" If the code json /code is the format code {'name' : 'John', 'age' : 23} /code , the function returns code 23 /code as there is a matching object at code .address</code>.</p> <p></p> <span id=\"example-2\" class=\"md-typeset\" style=\"display: block; color: rgba(0, 0, 0, 0.54); font-size: 12.8px; font-weight: bold;\">EXAMPLE 2</span> <pre class=\"codehilite\"><code>json:getObject(json,'$.age')</code></pre> <p></p> <p style=\"word-wrap: break-word;margin: 0;\">If the <code>json</code> is the format <code>{'name' : 'John', 'age' : 23}</code>, the function returns <code>23</code> as there is a matching object at <code> .age . EXAMPLE 3 json:getObject(json,'$.salary') If the json is the format {'name' : 'John', 'age' : 23} , the function returns null as there are no matching element at $.salary .","title":"getObject (Function)"},{"location":"docs/api/latest/#getstring-function","text":"Function retrieves value specified in the given path of the JSON element as a string. Origin: siddhi-execution-json:2.0.4 Syntax STRING json:getString( STRING|OBJECT json, STRING path) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json The JSON input containing value. STRING OBJECT No Yes path The JSON path to fetch the value. STRING No Yes Examples EXAMPLE 1 json:getString(json,'$.name') If the json is the format {'name' : 'John', 'age' : 23} , the function returns John as there is a matching string at .name /code . /p p /p span id=\"example-2\" class=\"md-typeset\" style=\"display: block; color: rgba(0, 0, 0, 0.54); font-size: 12.8px; font-weight: bold;\" EXAMPLE 2 /span json:getString(json,'$.salary') p /p p style=\"word-wrap: break-word;margin: 0;\" If the code json /code is the format code {'name' : 'John', 'age' : 23} /code , the function returns code null /code as there are no matching element at code .name</code>.</p> <p></p> <span id=\"example-2\" class=\"md-typeset\" style=\"display: block; color: rgba(0, 0, 0, 0.54); font-size: 12.8px; font-weight: bold;\">EXAMPLE 2</span> <pre class=\"codehilite\"><code>json:getString(json,'$.salary')</code></pre> <p></p> <p style=\"word-wrap: break-word;margin: 0;\">If the <code>json</code> is the format <code>{'name' : 'John', 'age' : 23}</code>, the function returns <code>null</code> as there are no matching element at <code> .salary . EXAMPLE 3 json:getString(json,'$.age') If the json is the format {'name' : 'John', 'age' : 23} , the function returns 23 as a string as there is a matching element at .age /code . /p p /p span id=\"example-4\" class=\"md-typeset\" style=\"display: block; color: rgba(0, 0, 0, 0.54); font-size: 12.8px; font-weight: bold;\" EXAMPLE 4 /span json:getString(json,'$.address') p /p p style=\"word-wrap: break-word;margin: 0;\" If the code json /code is the format code {'name' : 'John', 'address' : {'city' : 'NY', 'country' : 'USA'}} /code , the function returns code {'city' : 'NY', 'country' : 'USA'} /code as a string as there is a matching element at code .age</code>.</p> <p></p> <span id=\"example-4\" class=\"md-typeset\" style=\"display: block; color: rgba(0, 0, 0, 0.54); font-size: 12.8px; font-weight: bold;\">EXAMPLE 4</span> <pre class=\"codehilite\"><code>json:getString(json,'$.address')</code></pre> <p></p> <p style=\"word-wrap: break-word;margin: 0;\">If the <code>json</code> is the format <code>{'name' : 'John', 'address' : {'city' : 'NY', 'country' : 'USA'}}</code>, the function returns <code>{'city' : 'NY', 'country' : 'USA'}</code> as a string as there is a matching element at <code> .address .","title":"getString (Function)"},{"location":"docs/api/latest/#isexists-function","text":"Function checks whether there is a JSON element present in the given path or not. Origin: siddhi-execution-json:2.0.4 Syntax BOOL json:isExists( STRING|OBJECT json, STRING path) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json The JSON input that needs to be searched for an elements. STRING OBJECT No Yes path The JSON path to check for the element. STRING No Yes Examples EXAMPLE 1 json:isExists(json, '$.name') If the json is the format {'name' : 'John', 'age' : 23} , the function returns true as there is an element in the given path. EXAMPLE 2 json:isExists(json, '$.salary') If the json is the format {'name' : 'John', 'age' : 23} , the function returns false as there is no element in the given path.","title":"isExists (Function)"},{"location":"docs/api/latest/#setelement-function","text":"Function sets JSON element into a given JSON at the specific path. Origin: siddhi-execution-json:2.0.4 Syntax OBJECT json:setElement( STRING|OBJECT json, STRING path, STRING|BOOL|DOUBLE|FLOAT|INT|LONG|OBJECT json.element) OBJECT json:setElement( STRING|OBJECT json, STRING path, STRING|BOOL|DOUBLE|FLOAT|INT|LONG|OBJECT json.element, STRING key) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json The JSON to which a JSON element needs to be added/replaced. STRING OBJECT No Yes path The JSON path where the JSON element should be added/replaced. STRING No Yes json.element The JSON element being added. STRING BOOL DOUBLE FLOAT INT LONG OBJECT No Yes key The key to be used to refer the newly added element in the input JSON. Assumes the element is added to a JSON array, or the element selected by the JSON path will be updated. STRING Yes Yes Examples EXAMPLE 1 json:setElement(json, '$', \"{'country' : 'USA'}\", 'address') If the json is the format {'name' : 'John', 'married' : true} ,the function updates the json as {'name' : 'John', 'married' : true, 'address' : {'country' : 'USA'}} by adding 'address' element and returns the updated JSON. EXAMPLE 2 json:setElement(json, '$', 40, 'age') If the json is the format {'name' : 'John', 'married' : true} ,the function updates the json as {'name' : 'John', 'married' : true, 'age' : 40} by adding 'age' element and returns the updated JSON. EXAMPLE 3 json:setElement(json, '$', 45, 'age') If the json is the format {'name' : 'John', 'married' : true, 'age' : 40} , the function updates the json as {'name' : 'John', 'married' : true, 'age' : 45} by replacing 'age' element and returns the updated JSON. EXAMPLE 4 json:setElement(json, '$.items', 'book') If the json is the format {'name' : 'Stationary', 'items' : ['pen', 'pencil']} , the function updates the json as {'name' : 'John', 'items' : ['pen', 'pencil', 'book']} by adding 'book' in the items array and returns the updated JSON. EXAMPLE 5 json:setElement(json, '$.item', 'book') If the json is the format {'name' : 'Stationary', 'item' : 'pen'} , the function updates the json as {'name' : 'John', 'item' : 'book'} by replacing 'item' element and returns the updated JSON. EXAMPLE 6 json:setElement(json, '$.address', 'city', 'SF') If the json is the format {'name' : 'John', 'married' : true} ,the function will not update, but returns the original JSON as there are no valid path for $.address .","title":"setElement (Function)"},{"location":"docs/api/latest/#toobject-function","text":"Function generate JSON object from the given JSON string. Origin: siddhi-execution-json:2.0.4 Syntax OBJECT json:toObject( STRING json) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json A valid JSON string that needs to be converted to a JSON object. STRING No Yes Examples EXAMPLE 1 json:toJson(json) This returns the JSON object corresponding to the given JSON string.","title":"toObject (Function)"},{"location":"docs/api/latest/#tostring-function","text":"Function generates a JSON string corresponding to a given JSON object. Origin: siddhi-execution-json:2.0.4 Syntax STRING json:toString( OBJECT json) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json A valid JSON object to generates a JSON string. OBJECT No Yes Examples EXAMPLE 1 json:toString(json) This returns the JSON string corresponding to a given JSON object.","title":"toString (Function)"},{"location":"docs/api/latest/#tokenize-stream-processor","text":"Stream processor tokenizes the given JSON into to multiple JSON string elements and sends them as separate events. Origin: siddhi-execution-json:2.0.4 Syntax json:tokenize( STRING|OBJECT json, STRING path) json:tokenize( STRING|OBJECT json, STRING path, BOOL fail.on.missing.attribute) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json The input JSON that needs to be tokenized. STRING OBJECT No Yes path The path of the set of elements that will be tokenized. STRING No Yes fail.on.missing.attribute If there are no element on the given path, when set to true the system will drop the event, and when set to false the system will pass 'null' value to the jsonElement output attribute. true BOOL Yes No Extra Return Attributes Name Description Possible Types jsonElement The JSON element retrieved based on the given path will be returned as a JSON string. If the 'path' selects a JSON array then the system returns each element in the array as a JSON string via a separate events. STRING Examples EXAMPLE 1 define stream InputStream (json string, path string); @info(name = 'query1') from InputStream#json:tokenizeAsObject(json, path) select path, jsonElement insert into OutputStream; If the input 'json' is {name:'John', enrolledSubjects:['Mathematics', 'Physics']} , and the 'path' is passed as .enrolledSubjects /code then for both the elements in the selected JSON array, it generates it generates events as code (' .enrolledSubjects</code> then for both the elements in the selected JSON array, it generates it generates events as <code>(' .enrolledSubjects', 'Mathematics') , and (' .enrolledSubjects', 'Physics') /code . br For the same input JSON, if the 'path' is passed as code .enrolledSubjects', 'Physics')</code>.<br>For the same input JSON, if the 'path' is passed as <code> .name then it will only produce one event (' .name', 'John') /code as the 'path' provided a single JSON element. /p p /p span id=\"example-2\" class=\"md-typeset\" style=\"display: block; color: rgba(0, 0, 0, 0.54); font-size: 12.8px; font-weight: bold;\" EXAMPLE 2 /span define stream InputStream (json string, path string); @info(name = 'query1') from InputStream#json:tokenizeAsObject(json, path, true) select path, jsonElement insert into OutputStream; p /p p style=\"word-wrap: break-word;margin: 0;\" If the input 'json' is code {name:'John', age:25} /code ,and the 'path' is passed as code .name', 'John')</code> as the 'path' provided a single JSON element.</p> <p></p> <span id=\"example-2\" class=\"md-typeset\" style=\"display: block; color: rgba(0, 0, 0, 0.54); font-size: 12.8px; font-weight: bold;\">EXAMPLE 2</span> <pre class=\"codehilite\"><code>define stream InputStream (json string, path string); @info(name = 'query1') from InputStream#json:tokenizeAsObject(json, path, true) select path, jsonElement insert into OutputStream;</code></pre> <p></p> <p style=\"word-wrap: break-word;margin: 0;\">If the input 'json' is <code>{name:'John', age:25}</code>,and the 'path' is passed as <code> .salary then the system will produce (' .salary', null) /code , as the 'fail.on.missing.attribute' is code true /code and there are no matching element for code .salary', null)</code>, as the 'fail.on.missing.attribute' is <code>true</code> and there are no matching element for <code> .salary .","title":"tokenize (Stream Processor)"},{"location":"docs/api/latest/#tokenizeasobject-stream-processor","text":"Stream processor tokenizes the given JSON into to multiple JSON object elements and sends them as separate events. Origin: siddhi-execution-json:2.0.4 Syntax json:tokenizeAsObject( STRING|OBJECT json, STRING path) json:tokenizeAsObject( STRING|OBJECT json, STRING path, BOOL fail.on.missing.attribute) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json The input JSON that needs to be tokenized. STRING OBJECT No Yes path The path of the set of elements that will be tokenized. STRING No Yes fail.on.missing.attribute If there are no element on the given path, when set to true the system will drop the event, and when set to false the system will pass 'null' value to the jsonElement output attribute. true BOOL Yes No Extra Return Attributes Name Description Possible Types jsonElement The JSON element retrieved based on the given path will be returned as a JSON object. If the 'path' selects a JSON array then the system returns each element in the array as a JSON object via a separate events. OBJECT Examples EXAMPLE 1 define stream InputStream (json string, path string); @info(name = 'query1') from InputStream#json:tokenizeAsObject(json, path) select path, jsonElement insert into OutputStream; If the input 'json' is {name:'John', enrolledSubjects:['Mathematics', 'Physics']} , and the 'path' is passed as .enrolledSubjects /code then for both the elements in the selected JSON array, it generates it generates events as code (' .enrolledSubjects</code> then for both the elements in the selected JSON array, it generates it generates events as <code>(' .enrolledSubjects', 'Mathematics') , and (' .enrolledSubjects', 'Physics') /code . br For the same input JSON, if the 'path' is passed as code .enrolledSubjects', 'Physics')</code>.<br>For the same input JSON, if the 'path' is passed as <code> .name then it will only produce one event (' .name', 'John') /code as the 'path' provided a single JSON element. /p p /p span id=\"example-2\" class=\"md-typeset\" style=\"display: block; color: rgba(0, 0, 0, 0.54); font-size: 12.8px; font-weight: bold;\" EXAMPLE 2 /span define stream InputStream (json string, path string); @info(name = 'query1') from InputStream#json:tokenizeAsObject(json, path, true) select path, jsonElement insert into OutputStream; p /p p style=\"word-wrap: break-word;margin: 0;\" If the input 'json' is code {name:'John', age:25} /code ,and the 'path' is passed as code .name', 'John')</code> as the 'path' provided a single JSON element.</p> <p></p> <span id=\"example-2\" class=\"md-typeset\" style=\"display: block; color: rgba(0, 0, 0, 0.54); font-size: 12.8px; font-weight: bold;\">EXAMPLE 2</span> <pre class=\"codehilite\"><code>define stream InputStream (json string, path string); @info(name = 'query1') from InputStream#json:tokenizeAsObject(json, path, true) select path, jsonElement insert into OutputStream;</code></pre> <p></p> <p style=\"word-wrap: break-word;margin: 0;\">If the input 'json' is <code>{name:'John', age:25}</code>,and the 'path' is passed as <code> .salary then the system will produce (' .salary', null) /code , as the 'fail.on.missing.attribute' is code true /code and there are no matching element for code .salary', null)</code>, as the 'fail.on.missing.attribute' is <code>true</code> and there are no matching element for <code> .salary .","title":"tokenizeAsObject (Stream Processor)"},{"location":"docs/api/latest/#list","text":"","title":"List"},{"location":"docs/api/latest/#collect-aggregate-function","text":"Collects multiple values to construct a list. Origin: siddhi-execution-list:1.0.0 Syntax OBJECT list:collect( OBJECT|INT|LONG|FLOAT|DOUBLE|BOOL|STRING value) OBJECT list:collect( OBJECT|INT|LONG|FLOAT|DOUBLE|BOOL|STRING value, BOOL is.distinct) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic value Value of the list element OBJECT INT LONG FLOAT DOUBLE BOOL STRING No Yes is.distinct If true only distinct elements are collected false BOOL Yes Yes Examples EXAMPLE 1 from StockStream#window.lengthBatch(10) select list:collect(symbol) as stockSymbols insert into OutputStream; For the window expiry of 10 events, the collect() function will collect attributes of symbol to a single list and return as stockSymbols.","title":"collect (Aggregate Function)"},{"location":"docs/api/latest/#merge-aggregate-function","text":"Collects multiple lists to merge as a single list. Origin: siddhi-execution-list:1.0.0 Syntax OBJECT list:merge( OBJECT list) OBJECT list:merge( OBJECT list, BOOL is.distinct) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic list List to be merged OBJECT No Yes is.distinct Whether to return list with distinct values false BOOL Yes Yes Examples EXAMPLE 1 from StockStream#window.lengthBatch(2) select list:merge(list) as stockSymbols insert into OutputStream; For the window expiry of 2 events, the merge() function will collect attributes of list and merge them to a single list, returned as stockSymbols.","title":"merge (Aggregate Function)"},{"location":"docs/api/latest/#add-function","text":"Function returns the updated list after adding the given value. Origin: siddhi-execution-list:1.0.0 Syntax OBJECT list:add( OBJECT list, OBJECT|INT|LONG|FLOAT|DOUBLE|BOOL|STRING value) OBJECT list:add( OBJECT list, OBJECT|INT|LONG|FLOAT|DOUBLE|BOOL|STRING value, INT index) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic list The list to which the value should be added. OBJECT No Yes value The value to be added. OBJECT INT LONG FLOAT DOUBLE BOOL STRING No Yes index The index in which the value should to be added. last INT Yes Yes Examples EXAMPLE 1 list:add(stockSymbols, 'IBM') Function returns the updated list after adding the value IBM in the last index. EXAMPLE 2 list:add(stockSymbols, 'IBM', 0) Function returns the updated list after adding the value IBM in the 0 th index`.","title":"add (Function)"},{"location":"docs/api/latest/#addall-function","text":"Function returns the updated list after adding all the values from the given list. Origin: siddhi-execution-list:1.0.0 Syntax OBJECT list:addAll( OBJECT to.list, OBJECT from.list) OBJECT list:addAll( OBJECT to.list, OBJECT from.list, BOOL is.distinct) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic to.list The list into which the values need to copied. OBJECT No Yes from.list The list from which the values are copied. OBJECT No Yes is.distinct If true returns list with distinct values false BOOL Yes Yes Examples EXAMPLE 1 list:putAll(toList, fromList) If toList contains values ('IBM', 'WSO2), and if fromList contains values ('IBM', 'XYZ') then the function returns updated toList with values ('IBM', 'WSO2', 'IBM', 'XYZ'). EXAMPLE 2 list:putAll(toList, fromList, true) If toList contains values ('IBM', 'WSO2), and if fromList contains values ('IBM', 'XYZ') then the function returns updated toList with values ('IBM', 'WSO2', 'XYZ').","title":"addAll (Function)"},{"location":"docs/api/latest/#clear-function","text":"Function returns the cleared list. Origin: siddhi-execution-list:1.0.0 Syntax OBJECT list:clear( OBJECT list) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic list The list which needs to be cleared OBJECT No Yes Examples EXAMPLE 1 list:clear(stockDetails) Returns an empty list.","title":"clear (Function)"},{"location":"docs/api/latest/#clone-function","text":"Function returns the cloned list. Origin: siddhi-execution-list:1.0.0 Syntax OBJECT list:clone( OBJECT list) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic list The list to which needs to be cloned. OBJECT No Yes Examples EXAMPLE 1 list:clone(stockSymbols) Function returns cloned list of stockSymbols.","title":"clone (Function)"},{"location":"docs/api/latest/#contains-function","text":"Function checks whether the list contains the specific value. Origin: siddhi-execution-list:1.0.0 Syntax BOOL list:contains( OBJECT list, OBJECT|INT|LONG|FLOAT|DOUBLE|BOOL|STRING value) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic list The list that needs to be checked on whether it contains the value or not. OBJECT No Yes value The value that needs to be checked. OBJECT INT LONG FLOAT DOUBLE BOOL STRING No Yes Examples EXAMPLE 1 list:contains(stockSymbols, 'IBM') Returns 'true' if the stockSymbols list contains value IBM else it returns false .","title":"contains (Function)"},{"location":"docs/api/latest/#containsall-function","text":"Function checks whether the list contains all the values in the given list. Origin: siddhi-execution-list:1.0.0 Syntax BOOL list:containsAll( OBJECT list, OBJECT given.list) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic list The list that needs to be checked on whether it contains all the values or not. OBJECT No Yes given.list The list which contains all the values to be checked. OBJECT No Yes Examples EXAMPLE 1 list:containsAll(stockSymbols, latestStockSymbols) Returns 'true' if the stockSymbols list contains values in latestStockSymbols else it returns false .","title":"containsAll (Function)"},{"location":"docs/api/latest/#create-function","text":"Function creates a list containing all values provided. Origin: siddhi-execution-list:1.0.0 Syntax OBJECT list:create() OBJECT list:create( OBJECT|INT|LONG|FLOAT|DOUBLE|BOOL|STRING value1) OBJECT list:create( OBJECT|INT|LONG|FLOAT|DOUBLE|BOOL|STRING value1, OBJECT|INT|LONG|FLOAT|DOUBLE|BOOL|STRING ...) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic value1 Value 1 OBJECT INT LONG FLOAT DOUBLE BOOL STRING Yes Yes Examples EXAMPLE 1 list:create(1, 2, 3, 4, 5, 6) This returns a list with values 1 , 2 , 3 , 4 , 5 and 6 . EXAMPLE 2 list:create() This returns an empty list.","title":"create (Function)"},{"location":"docs/api/latest/#get-function","text":"Function returns the value at the specific index, null if index is out of range. Origin: siddhi-execution-list:1.0.0 Syntax OBJECT|INT|LONG|FLOAT|DOUBLE|BOOL|STRING list:get( OBJECT list, INT index) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic list Attribute containing the list OBJECT No Yes index Index of the element INT No Yes Examples EXAMPLE 1 list:get(stockSymbols, 1) This returns the element in the 1 st index in the stockSymbols list.","title":"get (Function)"},{"location":"docs/api/latest/#indexof-function","text":"Function returns the last index of the given element. Origin: siddhi-execution-list:1.0.0 Syntax INT list:indexOf( OBJECT list, OBJECT|INT|LONG|FLOAT|DOUBLE|BOOL|STRING value) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic list The list to be checked to get index of an element. OBJECT No Yes value Value for which last index needs to be identified. OBJECT INT LONG FLOAT DOUBLE BOOL STRING No Yes Examples EXAMPLE 1 list:indexOf(stockSymbols. `IBM`) Returns the last index of the element IBM if present else it returns -1.","title":"indexOf (Function)"},{"location":"docs/api/latest/#isempty-function","text":"Function checks if the list is empty. Origin: siddhi-execution-list:1.0.0 Syntax BOOL list:isEmpty( OBJECT list) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic list The list that needs to be checked whether it's empty or not. OBJECT No Yes Examples EXAMPLE 1 list:isEmpty(stockSymbols) Returns 'true' if the stockSymbols list is empty else it returns false .","title":"isEmpty (Function)"},{"location":"docs/api/latest/#islist-function","text":"Function checks if the object is type of a list. Origin: siddhi-execution-list:1.0.0 Syntax BOOL list:isList( OBJECT|INT|LONG|FLOAT|DOUBLE|BOOL|STRING arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The argument the need to be determined whether it's a list or not. OBJECT INT LONG FLOAT DOUBLE BOOL STRING No Yes Examples EXAMPLE 1 list:isList(stockSymbols) Returns 'true' if the stockSymbols is and an instance of java.util.List else it returns false .","title":"isList (Function)"},{"location":"docs/api/latest/#lastindexof-function","text":"Function returns the index of the given value. Origin: siddhi-execution-list:1.0.0 Syntax INT list:lastIndexOf( OBJECT list, OBJECT|INT|LONG|FLOAT|DOUBLE|BOOL|STRING value) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic list The list to be checked to get index of an element. OBJECT No Yes value Value for which last index needs to be identified. OBJECT INT LONG FLOAT DOUBLE BOOL STRING No Yes Examples EXAMPLE 1 list:lastIndexOf(stockSymbols. `IBM`) Returns the last index of the element IBM if present else it returns -1.","title":"lastIndexOf (Function)"},{"location":"docs/api/latest/#remove-function","text":"Function returns the updated list after removing the element with the specified value. Origin: siddhi-execution-list:1.0.0 Syntax OBJECT list:remove( OBJECT list, OBJECT|INT|LONG|FLOAT|DOUBLE|BOOL|STRING value) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic list The list that needs to be updated. OBJECT No Yes value The value of the element that needs to removed. OBJECT INT LONG FLOAT DOUBLE BOOL STRING No Yes Examples EXAMPLE 1 list:remove(stockSymbols, 'IBM') This returns the updated list, stockSymbols after stockSymbols the value IBM .","title":"remove (Function)"},{"location":"docs/api/latest/#removeall-function","text":"Function returns the updated list after removing all the element with the specified list. Origin: siddhi-execution-list:1.0.0 Syntax OBJECT list:removeAll( OBJECT list, OBJECT given.list) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic list The list that needs to be updated. OBJECT No Yes given.list The list with all the elements that needs to removed. OBJECT No Yes Examples EXAMPLE 1 list:removeAll(stockSymbols, latestStockSymbols) This returns the updated list, stockSymbols after removing all the values in latestStockSymbols.","title":"removeAll (Function)"},{"location":"docs/api/latest/#removebyindex-function","text":"Function returns the updated list after removing the element with the specified index. Origin: siddhi-execution-list:1.0.0 Syntax OBJECT list:removeByIndex( OBJECT list, INT index) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic list The list that needs to be updated. OBJECT No Yes index The index of the element that needs to removed. INT No Yes Examples EXAMPLE 1 list:removeByIndex(stockSymbols, 0) This returns the updated list, stockSymbols after removing value at 0 th index.","title":"removeByIndex (Function)"},{"location":"docs/api/latest/#retainall-function","text":"Function returns the updated list after retaining all the elements in the specified list. Origin: siddhi-execution-list:1.0.0 Syntax OBJECT list:retainAll( OBJECT list, OBJECT given.list) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic list The list that needs to be updated. OBJECT No Yes given.list The list with all the elements that needs to reatined. OBJECT No Yes Examples EXAMPLE 1 list:retainAll(stockSymbols, latestStockSymbols) This returns the updated list, stockSymbols after retaining all the values in latestStockSymbols.","title":"retainAll (Function)"},{"location":"docs/api/latest/#setvalue-function","text":"Function returns the updated list after replacing the element in the given index by the given value. Origin: siddhi-execution-list:1.0.0 Syntax OBJECT list:setValue( OBJECT list, INT index, OBJECT|INT|LONG|FLOAT|DOUBLE|BOOL|STRING value) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic list The list to which the value should be updated. OBJECT No Yes index The index in which the value should to be updated. INT No Yes value The value to be updated with. OBJECT INT LONG FLOAT DOUBLE BOOL STRING No Yes Examples EXAMPLE 1 list:set(stockSymbols, 0, 'IBM') Function returns the updated list after replacing the value at 0 th index with the value IBM","title":"setValue (Function)"},{"location":"docs/api/latest/#size-function","text":"Function to return the size of the list. Origin: siddhi-execution-list:1.0.0 Syntax INT list:size( OBJECT list) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic list The list for which size should be returned. OBJECT No Yes Examples EXAMPLE 1 list:size(stockSymbols) Returns size of the stockSymbols list.","title":"size (Function)"},{"location":"docs/api/latest/#sort-function","text":"Function returns lists sorted in ascending or descending order. Origin: siddhi-execution-list:1.0.0 Syntax OBJECT list:sort( OBJECT list) OBJECT list:sort( OBJECT list, STRING order) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic list The list which should be sorted. OBJECT No Yes order Order in which the list needs to be sorted (ASC/DESC/REV). REV STRING Yes No Examples EXAMPLE 1 list:sort(stockSymbols) Function returns the sorted list in ascending order. EXAMPLE 2 list:sort(stockSymbols, 'DESC') Function returns the sorted list in descending order.","title":"sort (Function)"},{"location":"docs/api/latest/#tokenize-stream-processor_1","text":"Tokenize the list and return each key, value as new attributes in events Origin: siddhi-execution-list:1.0.0 Syntax list:tokenize( OBJECT list) list:tokenize( OBJECT list, OBJECT ...) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic list Array list which needs to be tokenized OBJECT No Yes Extra Return Attributes Name Description Possible Types index Index of an entry consisted in the list INT value Value of an entry consisted in the list OBJECT Examples EXAMPLE 1 list:tokenize(customList) If custom list contains ('WSO2', 'IBM', 'XYZ') elements, then tokenize function will return 3 events with value attributes WSO2, IBM and XYZ respectively.","title":"tokenize (Stream Processor)"},{"location":"docs/api/latest/#map","text":"","title":"Map"},{"location":"docs/api/latest/#collect-aggregate-function_1","text":"Collect multiple key-value pairs to construct a map. Only distinct keys are collected, if a duplicate key arrives, it overrides the old value Origin: siddhi-execution-map:5.0.5 Syntax OBJECT map:collect( INT|LONG|FLOAT|DOUBLE|FLOAT|BOOL|STRING key, OBJECT|INT|LONG|FLOAT|DOUBLE|BOOL|STRING value) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic key Key of the map entry INT LONG FLOAT DOUBLE FLOAT BOOL STRING No Yes value Value of the map entry OBJECT INT LONG FLOAT DOUBLE BOOL STRING No Yes Examples EXAMPLE 1 from StockStream#window.lengthBatch(10) select map:collect(symbol, price) as stockDetails insert into OutputStream; For the window expiry of 10 events, the collect() function will collect attributes of key and value to a single map and return as stockDetails.","title":"collect (Aggregate Function)"},{"location":"docs/api/latest/#merge-aggregate-function_1","text":"Collect multiple maps to merge as a single map. Only distinct keys are collected, if a duplicate key arrives, it overrides the old value. Origin: siddhi-execution-map:5.0.5 Syntax OBJECT map:merge( OBJECT map) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic map Maps to be collected OBJECT No Yes Examples EXAMPLE 1 from StockStream#window.lengthBatch(2) select map:merge(map) as stockDetails insert into OutputStream; For the window expiry of 2 events, the merge() function will collect attributes of map and merge them to a single map, returned as stockDetails.","title":"merge (Aggregate Function)"},{"location":"docs/api/latest/#clear-function_1","text":"Function returns the cleared map. Origin: siddhi-execution-map:5.0.5 Syntax OBJECT map:clear( OBJECT map) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic map The map which needs to be cleared OBJECT No Yes Examples EXAMPLE 1 map:clear(stockDetails) Returns an empty map.","title":"clear (Function)"},{"location":"docs/api/latest/#clone-function_1","text":"Function returns the cloned map. Origin: siddhi-execution-map:5.0.5 Syntax OBJECT map:clone( OBJECT map) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic map The map to which needs to be cloned. OBJECT No Yes Examples EXAMPLE 1 map:clone(stockDetails) Function returns cloned map of stockDetails.","title":"clone (Function)"},{"location":"docs/api/latest/#combinebykey-function","text":"Function returns the map after combining all the maps given as parameters, such that the keys, of all the maps will be matched with an Array list of values from each map respectively. Origin: siddhi-execution-map:5.0.5 Syntax OBJECT map:combineByKey( OBJECT map, OBJECT map) OBJECT map:combineByKey( OBJECT map, OBJECT map, OBJECT ...) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic map The map into which the key-values need to copied. OBJECT No Yes Examples EXAMPLE 1 map:combineByKey(map1, map2) If map2 contains key-value pairs ('symbol': 'wso2'), ('volume' : 100), and if map2 contains key-value pairs ('symbol': 'IBM'), ('price' : 12), then the function returns the map with key value pairs as follows, (symbol: ArrayList('wso2, 'IBM')), (volume: ArrayList(100, null)) and (price: ArrayList(null, 12))","title":"combineByKey (Function)"},{"location":"docs/api/latest/#containskey-function","text":"Function checks if the map contains the key. Origin: siddhi-execution-map:5.0.5 Syntax BOOL map:containsKey( OBJECT map, INT|LONG|FLOAT|DOUBLE|BOOL|STRING key) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic map The map the needs to be checked on containing the key or not. OBJECT No Yes key The key to be checked. INT LONG FLOAT DOUBLE BOOL STRING No Yes Examples EXAMPLE 1 map:containsKey(stockDetails, '1234') Returns 'true' if the stockDetails map contains key 1234 else it returns false .","title":"containsKey (Function)"},{"location":"docs/api/latest/#containsvalue-function","text":"Function checks if the map contains the value. Origin: siddhi-execution-map:5.0.5 Syntax BOOL map:containsValue( OBJECT map, INT|LONG|FLOAT|DOUBLE|BOOL|STRING value) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic map The map the needs to be checked on containing the value or not. OBJECT No Yes value The value to be checked. INT LONG FLOAT DOUBLE BOOL STRING No Yes Examples EXAMPLE 1 map:containsValue(stockDetails, 'IBM') Returns 'true' if the stockDetails map contains value IBM else it returns false .","title":"containsValue (Function)"},{"location":"docs/api/latest/#create-function_1","text":"Function creates a map pairing the keys and their corresponding values. Origin: siddhi-execution-map:5.0.5 Syntax OBJECT map:create() OBJECT map:create( OBJECT|INT|LONG|FLOAT|DOUBLE|FLOAT|BOOL|STRING key1, OBJECT|INT|LONG|FLOAT|DOUBLE|FLOAT|BOOL|STRING value1) OBJECT map:create( OBJECT|INT|LONG|FLOAT|DOUBLE|FLOAT|BOOL|STRING key1, OBJECT|INT|LONG|FLOAT|DOUBLE|FLOAT|BOOL|STRING value1, OBJECT|INT|LONG|FLOAT|DOUBLE|FLOAT|BOOL|STRING ...) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic key1 Key 1 - OBJECT INT LONG FLOAT DOUBLE FLOAT BOOL STRING Yes Yes value1 Value 1 - OBJECT INT LONG FLOAT DOUBLE FLOAT BOOL STRING Yes Yes Examples EXAMPLE 1 map:create(1, 'one', 2, 'two', 3, 'three') This returns a map with keys 1 , 2 , 3 mapped with their corresponding values, one , two , three . EXAMPLE 2 map:create() This returns an empty map.","title":"create (Function)"},{"location":"docs/api/latest/#createfromjson-function","text":"Function returns the map created by pairing the keys with their corresponding values given in the JSON string. Origin: siddhi-execution-map:5.0.5 Syntax OBJECT map:createFromJSON( STRING json.string) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic json.string JSON as a string, which is used to create the map. STRING No Yes Examples EXAMPLE 1 map:createFromJSON(\"{\u2018symbol' : 'IBM', 'price' : 200, 'volume' : 100}\") This returns a map with the keys symbol , price , and volume , and their values, IBM , 200 and 100 respectively.","title":"createFromJSON (Function)"},{"location":"docs/api/latest/#createfromxml-function","text":"Function returns the map created by pairing the keys with their corresponding values,given as an XML string. Origin: siddhi-execution-map:5.0.5 Syntax OBJECT map:createFromXML( STRING xml.string) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic xml.string The XML string, which is used to create the map. STRING No Yes Examples EXAMPLE 1 map:createFromXML(\" stock symbol IBM /symbol price 200 /price volume 100 /volume /stock \") This returns a map with the keys symbol , price , volume , and with their values IBM , 200 and 100 respectively.","title":"createFromXML (Function)"},{"location":"docs/api/latest/#get-function_1","text":"Function returns the value corresponding to the given key from the map. Origin: siddhi-execution-map:5.0.5 Syntax OBJECT|INT|LONG|FLOAT|DOUBLE|FLOAT|BOOL|STRING map:get( OBJECT map, INT|LONG|FLOAT|DOUBLE|FLOAT|BOOL|STRING key) OBJECT|INT|LONG|FLOAT|DOUBLE|FLOAT|BOOL|STRING map:get( OBJECT map, INT|LONG|FLOAT|DOUBLE|FLOAT|BOOL|STRING key, OBJECT|INT|LONG|FLOAT|DOUBLE|FLOAT|BOOL|STRING default.value) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic map The map from where the value should be obtained. OBJECT No Yes key The key to fetch the value. INT LONG FLOAT DOUBLE FLOAT BOOL STRING No Yes default.value The value to be returned if the map does not have the key. OBJECT INT LONG FLOAT DOUBLE FLOAT BOOL STRING Yes Yes Examples EXAMPLE 1 map:get(companyMap, 1) If the companyMap has key 1 and value ABC in it's set of key value pairs. The function returns ABC . EXAMPLE 2 map:get(companyMap, 2) If the companyMap does not have any value for key 2 then the function returns null . EXAMPLE 3 map:get(companyMap, 2, 'two') If the companyMap does not have any value for key 2 then the function returns two .","title":"get (Function)"},{"location":"docs/api/latest/#isempty-function_1","text":"Function checks if the map is empty. Origin: siddhi-execution-map:5.0.5 Syntax BOOL map:isEmpty( OBJECT map) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic map The map the need to be checked whether it's empty or not. OBJECT No Yes Examples EXAMPLE 1 map:isEmpty(stockDetails) Returns 'true' if the stockDetails map is empty else it returns false .","title":"isEmpty (Function)"},{"location":"docs/api/latest/#ismap-function","text":"Function checks if the object is type of a map. Origin: siddhi-execution-map:5.0.5 Syntax BOOL map:isMap( OBJECT|INT|LONG|FLOAT|DOUBLE|FLOAT|BOOL|STRING arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The argument the need to be determined whether it's a map or not. OBJECT INT LONG FLOAT DOUBLE FLOAT BOOL STRING No Yes Examples EXAMPLE 1 map:isMap(stockDetails) Returns 'true' if the stockDetails is and an instance of java.util.Map else it returns false .","title":"isMap (Function)"},{"location":"docs/api/latest/#keys-function","text":"Function to return the keys of the map as a list. Origin: siddhi-execution-map:5.0.5 Syntax OBJECT map:keys( OBJECT map) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic map The map from which list of keys to be returned. OBJECT No Yes Examples EXAMPLE 1 map:keys(stockDetails) Returns keys of the stockDetails map.","title":"keys (Function)"},{"location":"docs/api/latest/#put-function","text":"Function returns the updated map after adding the given key-value pair. If the key already exist in the map the key is updated with the new value. Origin: siddhi-execution-map:5.0.5 Syntax OBJECT map:put( OBJECT map, OBJECT|INT|LONG|FLOAT|DOUBLE|FLOAT|BOOL|STRING key, OBJECT|INT|LONG|FLOAT|DOUBLE|FLOAT|BOOL|STRING value) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic map The map to which the value should be added. OBJECT No Yes key The key to be added. OBJECT INT LONG FLOAT DOUBLE FLOAT BOOL STRING No Yes value The value to be added. OBJECT INT LONG FLOAT DOUBLE FLOAT BOOL STRING No Yes Examples EXAMPLE 1 map:put(stockDetails , 'IBM' , '200') Function returns the updated map named stockDetails after adding the value 200 with the key IBM .","title":"put (Function)"},{"location":"docs/api/latest/#putall-function","text":"Function returns the updated map after adding all the key-value pairs from another map. If there are duplicate keys, the key will be assigned new values from the map that's being copied. Origin: siddhi-execution-map:5.0.5 Syntax OBJECT map:putAll( OBJECT to.map, OBJECT from.map) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic to.map The map into which the key-values need to copied. OBJECT No Yes from.map The map from which the key-values are copied. OBJECT No Yes Examples EXAMPLE 1 map:putAll(toMap, fromMap) If toMap contains key-value pairs ('symbol': 'wso2'), ('volume' : 100), and if fromMap contains key-value pairs ('symbol': 'IBM'), ('price' : 12), then the function returns updated toMap with key-value pairs ('symbol': 'IBM'), ('price' : 12), ('volume' : 100).","title":"putAll (Function)"},{"location":"docs/api/latest/#putifabsent-function","text":"Function returns the updated map after adding the given key-value pair if key is absent. Origin: siddhi-execution-map:5.0.5 Syntax OBJECT map:putIfAbsent( OBJECT map, INT|LONG|FLOAT|DOUBLE|BOOL|STRING key, INT|LONG|FLOAT|DOUBLE|BOOL|STRING value) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic map The map to which the value should be added. OBJECT No Yes key The key to be added. INT LONG FLOAT DOUBLE BOOL STRING No Yes value The value to be added. INT LONG FLOAT DOUBLE BOOL STRING No Yes Examples EXAMPLE 1 map:putIfAbsent(stockDetails , 1234 , 'IBM') Function returns the updated map named stockDetails after adding the value IBM with the key 1234 if key is absent from the original map.","title":"putIfAbsent (Function)"},{"location":"docs/api/latest/#remove-function_1","text":"Function returns the updated map after removing the element with the specified key. Origin: siddhi-execution-map:5.0.5 Syntax OBJECT map:remove( OBJECT map, OBJECT|INT|LONG|FLOAT|DOUBLE|FLOAT|BOOL|STRING key) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic map The map that needs to be updated. OBJECT No Yes key The key of the element that needs to removed. OBJECT INT LONG FLOAT DOUBLE FLOAT BOOL STRING No Yes Examples EXAMPLE 1 map:remove(stockDetails, 1234) This returns the updated map, stockDetails after removing the key-value pair corresponding to the key 1234 .","title":"remove (Function)"},{"location":"docs/api/latest/#replace-function","text":"Function returns the updated map after replacing the given key-value pair only if key is present. Origin: siddhi-execution-map:5.0.5 Syntax OBJECT map:replace( OBJECT map, INT|LONG|FLOAT|DOUBLE|FLOAT|BOOL|STRING key, INT|LONG|FLOAT|DOUBLE|BOOL|STRING value) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic map The map to which the key-value should be replaced. OBJECT No Yes key The key to be replaced. INT LONG FLOAT DOUBLE FLOAT BOOL STRING No Yes value The value to be replaced. INT LONG FLOAT DOUBLE BOOL STRING No Yes Examples EXAMPLE 1 map:replace(stockDetails , 1234 , 'IBM') Function returns the updated map named stockDetails after replacing the value IBM with the key 1234 if present.","title":"replace (Function)"},{"location":"docs/api/latest/#replaceall-function","text":"Function returns the updated map after replacing all the key-value pairs from another map, if keys are present. Origin: siddhi-execution-map:5.0.5 Syntax OBJECT map:replaceAll( OBJECT to.map, OBJECT from.map) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic to.map The map into which the key-values need to copied. OBJECT No Yes from.map The map from which the key-values are copied. OBJECT No Yes Examples EXAMPLE 1 map:replaceAll(toMap, fromMap) If toMap contains key-value pairs ('symbol': 'wso2'), ('volume' : 100), and if fromMap contains key-value pairs ('symbol': 'IBM'), ('price' : 12), then the function returns updated toMap with key-value pairs ('symbol': 'IBM'), ('volume' : 100).","title":"replaceAll (Function)"},{"location":"docs/api/latest/#size-function_1","text":"Function to return the size of the map. Origin: siddhi-execution-map:5.0.5 Syntax INT map:size( OBJECT map) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic map The map for which size should be returned. OBJECT No Yes Examples EXAMPLE 1 map:size(stockDetails) Returns size of the stockDetails map.","title":"size (Function)"},{"location":"docs/api/latest/#tojson-function","text":"Function converts a map into a JSON object and returns the JSON as a string. Origin: siddhi-execution-map:5.0.5 Syntax STRING map:toJSON( OBJECT map) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic map The map that needs to be converted to JSON OBJECT No Yes Examples EXAMPLE 1 map:toJSON(company) If company is a map with key-value pairs, ('symbol': 'wso2'),('volume' : 100), and ('price', 200), it returns the JSON string {\"symbol\" : \"wso2\", \"volume\" : 100 , \"price\" : 200} .","title":"toJSON (Function)"},{"location":"docs/api/latest/#toxml-function","text":"Function returns the map as an XML string. Origin: siddhi-execution-map:5.0.5 Syntax STRING map:toXML( OBJECT map) STRING map:toXML( OBJECT map, OBJECT|STRING root.element.name) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic map The map that needs to be converted to XML. OBJECT No Yes root.element.name The root element of the map. The XML root element will be ignored OBJECT STRING Yes Yes Examples EXAMPLE 1 toXML(company, 'abcCompany') If company is a map with key-value pairs, ('symbol' : 'wso2'), ('volume' : 100), and ('price' : 200), this function returns XML as a string, abcCompany symbol wso2 /symbol volume 100 /volume price 200 /price /abcCompany . EXAMPLE 2 toXML(company) If company is a map with key-value pairs, ('symbol' : 'wso2'), ('volume' : 100), and ('price' : 200), this function returns XML without root element as a string, symbol wso2 /symbol volume 100 /volume price 200 /price .","title":"toXML (Function)"},{"location":"docs/api/latest/#values-function","text":"Function to return the values of the map. Origin: siddhi-execution-map:5.0.5 Syntax OBJECT map:values( OBJECT map) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic map The map from which list if values to be returned. OBJECT No Yes Examples EXAMPLE 1 map:values(stockDetails) Returns values of the stockDetails map.","title":"values (Function)"},{"location":"docs/api/latest/#tokenize-stream-processor_2","text":"Tokenize the map and return each key, value as new attributes in events Origin: siddhi-execution-map:5.0.5 Syntax map:tokenize( OBJECT map) map:tokenize( OBJECT map, OBJECT ...) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic map Hash map containing key value pairs OBJECT No Yes Extra Return Attributes Name Description Possible Types key Key of an entry consisted in the map OBJECT value Value of an entry consisted in the map. If more than one map is given, then an Array List of values from each map is returned for the value attribute. OBJECT Examples EXAMPLE 1 define stream StockStream(symbol string, price float); from StockStream#window.lengthBatch(2) select map:collect(symbol, price) as symbolPriceMap insert into TempStream; from TempStream#map:tokenize(customMap) select key, value insert into SymbolStream; Based on the length batch window, symbolPriceMap will collect two events, and the map will then again tokenized to give 2 events with key and values being symbol name and price respectively.","title":"tokenize (Stream Processor)"},{"location":"docs/api/latest/#math","text":"","title":"Math"},{"location":"docs/api/latest/#percentile-aggregate-function","text":"This functions returns the pth percentile value of a given argument. Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:percentile( INT|LONG|FLOAT|DOUBLE arg, DOUBLE p) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value of the parameter whose percentile should be found. INT LONG FLOAT DOUBLE No Yes p Estimate of the percentile to be found (pth percentile) where p is any number greater than 0 or lesser than or equal to 100. DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (sensorId int, temperature double); from InValueStream select math:percentile(temperature, 97.0) as percentile insert into OutMediationStream; This function returns the percentile value based on the argument given. For example, math:percentile(temperature, 97.0) returns the 97 th percentile value of all the temperature events.","title":"percentile (Aggregate Function)"},{"location":"docs/api/latest/#abs-function","text":"This function returns the absolute value of the given parameter. It wraps the java.lang.Math.abs() function. Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:abs( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The parameter whose absolute value is found. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:abs(inValue) as absValue insert into OutMediationStream; Irrespective of whether the 'invalue' in the input stream holds a value of abs(3) or abs(-3),the function returns 3 since the absolute value of both 3 and -3 is 3. The result directed to OutMediationStream stream.","title":"abs (Function)"},{"location":"docs/api/latest/#acos-function","text":"If -1 = p1 = 1, this function returns the arc-cosine (inverse cosine) value of p1.If the domain is invalid, it returns NULL. The value returned is in radian scale. This function wraps the java.lang.Math.acos() function. Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:acos( FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose arc-cosine (inverse cosine) value is found. FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:acos(inValue) as acosValue insert into OutMediationStream; If the 'inValue' in the input stream is given, the function calculates the arc-cosine value of it and returns the arc-cosine value to the output stream, OutMediationStream. For example, acos(0.5) returns 1.0471975511965979.","title":"acos (Function)"},{"location":"docs/api/latest/#asin-function","text":"If -1 = p1 = 1, this function returns the arc-sin (inverse sine) value of p1. If the domain is invalid, it returns NULL. The value returned is in radian scale. This function wraps the java.lang.Math.asin() function. Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:asin( FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose arc-sin (inverse sine) value is found. FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:asin(inValue) as asinValue insert into OutMediationStream; If the 'inValue' in the input stream is given, the function calculates the arc-sin value of it and returns the arc-sin value to the output stream, OutMediationStream. For example, asin(0.5) returns 0.5235987755982989.","title":"asin (Function)"},{"location":"docs/api/latest/#atan-function","text":"1. If a single p1 is received, this function returns the arc-tangent (inverse tangent) value of p1 . 2. If p1 is received along with an optional p1 , it considers them as x and y coordinates and returns the arc-tangent (inverse tangent) value. The returned value is in radian scale. This function wraps the java.lang.Math.atan() function. Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:atan( INT|LONG|FLOAT|DOUBLE p1) DOUBLE math:atan( INT|LONG|FLOAT|DOUBLE p1, INT|LONG|FLOAT|DOUBLE p2) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose arc-tangent (inverse tangent) is found. If the optional second parameter is given this represents the x coordinate of the (x,y) coordinate pair. INT LONG FLOAT DOUBLE No Yes p2 This optional parameter represents the y coordinate of the (x,y) coordinate pair. 0D INT LONG FLOAT DOUBLE Yes Yes Examples EXAMPLE 1 define stream InValueStream (inValue1 double, inValue2 double); from InValueStream select math:atan(inValue1, inValue2) as convertedValue insert into OutMediationStream; If the 'inValue1' in the input stream is given, the function calculates the arc-tangent value of it and returns the arc-tangent value to the output stream, OutMediationStream. If both the 'inValue1' and 'inValue2' are given, then the function considers them to be x and y coordinates respectively and returns the calculated arc-tangent value to the output stream, OutMediationStream. For example, atan(12d, 5d) returns 1.1760052070951352.","title":"atan (Function)"},{"location":"docs/api/latest/#bin-function","text":"This function returns a string representation of the p1 argument, that is of either 'integer' or 'long' data type, as an unsigned integer in base 2. It wraps the java.lang.Integer.toBinaryString and java.lang.Long.toBinaryString` methods. Origin: siddhi-execution-math:5.0.4 Syntax STRING math:bin( INT|LONG p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value in either 'integer' or 'long', that should be converted into an unsigned integer of base 2. INT LONG No Yes Examples EXAMPLE 1 define stream InValueStream (inValue long); from InValueStream select math:bin(inValue) as binValue insert into OutMediationStream; If the 'inValue' in the input stream is given, the function converts it into an unsigned integer in base 2 and directs the output to the output stream, OutMediationStream. For example, bin(9) returns '1001'.","title":"bin (Function)"},{"location":"docs/api/latest/#cbrt-function","text":"This function returns the cube-root of 'p1' which is in radians. It wraps the java.lang.Math.cbrt() function. Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:cbrt( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose cube-root should be found. Input is required to be in radians. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:cbrt(inValue) as cbrtValue insert into OutMediationStream; If the 'inValue' is given, the function calculates the cube-root value for the same and directs the output to the output stream, OutMediationStream. For example, cbrt(17d) returns 2.5712815906582356.","title":"cbrt (Function)"},{"location":"docs/api/latest/#ceil-function","text":"This function returns the smallest double value, i.e., the closest to the negative infinity, that is greater than or equal to the p1 argument, and is equal to a mathematical integer. It wraps the java.lang.Math.ceil() method. Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:ceil( FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose ceiling value is found. FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:ceil(inValue) as ceilingValue insert into OutMediationStream; This function calculates the ceiling value of the given 'inValue' and directs the result to 'OutMediationStream' output stream. For example, ceil(423.187d) returns 424.0.","title":"ceil (Function)"},{"location":"docs/api/latest/#conv-function","text":"This function converts a from the fromBase base to the toBase base. Origin: siddhi-execution-math:5.0.4 Syntax STRING math:conv( STRING a, INT from.base, INT to.base) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic a The value whose base should be changed. Input should be given as a 'String'. STRING No Yes from.base The source base of the input parameter 'a'. INT No Yes to.base The target base that the input parameter 'a' should be converted into. INT No Yes Examples EXAMPLE 1 define stream InValueStream (inValue string,fromBase int,toBase int); from InValueStream select math:conv(inValue,fromBase,toBase) as convertedValue insert into OutMediationStream; If the 'inValue' in the input stream is given, and the base in which it currently resides in and the base to which it should be converted to is specified, the function converts it into a string in the target base and directs it to the output stream, OutMediationStream. For example, conv(\"7f\", 16, 10) returns \"127\".","title":"conv (Function)"},{"location":"docs/api/latest/#copysign-function","text":"This function returns a value of an input with the received magnitude and sign of another input. It wraps the java.lang.Math.copySign() function. Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:copySign( INT|LONG|FLOAT|DOUBLE magnitude, INT|LONG|FLOAT|DOUBLE sign) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic magnitude The magnitude of this parameter is used in the output attribute. INT LONG FLOAT DOUBLE No Yes sign The sign of this parameter is used in the output attribute. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue1 double, inValue2 double); from InValueStream select math:copySign(inValue1,inValue2) as copysignValue insert into OutMediationStream; If two values are provided as 'inValue1' and 'inValue2', the function copies the magnitude and sign of the second argument into the first one and directs the result to the output stream, OutMediatonStream. For example, copySign(5.6d, -3.0d) returns -5.6.","title":"copySign (Function)"},{"location":"docs/api/latest/#cos-function","text":"This function returns the cosine of p1 which is in radians. It wraps the java.lang.Math.cos() function. Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:cos( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose cosine value should be found.The input is required to be in radians. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:cos(inValue) as cosValue insert into OutMediationStream; If the 'inValue' is given, the function calculates the cosine value for the same and directs the output to the output stream, OutMediationStream. For example, cos(6d) returns 0.9601702866503661.","title":"cos (Function)"},{"location":"docs/api/latest/#cosh-function","text":"This function returns the hyperbolic cosine of p1 which is in radians. It wraps the java.lang.Math.cosh() function. Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:cosh( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose hyperbolic cosine should be found. The input is required to be in radians. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:cosh(inValue) as cosValue insert into OutMediationStream; If the 'inValue' is given, the function calculates the hyperbolic cosine value for the same and directs the output to the output stream, OutMediationStream. For example, cosh (6d) returns 201.7156361224559.","title":"cosh (Function)"},{"location":"docs/api/latest/#e-function","text":"This function returns the java.lang.Math.E constant, which is the closest double value to e, where e is the base of the natural logarithms. Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:e() Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:e() as eValue insert into OutMediationStream; This function returns the constant, 2.7182818284590452354 which is the closest double value to e and directs the output to 'OutMediationStream' output stream.","title":"e (Function)"},{"location":"docs/api/latest/#exp-function","text":"This function returns the Euler's number e raised to the power of p1 . It wraps the java.lang.Math.exp() function. Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:exp( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The power that the Euler's number e is raised to. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:exp(inValue) as expValue insert into OutMediationStream; If the 'inValue' in the inputstream holds a value, this function calculates the corresponding Euler's number 'e' and directs it to the output stream, OutMediationStream. For example, exp(10.23) returns 27722.51006805505.","title":"exp (Function)"},{"location":"docs/api/latest/#floor-function","text":"This function wraps the java.lang.Math.floor() function and returns the largest value, i.e., closest to the positive infinity, that is less than or equal to p1 , and is equal to a mathematical integer. Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:floor( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose floor value should be found. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:floor(inValue) as floorValue insert into OutMediationStream; This function calculates the floor value of the given 'inValue' input and directs the output to the 'OutMediationStream' output stream. For example, (10.23) returns 10.0.","title":"floor (Function)"},{"location":"docs/api/latest/#getexponent-function","text":"This function returns the unbiased exponent that is used in the representation of p1 . This function wraps the java.lang.Math.getExponent() function. Origin: siddhi-execution-math:5.0.4 Syntax INT math:getExponent( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of whose unbiased exponent representation should be found. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:getExponent(inValue) as expValue insert into OutMediationStream; This function calculates the unbiased exponent of a given input, 'inValue' and directs the result to the 'OutMediationStream' output stream. For example, getExponent(60984.1) returns 15.","title":"getExponent (Function)"},{"location":"docs/api/latest/#hex-function","text":"This function wraps the java.lang.Double.toHexString() function. It returns a hexadecimal string representation of the input, p1`. Origin: siddhi-execution-math:5.0.4 Syntax STRING math:hex( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose hexadecimal value should be found. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue int); from InValueStream select math:hex(inValue) as hexString insert into OutMediationStream; If the 'inValue' in the input stream is provided, the function converts this into its corresponding hexadecimal format and directs the output to the output stream, OutMediationStream. For example, hex(200) returns \"c8\".","title":"hex (Function)"},{"location":"docs/api/latest/#isinfinite-function","text":"This function wraps the java.lang.Float.isInfinite() and java.lang.Double.isInfinite() and returns true if p1 is infinitely large in magnitude and false if otherwise. Origin: siddhi-execution-math:5.0.4 Syntax BOOL math:isInfinite( FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 This is the value of the parameter that the function determines to be either infinite or finite. FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue1 double,inValue2 int); from InValueStream select math:isInfinite(inValue1) as isInfinite insert into OutMediationStream; If the value given in the 'inValue' in the input stream is of infinitely large magnitude, the function returns the value, 'true' and directs the result to the output stream, OutMediationStream'. For example, isInfinite(java.lang.Double.POSITIVE_INFINITY) returns true.","title":"isInfinite (Function)"},{"location":"docs/api/latest/#isnan-function","text":"This function wraps the java.lang.Float.isNaN() and java.lang.Double.isNaN() functions and returns true if p1 is NaN (Not-a-Number), and returns false if otherwise. Origin: siddhi-execution-math:5.0.4 Syntax BOOL math:isNan( FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter which the function determines to be either NaN or a number. FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue1 double,inValue2 int); from InValueStream select math:isNan(inValue1) as isNaN insert into OutMediationStream; If the 'inValue1' in the input stream has a value that is undefined, then the function considers it as an 'NaN' value and directs 'True' to the output stream, OutMediationStream. For example, isNan(java.lang.Math.log(-12d)) returns true.","title":"isNan (Function)"},{"location":"docs/api/latest/#ln-function","text":"This function returns the natural logarithm (base e) of p1 . Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:ln( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose natural logarithm (base e) should be found. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:ln(inValue) as lnValue insert into OutMediationStream; If the 'inValue' in the input stream is given, the function calculates its natural logarithm (base e) and directs the results to the output stream, 'OutMeditionStream'. For example, ln(11.453) returns 2.438251704415579.","title":"ln (Function)"},{"location":"docs/api/latest/#log-function","text":"This function returns the logarithm of the received number as per the given base . Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:log( INT|LONG|FLOAT|DOUBLE number, INT|LONG|FLOAT|DOUBLE base) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic number The value of the parameter whose base should be changed. INT LONG FLOAT DOUBLE No Yes base The base value of the ouput. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (number double, base double); from InValueStream select math:log(number, base) as logValue insert into OutMediationStream; If the number and the base to which it has to be converted into is given in the input stream, the function calculates the number to the base specified and directs the result to the output stream, OutMediationStream. For example, log(34, 2f) returns 5.08746284125034.","title":"log (Function)"},{"location":"docs/api/latest/#log10-function","text":"This function returns the base 10 logarithm of p1 . Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:log10( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose base 10 logarithm should be found. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:log10(inValue) as lnValue insert into OutMediationStream; If the 'inValue' in the input stream is given, the function calculates the base 10 logarithm of the same and directs the result to the output stream, OutMediatioStream. For example, log10(19.234) returns 1.2840696117100832.","title":"log10 (Function)"},{"location":"docs/api/latest/#log2-function","text":"This function returns the base 2 logarithm of p1 . Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:log2( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose base 2 logarithm should be found. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:log2(inValue) as lnValue insert into OutMediationStream; If the 'inValue' in the input stream is given, the function calculates the base 2 logarithm of the same and returns the value to the output stream, OutMediationStream. For example log2(91d) returns 6.507794640198696.","title":"log2 (Function)"},{"location":"docs/api/latest/#max-function","text":"This function returns the greater value of p1 and p2 . Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:max( INT|LONG|FLOAT|DOUBLE p1, INT|LONG|FLOAT|DOUBLE p2) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 One of the input values to be compared in order to find the larger value of the two INT LONG FLOAT DOUBLE No Yes p2 The input value to be compared with 'p1' in order to find the larger value of the two. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue1 double,inValue2 int); from InValueStream select math:max(inValue1,inValue2) as maxValue insert into OutMediationStream; If two input values 'inValue1, and 'inValue2' are given, the function compares them and directs the larger value to the output stream, OutMediationStream. For example, max(123.67d, 91) returns 123.67.","title":"max (Function)"},{"location":"docs/api/latest/#min-function","text":"This function returns the smaller value of p1 and p2 . Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:min( INT|LONG|FLOAT|DOUBLE p1, INT|LONG|FLOAT|DOUBLE p2) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 One of the input values that are to be compared in order to find the smaller value. INT LONG FLOAT DOUBLE No Yes p2 The input value that is to be compared with 'p1' in order to find the smaller value. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue1 double,inValue2 int); from InValueStream select math:min(inValue1,inValue2) as minValue insert into OutMediationStream; If two input values, 'inValue1' and 'inValue2' are given, the function compares them and directs the smaller value of the two to the output stream, OutMediationStream. For example, min(123.67d, 91) returns 91.","title":"min (Function)"},{"location":"docs/api/latest/#oct-function","text":"This function converts the input parameter p1 to octal. Origin: siddhi-execution-math:5.0.4 Syntax STRING math:oct( INT|LONG p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose octal representation should be found. INT LONG No Yes Examples EXAMPLE 1 define stream InValueStream (inValue long); from InValueStream select math:oct(inValue) as octValue insert into OutMediationStream; If the 'inValue' in the input stream is given, this function calculates the octal value corresponding to the same and directs it to the output stream, OutMediationStream. For example, oct(99l) returns \"143\".","title":"oct (Function)"},{"location":"docs/api/latest/#parsedouble-function","text":"This function returns the double value of the string received. Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:parseDouble( STRING p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that should be converted into a double value. STRING No Yes Examples EXAMPLE 1 define stream InValueStream (inValue string); from InValueStream select math:parseDouble(inValue) as output insert into OutMediationStream; If the 'inValue' in the input stream holds a value, this function converts it into the corresponding double value and directs it to the output stream, OutMediationStream. For example, parseDouble(\"123\") returns 123.0.","title":"parseDouble (Function)"},{"location":"docs/api/latest/#parsefloat-function","text":"This function returns the float value of the received string. Origin: siddhi-execution-math:5.0.4 Syntax FLOAT math:parseFloat( STRING p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that should be converted into a float value. STRING No Yes Examples EXAMPLE 1 define stream InValueStream (inValue string); from InValueStream select math:parseFloat(inValue) as output insert into OutMediationStream; The function converts the input value given in 'inValue',into its corresponding float value and directs the result into the output stream, OutMediationStream. For example, parseFloat(\"123\") returns 123.0.","title":"parseFloat (Function)"},{"location":"docs/api/latest/#parseint-function","text":"This function returns the integer value of the received string. Origin: siddhi-execution-math:5.0.4 Syntax INT math:parseInt( STRING p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that should be converted to an integer. STRING No Yes Examples EXAMPLE 1 define stream InValueStream (inValue string); from InValueStream select math:parseInt(inValue) as output insert into OutMediationStream; The function converts the 'inValue' into its corresponding integer value and directs the output to the output stream, OutMediationStream. For example, parseInt(\"123\") returns 123.","title":"parseInt (Function)"},{"location":"docs/api/latest/#parselong-function","text":"This function returns the long value of the string received. Origin: siddhi-execution-math:5.0.4 Syntax LONG math:parseLong( STRING p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that should be converted to a long value. STRING No Yes Examples EXAMPLE 1 define stream InValueStream (inValue string); from InValueStream select math:parseLong(inValue) as output insert into OutMediationStream; The function converts the 'inValue' to its corresponding long value and directs the result to the output stream, OutMediationStream. For example, parseLong(\"123\") returns 123.","title":"parseLong (Function)"},{"location":"docs/api/latest/#pi-function","text":"This function returns the java.lang.Math.PI constant, which is the closest value to pi, i.e., the ratio of the circumference of a circle to its diameter. Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:pi() Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:pi() as piValue insert into OutMediationStream; pi() always returns 3.141592653589793.","title":"pi (Function)"},{"location":"docs/api/latest/#power-function","text":"This function raises the given value to a given power. Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:power( INT|LONG|FLOAT|DOUBLE value, INT|LONG|FLOAT|DOUBLE to.power) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic value The value that should be raised to the power of 'to.power' input parameter. INT LONG FLOAT DOUBLE No Yes to.power The power to which the 'value' input parameter should be raised. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue1 double, inValue2 double); from InValueStream select math:power(inValue1,inValue2) as powerValue insert into OutMediationStream; This function raises the 'inValue1' to the power of 'inValue2' and directs the output to the output stream, 'OutMediationStream. For example, (5.6d, 3.0d) returns 175.61599999999996.","title":"power (Function)"},{"location":"docs/api/latest/#rand-function","text":"This returns a stream of pseudo-random numbers when a sequence of calls are sent to the rand() . Optionally, it is possible to define a seed, i.e., rand(seed) using which the pseudo-random numbers are generated. These functions internally use the java.util.Random class. Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:rand() DOUBLE math:rand( INT|LONG seed) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic seed An optional seed value that will be used to generate the random number sequence. defaultSeed INT LONG Yes Yes Examples EXAMPLE 1 define stream InValueStream (symbol string, price long, volume long); from InValueStream select symbol, math:rand() as randNumber select math:oct(inValue) as octValue insert into OutMediationStream; In the example given above, a random double value between 0 and 1 will be generated using math:rand().","title":"rand (Function)"},{"location":"docs/api/latest/#round-function","text":"This function returns the value of the input argument rounded off to the closest integer/long value. Origin: siddhi-execution-math:5.0.4 Syntax INT|LONG math:round( FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that should be rounded off to the closest integer/long value. FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:round(inValue) as roundValue insert into OutMediationStream; The function rounds off 'inValue1' to the closest int/long value and directs the output to the output stream, 'OutMediationStream'. For example, round(3252.353) returns 3252.","title":"round (Function)"},{"location":"docs/api/latest/#signum-function","text":"This returns +1, 0, or -1 for the given positive, zero and negative values respectively. This function wraps the java.lang.Math.signum() function. Origin: siddhi-execution-math:5.0.4 Syntax INT math:signum( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that should be checked to be positive, negative or zero. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:signum(inValue) as sign insert into OutMediationStream; The function evaluates the 'inValue' given to be positive, negative or zero and directs the result to the output stream, 'OutMediationStream'. For example, signum(-6.32d) returns -1.","title":"signum (Function)"},{"location":"docs/api/latest/#sin-function","text":"This returns the sine of the value given in radians. This function wraps the java.lang.Math.sin() function. Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:sin( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose sine value should be found. Input is required to be in radians. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:sin(inValue) as sinValue insert into OutMediationStream; The function calculates the sine value of the given 'inValue' and directs the output to the output stream, 'OutMediationStream. For example, sin(6d) returns -0.27941549819892586.","title":"sin (Function)"},{"location":"docs/api/latest/#sinh-function","text":"This returns the hyperbolic sine of the value given in radians. This function wraps the java.lang.Math.sinh() function. Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:sinh( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose hyperbolic sine value should be found. Input is required to be in radians. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:sinh(inValue) as sinhValue insert into OutMediationStream; This function calculates the hyperbolic sine value of 'inValue' and directs the output to the output stream, 'OutMediationStream'. For example, sinh(6d) returns 201.71315737027922.","title":"sinh (Function)"},{"location":"docs/api/latest/#sqrt-function","text":"This function returns the square-root of the given value. It wraps the java.lang.Math.sqrt() s function. Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:sqrt( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose square-root value should be found. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:sqrt(inValue) as sqrtValue insert into OutMediationStream; The function calculates the square-root value of the 'inValue' and directs the output to the output stream, 'OutMediationStream'. For example, sqrt(4d) returns 2.","title":"sqrt (Function)"},{"location":"docs/api/latest/#tan-function","text":"This function returns the tan of the given value in radians. It wraps the java.lang.Math.tan() function. Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:tan( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose tan value should be found. Input is required to be in radians. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:tan(inValue) as tanValue insert into OutMediationStream; This function calculates the tan value of the 'inValue' given and directs the output to the output stream, 'OutMediationStream'. For example, tan(6d) returns -0.29100619138474915.","title":"tan (Function)"},{"location":"docs/api/latest/#tanh-function","text":"This function returns the hyperbolic tangent of the value given in radians. It wraps the java.lang.Math.tanh() function. Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:tanh( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value of the parameter whose hyperbolic tangent value should be found. Input is required to be in radians. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:tanh(inValue) as tanhValue insert into OutMediationStream; If the 'inVaue' in the input stream is given, this function calculates the hyperbolic tangent value of the same and directs the output to 'OutMediationStream' stream. For example, tanh(6d) returns 0.9999877116507956.","title":"tanh (Function)"},{"location":"docs/api/latest/#todegrees-function","text":"This function converts the value given in radians to degrees. It wraps the java.lang.Math.toDegrees() function. Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:toDegrees( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The input value in radians that should be converted to degrees. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:toDegrees(inValue) as degreesValue insert into OutMediationStream; The function converts the 'inValue' in the input stream from radians to degrees and directs the output to 'OutMediationStream' output stream. For example, toDegrees(6d) returns 343.77467707849394.","title":"toDegrees (Function)"},{"location":"docs/api/latest/#toradians-function","text":"This function converts the value given in degrees to radians. It wraps the java.lang.Math.toRadians() function. Origin: siddhi-execution-math:5.0.4 Syntax DOUBLE math:toRadians( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The input value in degrees that should be converted to radians. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 define stream InValueStream (inValue double); from InValueStream select math:toRadians(inValue) as radiansValue insert into OutMediationStream; This function converts the input, from degrees to radians and directs the result to 'OutMediationStream' output stream. For example, toRadians(6d) returns 0.10471975511965977.","title":"toRadians (Function)"},{"location":"docs/api/latest/#rdbms","text":"","title":"Rdbms"},{"location":"docs/api/latest/#cud-stream-processor","text":"This function performs SQL CUD (INSERT, UPDATE, DELETE) queries on data sources. Note: This function to work data sources should be set at the Siddhi Manager level. Origin: siddhi-store-rdbms:7.0.2 Syntax rdbms:cud( STRING datasource.name, STRING query) rdbms:cud( STRING datasource.name, STRING query, STRING|BOOL|INT|DOUBLE|FLOAT|LONG parameter) rdbms:cud( STRING datasource.name, STRING query, STRING|BOOL|INT|DOUBLE|FLOAT|LONG parameter, STRING|BOOL|INT|DOUBLE|FLOAT|LONG ...) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic datasource.name The name of the datasource for which the query should be performed. If Siddhi is used as a Java/Python library the datasource should be explicitly set in the siddhi manager in order for the function to work. STRING No No query The update, delete, or insert query(formatted according to the relevant database type) that needs to be performed. STRING No Yes parameter If the second parameter is a parametrised SQL query, then siddhi attributes can be passed to set the values of the parameters STRING BOOL INT DOUBLE FLOAT LONG Yes Yes System Parameters Name Description Default Value Possible Parameters perform.CUD.operations If this parameter is set to 'true', the RDBMS CUD function is enabled to perform CUD operations. false true false Extra Return Attributes Name Description Possible Types numRecords The number of records manipulated by the query. INT Examples EXAMPLE 1 from TriggerStream#rdbms:cud(\"SAMPLE_DB\", \"UPDATE Customers_Table SET customerName='abc' where customerName='xyz'\") select numRecords insert into RecordStream; This query updates the events from the input stream named 'TriggerStream' with an additional attribute named 'numRecords', of which the value indicates the number of records manipulated. The updated events are inserted into an output stream named 'RecordStream'. EXAMPLE 2 from TriggerStream#rdbms:cud(\"SAMPLE_DB\", \"UPDATE Customers_Table SET customerName=? where customerName=?\", changedName, previousName) select numRecords insert into RecordStream; This query updates the events from the input stream named 'TriggerStream' with an additional attribute named 'numRecords', of which the value indicates the number of records manipulated. The updated events are inserted into an output stream named 'RecordStream'. Here the values of attributes changedName and previousName in the event will be set to the query.","title":"cud (Stream Processor)"},{"location":"docs/api/latest/#query-stream-processor","text":"This function performs SQL retrieval queries on data sources. Note: This function to work data sources should be set at the Siddhi Manager level. Origin: siddhi-store-rdbms:7.0.2 Syntax rdbms:query( STRING datasource.name, STRING attribute.definition.list, STRING query) rdbms:query( STRING datasource.name, STRING attribute.definition.list, STRING query, STRING|BOOL|INT|DOUBLE|FLOAT|LONG parameter) rdbms:query( STRING datasource.name, STRING attribute.definition.list, STRING query, STRING|BOOL|INT|DOUBLE|FLOAT|LONG parameter, STRING|BOOL|INT|DOUBLE|FLOAT|LONG ...) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic datasource.name The name of the datasource for which the query should be performed. If Siddhi is used as a Java/Python library the datasource should be explicitly set in the siddhi manager in order for the function to work. STRING No No attribute.definition.list This is provided as a comma-separated list in the ' AttributeName AttributeType ' format. The SQL query is expected to return the attributes in the given order. e.g., If one attribute is defined here, the SQL query should return one column result set. If more than one column is returned, then the first column is processed. The Siddhi data types supported are 'STRING', 'INT', 'LONG', 'DOUBLE', 'FLOAT', and 'BOOL'. Mapping of the Siddhi data type to the database data type can be done as follows, Siddhi Datatype - Datasource Datatype STRING - CHAR , VARCHAR , LONGVARCHAR INT - INTEGER LONG - BIGINT DOUBLE - DOUBLE FLOAT - REAL BOOL - BIT STRING No No query The select query(formatted according to the relevant database type) that needs to be performed STRING No Yes parameter If the second parameter is a parametrised SQL query, then siddhi attributes can be passed to set the values of the parameters STRING BOOL INT DOUBLE FLOAT LONG Yes Yes Extra Return Attributes Name Description Possible Types attributeName The return attributes will be the ones defined in the parameter attribute.definition.list . STRING INT LONG DOUBLE FLOAT BOOL Examples EXAMPLE 1 from TriggerStream#rdbms:query('SAMPLE_DB', 'creditcardno string, country string, transaction string, amount int', 'select * from Transactions_Table') select creditcardno, country, transaction, amount insert into recordStream; Events inserted into recordStream includes all records matched for the query i.e an event will be generated for each record retrieved from the datasource. The event will include as additional attributes, the attributes defined in the attribute.definition.list (creditcardno, country, transaction, amount). EXAMPLE 2 from TriggerStream#rdbms:query('SAMPLE_DB', 'creditcardno string, country string,transaction string, amount int', 'select * from where country=?', countrySearchWord) select creditcardno, country, transaction, amount insert into recordStream; Events inserted into recordStream includes all records matched for the query i.e an event will be generated for each record retrieved from the datasource. The event will include as additional attributes, the attributes defined in the attribute.definition.list (creditcardno, country, transaction, amount). countrySearchWord value from the event will be set in the query when querying the datasource.","title":"query (Stream Processor)"},{"location":"docs/api/latest/#regex","text":"","title":"Regex"},{"location":"docs/api/latest/#find-function","text":"Finds the subsequence that matches the given regex pattern. Origin: siddhi-execution-regex:5.0.5 Syntax BOOL regex:find( STRING regex, STRING input.sequence) BOOL regex:find( STRING regex, STRING input.sequence, INT starting.index) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic regex A regular expression that is matched to a sequence in order to find the subsequence of the same. For example, \\d\\d(.*)WSO2 . STRING No Yes input.sequence The input sequence to be matched with the regular expression. For example, 21 products are produced by WSO2 . STRING No Yes starting.index The starting index of the input sequence from where the input sequence ismatched with the given regex pattern.For example, 10 . 0 INT Yes Yes Examples EXAMPLE 1 regex:find('\\d\\d(.*)WSO2', '21 products are produced by WSO2 currently') This method attempts to find the subsequence of the input.sequence that matches the regex pattern, \\d\\d(. )WSO2 . It returns true as a subsequence exists. EXAMPLE 2 regex:find('\\d\\d(.*)WSO2', '21 products are produced by WSO2.', 4) This method attempts to find the subsequence of the input.sequence that matches the regex pattern, \\d\\d(. )WSO2 starting from index 4 . It returns 'false' as subsequence does not exists.","title":"find (Function)"},{"location":"docs/api/latest/#group-function","text":"Returns the subsequence captured by the given group during the regex match operation. Origin: siddhi-execution-regex:5.0.5 Syntax STRING regex:group( STRING regex, STRING input.sequence, INT group.id) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic regex A regular expression. For example, \\d\\d(.*)WSO2. STRING No Yes input.sequence The input sequence to be matched with the regular expression. For example, 2 1 products are produced by WSO2 . STRING No Yes group.id The given group id of the regex expression. For example, 2 . INT No Yes Examples EXAMPLE 1 regex:group('\\d\\d(.*)(WSO2.*)(WSO2.*)', '21 products are produced within 10 years by WSO2 currently by WSO2 employees', 3) Function returns 'WSO2 employees', the subsequence captured by the groupID 3 according to the regex pattern, \\d\\d(. )(WSO2. )(WSO2.*) .","title":"group (Function)"},{"location":"docs/api/latest/#lookingat-function","text":"Matches the input.sequence from the beginning against the regex pattern, and unlike regex:matches() it does not require that the entire input.sequence be matched. Origin: siddhi-execution-regex:5.0.5 Syntax BOOL regex:lookingAt( STRING regex, STRING input.sequence) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic regex A regular expression. For example, \\d\\d(.*)WSO2 . STRING No Yes input.sequence The input sequence to be matched with the regular expression. For example, 21 products are produced by WSO2 . STRING No Yes Examples EXAMPLE 1 regex:lookingAt('\\d\\d(.*)(WSO2.*)', '21 products are produced by WSO2 currently in Sri Lanka') Function matches the input.sequence against the regex pattern, \\d\\d(. )(WSO2. ) from the beginning, and as it matches it returns true . EXAMPLE 2 regex:lookingAt('WSO2(.*)middleware(.*)', 'sample test string and WSO2 is situated in trace and it's a middleware company') Function matches the input.sequence against the regex pattern, WSO2(. )middleware(. ) from the beginning, and as it does not match it returns false .","title":"lookingAt (Function)"},{"location":"docs/api/latest/#matches-function","text":"Matches the entire input.sequence against the regex pattern. Origin: siddhi-execution-regex:5.0.5 Syntax BOOL regex:matches( STRING regex, STRING input.sequence) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic regex A regular expression. For example, \\d\\d(.*)WSO2 . STRING No Yes input.sequence The input sequence to be matched with the regular expression. For example, 21 products are produced by WSO2 . STRING No Yes Examples EXAMPLE 1 regex:matches('WSO2(.*)middleware(.*)', 'WSO2 is situated in trace and its a middleware company') Function matches the entire input.sequence against WSO2(. )middleware(. ) regex pattern, and as it matches it returns true . EXAMPLE 2 regex:matches('WSO2(.*)middleware', 'WSO2 is situated in trace and its a middleware company') Function matches the entire input.sequence against WSO2(.*)middleware regex pattern. As it does not match it returns false .","title":"matches (Function)"},{"location":"docs/api/latest/#reorder","text":"","title":"Reorder"},{"location":"docs/api/latest/#akslack-stream-processor","text":"Stream processor performs reordering of out-of-order events optimized for a givenparameter using AQ-K-Slack algorithm . This is best for reordering events on attributes those are used for aggregations.data . Origin: siddhi-execution-reorder:5.0.3 Syntax reorder:akslack( LONG timestamp, INT|FLOAT|LONG|DOUBLE correlation.field) reorder:akslack( LONG timestamp, INT|FLOAT|LONG|DOUBLE correlation.field, LONG batch.size) reorder:akslack( LONG timestamp, INT|FLOAT|LONG|DOUBLE correlation.field, LONG batch.size, LONG timeout) reorder:akslack( LONG timestamp, INT|FLOAT|LONG|DOUBLE correlation.field, LONG batch.size, LONG timeout, LONG max.k) reorder:akslack( LONG timestamp, INT|FLOAT|LONG|DOUBLE correlation.field, LONG batch.size, LONG timeout, LONG max.k, BOOL discard.late.arrival) reorder:akslack( LONG timestamp, INT|FLOAT|LONG|DOUBLE correlation.field, LONG batch.size, LONG timeout, LONG max.k, BOOL discard.late.arrival, DOUBLE error.threshold, DOUBLE confidence.level) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic timestamp The event timestamp on which the events should be ordered. LONG No Yes correlation.field By monitoring the changes in this field Alpha K-Slack dynamically optimises its behavior. This field is used to calculate the runtime window coverage threshold, which represents the upper limit set for unsuccessfully handled late arrivals. INT FLOAT LONG DOUBLE No Yes batch.size The parameter 'batch.size' denotes the number of events that should be considered in the calculation of an alpha value. This should be greater than or equal to 15. 10,000 LONG Yes No timeout A timeout value in milliseconds, where the buffered events who are older than the given timeout period get flushed every second. -1 (timeout is infinite) LONG Yes No max.k The maximum K-Slack window threshold ('K' parameter). 9,223,372,036,854,775,807 (The maximum Long value) LONG Yes No discard.late.arrival If set to true the processor would discarded the out-of-order events arriving later than the K-Slack window, and in otherwise it allows the late arrivals to proceed. false BOOL Yes No error.threshold The error threshold to be applied in Alpha K-Slack algorithm. 0.03 (3%) DOUBLE Yes No confidence.level The confidence level to be applied in Alpha K-Slack algorithm. 0.95 (95%) DOUBLE Yes No Examples EXAMPLE 1 define stream StockStream (eventTime long, symbol string, volume long); @info(name = 'query1') from StockStream#reorder:akslack(eventTime, volume, 20)#window.time(5 min) select eventTime, symbol, sum(volume) as total insert into OutputStream; The query reorders events based on the 'eventTime' attribute value and optimises for aggregating 'volume' attribute considering last 20 events.","title":"akslack (Stream Processor)"},{"location":"docs/api/latest/#kslack-stream-processor","text":"Stream processor performs reordering of out-of-order events using K-Slack algorithm . Origin: siddhi-execution-reorder:5.0.3 Syntax reorder:kslack( LONG timestamp) reorder:kslack( LONG timestamp, LONG timeout) reorder:kslack( LONG timestamp, BOOL discard.late.arrival) reorder:kslack( LONG timestamp, LONG timeout, LONG max.k) reorder:kslack( LONG timestamp, LONG timeout, BOOL discard.late.arrival) reorder:kslack( LONG timestamp, LONG timeout, LONG max.k, BOOL discard.late.arrival) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic timestamp The event timestamp on which the events should be ordered. LONG No Yes timeout A timeout value in milliseconds, where the buffered events who are older than the given timeout period get flushed every second. -1 (timeout is infinite) LONG Yes No max.k The maximum K-Slack window threshold ('K' parameter). 9,223,372,036,854,775,807 (The maximum Long value) LONG Yes No discard.late.arrival If set to true the processor would discarded the out-of-order events arriving later than the K-Slack window, and in otherwise it allows the late arrivals to proceed. false BOOL Yes No Examples EXAMPLE 1 define stream StockStream (eventTime long, symbol string, volume long); @info(name = 'query1') from StockStream#reorder:kslack(eventTime, 5000) select eventTime, symbol, volume insert into OutputStream; The query reorders events based on the 'eventTime' attribute value, and it forcefully flushes all the events who have arrived older than the given 'timeout' value ( 5000 milliseconds) every second.","title":"kslack (Stream Processor)"},{"location":"docs/api/latest/#script","text":"","title":"Script"},{"location":"docs/api/latest/#javascript-script","text":"This extension allows you to include JavaScript functions within the Siddhi Query Language. Origin: siddhi-script-js:5.0.2 Syntax define function FunctionName [javascript] return type { // Script code }; Examples EXAMPLE 1 define function concatJ[JavaScript] return string {\" var str1 = data[0]; var str2 = data[1]; var str3 = data[2]; var res = str1.concat(str2,str3); return res; }; This JS function will consume 3 var variables, concatenate them and will return as a string","title":"javascript (Script)"},{"location":"docs/api/latest/#sink","text":"","title":"Sink"},{"location":"docs/api/latest/#email-sink","text":"The email sink uses the 'smtp' server to publish events via emails. The events can be published in 'text', 'xml' or 'json' formats. The user can define email sink parameters in either the ' SP_HOME /conf/ PROFILE /deployment yaml' file or in the stream definition. The email sink first checks the stream definition for parameters, and if they are no configured there, it checks the 'deployment.yaml' file. If the parameters are not configured in either place, default values are considered for optional parameters. If you need to configure server system parameters that are not provided as options in the stream definition, then those parameters need to be defined them in the 'deployment.yaml' file under 'email sink properties'. For more information about the SMTP server parameters, see https://javaee.github.io/javamail/SMTP-Transport. Further, some email accounts are required to enable the 'access to less secure apps' option. For gmail accounts, you can enable this option via https://myaccount.google.com/lesssecureapps. Origin: siddhi-io-email:2.0.5 Syntax @sink(type=\"email\", username=\" STRING \", address=\" STRING \", password=\" STRING \", host=\" STRING \", port=\" INT \", ssl.enable=\" BOOL \", auth=\" BOOL \", content.type=\" STRING \", subject=\" STRING \", to=\" STRING \", cc=\" STRING \", bcc=\" STRING \", attachments=\" STRING \", connection.pool.size=\" INT \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic username The username of the email account that is used to send emails. e.g., 'abc' is the username of the 'abc@gmail.com' account. STRING No No address The address of the email account that is used to send emails. STRING No No password The password of the email account. STRING No No host The host name of the SMTP server. e.g., 'smtp.gmail.com' is a host name for a gmail account. The default value 'smtp.gmail.com' is only valid if the email account is a gmail account. smtp.gmail.com STRING Yes No port The port that is used to create the connection. '465' the default value is only valid is SSL is enabled. INT Yes No ssl.enable This parameter specifies whether the connection should be established via a secure connection or not. The value can be either 'true' or 'false'. If it is 'true', then the connection is establish via the 493 port which is a secure connection. true BOOL Yes No auth This parameter specifies whether to use the 'AUTH' command when authenticating or not. If the parameter is set to 'true', an attempt is made to authenticate the user using the 'AUTH' command. true BOOL Yes No content.type The content type can be either 'text/plain' or 'text/html'. text/plain STRING Yes No subject The subject of the mail to be send. STRING No Yes to The address of the 'to' recipient. If there are more than one 'to' recipients, then all the required addresses can be given as a comma-separated list. STRING No Yes cc The address of the 'cc' recipient. If there are more than one 'cc' recipients, then all the required addresses can be given as a comma-separated list. None STRING Yes No bcc The address of the 'bcc' recipient. If there are more than one 'bcc' recipients, then all the required addresses can be given as a comma-separated list. None STRING Yes No attachments File paths of the files that need to be attached to the email. These paths should be absolute paths. They can be either directories or files . If the path is to a directory, all the files located at the first level (i.e., not within another sub directory) are attached. None STRING Yes Yes connection.pool.size Number of concurrent Email client connections. 1 INT Yes No System Parameters Name Description Default Value Possible Parameters mail.smtp.ssl.trust If this parameter is se, and a socket factory has not been specified, it enables the use of a MailSSLSocketFactory. If this parameter is set to \" \", all the hosts are trusted. If it is set to a whitespace-separated list of hosts, only those specified hosts are trusted. If not, the hosts trusted depends on the certificate presented by the server. String mail.smtp.connectiontimeout The socket connection timeout value in milliseconds. infinite timeout Any Integer mail.smtp.timeout The socket I/O timeout value in milliseconds. infinite timeout Any Integer mail.smtp.from The email address to use for the SMTP MAIL command. This sets the envelope return address. Defaults to msg.getFrom() or InternetAddress.getLocalAddress(). Any valid email address mail.smtp.localport The local port number to bind to when creating the SMTP socket. Defaults to the port number picked by the Socket class. Any Integer mail.smtp.ehlo If this parameter is set to 'false', you must not attempt to sign in with the EHLO command. true true or false mail.smtp.auth.login.disable If this is set to 'true', it is not allowed to use the 'AUTH LOGIN' command. false true or false mail.smtp.auth.plain.disable If this parameter is set to 'true', it is not allowed to use the 'AUTH PLAIN' command. false true or false mail.smtp.auth.digest-md5.disable If this parameter is set to 'true', it is not allowed to use the 'AUTH DIGEST-MD5' command. false true or false mail.smtp.auth.ntlm.disable If this parameter is set to 'true', it is not allowed to use the 'AUTH NTLM' command false true or false mail.smtp.auth.ntlm.domain The NTLM authentication domain. None The valid NTLM authentication domain name. mail.smtp.auth.ntlm.flags NTLM protocol-specific flags. For more details, see http://curl.haxx.se/rfc/ntlm.html#theNtlmFlags. None Valid NTLM protocol-specific flags. mail.smtp.dsn.notify The NOTIFY option to the RCPT command. None Either 'NEVER', or a combination of 'SUCCESS', 'FAILURE', and 'DELAY' (separated by commas). mail.smtp.dsn.ret The 'RET' option to the 'MAIL' command. None Either 'FULL' or 'HDRS'. mail.smtp.sendpartial If this parameter is set to 'true' and a message is addressed to both valid and invalid addresses, the message is sent with a log that reports the partial failure with a 'SendFailedException' error. If this parameter is set to 'false' (which is default), the message is not sent to any of the recipients when the recipient lists contain one or more invalid addresses. false true or false mail.smtp.sasl.enable If this parameter is set to 'true', the system attempts to use the 'javax.security.sasl' package to choose an authentication mechanism for the login. false true or false mail.smtp.sasl.mechanisms Enter a space or a comma-separated list of SASL mechanism names that the system shouldt try to use. None mail.smtp.sasl.authorizationid The authorization ID to be used in the SASL authentication. If no value is specified, the authentication ID (i.e., username) is used. username Valid ID mail.smtp.sasl.realm The realm to be used with the 'DIGEST-MD5' authentication. None mail.smtp.quitwait If this parameter is set to 'false', the 'QUIT' command is issued and the connection is immediately closed. If this parameter is set to 'true' (which is default), the transport waits for the response to the QUIT command. false true or false mail.smtp.reportsuccess If this parameter is set to 'true', the transport to includes an 'SMTPAddressSucceededException' for each address to which the message is successfully delivered. false true or false mail.smtp.socketFactory If this parameter is set to a class that implements the 'javax.net.SocketFactory' interface, this class is used to create SMTP sockets. None Socket Factory mail.smtp.socketFactory.class If this parameter is set, it specifies the name of a class that implements the 'javax.net.SocketFactory interface'. This class is used to create SMTP sockets. None mail.smtp.socketFactory.fallback If this parameter is set to 'true', the failure to create a socket using the specified socket factory class causes the socket to be created using the 'java.net.Socket' class. true true or false mail.smtp.socketFactory.port This specifies the port to connect to when using the specified socket factory. 25 Valid port number mail.smtp.ssl.protocols This specifies the SSL protocols that need to be enabled for the SSL connections. None This parameter specifies a whitespace separated list of tokens that are acceptable to the 'javax.net.ssl.SSLSocket.setEnabledProtocols' method. mail.smtp.starttls.enable If this parameter is set to 'true', it is possible to issue the 'STARTTLS' command (if supported by the server) to switch the connection to a TLS-protected connection before issuing any login commands. false true or false mail.smtp.starttls.required If this parameter is set to 'true', it is required to use the 'STARTTLS' command. If the server does not support the 'STARTTLS' command, or if the command fails, the connection method will fail. false true or false mail.smtp.socks.host This specifies the host name of a SOCKS5 proxy server to be used for the connections to the mail server. None mail.smtp.socks.port This specifies the port number for the SOCKS5 proxy server. This needs to be used only if the proxy server is not using the standard port number 1080. 1080 valid port number mail.smtp.auth.ntlm.disable If this parameter is set to 'true', the AUTH NTLM command cannot be issued. false true or false mail.smtp.mailextension The extension string to be appended to the MAIL command. None mail.smtp.userset If this parameter is set to 'true', you should use the 'RSET' command instead of the 'NOOP' command in the 'isConnected' method. In some scenarios, 'sendmail' responds slowly after many 'NOOP' commands. This is avoided by using 'RSET' instead. false true or false Examples EXAMPLE 1 @sink(type='email', @map(type ='json'), username='sender.account', address='sender.account@gmail.com',password='account.password',subject='Alerts from Wso2 Stream Processor',to='{{email}}',)define stream FooStream (email string, loginId int, name string); This example illustrates how to publish events via an email sink based on the values provided for the mandatory parameters. As shown in the example, it publishes events from the 'FooStream' in 'json' format as emails to the specified 'to' recipients via the email sink. The email is sent from the 'sender.account@gmail.com' email address via a secure connection. EXAMPLE 2 @sink(type='email', @map(type ='json'), subject='Alerts from Wso2 Stream Processor',to='{{email}}',)define stream FooStream (email string, loginId int, name string); This example illustrates how to configure the query parameters and the system parameters in the 'deployment.yaml' file. Corresponding parameters need to be configured under 'email', and namespace:'sink' as follows: siddhi: extensions: - extension: name:'email' namespace:'sink' properties: username: sender's email username address: sender's email address password: sender's email password As shown in the example, events from the FooStream are published in 'json' format via the email sink as emails to the given 'to' recipients. The email is sent from the 'sender.account@gmail.com' address via a secure connection. EXAMPLE 3 @sink(type='email', @map(type ='json'), username='sender.account', address='sender.account@gmail.com',password='account.password',host='smtp.gmail.com',port='465',ssl.enable='true',auth='true',content.type='text/html',subject='Alerts from Wso2 Stream Processor-{{name}}',to='to1.account@gmail.com, to2.account@gmail.com',cc='cc1.account@gmail.com, cc2.account@gmail.com',bcc='bcc1.account@gmail.com)define stream FooStream (name string, age int, country string); This example illustrates how to publish events via the email sink. Events from the 'FooStream' stream are published in 'xml' format via the email sink as a text/html message and sent to the specified 'to', 'cc', and 'bcc' recipients via a secure connection. The 'name' namespace in the 'subject' attribute is the value of the 'name' parameter in the corresponding output event. EXAMPLE 4 @sink(type='email', @map(type ='json'), username='sender.account', address='sender.account@gmail.com',password='account.password',host='smtp.gmail.com',port='465',ssl.enable='true',auth='true',content.type='text/html',subject='Alerts from Wso2 Stream Processor-{{name}}',to='to1.account@gmail.com, to2.account@gmail.com',cc='cc1.account@gmail.com, cc2.account@gmail.com',bcc='bcc1.account@gmail.comattachments= '{{attachments}}')define stream FooStream (name string, age int, country string, attachments string); This example illustrates how to publish events via the email sink. Here, the email also contains attachments. Events from the FooStream are published in 'xml' format via the email sink as a 'text/html' message to the specified 'to','cc', and 'bcc' recipients via a secure connection. The 'name' namespace in the 'subject' attribute is the value for the 'name' parameter in the corresponding output event. The attachments included in the email message are the local files available in the path specified as the value for the 'attachments' attribute.","title":"email (Sink)"},{"location":"docs/api/latest/#file-sink","text":"File Sink can be used to publish (write) event data which is processed within siddhi to files. Siddhi-io-file sink provides support to write both textual and binary data into files Origin: siddhi-io-file:2.0.3 Syntax @sink(type=\"file\", file.uri=\" STRING \", append=\" BOOL \", add.line.separator=\" BOOL \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic file.uri Used to specify the file for data to be written. STRING No Yes append This parameter is used to specify whether the data should be append to the file or not. If append = 'true', data will be write at the end of the file without changing the existing content. If file does not exist, a new fill will be crated and then data will be written. If append append = 'false', If given file exists, existing content will be deleted and then data will be written back to the file. If given file does not exist, a new file will be created and then data will be written on it. true BOOL Yes No add.line.separator This parameter is used to specify whether events added to the file should be separated by a newline. If add.event.separator= 'true',then a newline will be added after data is added to the file. true. (However, if csv mapper is used, it is false) BOOL Yes No Examples EXAMPLE 1 @sink(type='file', @map(type='json'), append='false', file.uri='/abc/{{symbol}}.txt') define stream BarStream (symbol string, price float, volume long); Under above configuration, for each event, a file will be generated if there's no such a file,and then data will be written to that file as json messagesoutput will looks like below. { \"event\":{ \"symbol\":\"WSO2\", \"price\":55.6, \"volume\":100 } }","title":"file (Sink)"},{"location":"docs/api/latest/#grpc-sink","text":"This extension publishes event data encoded into GRPC Classes as defined in the user input jar. This extension has a default gRPC service classes added. The default service is called \"EventService\". Please find the protobuf definition here . If we want to use our custom gRPC services, we have to pack auto-generated gRPC service classes and protobuf classes into a jar file and add it into the project classpath (or to the jars folder in the siddhi-tooling folder if we use it with siddhi-tooling ). Please find the custom protobuf definition that uses in examples here . This grpc sink is used for scenarios where we send a request and don't expect a response back. I.e getting a google.protobuf.Empty response back. Origin: siddhi-io-grpc:1.0.5 Syntax @sink(type=\"grpc\", publisher.url=\" STRING \", headers=\" STRING \", idle.timeout=\" LONG \", keep.alive.time=\" LONG \", keep.alive.timeout=\" LONG \", keep.alive.without.calls=\" BOOL \", enable.retry=\" BOOL \", max.retry.attempts=\" INT \", retry.buffer.size=\" LONG \", per.rpc.buffer.size=\" LONG \", channel.termination.waiting.time=\" LONG \", truststore.file=\" STRING \", truststore.password=\" STRING \", truststore.algorithm=\" STRING \", tls.store.type=\" STRING \", keystore.file=\" STRING \", keystore.password=\" STRING \", keystore.algorithm=\" STRING \", enable.ssl=\" BOOL \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic publisher.url The url to which the outgoing events should be published via this extension. This url should consist the host hostPort, port, fully qualified service name, method name in the following format. grpc://0.0.0.0:9763/ serviceName / methodName For example: grpc://0.0.0.0:9763/org.wso2.grpc.EventService/consume STRING No No headers GRPC Request headers in format \"' key : value ',' key : value '\" . If header parameter is not provided just the payload is sent - STRING Yes No idle.timeout Set the duration in seconds without ongoing RPCs before going to idle mode. 1800 LONG Yes No keep.alive.time Sets the time in seconds without read activity before sending a keepalive ping. Keepalives can increase the load on services so must be used with caution. By default set to Long.MAX_VALUE which disables keep alive pinging. Long.MAX_VALUE LONG Yes No keep.alive.timeout Sets the time in seconds waiting for read activity after sending a keepalive ping. 20 LONG Yes No keep.alive.without.calls Sets whether keepalive will be performed when there are no outstanding RPC on a connection. false BOOL Yes No enable.retry Enables the retry mechanism provided by the gRPC library. false BOOL Yes No max.retry.attempts Sets max number of retry attempts. The total number of retry attempts for each RPC will not exceed this number even if service config may allow a higher number. 5 INT Yes No retry.buffer.size Sets the retry buffer size in bytes. If the buffer limit is exceeded, no RPC could retry at the moment, and in hedging case all hedges but one of the same RPC will cancel. 16777216 LONG Yes No per.rpc.buffer.size Sets the per RPC buffer limit in bytes used for retry. The RPC is not retriable if its buffer limit is exceeded. 1048576 LONG Yes No channel.termination.waiting.time The time in seconds to wait for the channel to become terminated, giving up if the timeout is reached. 5 LONG Yes No truststore.file the file path of truststore. If this is provided then server authentication is enabled - STRING Yes No truststore.password the password of truststore. If this is provided then the integrity of the keystore is checked - STRING Yes No truststore.algorithm the encryption algorithm to be used for server authentication - STRING Yes No tls.store.type TLS store type - STRING Yes No keystore.file the file path of keystore. If this is provided then client authentication is enabled - STRING Yes No keystore.password the password of keystore - STRING Yes No keystore.algorithm the encryption algorithm to be used for client authentication - STRING Yes No enable.ssl to enable ssl. If set to true and truststore.file is not given then it will be set to default carbon jks by default FALSE BOOL Yes No Examples EXAMPLE 1 @sink(type='grpc', publisher.url = 'grpc://134.23.43.35:8080/org.wso2.grpc.EventService/consume', @map(type='json')) define stream FooStream (message String); Here a stream named FooStream is defined with grpc sink. A grpc server should be running at 194.23.98.100 listening to port 8080. sink.id is set to 1 here. So we can write a source with sink.id 1 so that it will listen to responses for requests published from this stream. Note that since we are using EventService/consume the sink will be operating in default mode EXAMPLE 2 @sink(type='grpc', publisher.url = 'grpc://134.23.43.35:8080/org.wso2.grpc.EventService/consume', headers='{{headers}}', @map(type='json'), @payload('{{message}}')) define stream FooStream (message String, headers String); A similar example to above but with headers. Headers are also send into the stream as a data. In the sink headers dynamic property reads the value and sends it as MetaData with the request EXAMPLE 3 @sink(type='grpc', publisher.url = 'grpc://134.23.43.35:8080/org.wso2.grpc.MyService/send', @map(type='protobuf'), define stream FooStream (stringValue string, intValue int,longValue long,booleanValue bool,floatValue float,doubleValue double); Here a stream named FooStream is defined with grpc sink. A grpc server should be running at 134.23.43.35 listening to port 8080 since there is no mapper provided, attributes of stream definition should be as same as the attributes of protobuf message definition. EXAMPLE 4 @sink(type='grpc', publisher.url = 'grpc://134.23.43.35:8080/org.wso2.grpc.MyService/testMap', @map(type='protobuf'), define stream FooStream (stringValue string, intValue int,map object); Here a stream named FooStream is defined with grpc sink. A grpc server should be running at 134.23.43.35 listening to port 8080. The 'map object' in the stream definition defines that this stream is going to use Map object with grpc service. We can use any map object that extends 'java.util.AbstractMap' class. EXAMPLE 5 @sink(type='grpc', publisher.url = 'grpc://134.23.43.35:8080/org.wso2.grpc.MyService/testMap', @map(type='protobuf', @payload(stringValue='a',longValue='b',intValue='c',booleanValue='d',floatValue = 'e', doubleValue = 'f'))) define stream FooStream (a string, b long, c int,d bool,e float,f double); Here a stream named FooStream is defined with grpc sink. A grpc server should be running at 194.23.98.100 listening to port 8080. @payload is provided in this stream, therefore we can use any name for the attributes in the stream definition, but we should correctly map those names with protobuf message attributes. If we are planning to send metadata within a stream we should use @payload to map attributes to identify the metadata attribute and the protobuf attributes separately. EXAMPLE 6 @sink(type='grpc', publisher.url = 'grpc://194.23.98.100:8888/org.wso2.grpc.test.StreamService/clientStream', @map(type='protobuf')) define stream FooStream (stringValue string, intValue int,longValue long,booleanValue bool,floatValue float,doubleValue double); Here in the grpc sink, we are sending a stream of requests to the server that runs on 194.23.98.100 and port 8888. When we need to send a stream of requests from the grpc sink we have to define a client stream RPC method.Then the siddhi will identify whether it's a unary method or a stream method and send requests according to the method type.","title":"grpc (Sink)"},{"location":"docs/api/latest/#grpc-call-sink","text":"This extension publishes event data encoded into GRPC Classes as defined in the user input jar. This extension has a default gRPC service classes jar added. The default service is called \"EventService\". Please find the protobuf definition here . If we want to use our custom gRPC services, we have to pack auto-generated gRPC service classes and protobuf classes into a jar file and add it into the project classpath (or to the jars folder in the siddhi-tooling folder if we use it with siddhi-tooling ). Please find the custom protobuf definition that uses in examples here . This grpc-call sink is used for scenarios where we send a request out and expect a response back. In default mode this will use EventService process method. grpc-call-response source is used to receive the responses. A unique sink.id is used to correlate between the sink and its corresponding source. Origin: siddhi-io-grpc:1.0.5 Syntax @sink(type=\"grpc-call\", publisher.url=\" STRING \", sink.id=\" INT \", headers=\" STRING \", idle.timeout=\" LONG \", keep.alive.time=\" LONG \", keep.alive.timeout=\" LONG \", keep.alive.without.calls=\" BOOL \", enable.retry=\" BOOL \", max.retry.attempts=\" INT \", retry.buffer.size=\" LONG \", per.rpc.buffer.size=\" LONG \", channel.termination.waiting.time=\" LONG \", max.inbound.message.size=\" LONG \", max.inbound.metadata.size=\" LONG \", truststore.file=\" STRING \", truststore.password=\" STRING \", truststore.algorithm=\" STRING \", tls.store.type=\" STRING \", keystore.file=\" STRING \", keystore.password=\" STRING \", keystore.algorithm=\" STRING \", enable.ssl=\" BOOL \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic publisher.url The url to which the outgoing events should be published via this extension. This url should consist the host hostPort, port, fully qualified service name, method name in the following format. grpc://0.0.0.0:9763/ serviceName / methodName For example: grpc://0.0.0.0:9763/org.wso2.grpc.EventService/consume STRING No No sink.id a unique ID that should be set for each grpc-call-sink. There is a 1:1 mapping between grpc-call sinks and grpc-call-response sources. Each sink has one particular source listening to the responses to requests published from that sink. So the same sink.id should be given when writing the source also. INT No No headers GRPC Request headers in format \"' key : value ',' key : value '\" . If header parameter is not provided just the payload is sent - STRING Yes No idle.timeout Set the duration in seconds without ongoing RPCs before going to idle mode. 1800 LONG Yes No keep.alive.time Sets the time in seconds without read activity before sending a keepalive ping. Keepalives can increase the load on services so must be used with caution. By default set to Long.MAX_VALUE which disables keep alive pinging. Long.MAX_VALUE LONG Yes No keep.alive.timeout Sets the time in seconds waiting for read activity after sending a keepalive ping. 20 LONG Yes No keep.alive.without.calls Sets whether keepalive will be performed when there are no outstanding RPC on a connection. false BOOL Yes No enable.retry Enables the retry and hedging mechanism provided by the gRPC library. false BOOL Yes No max.retry.attempts Sets max number of retry attempts. The total number of retry attempts for each RPC will not exceed this number even if service config may allow a higher number. 5 INT Yes No retry.buffer.size Sets the retry buffer size in bytes. If the buffer limit is exceeded, no RPC could retry at the moment, and in hedging case all hedges but one of the same RPC will cancel. 16777216 LONG Yes No per.rpc.buffer.size Sets the per RPC buffer limit in bytes used for retry. The RPC is not retriable if its buffer limit is exceeded. 1048576 LONG Yes No channel.termination.waiting.time The time in seconds to wait for the channel to become terminated, giving up if the timeout is reached. 5 LONG Yes No max.inbound.message.size Sets the maximum message size allowed to be received on the channel in bytes 4194304 LONG Yes No max.inbound.metadata.size Sets the maximum size of metadata allowed to be received in bytes 8192 LONG Yes No truststore.file the file path of truststore. If this is provided then server authentication is enabled - STRING Yes No truststore.password the password of truststore. If this is provided then the integrity of the keystore is checked - STRING Yes No truststore.algorithm the encryption algorithm to be used for server authentication - STRING Yes No tls.store.type TLS store type - STRING Yes No keystore.file the file path of keystore. If this is provided then client authentication is enabled - STRING Yes No keystore.password the password of keystore - STRING Yes No keystore.algorithm the encryption algorithm to be used for client authentication - STRING Yes No enable.ssl to enable ssl. If set to true and truststore.file is not given then it will be set to default carbon jks by default FALSE BOOL Yes No Examples EXAMPLE 1 @sink(type='grpc-call', publisher.url = 'grpc://194.23.98.100:8080/EventService/process', sink.id= '1', @map(type='json')) define stream FooStream (message String); @source(type='grpc-call-response', sink.id= '1') define stream BarStream (message String); Here a stream named FooStream is defined with grpc sink. A grpc server should be running at 194.23.98.100 listening to port 8080. sink.id is set to 1 here. So we can write a source with sink.id 1 so that it will listen to responses for requests published from this stream. Note that since we are using EventService/process the sink will be operating in default mode EXAMPLE 2 @sink(type='grpc-call', publisher.url = 'grpc://194.23.98.100:8080/EventService/process', sink.id= '1', @map(type='json')) define stream FooStream (message String); @source(type='grpc-call-response', sink.id= '1') define stream BarStream (message String); Here with the same FooStream definition we have added a BarStream which has a grpc-call-response source with the same sink.id 1. So the responses for calls sent from the FooStream will be added to BarStream. EXAMPLE 3 @sink(type='grpc-call', publisher.url = 'grpc://194.23.98.100:8888/org.wso2.grpc.test.MyService/process', sink.id= '1', @map(type='protobuf')) define stream FooStream (stringValue string, intValue int,longValue long,booleanValue bool,floatValue float,doubleValue double); @source(type='grpc-call-response', receiver.url = 'grpc://localhost:8888/org.wso2.grpc.MyService/process', sink.id= '1', @map(type='protobuf'))define stream FooStream (stringValue string, intValue int,longValue long,booleanValue bool,floatValue float,doubleValue double); Here a stream named FooStream is defined with grpc sink. A grpc server should be running at 194.23.98.100 listening to port 8080. We have added another stream called BarStream which is a grpc-call-response source with the same sink.id 1 and as same as FooStream definition. So the responses for calls sent from the FooStream will be added to BarStream. Since there is no mapping available in the stream definition attributes names should be as same as the attributes of the protobuf message definition. (Here the only reason we provide receiver.url in the grpc-call-response source is for protobuf mapper to map Response into a siddhi event, we can give any address and any port number in the URL, but we should provide the service name and the method name correctly) EXAMPLE 4 @sink(type='grpc-call', publisher.url = 'grpc://194.23.98.100:8888/org.wso2.grpc.test.MyService/process', sink.id= '1', @map(type='protobuf', @payload(stringValue='a',longValue='c',intValue='b',booleanValue='d',floatValue = 'e', doubleValue = 'f')))define stream FooStream (a string, b int,c long,d bool,e float,f double); @source(type='grpc-call-response', receiver.url = 'grpc://localhost:8888/org.wso2.grpc.test.MyService/process', sink.id= '1', @map(type='protobuf',@attributes(a = 'stringValue', b = 'intValue', c = 'longValue',d = 'booleanValue', e ='floatValue', f ='doubleValue')))define stream FooStream (a string, b int,c long,d bool,e float,f double); Here with the same FooStream definition we have added a BarStream which has a grpc-call-response source with the same sink.id 1. So the responses for calls sent from the FooStream will be added to BarStream. In this stream we provided mapping for both the sink and the source. so we can use any name for the attributes in the stream definition, but we have to map those attributes with correct protobuf attributes. As same as the grpc-sink, if we are planning to use metadata we should map the attributes.","title":"grpc-call (Sink)"},{"location":"docs/api/latest/#grpc-service-response-sink","text":"This extension is used to send responses back to a gRPC client after receiving requests through grpc-service source. This correlates with the particular source using a unique source.id Origin: siddhi-io-grpc:1.0.5 Syntax @sink(type=\"grpc-service-response\", source.id=\" INT \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic source.id A unique id to identify the correct source to which this sink is mapped. There is a 1:1 mapping between source and sink INT No No Examples EXAMPLE 1 @sink(type='grpc-service-response', source.id='1', @map(type='json')) define stream BarStream (messageId String, message String); @source(type='grpc-service', url='grpc://134.23.43.35:8080/org.wso2.grpc.EventService/process', source.id='1', @map(type='json', @attributes(messageId='trp:messageId', message='message'))) define stream FooStream (messageId String, message String); from FooStream select * insert into BarStream; The grpc requests are received through the grpc-service sink. Each received event is sent back through grpc-service-source. This is just a passthrough through Siddhi as we are selecting everything from FooStream and inserting into BarStream.","title":"grpc-service-response (Sink)"},{"location":"docs/api/latest/#http-sink","text":"HTTP sink publishes messages via HTTP or HTTPS protocols using methods such as POST, GET, PUT, and DELETE on formats text , XML and JSON . It can also publish to endpoints protected by basic authentication or OAuth 2.0. Origin: siddhi-io-http:2.2.0 Syntax @sink(type=\"http\", publisher.url=\" STRING \", basic.auth.username=\" STRING \", basic.auth.password=\" STRING \", https.truststore.file=\" STRING \", https.truststore.password=\" STRING \", oauth.username=\" STRING \", oauth.password=\" STRING \", consumer.key=\" STRING \", consumer.secret=\" STRING \", token.url=\" STRING \", refresh.token=\" STRING \", headers=\" STRING \", method=\" STRING \", socket.idle.timeout=\" INT \", chunk.disabled=\" BOOL \", ssl.protocol=\" STRING \", ssl.verification.disabled=\" BOOL \", tls.store.type=\" STRING \", ssl.configurations=\" STRING \", proxy.host=\" STRING \", proxy.port=\" STRING \", proxy.username=\" STRING \", proxy.password=\" STRING \", client.bootstrap.configurations=\" STRING \", max.pool.active.connections=\" INT \", min.pool.idle.connections=\" INT \", max.pool.idle.connections=\" INT \", min.evictable.idle.time=\" STRING \", time.between.eviction.runs=\" STRING \", max.wait.time=\" STRING \", test.on.borrow=\" BOOL \", test.while.idle=\" BOOL \", exhausted.action=\" INT \", hostname.verification.enabled=\" BOOL \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic publisher.url The URL to which the outgoing events should be published. Examples: http://localhost:8080/endpoint , https://localhost:8080/endpoint STRING No No basic.auth.username The username to be included in the authentication header when calling endpoints protected by basic authentication. basic.auth.password property should be also set when using this property. - STRING Yes No basic.auth.password The password to be included in the authentication header when calling endpoints protected by basic authentication. basic.auth.username property should be also set when using this property. - STRING Yes No https.truststore.file The file path of the client truststore when sending messages through https protocol. ${carbon.home}/resources/security/client-truststore.jks STRING Yes No https.truststore.password The password for the client-truststore. wso2carbon STRING Yes No oauth.username The username to be included in the authentication header when calling endpoints protected by OAuth 2.0. oauth.password property should be also set when using this property. - STRING Yes No oauth.password The password to be included in the authentication header when calling endpoints protected by OAuth 2.0. oauth.username property should be also set when using this property. - STRING Yes No consumer.key Consumer key used for calling endpoints protected by OAuth 2.0 - STRING Yes No consumer.secret Consumer secret used for calling endpoints protected by OAuth 2.0 - STRING Yes No token.url Token URL to generate a new access tokens when calling endpoints protected by OAuth 2.0 - STRING Yes No refresh.token Refresh token used for generating new access tokens when calling endpoints protected by OAuth 2.0 - STRING Yes No headers HTTP request headers in format \"' key : value ',' key : value '\" . When Content-Type header is not provided the system derives the Content-Type based on the provided sink mapper as following: - @map(type='xml') : application/xml - @map(type='json') : application/json - @map(type='text') : plain/text - @map(type='keyvalue') : application/x-www-form-urlencoded - For all other cases system defaults to plain/text Also the Content-Length header need not to be provided, as the system automatically defines it by calculating the size of the payload. Content-Type and Content-Length headers STRING Yes No method The HTTP method used for calling the endpoint. POST STRING Yes No socket.idle.timeout Socket timeout in millis. 6000 INT Yes No chunk.disabled Disable chunked transfer encoding. false BOOL Yes No ssl.protocol SSL/TLS protocol. TLS STRING Yes No ssl.verification.disabled Disable SSL verification. false BOOL Yes No tls.store.type TLS store type. JKS STRING Yes No ssl.configurations SSL/TSL configurations in format \"' key : value ',' key : value '\" . Some supported parameters: - SSL/TLS protocols: 'sslEnabledProtocols:TLSv1.1,TLSv1.2' - List of ciphers: 'ciphers:TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256' - Enable session creation: 'client.enable.session.creation:true' - Supported server names: 'server.suported.server.names:server' - Add HTTP SNIMatcher: 'server.supported.snimatchers:SNIMatcher' - STRING Yes No proxy.host Proxy server host - STRING Yes No proxy.port Proxy server port - STRING Yes No proxy.username Proxy server username - STRING Yes No proxy.password Proxy server password - STRING Yes No client.bootstrap.configurations Client bootstrap configurations in format \"' key : value ',' key : value '\" . Some supported configurations : - Client connect timeout in millis: 'client.bootstrap.connect.timeout:15000' - Client socket timeout in seconds: 'client.bootstrap.socket.timeout:15' - Client socket reuse: 'client.bootstrap.socket.reuse:true' - Enable TCP no delay: 'client.bootstrap.nodelay:true' - Enable client keep alive: 'client.bootstrap.keepalive:true' - Send buffer size: 'client.bootstrap.sendbuffersize:1048576' - Receive buffer size: 'client.bootstrap.recievebuffersize:1048576' - STRING Yes No max.pool.active.connections Maximum possible number of active connection per client pool. -1 INT Yes No min.pool.idle.connections Minimum number of idle connections that can exist per client pool. 0 INT Yes No max.pool.idle.connections Maximum number of idle connections that can exist per client pool. 100 INT Yes No min.evictable.idle.time Minimum time (in millis) a connection may sit idle in the client pool before it become eligible for eviction. 300000 STRING Yes No time.between.eviction.runs Time between two eviction operations (in millis) on the client pool. 30000 STRING Yes No max.wait.time The maximum time (in millis) the pool will wait (when there are no available connections) for a connection to be returned to the pool. 60000 STRING Yes No test.on.borrow Enable connections to be validated before being borrowed from the client pool. true BOOL Yes No test.while.idle Enable connections to be validated during the eviction operation (if any). true BOOL Yes No exhausted.action Action that should be taken when the maximum number of active connections are being used. This action should be indicated as an int and possible action values are following. 0 - Fail the request. 1 - Block the request, until a connection returns to the pool. 2 - Grow the connection pool size. 1 (Block when exhausted) INT Yes No hostname.verification.enabled Enable hostname verification. true BOOL Yes No System Parameters Name Description Default Value Possible Parameters clientBootstrapClientGroupSize Number of client threads to perform non-blocking read and write to one or more channels. (Number of available processors) * 2 Any positive integer clientBootstrapBossGroupSize Number of boss threads to accept incoming connections. Number of available processors Any positive integer clientBootstrapWorkerGroupSize Number of worker threads to accept the connections from boss threads and perform non-blocking read and write from one or more channels. (Number of available processors) * 2 Any positive integer trustStoreLocation The default truststore file path. ${carbon.home}/resources/security/client-truststore.jks Path to client truststore .jks file trustStorePassword The default truststore password. wso2carbon Truststore password as string Examples EXAMPLE 1 @sink(type = 'http', publisher.url = 'http://stocks.com/stocks', @map(type = 'json')) define stream StockStream (symbol string, price float, volume long); Events arriving on the StockStream will be published to the HTTP endpoint http://stocks.com/stocks using POST method with Content-Type application/json by converting those events to the default JSON format as following: { \"event\": { \"symbol\": \"FB\", \"price\": 24.5, \"volume\": 5000 } } EXAMPLE 2 @sink(type='http', publisher.url = 'http://localhost:8009/foo', client.bootstrap.configurations = \"'client.bootstrap.socket.timeout:20'\", max.pool.active.connections = '1', headers = \"{{headers}}\", @map(type='xml', @payload(\"\"\" stock {{payloadBody}} /stock \"\"\"))) define stream FooStream (payloadBody String, headers string); Events arriving on FooStream will be published to the HTTP endpoint http://localhost:8009/foo using POST method with Content-Type application/xml and setting payloadBody and header attribute values. If the payloadBody contains symbol WSO2 /symbol price 55.6 /price volume 100 /volume and header contains 'topic:foobar' values, then the system will generate an output with the body: stock symbol WSO2 /symbol price 55.6 /price volume 100 /volume /stock and HTTP headers: Content-Length:xxx , Content-Location:'xxx' , Content-Type:'application/xml' , HTTP_METHOD:'POST'","title":"http (Sink)"},{"location":"docs/api/latest/#http-call-sink","text":"The http-call sink publishes messages to endpoints via HTTP or HTTPS protocols using methods such as POST, GET, PUT, and DELETE on formats text , XML or JSON and consume responses through its corresponding http-call-response source. It also supports calling endpoints protected with basic authentication or OAuth 2.0. Origin: siddhi-io-http:2.2.0 Syntax @sink(type=\"http-call\", publisher.url=\" STRING \", sink.id=\" STRING \", basic.auth.username=\" STRING \", basic.auth.password=\" STRING \", https.truststore.file=\" STRING \", https.truststore.password=\" STRING \", oauth.username=\" STRING \", oauth.password=\" STRING \", consumer.key=\" STRING \", consumer.secret=\" STRING \", token.url=\" STRING \", refresh.token=\" STRING \", headers=\" STRING \", method=\" STRING \", downloading.enabled=\" BOOL \", download.path=\" STRING \", blocking.io=\" BOOL \", socket.idle.timeout=\" INT \", chunk.disabled=\" BOOL \", ssl.protocol=\" STRING \", ssl.verification.disabled=\" BOOL \", ssl.configurations=\" STRING \", proxy.host=\" STRING \", proxy.port=\" STRING \", proxy.username=\" STRING \", proxy.password=\" STRING \", client.bootstrap.configurations=\" STRING \", max.pool.active.connections=\" INT \", min.pool.idle.connections=\" INT \", max.pool.idle.connections=\" INT \", min.evictable.idle.time=\" STRING \", time.between.eviction.runs=\" STRING \", max.wait.time=\" STRING \", test.on.borrow=\" BOOL \", test.while.idle=\" BOOL \", exhausted.action=\" INT \", hostname.verification.enabled=\" BOOL \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic publisher.url The URL which should be called. Examples: http://localhost:8080/endpoint , https://localhost:8080/endpoint STRING No No sink.id Identifier to correlate the http-call sink to its corresponding http-call-response sources to retrieved the responses. STRING No No basic.auth.username The username to be included in the authentication header when calling endpoints protected by basic authentication. basic.auth.password property should be also set when using this property. - STRING Yes No basic.auth.password The password to be included in the authentication header when calling endpoints protected by basic authentication. basic.auth.username property should be also set when using this property. - STRING Yes No https.truststore.file The file path of the client truststore when sending messages through https protocol. ${carbon.home}/resources/security/client-truststore.jks STRING Yes No https.truststore.password The password for the client-truststore. wso2carbon STRING Yes No oauth.username The username to be included in the authentication header when calling endpoints protected by OAuth 2.0. oauth.password property should be also set when using this property. - STRING Yes No oauth.password The password to be included in the authentication header when calling endpoints protected by OAuth 2.0. oauth.username property should be also set when using this property. - STRING Yes No consumer.key Consumer key used for calling endpoints protected by OAuth 2.0 - STRING Yes No consumer.secret Consumer secret used for calling endpoints protected by OAuth 2.0 - STRING Yes No token.url Token URL to generate a new access tokens when calling endpoints protected by OAuth 2.0 - STRING Yes No refresh.token Refresh token used for generating new access tokens when calling endpoints protected by OAuth 2.0 - STRING Yes No headers HTTP request headers in format \"' key : value ',' key : value '\" . When the Content-Type header is not provided the system decides the Content-Type based on the provided sink mapper as following: - @map(type='xml') : application/xml - @map(type='json') : application/json - @map(type='text') : plain/text - @map(type='keyvalue') : application/x-www-form-urlencoded - For all other cases system defaults to plain/text Also the Content-Length header need not to be provided, as the system automatically defines it by calculating the size of the payload. Content-Type and Content-Length headers STRING Yes No method The HTTP method used for calling the endpoint. POST STRING Yes No downloading.enabled Enable response received by the http-call-response source to be written to a file. When this is enabled the download.path property should be also set. false BOOL Yes No download.path The absolute file path along with the file name where the downloads should be saved. - STRING Yes Yes blocking.io Blocks the request thread until a response it received from HTTP call-response source before sending any other request. false BOOL Yes No socket.idle.timeout Socket timeout in millis. 6000 INT Yes No chunk.disabled Disable chunked transfer encoding. false BOOL Yes No ssl.protocol SSL/TLS protocol. TLS STRING Yes No ssl.verification.disabled Disable SSL verification. false BOOL Yes No ssl.configurations SSL/TSL configurations. Expected format \"' key : value ',' key : value '\" . Some supported parameters: - SSL/TLS protocols: 'sslEnabledProtocols:TLSv1.1,TLSv1.2' - List of ciphers: 'ciphers:TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256' - Enable session creation: 'client.enable.session.creation:true' - Supported server names: 'server.suported.server.names:server' - Add HTTP SNIMatcher: 'server.supported.snimatchers:SNIMatcher' - STRING Yes No proxy.host Proxy server host - STRING Yes No proxy.port Proxy server port - STRING Yes No proxy.username Proxy server username - STRING Yes No proxy.password Proxy server password - STRING Yes No client.bootstrap.configurations Client bootstrap configurations in format \"' key : value ',' key : value '\" . Some supported configurations : - Client connect timeout in millis: 'client.bootstrap.connect.timeout:15000' - Client socket timeout in seconds: 'client.bootstrap.socket.timeout:15' - Client socket reuse: 'client.bootstrap.socket.reuse:true' - Enable TCP no delay: 'client.bootstrap.nodelay:true' - Enable client keep alive: 'client.bootstrap.keepalive:true' - Send buffer size: 'client.bootstrap.sendbuffersize:1048576' - Receive buffer size: 'client.bootstrap.recievebuffersize:1048576' - STRING Yes No max.pool.active.connections Maximum possible number of active connection per client pool. -1 INT Yes No min.pool.idle.connections Minimum number of idle connections that can exist per client pool. 0 INT Yes No max.pool.idle.connections Maximum number of idle connections that can exist per client pool. 100 INT Yes No min.evictable.idle.time Minimum time (in millis) a connection may sit idle in the client pool before it become eligible for eviction. 300000 STRING Yes No time.between.eviction.runs Time between two eviction operations (in millis) on the client pool. 30000 STRING Yes No max.wait.time The maximum time (in millis) the pool will wait (when there are no available connections) for a connection to be returned to the pool. 60000 STRING Yes No test.on.borrow Enable connections to be validated before being borrowed from the client pool. true BOOL Yes No test.while.idle Enable connections to be validated during the eviction operation (if any). true BOOL Yes No exhausted.action Action that should be taken when the maximum number of active connections are being used. This action should be indicated as an int and possible action values are following. 0 - Fail the request. 1 - Block the request, until a connection returns to the pool. 2 - Grow the connection pool size. 1 (Block when exhausted) INT Yes No hostname.verification.enabled Enable hostname verification true BOOL Yes No System Parameters Name Description Default Value Possible Parameters clientBootstrapClientGroupSize Number of client threads to perform non-blocking read and write to one or more channels. (Number of available processors) * 2 Any positive integer clientBootstrapBossGroupSize Number of boss threads to accept incoming connections. Number of available processors Any positive integer clientBootstrapWorkerGroupSize Number of worker threads to accept the connections from boss threads and perform non-blocking read and write from one or more channels. (Number of available processors) * 2 Any positive integer trustStoreLocation The default truststore file path. ${carbon.home}/resources/security/client-truststore.jks Path to client truststore .jks file trustStorePassword The default truststore password. wso2carbon Truststore password as string Examples EXAMPLE 1 @sink(type='http-call', sink.id='foo', publisher.url='http://localhost:8009/foo', @map(type='xml', @payload('{{payloadBody}}'))) define stream FooStream (payloadBody string); @source(type='http-call-response', sink.id='foo', @map(type='text', regex.A='((.|\\n)*)', @attributes(headers='trp:headers', message='A[1]'))) define stream ResponseStream(message string, headers string); When events arrive in FooStream , http-call sink makes calls to endpoint on url http://localhost:8009/foo with POST method and Content-Type application/xml . If the event payloadBody attribute contains following XML: item name apple /name price 55 /price quantity 5 /quantity /item the http-call sink maps that and sends it to the endpoint. When endpoint sends a response it will be consumed by the corresponding http-call-response source correlated via the same sink.id foo and that will map the response message and send it via ResponseStream steam by assigning the message body as message attribute and response headers as headers attribute of the event. EXAMPLE 2 @sink(type='http-call', publisher.url='http://localhost:8005/files/{{name}}' downloading.enabled='true', download.path='{{downloadPath}}{{name}}', method='GET', sink.id='download', @map(type='json')) define stream DownloadRequestStream(name String, id int, downloadPath string); @source(type='http-call-response', sink.id='download', http.status.code='2\\\\d+', @map(type='text', regex.A='((.|\\n)*)', @attributes(name='trp:name', id='trp:id', file='A[1]'))) define stream ResponseStream2xx(name string, id string, file string); @source(type='http-call-response', sink.id='download', http.status.code='4\\\\d+', @map(type='text', regex.A='((.|\\n)*)', @attributes(errorMsg='A[1]'))) define stream ResponseStream4xx(errorMsg string); When events arrive in DownloadRequestStream with name : foo.txt , id : 75 and downloadPath : /user/download/ the http-call sink sends a GET request to the url http://localhost:8005/files/foo.txt to download the file to the given path /user/download/foo.txt and capture the response via its corresponding http-call-response source based on the response status code. If the response status code is in the range of 200 the message will be received by the http-call-response source associated with the ResponseStream2xx stream which expects http.status.code with regex 2\\d+ while downloading the file to the local file system on the path /user/download/foo.txt and mapping the response message having the absolute file path to event's file attribute. If the response status code is in the range of 400 then the message will be received by the http-call-response source associated with the ResponseStream4xx stream which expects http.status.code with regex 4\\d+ while mapping the error response to the errorMsg attribute of the event.","title":"http-call (Sink)"},{"location":"docs/api/latest/#http-request-sink","text":"Deprecated (Use http-call sink instead). The http-request sink publishes messages to endpoints via HTTP or HTTPS protocols using methods such as POST, GET, PUT, and DELETE on formats text , XML or JSON and consume responses through its corresponding http-response source. It also supports calling endpoints protected with basic authentication or OAuth 2.0. Origin: siddhi-io-http:2.2.0 Syntax @sink(type=\"http-request\", publisher.url=\" STRING \", sink.id=\" STRING \", basic.auth.username=\" STRING \", basic.auth.password=\" STRING \", https.truststore.file=\" STRING \", https.truststore.password=\" STRING \", oauth.username=\" STRING \", oauth.password=\" STRING \", consumer.key=\" STRING \", consumer.secret=\" STRING \", token.url=\" STRING \", refresh.token=\" STRING \", headers=\" STRING \", method=\" STRING \", downloading.enabled=\" BOOL \", download.path=\" STRING \", blocking.io=\" BOOL \", socket.idle.timeout=\" INT \", chunk.disabled=\" BOOL \", ssl.protocol=\" STRING \", ssl.verification.disabled=\" BOOL \", ssl.configurations=\" STRING \", proxy.host=\" STRING \", proxy.port=\" STRING \", proxy.username=\" STRING \", proxy.password=\" STRING \", client.bootstrap.configurations=\" STRING \", max.pool.active.connections=\" INT \", min.pool.idle.connections=\" INT \", max.pool.idle.connections=\" INT \", min.evictable.idle.time=\" STRING \", time.between.eviction.runs=\" STRING \", max.wait.time=\" STRING \", test.on.borrow=\" BOOL \", test.while.idle=\" BOOL \", exhausted.action=\" INT \", hostname.verification.enabled=\" BOOL \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic publisher.url The URL which should be called. Examples: http://localhost:8080/endpoint , https://localhost:8080/endpoint STRING No No sink.id Identifier to correlate the http-request sink to its corresponding http-response sources to retrieved the responses. STRING No No basic.auth.username The username to be included in the authentication header when calling endpoints protected by basic authentication. basic.auth.password property should be also set when using this property. - STRING Yes No basic.auth.password The password to be included in the authentication header when calling endpoints protected by basic authentication. basic.auth.username property should be also set when using this property. - STRING Yes No https.truststore.file The file path of the client truststore when sending messages through https protocol. ${carbon.home}/resources/security/client-truststore.jks STRING Yes No https.truststore.password The password for the client-truststore. wso2carbon STRING Yes No oauth.username The username to be included in the authentication header when calling endpoints protected by OAuth 2.0. oauth.password property should be also set when using this property. - STRING Yes No oauth.password The password to be included in the authentication header when calling endpoints protected by OAuth 2.0. oauth.username property should be also set when using this property. - STRING Yes No consumer.key Consumer key used for calling endpoints protected by OAuth 2.0 - STRING Yes No consumer.secret Consumer secret used for calling endpoints protected by OAuth 2.0 - STRING Yes No token.url Token URL to generate a new access tokens when calling endpoints protected by OAuth 2.0 - STRING Yes No refresh.token Refresh token used for generating new access tokens when calling endpoints protected by OAuth 2.0 - STRING Yes No headers HTTP request headers in format \"' key : value ',' key : value '\" . When the Content-Type header is not provided the system decides the Content-Type based on the provided sink mapper as following: - @map(type='xml') : application/xml - @map(type='json') : application/json - @map(type='text') : plain/text - @map(type='keyvalue') : application/x-www-form-urlencoded - For all other cases system defaults to plain/text Also the Content-Length header need not to be provided, as the system automatically defines it by calculating the size of the payload. Content-Type and Content-Length headers STRING Yes No method The HTTP method used for calling the endpoint. POST STRING Yes No downloading.enabled Enable response received by the http-response source to be written to a file. When this is enabled the download.path property should be also set. false BOOL Yes No download.path The absolute file path along with the file name where the downloads should be saved. - STRING Yes Yes blocking.io Blocks the request thread until a response it received from HTTP call-response source before sending any other request. false BOOL Yes No socket.idle.timeout Socket timeout in millis. 6000 INT Yes No chunk.disabled Disable chunked transfer encoding. false BOOL Yes No ssl.protocol SSL/TLS protocol. TLS STRING Yes No ssl.verification.disabled Disable SSL verification. false BOOL Yes No ssl.configurations SSL/TSL configurations in format \"' key : value ',' key : value '\" . Some supported parameters: - SSL/TLS protocols: 'sslEnabledProtocols:TLSv1.1,TLSv1.2' - List of ciphers: 'ciphers:TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256' - Enable session creation: 'client.enable.session.creation:true' - Supported server names: 'server.suported.server.names:server' - Add HTTP SNIMatcher: 'server.supported.snimatchers:SNIMatcher' - STRING Yes No proxy.host Proxy server host - STRING Yes No proxy.port Proxy server port - STRING Yes No proxy.username Proxy server username - STRING Yes No proxy.password Proxy server password - STRING Yes No client.bootstrap.configurations Client bootstrap configurations in format \"' key : value ',' key : value '\" . Some supported configurations : - Client connect timeout in millis: 'client.bootstrap.connect.timeout:15000' - Client socket timeout in seconds: 'client.bootstrap.socket.timeout:15' - Client socket reuse: 'client.bootstrap.socket.reuse:true' - Enable TCP no delay: 'client.bootstrap.nodelay:true' - Enable client keep alive: 'client.bootstrap.keepalive:true' - Send buffer size: 'client.bootstrap.sendbuffersize:1048576' - Receive buffer size: 'client.bootstrap.recievebuffersize:1048576' - STRING Yes No max.pool.active.connections Maximum possible number of active connection per client pool. -1 INT Yes No min.pool.idle.connections Minimum number of idle connections that can exist per client pool. 0 INT Yes No max.pool.idle.connections Maximum number of idle connections that can exist per client pool. 100 INT Yes No min.evictable.idle.time Minimum time (in millis) a connection may sit idle in the client pool before it become eligible for eviction. 300000 STRING Yes No time.between.eviction.runs Time between two eviction operations (in millis) on the client pool. 30000 STRING Yes No max.wait.time The maximum time (in millis) the pool will wait (when there are no available connections) for a connection to be returned to the pool. 60000 STRING Yes No test.on.borrow Enable connections to be validated before being borrowed from the client pool. true BOOL Yes No test.while.idle Enable connections to be validated during the eviction operation (if any). true BOOL Yes No exhausted.action Action that should be taken when the maximum number of active connections are being used. This action should be indicated as an int and possible action values are following. 0 - Fail the request. 1 - Block the request, until a connection returns to the pool. 2 - Grow the connection pool size. 1 (Block when exhausted) INT Yes No hostname.verification.enabled Enable hostname verification true BOOL Yes No System Parameters Name Description Default Value Possible Parameters clientBootstrapClientGroupSize Number of client threads to perform non-blocking read and write to one or more channels. (Number of available processors) * 2 Any positive integer clientBootstrapBossGroupSize Number of boss threads to accept incoming connections. Number of available processors Any positive integer clientBootstrapWorkerGroupSize Number of worker threads to accept the connections from boss threads and perform non-blocking read and write from one or more channels. (Number of available processors) * 2 Any positive integer trustStoreLocation The default truststore file path. ${carbon.home}/resources/security/client-truststore.jks Path to client truststore .jks file trustStorePassword The default truststore password. wso2carbon Truststore password as string Examples EXAMPLE 1 @sink(type='http-request', sink.id='foo', publisher.url='http://localhost:8009/foo', @map(type='xml', @payload('{{payloadBody}}'))) define stream FooStream (payloadBody string); @source(type='http-response', sink.id='foo', @map(type='text', regex.A='((.|\\n)*)', @attributes(headers='trp:headers', message='A[1]'))) define stream ResponseStream(message string, headers string); When events arrive in FooStream , http-request sink makes calls to endpoint on url http://localhost:8009/foo with POST method and Content-Type application/xml . If the event payloadBody attribute contains following XML: item name apple /name price 55 /price quantity 5 /quantity /item the http-request sink maps that and sends it to the endpoint. When endpoint sends a response it will be consumed by the corresponding http-response source correlated via the same sink.id foo and that will map the response message and send it via ResponseStream steam by assigning the message body as message attribute and response headers as headers attribute of the event. EXAMPLE 2 @sink(type='http-request', publisher.url='http://localhost:8005/files/{{name}}' downloading.enabled='true', download.path='{{downloadPath}}{{name}}', method='GET', sink.id='download', @map(type='json')) define stream DownloadRequestStream(name String, id int, downloadPath string); @source(type='http-response', sink.id='download', http.status.code='2\\\\d+', @map(type='text', regex.A='((.|\\n)*)', @attributes(name='trp:name', id='trp:id', file='A[1]'))) define stream ResponseStream2xx(name string, id string, file string); @source(type='http-response', sink.id='download', http.status.code='4\\\\d+', @map(type='text', regex.A='((.|\\n)*)', @attributes(errorMsg='A[1]'))) define stream ResponseStream4xx(errorMsg string); When events arrive in DownloadRequestStream with name : foo.txt , id : 75 and downloadPath : /user/download/ the http-request sink sends a GET request to the url http://localhost:8005/files/foo.txt to download the file to the given path /user/download/foo.txt and capture the response via its corresponding http-response source based on the response status code. If the response status code is in the range of 200 the message will be received by the http-response source associated with the ResponseStream2xx stream which expects http.status.code with regex 2\\d+ while downloading the file to the local file system on the path /user/download/foo.txt and mapping the response message having the absolute file path to event's file attribute. If the response status code is in the range of 400 then the message will be received by the http-response source associated with the ResponseStream4xx stream which expects http.status.code with regex 4\\d+ while mapping the error response to the errorMsg attribute of the event.","title":"http-request (Sink)"},{"location":"docs/api/latest/#http-response-sink","text":"Deprecated (Use http-service-response sink instead). The http-response sink send responses of the requests consumed by its corresponding http-request source, by mapping the response messages to formats such as text , XML and JSON . Origin: siddhi-io-http:2.2.0 Syntax @sink(type=\"http-response\", source.id=\" STRING \", message.id=\" STRING \", headers=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic source.id Identifier to correlate the http-response sink to its corresponding http-request source which consumed the request. STRING No No message.id Identifier to correlate the response with the request received by http-request source. STRING No Yes headers HTTP request headers in format \"' key : value ',' key : value '\" . When the Content-Type header is not provided the system decides the Content-Type based on the provided sink mapper as following: - @map(type='xml') : application/xml - @map(type='json') : application/json - @map(type='text') : plain/text - @map(type='keyvalue') : application/x-www-form-urlencoded - For all other cases system defaults to plain/text Also the Content-Length header need not to be provided, as the system automatically defines it by calculating the size of the payload. Content-Type and Content-Length headers STRING Yes No Examples EXAMPLE 1 @source(type='http-request', receiver.url='http://localhost:5005/add', source.id='adder', @map(type='json, @attributes(messageId='trp:messageId', value1='$.event.value1', value2='$.event.value2'))) define stream AddStream (messageId string, value1 long, value2 long); @sink(type='http-response', source.id='adder', message.id='{{messageId}}', @map(type = 'json')) define stream ResultStream (messageId string, results long); @info(name = 'query1') from AddStream select messageId, value1 + value2 as results insert into ResultStream; The http-request source on stream AddStream listens on url http://localhost:5005/stocks for JSON messages with format: { \"event\": { \"value1\": 3, \"value2\": 4 } } and when events arrive it maps to AddStream events and pass them to query query1 for processing. The query results produced on ResultStream are sent as a response via http-response sink with format: { \"event\": { \"results\": 7 } } Here the request and response are correlated by passing the messageId produced by the http-request to the respective http-response sink.","title":"http-response (Sink)"},{"location":"docs/api/latest/#http-service-response-sink","text":"The http-service-response sink send responses of the requests consumed by its corresponding http-service source, by mapping the response messages to formats such as text , XML and JSON . Origin: siddhi-io-http:2.2.0 Syntax @sink(type=\"http-service-response\", source.id=\" STRING \", message.id=\" STRING \", headers=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic source.id Identifier to correlate the http-service-response sink to its corresponding http-service source which consumed the request. STRING No No message.id Identifier to correlate the response with the request received by http-service source. STRING No Yes headers HTTP request headers in format \"' key : value ',' key : value '\" . When the Content-Type header is not provided the system decides the Content-Type based on the provided sink mapper as following: - @map(type='xml') : application/xml - @map(type='json') : application/json - @map(type='text') : plain/text - @map(type='keyvalue') : application/x-www-form-urlencoded - For all other cases system defaults to plain/text Also the Content-Length header need not to be provided, as the system automatically defines it by calculating the size of the payload. Content-Type and Content-Length headers STRING Yes No Examples EXAMPLE 1 @source(type='http-service', receiver.url='http://localhost:5005/add', source.id='adder', @map(type='json, @attributes(messageId='trp:messageId', value1='$.event.value1', value2='$.event.value2'))) define stream AddStream (messageId string, value1 long, value2 long); @sink(type='http-service-response', source.id='adder', message.id='{{messageId}}', @map(type = 'json')) define stream ResultStream (messageId string, results long); @info(name = 'query1') from AddStream select messageId, value1 + value2 as results insert into ResultStream; The http-service source on stream AddStream listens on url http://localhost:5005/stocks for JSON messages with format: { \"event\": { \"value1\": 3, \"value2\": 4 } } and when events arrive it maps to AddStream events and pass them to query query1 for processing. The query results produced on ResultStream are sent as a response via http-service-response sink with format: { \"event\": { \"results\": 7 } } Here the request and response are correlated by passing the messageId produced by the http-service to the respective http-service-response sink.","title":"http-service-response (Sink)"},{"location":"docs/api/latest/#inmemory-sink","text":"In-memory sink publishes events to In-memory sources that are subscribe to the same topic to which the sink publishes. This provides a way to connect multiple Siddhi Apps deployed under the same Siddhi Manager (JVM). Here both the publisher and subscriber should have the same event schema (stream definition) for successful data transfer. Origin: siddhi-core:5.1.8 Syntax @sink(type=\"inMemory\", topic=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic topic Event are delivered to allthe subscribers subscribed on this topic. STRING No No Examples EXAMPLE 1 @sink(type='inMemory', topic='Stocks', @map(type='passThrough')) define stream StocksStream (symbol string, price float, volume long); Here the StocksStream uses inMemory sink to emit the Siddhi events to all the inMemory sources deployed in the same JVM and subscribed to the topic Stocks .","title":"inMemory (Sink)"},{"location":"docs/api/latest/#jms-sink","text":"JMS Sink allows users to subscribe to a JMS broker and publish JMS messages. Origin: siddhi-io-jms:2.0.3 Syntax @sink(type=\"jms\", destination=\" STRING \", connection.factory.jndi.name=\" STRING \", factory.initial=\" STRING \", provider.url=\" STRING \", connection.factory.type=\" STRING \", connection.username=\" STRING \", connection.password=\" STRING \", connection.factory.nature=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic destination Queue/Topic name which JMS Source should subscribe to STRING No Yes connection.factory.jndi.name JMS Connection Factory JNDI name. This value will be used for the JNDI lookup to find the JMS Connection Factory. QueueConnectionFactory STRING Yes No factory.initial Naming factory initial value STRING No No provider.url Java naming provider URL. Property for specifying configuration information for the service provider to use. The value of the property should contain a URL string (e.g. \"ldap://somehost:389\") STRING No No connection.factory.type Type of the connection connection factory. This can be either queue or topic. queue STRING Yes No connection.username username for the broker. None STRING Yes No connection.password Password for the broker None STRING Yes No connection.factory.nature Connection factory nature for the broker(cached/pooled). default STRING Yes No Examples EXAMPLE 1 @sink(type='jms', @map(type='xml'), factory.initial='org.apache.activemq.jndi.ActiveMQInitialContextFactory', provider.url='vm://localhost',destination='DAS_JMS_OUTPUT_TEST', connection.factory.type='topic',connection.factory.jndi.name='TopicConnectionFactory') define stream inputStream (name string, age int, country string); This example shows how to publish to an ActiveMQ topic. EXAMPLE 2 @sink(type='jms', @map(type='xml'), factory.initial='org.apache.activemq.jndi.ActiveMQInitialContextFactory', provider.url='vm://localhost',destination='DAS_JMS_OUTPUT_TEST') define stream inputStream (name string, age int, country string); This example shows how to publish to an ActiveMQ queue. Note that we are not providing properties like connection factory type","title":"jms (Sink)"},{"location":"docs/api/latest/#kafka-sink","text":"A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to use the Kafka transport, the type parameter should have kafka as its value. Origin: siddhi-io-kafka:5.0.5 Syntax @sink(type=\"kafka\", bootstrap.servers=\" STRING \", topic=\" STRING \", partition.no=\" INT \", sequence.id=\" STRING \", key=\" STRING \", is.binary.message=\" BOOL \", optional.configuration=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No is.binary.message In order to send the binary events via kafka sink, this parameter is set to 'True'. null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='topic_with_partitions', partition.no='0', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka sink configuration publishes to 0 th partition of the topic named topic_with_partitions . EXAMPLE 2 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='{{symbol}}', partition.no='{{volume}}', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes dynamic topic and partitions that are taken from the Siddhi event. The value for partition.no is taken from the volume attribute, and the topic value is taken from the symbol attribute.","title":"kafka (Sink)"},{"location":"docs/api/latest/#kafkamultidc-sink","text":"A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to publish events via the Kafka transport, and using two Kafka brokers to publish events to the same topic, the type parameter must have kafkaMultiDC as its value. Origin: siddhi-io-kafka:5.0.5 Syntax @sink(type=\"kafkaMultiDC\", bootstrap.servers=\" STRING \", topic=\" STRING \", sequence.id=\" STRING \", key=\" STRING \", partition.no=\" INT \", is.binary.message=\" BOOL \", optional.configuration=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma -separated values. There must be at least two servers in this list. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No is.binary.message In order to send the binary events via kafkaMultiDCSink, it is required to set this parameter to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink(type='kafkaMultiDC', topic='myTopic', partition.no='0',bootstrap.servers='host1:9092, host2:9092', @map(type='xml'))Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes to the default (i.e., 0 th ) partition of the brokers in two data centers","title":"kafkaMultiDC (Sink)"},{"location":"docs/api/latest/#log-sink","text":"This is a sink that can be used as a logger. This will log the output events in the output stream with user specified priority and a prefix Origin: siddhi-core:5.1.8 Syntax @sink(type=\"log\", priority=\" STRING \", prefix=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic priority This will set the logger priority i.e log level. Accepted values are INFO, DEBUG, WARN, FATAL, ERROR, OFF, TRACE INFO STRING Yes No prefix This will be the prefix to the output message. If the output stream has event [2,4] and the prefix is given as \"Hello\" then the log will show \"Hello : [2,4]\" default prefix will be : STRING Yes No Examples EXAMPLE 1 @sink(type='log', prefix='My Log', priority='DEBUG') define stream BarStream (symbol string, price float, volume long) In this example BarStream uses log sink and the prefix is given as My Log. Also the priority is set to DEBUG. EXAMPLE 2 @sink(type='log', priority='DEBUG') define stream BarStream (symbol string, price float, volume long) In this example BarStream uses log sink and the priority is set to DEBUG. User has not specified prefix so the default prefix will be in the form Siddhi App Name : Stream Name EXAMPLE 3 @sink(type='log', prefix='My Log') define stream BarStream (symbol string, price float, volume long) In this example BarStream uses log sink and the prefix is given as My Log. User has not given a priority so it will be set to default INFO. EXAMPLE 4 @sink(type='log') define stream BarStream (symbol string, price float, volume long) In this example BarStream uses log sink. The user has not given prefix or priority so they will be set to their default values.","title":"log (Sink)"},{"location":"docs/api/latest/#nats-sink","text":"NATS Sink allows users to subscribe to a NATS broker and publish messages. Origin: siddhi-io-nats:2.0.8 Syntax @sink(type=\"nats\", destination=\" STRING \", bootstrap.servers=\" STRING \", client.id=\" STRING \", cluster.id=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic destination Subject name which NATS sink should publish to. STRING No Yes bootstrap.servers The NATS based url of the NATS server. nats://localhost:4222 STRING Yes No client.id The identifier of the client publishing/connecting to the NATS broker. Should be unique for each client connecting to the server/cluster. None STRING Yes No cluster.id The identifier of the NATS server/cluster. test-cluster STRING Yes No Examples EXAMPLE 1 @sink(type='nats', @map(type='xml'), destination='SP_NATS_OUTPUT_TEST', bootstrap.servers='nats://localhost:4222',client.id='nats_client',server.id='test-cluster') define stream outputStream (name string, age int, country string); This example shows how to publish to a NATS subject with all supporting configurations. With the following configuration the sink identified as 'nats-client' will publish to a subject named as 'SP_NATS_OUTPUT_TEST' which resides in a nats instance with a cluster id of 'test-cluster', running in localhost and listening to the port 4222 for client connection. EXAMPLE 2 @sink(type='nats', @map(type='xml'), destination='SP_NATS_OUTPUT_TEST') define stream outputStream (name string, age int, country string); This example shows how to publish to a NATS subject with mandatory configurations. With the following configuration the sink identified with an auto generated client id will publish to a subject named as 'SP_NATS_OUTPUT_TEST' which resides in a nats instance with a cluster id of 'test-cluster', running in localhost and listening to the port 4222 for client connection.","title":"nats (Sink)"},{"location":"docs/api/latest/#prometheus-sink","text":"This sink publishes events processed by Siddhi into Prometheus metrics and exposes them to the Prometheus server at the specified URL. The created metrics can be published to Prometheus via 'server' or 'pushGateway', depending on your preference. The metric types that are supported by the Prometheus sink are 'counter', 'gauge', 'histogram', and 'summary'. The values and labels of the Prometheus metrics can be updated through the events. Origin: siddhi-io-prometheus:2.1.0 Syntax @sink(type=\"prometheus\", job=\" STRING \", publish.mode=\" STRING \", push.url=\" STRING \", server.url=\" STRING \", metric.type=\" STRING \", metric.help=\" STRING \", metric.name=\" STRING \", buckets=\" STRING \", quantiles=\" STRING \", quantile.error=\" DOUBLE \", value.attribute=\" STRING \", push.operation=\" STRING \", grouping.key=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic job This parameter specifies the job name of the metric. This must be the same job name that is defined in the Prometheus configuration file. siddhiJob STRING Yes No publish.mode The mode in which the metrics need to be exposed to the Prometheus server.The possible publishing modes are 'server' and 'pushgateway'.The server mode exposes the metrics through an HTTP server at the specified URL, and the 'pushGateway' mode pushes the metrics to the pushGateway that needs to be running at the specified URL. server STRING Yes No push.url This parameter specifies the target URL of the Prometheus pushGateway. This is the URL at which the pushGateway must be listening. This URL needs to be defined in the Prometheus configuration file as a target before it can be used here. http://localhost:9091 STRING Yes No server.url This parameter specifies the URL where the HTTP server is initiated to expose metrics in the 'server' publish mode. This URL needs to be defined in the Prometheus configuration file as a target before it can be used here. http://localhost:9080 STRING Yes No metric.type The type of Prometheus metric that needs to be created at the sink. The supported metric types are 'counter', 'gauge',c'histogram' and 'summary'. STRING No No metric.help A brief description of the metric and its purpose. STRING Yes No metric.name This parameter allows you to assign a preferred name for the metric. The metric name must match the regex format, i.e., [a-zA-Z_:][a-zA-Z0-9_:]*. STRING Yes No buckets The bucket values preferred by the user for histogram metrics. The bucket values must be in the 'string' format with each bucket value separated by a comma as shown in the example below. \"2,4,6,8\" null STRING Yes No quantiles This parameter allows you to specify quantile values for summary metrics as preferred. The quantile values must be in the 'string' format with each quantile value separated by a comma as shown in the example below. \"0.5,0.75,0.95\" null STRING Yes No quantile.error The error tolerance value for calculating quantiles in summary metrics. This must be a positive value, but less than 1. 0.001 DOUBLE Yes No value.attribute The name of the attribute in the stream definition that specifies the metric value. The defined 'value' attribute must be included in the stream definition. The system increases the metric value for the counter and gauge metric types by the value of the 'value attribute. The system observes the value of the 'value' attribute for the calculations of 'summary' and 'histogram' metric types. value STRING Yes No push.operation This parameter defines the mode for pushing metrics to the pushGateway. The available push operations are 'push' and 'pushadd'. The operations differ according to the existing metrics in pushGateway where 'push' operation replaces the existing metrics, and 'pushadd' operation only updates the newly created metrics. pushadd STRING Yes No grouping.key This parameter specifies the grouping key of created metrics in key-value pairs. The grouping key is used only in pushGateway mode in order to distinguish the metrics from already existing metrics. The expected format of the grouping key is as follows: \"'key1:value1','key2:value2'\" STRING Yes No System Parameters Name Description Default Value Possible Parameters jobName This property specifies the default job name for the metric. This job name must be the same as the job name defined in the Prometheus configuration file. siddhiJob Any string publishMode The default publish mode for the Prometheus sink for exposing metrics to the Prometheus server. The mode can be either 'server' or 'pushgateway'. server server or pushgateway serverURL This property configures the URL where the HTTP server is initiated to expose metrics. This URL needs to be defined in the Prometheus configuration file as a target to be identified by Prometheus before it can be used here. By default, the HTTP server is initiated at 'http://localhost:9080'. http://localhost:9080 Any valid URL pushURL This property configures the target URL of the Prometheus pushGateway (where the pushGateway needs to listen). This URL needs to be defined in the Prometheus configuration file as a target to be identified by Prometheus before it can be used here. http://localhost:9091 Any valid URL groupingKey This property configures the grouping key of created metrics in key-value pairs. Grouping key is used only in pushGateway mode in order to distinguish these metrics from already existing metrics under the same job. The expected format of the grouping key is as follows: \"'key1:value1','key2:value2'\" . null Any key value pairs in the supported format Examples EXAMPLE 1 @sink(type='prometheus',job='fooOrderCount', server.url ='http://localhost:9080', publish.mode='server', metric.type='counter', metric.help= 'Number of foo orders', @map(type='keyvalue')) define stream FooCountStream (Name String, quantity int, value int); In the above example, the Prometheus-sink creates a counter metric with the stream name and defined attributes as labels. The metric is exposed through an HTTP server at the target URL. EXAMPLE 2 @sink(type='prometheus',job='inventoryLevel', push.url='http://localhost:9080', publish.mode='pushGateway', metric.type='gauge', metric.help= 'Current level of inventory', @map(type='keyvalue')) define stream InventoryLevelStream (Name String, value int); In the above example, the Prometheus-sink creates a gauge metric with the stream name and defined attributes as labels.The metric is pushed to the Prometheus pushGateway at the target URL.","title":"prometheus (Sink)"},{"location":"docs/api/latest/#rabbitmq-sink","text":"The rabbitmq sink pushes the events into a rabbitmq broker using the AMQP protocol Origin: siddhi-io-rabbitmq:3.0.2 Syntax @sink(type=\"rabbitmq\", uri=\" STRING \", heartbeat=\" INT \", exchange.name=\" STRING \", exchange.type=\" STRING \", exchange.durable.enabled=\" BOOL \", exchange.autodelete.enabled=\" BOOL \", delivery.mode=\" INT \", content.type=\" STRING \", content.encoding=\" STRING \", priority=\" INT \", correlation.id=\" STRING \", reply.to=\" STRING \", expiration=\" STRING \", message.id=\" STRING \", timestamp=\" STRING \", type=\" STRING \", user.id=\" STRING \", app.id=\" STRING \", routing.key=\" STRING \", headers=\" STRING \", tls.enabled=\" BOOL \", tls.truststore.path=\" STRING \", tls.truststore.password=\" STRING \", tls.truststore.type=\" STRING \", tls.version=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic uri The URI that used to connect to an AMQP server. If no URI is specified, an error is logged in the CLI.e.g., amqp://guest:guest , amqp://guest:guest@localhost:5672 STRING No No heartbeat The period of time (in seconds) after which the peer TCP connection should be considered unreachable (down) by RabbitMQ and client libraries. 60 INT Yes No exchange.name The name of the exchange that decides what to do with a message it sends.If the exchange.name already exists in the RabbitMQ server, then the system uses that exchange.name instead of redeclaring. STRING No Yes exchange.type The type of the exchange.name. The exchange types available are direct , fanout , topic and headers . For a detailed description of each type, see RabbitMQ - AMQP Concepts direct STRING Yes Yes exchange.durable.enabled If this is set to true , the exchange remains declared even if the broker restarts. false BOOL Yes Yes exchange.autodelete.enabled If this is set to true , the exchange is automatically deleted when it is not used anymore. false BOOL Yes Yes delivery.mode This determines whether the connection should be persistent or not. The value must be either 1 or 2 .If the delivery.mode = 1, then the connection is not persistent. If the delivery.mode = 2, then the connection is persistent. 1 INT Yes No content.type The message content type. This should be the MIME content type. null STRING Yes No content.encoding The message content encoding. The value should be MIME content encoding. null STRING Yes No priority Specify a value within the range 0 to 9 in this parameter to indicate the message priority. 0 INT Yes Yes correlation.id The message correlated to the current message. e.g., The request to which this message is a reply. When a request arrives, a message describing the task is pushed to the queue by the front end server. After that the frontend server blocks to wait for a response message with the same correlation ID. A pool of worker machines listen on queue, and one of them picks up the task, performs it, and returns the result as message. Once a message with right correlation ID arrives, thefront end server continues to return the response to the caller. null STRING Yes Yes reply.to This is an anonymous exclusive callback queue. When the RabbitMQ receives a message with the reply.to property, it sends the response to the mentioned queue. This is commonly used to name a reply queue (or any other identifier that helps a consumer application to direct its response). null STRING Yes No expiration The expiration time after which the message is deleted. The value of the expiration field describes the TTL (Time To Live) period in milliseconds. null STRING Yes No message.id The message identifier. If applications need to identify messages, it is recommended that they use this attribute instead of putting it into the message payload. null STRING Yes Yes timestamp Timestamp of the moment when the message was sent. If you do not specify a value for this parameter, the system automatically generates the current date and time as the timestamp value. The format of the timestamp value is dd/mm/yyyy . current timestamp STRING Yes No type The type of the message. e.g., The type of the event or the command represented by the message. null STRING Yes No user.id The user ID specified here is verified by RabbitMQ against theuser name of the actual connection. This is an optional parameter. null STRING Yes No app.id The identifier of the application that produced the message. null STRING Yes No routing.key The key based on which the excahnge determines how to route the message to the queue. The routing key is similar to an address for the message. empty STRING Yes Yes headers The headers of the message. The attributes used for routing are taken from the this paremeter. A message is considered matching if the value of the header equals the value specified upon binding. null STRING Yes Yes tls.enabled This parameter specifies whether an encrypted communication channel should be established or not. When this parameter is set to true , the tls.truststore.path and tls.truststore.password parameters are initialized. false BOOL Yes No tls.truststore.path The file path to the location of the truststore of the client that sends the RabbitMQ events via the AMQP protocol. A custom client-truststore can be specified if required. If a custom truststore is not specified, then the system uses the default client-trustore in the {carbon.home}/resources/security /code directory. /p /td td style=\"vertical-align: top\" {carbon.home}/resources/security</code> directory.</p></td> <td style=\"vertical-align: top\"> /resources/security/client-truststore.jks STRING Yes No tls.truststore.password The password for the client-truststore. A custom password can be specified if required. If no custom password is specified, then the system uses wso2carbon as the default password. wso2carbon STRING Yes No tls.truststore.type The type of the truststore. JKS STRING Yes No tls.version The version of the tls/ssl. SSL STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink(type ='rabbitmq', uri = 'amqp://guest:guest@localhost:5672', exchange.name = 'direct', routing.key= 'direct', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes events to the direct exchange with the direct exchange type and the directTest routing key.","title":"rabbitmq (Sink)"},{"location":"docs/api/latest/#s3-sink","text":"S3 sink publishes events as Amazon AWS S3 buckets. Origin: siddhi-io-s3:1.0.2 Syntax @sink(type=\"s3\", credential.provider.class=\" STRING \", aws.access.key=\" STRING \", aws.secret.key=\" STRING \", bucket.name=\" STRING \", aws.region=\" STRING \", versioning.enabled=\" BOOL \", object.path=\" STRING \", storage.class=\" STRING \", content.type=\" STRING \", bucket.acl=\" STRING \", node.id=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic credential.provider.class AWS credential provider class to be used. If blank along with the username and the password, default credential provider will be used. EMPTY_STRING STRING Yes No aws.access.key AWS access key. This cannot be used along with the credential.provider.class EMPTY_STRING STRING Yes No aws.secret.key AWS secret key. This cannot be used along with the credential.provider.class EMPTY_STRING STRING Yes No bucket.name Name of the S3 bucket STRING No No aws.region The region to be used to create the bucket EMPTY_STRING STRING Yes No versioning.enabled Flag to enable versioning support in the bucket false BOOL Yes No object.path Path for each S3 object STRING No Yes storage.class AWS storage class standard STRING Yes No content.type Content type of the event application/octet-stream STRING Yes Yes bucket.acl Access control list for the bucket EMPTY_STRING STRING Yes No node.id The node ID of the current publisher. This needs to be unique for each publisher instance as it may cause object overwrites while uploading the objects to same S3 bucket from different publishers. EMPTY_STRING STRING Yes No Examples EXAMPLE 1 @sink(type='s3', bucket.name='user-stream-bucket',object.path='bar/users', credential.provider='software.amazon.awssdk.auth.credentials.ProfileCredentialsProvider', flush.size='3', @map(type='json', enclosing.element='$.user', @payload(\"\"\"{\"name\": \"{{name}}\", \"age\": {{age}}}\"\"\"))) define stream UserStream(name string, age int); This creates a S3 bucket named 'user-stream-bucket'. Then this will collect 3 events together and create a JSON object and save that in S3.","title":"s3 (Sink)"},{"location":"docs/api/latest/#tcp-sink","text":"A Siddhi application can be configured to publish events via the TCP transport by adding the @Sink(type = 'tcp') annotation at the top of an event stream definition. Origin: siddhi-io-tcp:3.0.4 Syntax @sink(type=\"tcp\", url=\" STRING \", sync=\" STRING \", tcp.no.delay=\" BOOL \", keep.alive=\" BOOL \", worker.threads=\" INT|LONG \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic url The URL to which outgoing events should be published via TCP. The URL should adhere to tcp:// host : port / context format. STRING No No sync This parameter defines whether the events should be published in a synchronized manner or not. If sync = 'true', then the worker will wait for the ack after sending the message. Else it will not wait for an ack. false STRING Yes Yes tcp.no.delay This is to specify whether to disable Nagle algorithm during message passing. If tcp.no.delay = 'true', the execution of Nagle algorithm will be disabled in the underlying TCP logic. Hence there will be no delay between two successive writes to the TCP connection. Else there can be a constant ack delay. true BOOL Yes No keep.alive This property defines whether the server should be kept alive when there are no connections available. true BOOL Yes No worker.threads Number of threads to publish events. 10 INT LONG Yes No Examples EXAMPLE 1 @Sink(type = 'tcp', url='tcp://localhost:8080/abc', sync='true' @map(type='binary')) define stream Foo (attribute1 string, attribute2 int); A sink of type 'tcp' has been defined. All events arriving at Foo stream via TCP transport will be sent to the url tcp://localhost:8080/abc in a synchronous manner.","title":"tcp (Sink)"},{"location":"docs/api/latest/#sinkmapper","text":"","title":"Sinkmapper"},{"location":"docs/api/latest/#avro-sink-mapper","text":"This extension is a Siddhi Event to Avro Message output mapper.Transports that publish messages to Avro sink can utilize this extension to convert Siddhi events to Avro messages. You can either specify the Avro schema or provide the schema registry URL and the schema reference ID as parameters in the stream definition. If no Avro schema is specified, a flat Avro schema of the 'record' type is generated with the stream attributes as schema fields. Origin: siddhi-map-avro:2.0.6 Syntax @sink(..., @map(type=\"avro\", schema.def=\" STRING \", schema.registry=\" STRING \", schema.id=\" STRING \") QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic schema.def This specifies the required Avro schema to be used to convert Siddhi events to Avro messages. The schema needs to be specified as a quoted JSON string. STRING No No schema.registry This specifies the URL of the schema registry. STRING No No schema.id This specifies the ID of the avro schema. This ID is the global ID that is returned from the schema registry when posting the schema to the registry. The specified ID is used to retrieve the schema from the schema registry. STRING No No Examples EXAMPLE 1 @sink(type='inMemory', topic='stock', @map(type='avro',schema.def = \"\"\"{\"type\":\"record\",\"name\":\"stock\",\"namespace\":\"stock.example\",\"fields\":[{\"name\":\"symbol\",\"type\":\"string\"},{\"name\":\"price\",\"type\":\"float\"},{\"name\":\"volume\",\"type\":\"long\"}]}\"\"\")) define stream StockStream (symbol string, price float, volume long); The above configuration performs a default Avro mapping that generates an Avro message as an output ByteBuffer. EXAMPLE 2 @sink(type='inMemory', topic='stock', @map(type='avro',schema.registry = 'http://localhost:8081', schema.id ='22',@payload(\"\"\"{\"Symbol\":{{symbol}},\"Price\":{{price}},\"Volume\":{{volume}}}\"\"\" ))) define stream StockStream (symbol string, price float, volume long); The above configuration performs a custom Avro mapping that generates an Avro message as an output ByteBuffer. The Avro schema is retrieved from the given schema registry (localhost:8081) using the schema ID provided.","title":"avro (Sink Mapper)"},{"location":"docs/api/latest/#binary-sink-mapper","text":"This section explains how to map events processed via Siddhi in order to publish them in the binary format. Origin: siddhi-map-binary:2.0.4 Syntax @sink(..., @map(type=\"binary\") Examples EXAMPLE 1 @sink(type='inMemory', topic='WSO2', @map(type='binary')) define stream FooStream (symbol string, price float, volume long); This will publish Siddhi event in binary format.","title":"binary (Sink Mapper)"},{"location":"docs/api/latest/#csv-sink-mapper","text":"This output mapper extension allows you to convert Siddhi events processed by the WSO2 SP to CSV message before publishing them. You can either use custom placeholder to map a custom CSV message or use pre-defined CSV format where event conversion takes place without extra configurations. Origin: siddhi-map-csv:2.0.3 Syntax @sink(..., @map(type=\"csv\", delimiter=\" STRING \", header=\" BOOL \", event.grouping.enabled=\" BOOL \") QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic delimiter This parameter used to separate the output CSV data, when converting a Siddhi event to CSV format, , STRING Yes No header This parameter specifies whether the CSV messages will be generated with header or not. If this parameter is set to true, message will be generated with header false BOOL Yes No event.grouping.enabled If this parameter is set to true , events are grouped via a line.separator when multiple events are received. It is required to specify a value for the System.lineSeparator() when the value for this parameter is true . false BOOL Yes No Examples EXAMPLE 1 @sink(type='inMemory', topic='{{symbol}}', @map(type='csv')) define stream BarStream (symbol string, price float, volume long); Above configuration will perform a default CSV output mapping, which will generate output as follows: WSO2,55.6,100 OS supported line separator If header is true and delimiter is \"-\", then the output will be as follows: symbol-price-volume OS supported line separator WSO2-55.6-100 OS supported line separator EXAMPLE 2 @sink(type='inMemory', topic='{{symbol}}', @map(type='csv',header='true',delimiter='-',@payload(symbol='0',price='2',volume='1')))define stream BarStream (symbol string, price float,volume long); Above configuration will perform a custom CSV mapping. Here, user can add custom place order in the @payload. The place order indicates that where the attribute name's value will be appear in the output message, The output will be produced output as follows: WSO2,100,55.6 If header is true and delimiter is \"-\", then the output will be as follows: symbol-price-volume WSO2-55.6-100 OS supported line separator If event grouping is enabled, then the output is as follows: WSO2-55.6-100 OS supported line separator WSO2-55.6-100 OS supported line separator WSO2-55.6-100 OS supported line separator","title":"csv (Sink Mapper)"},{"location":"docs/api/latest/#json-sink-mapper","text":"This extension is an Event to JSON output mapper. Transports that publish messages can utilize this extension to convert Siddhi events to JSON messages. You can either send a pre-defined JSON format or a custom JSON message. Origin: siddhi-map-json:5.0.5 Syntax @sink(..., @map(type=\"json\", validate.json=\" BOOL \", enclosing.element=\" STRING \") QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic validate.json If this property is set to true , it enables JSON validation for the JSON messages generated. When validation is carried out, messages that do not adhere to proper JSON standards are dropped. This property is set to 'false' by default. false BOOL Yes No enclosing.element This specifies the enclosing element to be used if multiple events are sent in the same JSON message. Siddhi treats the child elements of the given enclosing element as events and executes JSON expressions on them. If an enclosing.element is not provided, the multiple event scenario is disregarded and JSON path is evaluated based on the root element. $ STRING Yes No Examples EXAMPLE 1 @sink(type='inMemory', topic='stock', @map(type='json')) define stream FooStream (symbol string, price float, volume long); Above configuration does a default JSON input mapping that generates the output given below. { \"event\":{ \"symbol\":WSO2, \"price\":55.6, \"volume\":100 } } EXAMPLE 2 @sink(type='inMemory', topic='{{symbol}}', @map(type='json', enclosing.element='$.portfolio', validate.json='true', @payload( \"\"\"{\"StockData\":{\"Symbol\":\"{{symbol}}\",\"Price\":{{price}}}\"\"\"))) define stream BarStream (symbol string, price float, volume long); The above configuration performs a custom JSON mapping that generates the following JSON message as the output. {\"portfolio\":{ \"StockData\":{ \"Symbol\":WSO2, \"Price\":55.6 } } }","title":"json (Sink Mapper)"},{"location":"docs/api/latest/#keyvalue-sink-mapper","text":"The Event to Key-Value Map output mapper extension allows you to convert Siddhi events processed by WSO2 SP to key-value map events before publishing them. You can either use pre-defined keys where conversion takes place without extra configurations, or use custom keys with which the messages can be published. Origin: siddhi-map-keyvalue:2.0.5 Syntax @sink(..., @map(type=\"keyvalue\") Examples EXAMPLE 1 @sink(type='inMemory', topic='stock', @map(type='keyvalue')) define stream FooStream (symbol string, price float, volume long); This query performs a default Key-Value output mapping. The expected output is something similar to the following: symbol:'WSO2' price : 55.6f volume: 100L EXAMPLE 2 @sink(type='inMemory', topic='stock', @map(type='keyvalue', @payload(a='symbol',b='price',c='volume'))) define stream FooStream (symbol string, price float, volume long); This query performs a custom Key-Value output mapping where values are passed as objects. Values for symbol , price , and volume attributes are published with the keys a , b and c respectively. The expected output is a map similar to the following: a:'WSO2' b : 55.6f c: 100L EXAMPLE 3 @sink(type='inMemory', topic='stock', @map(type='keyvalue', @payload(a='{{symbol}} is here',b='`price`',c='volume'))) define stream FooStream (symbol string, price float, volume long); This query performs a custom Key-Value output mapping where the values of the a and b attributes are strings and c is object. The expected output should be a Map similar to the following: a:'WSO2 is here' b : 'price' c: 100L","title":"keyvalue (Sink Mapper)"},{"location":"docs/api/latest/#passthrough-sink-mapper","text":"Pass-through mapper passed events (Event[]) through without any mapping or modifications. Origin: siddhi-core:5.1.8 Syntax @sink(..., @map(type=\"passThrough\") Examples EXAMPLE 1 @sink(type='inMemory', @map(type='passThrough')) define stream BarStream (symbol string, price float, volume long); In the following example BarStream uses passThrough outputmapper which emit Siddhi event directly without any transformation into sink.","title":"passThrough (Sink Mapper)"},{"location":"docs/api/latest/#protobuf-sink-mapper","text":"This output mapper allows you to convert Events to protobuf messages before publishing them. To work with this mapper you have to add auto-generated protobuf classes to the project classpath. When you use this output mapper, you can either define stream attributes as the same names as the protobuf message attributes or you can use custom mapping to map stream definition attributes with the protobuf attributes..Please find the sample proto definition here Origin: siddhi-map-protobuf:1.0.2 Syntax @sink(..., @map(type=\"protobuf\", class=\" STRING \") QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic class This specifies the class name of the protobuf message class, If sink type is grpc then it's not necessary to provide this parameter. - STRING Yes No Examples EXAMPLE 1 @sink(type='grpc', url = 'grpc://localhost:2000/org.wso2.grpc.test.MyService/process @map(type='protobuf')) define stream BarStream (stringValue string, intValue int,longValue long,booleanValue bool,floatValue float,doubleValue double) Above definition will map BarStream values into the protobuf message type of the 'process' method in 'MyService' service EXAMPLE 2 @sink(type='grpc', url = 'grpc://localhost:2000/org.wso2.grpc.test.MyService/process @map(type='protobuf'), @payload(stringValue='a',longValue='b',intValue='c',booleanValue='d',floatValue = 'e', doubleValue = 'f'))) define stream BarStream (a string, b long, c int,d bool,e float,f double); The above definition will map BarStream values to request message type of the 'process' method in 'MyService' service. and stream values will map like this, - value of 'a' will be assign 'stringValue' variable in the message class - value of 'b' will be assign 'longValue' variable in the message class - value of 'c' will be assign 'intValue' variable in the message class - value of 'd' will be assign 'booleanValue' variable in the message class - value of 'e' will be assign 'floatValue' variable in the message class - value of 'f' will be assign 'doubleValue' variable in the message class EXAMPLE 3 @sink(type='grpc', url = 'grpc://localhost:2000/org.wso2.grpc.test.MyService/testMap' @map(type='protobuf')) define stream BarStream (stringValue string,intValue int,map object); The above definition will map BarStream values to request message type of the 'testMap' method in 'MyService' service and since there is an object data type is inthe stream(map object) , mapper will assume that 'map' is an instance of 'java.util.Map' class, otherwise it will throws and error. EXAMPLE 4 @sink(type='inMemory', topic='test01', @map(type='protobuf', class='org.wso2.grpc.test.Request')) define stream BarStream (stringValue string, intValue int,longValue long,booleanValue bool,floatValue float,doubleValue double); The above definition will map BarStream values to 'org.wso2.grpc.test.Request'protobuf class type. If sink type is not a grpc, sink is expecting to get the mapping protobuf class from the 'class' parameter in the @map extension","title":"protobuf (Sink Mapper)"},{"location":"docs/api/latest/#text-sink-mapper","text":"This extension is a Event to Text output mapper. Transports that publish text messages can utilize this extension to convert the Siddhi events to text messages. Users can use a pre-defined text format where event conversion is carried out without any additional configurations, or use custom placeholder(using {{ and }} ) to map custom text messages. Again, you can also enable mustache based custom mapping. In mustache based custom mapping you can use custom placeholder (using {{ and }} or {{{ and }}} ) to map custom text. In mustache based custom mapping, all variables are HTML escaped by default. For example: is replaced with amp; \" is replaced with quot; = is replaced with #61; If you want to return unescaped HTML, use the triple mustache {{{ instead of double {{ . Origin: siddhi-map-text:2.0.4 Syntax @sink(..., @map(type=\"text\", event.grouping.enabled=\" BOOL \", delimiter=\" STRING \", new.line.character=\" STRING \", mustache.enabled=\" BOOL \") QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic event.grouping.enabled If this parameter is set to true , events are grouped via a delimiter when multiple events are received. It is required to specify a value for the delimiter parameter when the value for this parameter is true . false BOOL Yes No delimiter This parameter specifies how events are separated when a grouped event is received. This must be a whole line and not a single character. ~ ~ ~ ~ STRING Yes No new.line.character This attribute indicates the new line character of the event that is expected to be received. This is used mostly when communication between 2 types of operating systems is expected. For example, Linux uses \\n whereas Windows uses \\r\\n as the end of line character. \\n STRING Yes No mustache.enabled If this parameter is set to true , then mustache mapping gets enabled forcustom text mapping. false BOOL Yes No Examples EXAMPLE 1 @sink(type='inMemory', topic='stock', @map(type='text')) define stream FooStream (symbol string, price float, volume long); This query performs a default text input mapping. The expected output is as follows: symbol:\"WSO2\", price:55.6, volume:100 EXAMPLE 2 @sink(type='inMemory', topic='stock', @map(type='text', event.grouping.enabled='true')) define stream FooStream (symbol string, price float, volume long); This query performs a default text input mapping with event grouping. The expected output is as follows: symbol:\"WSO2\", price:55.6, volume:100 ~ ~ ~ ~ symbol:\"WSO2\", price:55.6, volume:100 EXAMPLE 3 @sink(type='inMemory', topic='stock', @map(type='text', @payload(\"SensorID : {{symbol}}/{{volume}}, SensorPrice : Rs{{price}}/=, Value : {{volume}}ml\"))) define stream FooStream (symbol string, price float, volume long); This query performs a custom text mapping. The expected output is as follows: SensorID : wso2/100, SensorPrice : Rs1000/=, Value : 100ml for the following siddhi event. {wso2,1000,100} EXAMPLE 4 @sink(type='inMemory', topic='stock', @map(type='text', event.grouping.enabled='true', @payload(\"Stock price of {{symbol}} is {{price}}\"))) define stream FooStream (symbol string, price float, volume long); This query performs a custom text mapping with event grouping. The expected output is as follows: Stock price of WSO2 is 55.6 ~ ~ ~ ~ Stock price of WSO2 is 55.6 ~ ~ ~ ~ Stock price of WSO2 is 55.6 for the following siddhi event. {WSO2,55.6,10} EXAMPLE 5 @sink(type='inMemory', topic='stock', @map(type='text', mustache.enabled='true', @payload(\"SensorID : {{{symbol}}}/{{{volume}}}, SensorPrice : Rs{{{price}}}/=, Value : {{{volume}}}ml\"))) define stream FooStream (symbol string, price float, volume long); This query performs a custom text mapping to return unescaped HTML. The expected output is as follows: SensorID : a b/100, SensorPrice : Rs1000/=, Value : 100ml for the following siddhi event. {a b,1000,100}","title":"text (Sink Mapper)"},{"location":"docs/api/latest/#xml-sink-mapper","text":"This mapper converts Siddhi output events to XML before they are published via transports that publish in XML format. Users can either send a pre-defined XML format or a custom XML message containing event data. Origin: siddhi-map-xml:5.0.3 Syntax @sink(..., @map(type=\"xml\", validate.xml=\" BOOL \", enclosing.element=\" STRING \") QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic validate.xml This parameter specifies whether the XML messages generated should be validated or not. If this parameter is set to true, messages that do not adhere to proper XML standards are dropped. false BOOL Yes No enclosing.element When an enclosing element is specified, the child elements (e.g., the immediate child elements) of that element are considered as events. This is useful when you need to send multiple events in a single XML message. When an enclosing element is not specified, one XML message per every event will be emitted without enclosing. None in custom mapping and events in default mapping STRING Yes No Examples EXAMPLE 1 @sink(type='inMemory', topic='stock', @map(type='xml')) define stream FooStream (symbol string, price float, volume long); Above configuration will do a default XML input mapping which will generate below output events event symbol WSO2 /symbol price 55.6 /price volume 100 /volume /event /events EXAMPLE 2 @sink(type='inMemory', topic='{{symbol}}', @map(type='xml', enclosing.element=' portfolio ', validate.xml='true', @payload( \" StockData Symbol {{symbol}} /Symbol Price {{price}} /Price /StockData \"))) define stream BarStream (symbol string, price float, volume long); Above configuration will perform a custom XML mapping. Inside @payload you can specify the custom template that you want to send the messages out and addd placeholders to places where you need to add event attributes.Above config will produce below output XML message portfolio StockData Symbol WSO2 /Symbol Price 55.6 /Price /StockData /portfolio","title":"xml (Sink Mapper)"},{"location":"docs/api/latest/#source","text":"","title":"Source"},{"location":"docs/api/latest/#cdc-source","text":"The CDC source receives events when change events (i.e., INSERT, UPDATE, DELETE) are triggered for a database table. Events are received in the 'key-value' format. There are two modes you could perform CDC: Listening mode and Polling mode. In polling mode, the datasource is periodically polled for capturing the changes. The polling period can be configured. In polling mode, you can only capture INSERT and UPDATE changes. On listening mode, the Source will keep listening to the Change Log of the database and notify in case a change has taken place. Here, you are immediately notified about the change, compared to polling mode. The key values of the map of a CDC change event are as follows. For 'listening' mode: For insert: Keys are specified as columns of the table. For delete: Keys are followed by the specified table columns. This is achieved via 'before_'. e.g., specifying 'before_X' results in the key being added before the column named 'X'. For update: Keys are followed followed by the specified table columns. This is achieved via 'before_'. e.g., specifying 'before_X' results in the key being added before the column named 'X'. For 'polling' mode: Keys are specified as the columns of the table.#### Preparations required for working with Oracle Databases in listening mode Using the extension in Windows, Mac OSX and AIX are pretty straight forward inorder to achieve the required behaviour please follow the steps given below - Download the compatible version of oracle instantclient for the database version from here and extract - Extract and set the environment variable LD_LIBRARY_PATH to the location of instantclient which was exstracted as shown below export LD_LIBRARY_PATH= path to the instant client location - Inside the instantclient folder which was download there are two jars xstreams.jar and ojdbc version .jar convert them to OSGi bundles using the tools which were provided in the distribution /bin for converting the ojdbc.jar use the tool spi-provider.sh|bat and for the conversion of xstreams.jar use the jni-provider.sh as shown below(Note: this way of converting Xstreams jar is applicable only for Linux environments for other OSs this step is not required and converting it through the jartobundle.sh tool is enough) ./jni-provider.sh input-jar destination comma seperated native library names once ojdbc and xstreams jars are converted to OSGi copy the generated jars to the distribution /lib . Currently siddhi-io-cdc only supports the oracle database distributions 12 and above See parameter: mode for supported databases and change events. Origin: siddhi-io-cdc:2.0.4 Syntax @source(type=\"cdc\", url=\" STRING \", mode=\" STRING \", jdbc.driver.name=\" STRING \", username=\" STRING \", password=\" STRING \", pool.properties=\" STRING \", datasource.name=\" STRING \", table.name=\" STRING \", polling.column=\" STRING \", polling.interval=\" INT \", operation=\" STRING \", connector.properties=\" STRING \", database.server.id=\" STRING \", database.server.name=\" STRING \", wait.on.missed.record=\" BOOL \", missed.record.waiting.timeout=\" INT \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic url The connection URL to the database. F=The format used is: 'jdbc:mysql:// host : port / database_name ' STRING No No mode Mode to capture the change data. The type of events that can be received, and the required parameters differ based on the mode. The mode can be one of the following: 'polling': This mode uses a column named 'polling.column' to monitor the given table. It captures change events of the 'RDBMS', 'INSERT, and 'UPDATE' types. 'listening': This mode uses logs to monitor the given table. It currently supports change events only of the 'MySQL', 'INSERT', 'UPDATE', and 'DELETE' types. listening STRING Yes No jdbc.driver.name The driver class name for connecting the database. It is required to specify a value for this parameter when the mode is 'polling'. STRING Yes No username The username to be used for accessing the database. This user needs to have the 'SELECT', 'RELOAD', 'SHOW DATABASES', 'REPLICATION SLAVE', and 'REPLICATION CLIENT'privileges for the change data capturing table (specified via the 'table.name' parameter). To operate in the polling mode, the user needs 'SELECT' privileges. STRING No No password The password of the username you specified for accessing the database. STRING No No pool.properties The pool parameters for the database connection can be specified as key-value pairs. STRING Yes No datasource.name Name of the wso2 datasource to connect to the database. When datasource name is provided, the URL, username and password are not needed. A datasource based connection is given more priority over the URL based connection. This parameter is applicable only when the mode is set to 'polling', and it can be applied only when you use this extension with WSO2 Stream Processor. STRING Yes No table.name The name of the table that needs to be monitored for data changes. STRING No No polling.column The column name that is polled to capture the change data. It is recommended to have a TIMESTAMP field as the 'polling.column' in order to capture the inserts and updates. Numeric auto-incremental fields and char fields can also be used as 'polling.column'. However, note that fields of these types only support insert change capturing, and the possibility of using a char field also depends on how the data is input. It is required to enter a value for this parameter only when the mode is 'polling'. STRING Yes No polling.interval The time interval (specified in seconds) to poll the given table for changes. This parameter is applicable only when the mode is set to 'polling'. 1 INT Yes No operation The change event operation you want to carry out. Possible values are 'insert', 'update' or 'delete'. This parameter is not case sensitive. It is required to specify a value only when the mode is 'listening'. STRING No No connector.properties Here, you can specify Debezium connector properties as a comma-separated string. The properties specified here are given more priority over the parameters. This parameter is applicable only for the 'listening' mode. Empty_String STRING Yes No database.server.id An ID to be used when joining MySQL database cluster to read the bin log. This should be a unique integer between 1 to 2^32. This parameter is applicable only when the mode is 'listening'. Random integer between 5400 and 6400 STRING Yes No database.server.name A logical name that identifies and provides a namespace for the database server. This parameter is applicable only when the mode is 'listening'. {host}_{port} STRING Yes No wait.on.missed.record Indicates whether the process needs to wait on missing/out-of-order records. When this flag is set to 'true' the process will be held once it identifies a missing record. The missing recrod is identified by the sequence of the polling.column value. This can be used only with number fields and not recommended to use with time values as it will not be sequential. This should be enabled ONLY where the records can be written out-of-order, (eg. concurrent writers) as this degrades the performance. false BOOL Yes No missed.record.waiting.timeout The timeout (specified in seconds) to retry for missing/out-of-order record. This should be used along with the wait.on.missed.record parameter. If the parameter is not set, the process will indefinitely wait for the missing record. -1 INT Yes No Examples EXAMPLE 1 @source(type = 'cdc' , url = 'jdbc:mysql://localhost:3306/SimpleDB', username = 'cdcuser', password = 'pswd4cdc', table.name = 'students', operation = 'insert', @map(type='keyvalue', @attributes(id = 'id', name = 'name'))) define stream inputStream (id string, name string); In this example, the CDC source listens to the row insertions that are made in the 'students' table with the column name, and the ID. This table belongs to the 'SimpleDB' MySQL database that can be accessed via the given URL. EXAMPLE 2 @source(type = 'cdc' , url = 'jdbc:mysql://localhost:3306/SimpleDB', username = 'cdcuser', password = 'pswd4cdc', table.name = 'students', operation = 'update', @map(type='keyvalue', @attributes(id = 'id', name = 'name', before_id = 'before_id', before_name = 'before_name'))) define stream inputStream (before_id string, id string, before_name string , name string); In this example, the CDC source listens to the row updates that are made in the 'students' table. This table belongs to the 'SimpleDB' MySQL database that can be accessed via the given URL. EXAMPLE 3 @source(type = 'cdc' , url = 'jdbc:mysql://localhost:3306/SimpleDB', username = 'cdcuser', password = 'pswd4cdc', table.name = 'students', operation = 'delete', @map(type='keyvalue', @attributes(before_id = 'before_id', before_name = 'before_name'))) define stream inputStream (before_id string, before_name string); In this example, the CDC source listens to the row deletions made in the 'students' table. This table belongs to the 'SimpleDB' database that can be accessed via the given URL. EXAMPLE 4 @source(type = 'cdc', mode='polling', polling.column = 'id', jdbc.driver.name = 'com.mysql.jdbc.Driver', url = 'jdbc:mysql://localhost:3306/SimpleDB', username = 'cdcuser', password = 'pswd4cdc', table.name = 'students', @map(type='keyvalue'), @attributes(id = 'id', name = 'name')) define stream inputStream (id int, name string); In this example, the CDC source polls the 'students' table for inserts. 'id' that is specified as the polling colum' is an auto incremental field. The connection to the database is made via the URL, username, password, and the JDBC driver name. EXAMPLE 5 @source(type = 'cdc', mode='polling', polling.column = 'id', datasource.name = 'SimpleDB', table.name = 'students', @map(type='keyvalue'), @attributes(id = 'id', name = 'name')) define stream inputStream (id int, name string); In this example, the CDC source polls the 'students' table for inserts. The given polling column is a char column with the 'S001, S002, ... .' pattern. The connection to the database is made via a data source named 'SimpleDB'. Note that the 'datasource.name' parameter works only with the Stream Processor. EXAMPLE 6 @source(type = 'cdc', mode='polling', polling.column = 'last_updated', datasource.name = 'SimpleDB', table.name = 'students', @map(type='keyvalue')) define stream inputStream (name string); In this example, the CDC source polls the 'students' table for inserts and updates. The polling column is a timestamp field. EXAMPLE 7 @source(type='cdc', jdbc.driver.name='com.mysql.jdbc.Driver', url='jdbc:mysql://localhost:3306/SimpleDB', username='cdcuser', password='pswd4cdc', table.name='students', mode='polling', polling.column='id', operation='insert', wait.on.missed.record='true', missed.record.waiting.timeout='10', @map(type='keyvalue'), @attributes(batch_no='batch_no', item='item', qty='qty')) define stream inputStream (id int, name string); In this example, the CDC source polls the 'students' table for inserts. The polling column is a numeric field. This source expects the records in the database to be written concurrently/out-of-order so it waits if it encounters a missing record. If the record doesn't appear within 10 seconds it resumes the process. EXAMPLE 8 @source(type = 'cdc', url = 'jdbc:oracle:thin://localhost:1521/ORCLCDB', username='c##xstrm', password='xs', table.name='DEBEZIUM.sweetproductiontable', operation = 'insert', connector.properties='oracle.outserver.name=DBZXOUT,oracle.pdb=ORCLPDB1' @map(type = 'keyvalue')) define stream insertSweetProductionStream (ID int, NAME string, WEIGHT int); In this example, the CDC source connect to an Oracle database and listens for insert queries of sweetproduction table","title":"cdc (Source)"},{"location":"docs/api/latest/#email-source","text":"The 'Email' source allows you to receive events via emails. An 'Email' source can be configured using the 'imap' or 'pop3' server to receive events. This allows you to filter the messages that satisfy the criteria specified under the 'search term' option. The email source parameters can be defined in either the ' SP_HOME /conf/ PROFILE /deployment yaml' file or the stream definition. If the parameter configurations are not available in either place, the default values are considered (i.e., if default values are available). If you need to configure server system parameters that are not provided as options in the stream definition, they need to be defined in the 'deployment yaml' file under 'email source properties'. For more information about 'imap' and 'pop3' server system parameters, see the following. JavaMail Reference Implementation - IMAP Store JavaMail Reference Implementation - POP3 Store Store Origin: siddhi-io-email:2.0.5 Syntax @source(type=\"email\", username=\" STRING \", password=\" STRING \", store=\" STRING \", host=\" STRING \", port=\" INT \", folder=\" STRING \", search.term=\" STRING \", polling.interval=\" LONG \", action.after.processed=\" STRING \", folder.to.move=\" STRING \", content.type=\" STRING \", ssl.enable=\" BOOL \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic username The user name of the email account. e.g., 'wso2mail' is the username of the 'wso2mail@gmail.com' mail account. STRING No No password The password of the email account STRING No No store The store type that used to receive emails. Possible values are 'imap' and 'pop3'. imap STRING Yes No host The host name of the server (e.g., 'imap.gmail.com' is the host name for a gmail account with an IMAP store.). The default value 'imap.gmail.com' is only valid if the email account is a gmail account with IMAP enabled. If store type is 'imap', then the default value is 'imap.gmail.com'. If the store type is 'pop3', then thedefault value is 'pop3.gmail.com'. STRING Yes No port The port that is used to create the connection. '993', the default value is valid only if the store is 'imap' and ssl-enabled. INT Yes No folder The name of the folder to which the emails should be fetched. INBOX STRING Yes No search.term The option that includes conditions such as key-value pairs to search for emails. In a string search term, the key and the value should be separated by a semicolon (';'). Each key-value pair must be within inverted commas (' '). The string search term can define multiple comma-separated key-value pairs. This string search term currently supports only the 'subject', 'from', 'to', 'bcc', and 'cc' keys. e.g., if you enter 'subject:DAS, from:carbon, bcc:wso2', the search term creates a search term instance that filters emails that contain 'DAS' in the subject, 'carbon' in the 'from' address, and 'wso2' in one of the 'bcc' addresses. The string search term carries out sub string matching that is case-sensitive. If '@' in included in the value for any key other than the 'subject' key, it checks for an address that is equal to the value given. e.g., If you search for 'abc@', the string search terms looks for an address that contains 'abc' before the '@' symbol. None STRING Yes No polling.interval This defines the time interval in seconds at which th email source should poll the account to check for new mail arrivals.in seconds. 600 LONG Yes No action.after.processed The action to be performed by the email source for the processed mail. Possible values are as follows: 'FLAGGED': Sets the flag as 'flagged'. 'SEEN': Sets the flag as 'read'. 'ANSWERED': Sets the flag as 'answered'. 'DELETE': Deletes tha mail after the polling cycle. 'MOVE': Moves the mail to the folder specified in the 'folder.to.move' parameter. If the folder specified is 'pop3', then the only option available is 'DELETE'. NONE STRING Yes No folder.to.move The name of the folder to which the mail must be moved once it is processed. If the action after processing is 'MOVE', it is required to specify a value for this parameter. STRING No No content.type The content type of the email. It can be either 'text/plain' or 'text/html.' text/plain STRING Yes No ssl.enable If this is set to 'true', a secure port is used to establish the connection. The possible values are 'true' and 'false'. true BOOL Yes No System Parameters Name Description Default Value Possible Parameters mail.imap.partialfetch This determines whether the IMAP partial-fetch capability should be used. true true or false mail.imap.fetchsize The partial fetch size in bytes. 16K value in bytes mail.imap.peek If this is set to 'true', the IMAP PEEK option should be used when fetching body parts to avoid setting the 'SEEN' flag on messages. The default value is 'false'. This can be overridden on a per-message basis by the 'setPeek method' in 'IMAPMessage'. false true or false mail.imap.connectiontimeout The socket connection timeout value in milliseconds. This timeout is implemented by 'java.net.Socket'. infinity timeout Any Integer value mail.imap.timeout The socket read timeout value in milliseconds. This timeout is implemented by 'java.net.Socket'. infinity timeout Any Integer value mail.imap.writetimeout The socket write timeout value in milliseconds. This timeout is implemented by using a 'java.util.concurrent.ScheduledExecutorService' per connection that schedules a thread to close the socket if the timeout period elapses. Therefore, the overhead of using this timeout is one thread per connection. infinity timeout Any Integer value mail.imap.statuscachetimeout The timeout value in milliseconds for the cache of 'STATUS' command response. 1000ms Time out in miliseconds mail.imap.appendbuffersize The maximum size of a message to buffer in memory when appending to an IMAP folder. None Any Integer value mail.imap.connectionpoolsize The maximum number of available connections in the connection pool. 1 Any Integer value mail.imap.connectionpooltimeout The timeout value in milliseconds for connection pool connections. 45000ms Any Integer mail.imap.separatestoreconnection If this parameter is set to 'true', it indicates that a dedicated store connection needs to be used for store commands. true true or false mail.imap.auth.login.disable If this is set to 'true', it is not possible to use the non-standard 'AUTHENTICATE LOGIN' command instead of the plain 'LOGIN' command. false true or false mail.imap.auth.plain.disable If this is set to 'true', the 'AUTHENTICATE PLAIN' command cannot be used. false true or false mail.imap.auth.ntlm.disable If true, prevents use of the AUTHENTICATE NTLM command. false true or false mail.imap.proxyauth.user If the server supports the PROXYAUTH extension, this property specifies the name of the user to act as. Authentication to log in to the server is carried out using the administrator's credentials. After authentication, the IMAP provider issues the 'PROXYAUTH' command with the user name specified in this property. None Valid string value mail.imap.localaddress The local address (host name) to bind to when creating the IMAP socket. Defaults to the address picked by the Socket class. Valid string value mail.imap.localport The local port number to bind to when creating the IMAP socket. Defaults to the port number picked by the Socket class. Valid String value mail.imap.sasl.enable If this parameter is set to 'true', the system attempts to use the 'javax.security.sasl' package to choose an authentication mechanism for the login. false true or false mail.imap.sasl.mechanisms A list of SASL mechanism names that the system should to try to use. The names can be separated by spaces or commas. None Valid string value mail.imap.sasl.authorizationid The authorization ID to use in the SASL authentication. If this parameter is not set, the authentication ID (username) is used. Valid string value mail.imap.sasl.realm The realm to use with SASL authentication mechanisms that require a realm, such as 'DIGEST-MD5'. None Valid string value mail.imap.auth.ntlm.domain The NTLM authentication domain. None Valid string value The NTLM authentication domain. NTLM protocol-specific flags. None Valid integer value mail.imap.socketFactory If this parameter is set to a class that implements the 'javax.net.SocketFactory' interface, this class is used to create IMAP sockets. None Valid SocketFactory mail.imap.socketFactory.class If this parameter is set, it specifies the name of a class that implements the 'javax.net.SocketFactory' interface. This class is used to create IMAP sockets. None Valid string mail.imap.socketFactory.fallback If this parameter is set to 'true', failure to create a socket using the specified socket factory class results in the socket being created using the 'java.net.Socket' class. true true or false mail.imap.socketFactory.port This specifies the port to connect to when using the specified socket factory. If this parameter is not set, the default port is used. 143 Valid Integer mail.imap.ssl.checkserveridentity If this parameter is set to 'true', the system checks the server identity as specified by RFC 2595. false true or false mail.imap.ssl.trust If this parameter is set and a socket factory has not been specified, it enables the use of a 'MailSSLSocketFactory'. If this parameter is set to ' ', all the hosts are trusted. If this parameter specifies list of hosts separated by white spaces, only those hosts are trusted. If the parameter is not set to any of the values mentioned above, trust depends on the certificate presented by the server. Valid String mail.imap.ssl.socketFactory If this parameter is set to a class that extends the 'javax.net.ssl.SSLSocketFactory' class this class is used to create IMAP SSL sockets. None SSL Socket Factory mail.imap.ssl.socketFactory.class If this parameter is set, it specifies the name of a class that extends the 'javax.net.ssl.SSLSocketFactory' class. This class is used to create IMAP SSL sockets. None Valid String mail.imap.ssl.socketFactory.port This specifies the port to connect to when using the specified socket factory. the default port 993 is used. valid port number mail.imap.ssl.protocols This specifies the SSL protocols that are enabled for SSL connections. The property value is a whitespace-separated list of tokens acceptable to the 'javax.net.ssl.SSLSocket.setEnabledProtocols' method. None Valid string mail.imap.starttls.enable If this parameter is set to 'true', it is possible to use the 'STARTTLS' command (if supported by the server) to switch the connection to a TLS-protected connection before issuing any login commands. false true or false mail.imap.socks.host This specifies the host name of a 'SOCKS5' proxy server that is used to connect to the mail server. None Valid String mail.imap.socks.port This specifies the port number for the 'SOCKS5' proxy server. This is needed if the proxy server is not using the standard port number 1080. 1080 Valid String mail.imap.minidletime This property sets the delay in milliseconds. 10 milliseconds time in seconds (Integer) mail.imap.enableimapevents If this property is set to 'true', it enables special IMAP-specific events to be delivered to the 'ConnectionListener' of the store. The unsolicited responses received during the idle method of the store are sent as connection events with 'IMAPStore.RESPONSE' as the type. The event's message is the raw IMAP response string. false true or false mail.imap.folder.class The class name of a subclass of 'com.sun.mail.imap.IMAPFolder'. The subclass can be used to provide support for additional IMAP commands. The subclass must have public constructors of the form 'public MyIMAPFolder'(String fullName, char separator, IMAPStore store, Boolean isNamespace) and public 'MyIMAPFolder'(ListInfo li, IMAPStore store) None Valid String mail.pop3.connectiontimeout The socket connection timeout value in milliseconds. Infinite timeout Integer value mail.pop3.timeout The socket I/O timeout value in milliseconds. Infinite timeout Integer value mail.pop3.message.class The class name of a subclass of 'com.sun.mail.pop3.POP3Message'. None Valid String mail.pop3.localaddress The local address (host name) to bind to when creating the POP3 socket. Defaults to the address picked by the Socket class. Valid String mail.pop3.localport The local port number to bind to when creating the POP3 socket. Defaults to the port number picked by the Socket class. Valid port number mail.pop3.apop.enable If this parameter is set to 'true', use 'APOP' instead of 'USER/PASS' to log in to the 'POP3' server (if the 'POP3' server supports 'APOP'). APOP sends a digest of the password instead of clearing the text password. false true or false mail.pop3.socketFactory If this parameter is set to a class that implements the 'javax.net.SocketFactory' interface, this class is used to create 'POP3' sockets. None Socket Factory mail.pop3.socketFactory.class If this parameter is set, it specifies the name of a class that implements the 'javax.net.SocketFactory' interface. This class is used to create 'POP3' sockets. None Valid String mail.pop3.socketFactory.fallback If this parameter is set to 'true', failure to create a socket using the specified socket factory class results in the socket being created using the 'java.net.Socket' class. false true or false mail.pop3.socketFactory.port This specifies the port to connect to when using the specified socket factory. Default port Valid port number mail.pop3.ssl.checkserveridentity If this parameter is set to 'true', check the server identity as specified by RFC 2595. false true or false mail.pop3.ssl.trust If this parameter is set and a socket factory has not been specified, it is possible to use a 'MailSSLSocketFactory'. If this parameter is set to ' ', all the hosts are trusted. If the parameter is set to a whitespace-separated list of hosts, only those hosts are trusted. If the parameter is not set to any of the values mentioned above, trust depends on the certificate presented by the server. Valid String mail.pop3.ssl.socketFactory If this parameter is set to a class that extends the 'javax.net.ssl.SSLSocketFactory' class, this class is used to create 'POP3' SSL sockets. None SSL Socket Factory mail.pop3.ssl.checkserveridentity If this parameter is set to 'true', the system checks the server identity as specified by 'RFC 2595'. false true or false mail.pop3.ssl.trust If this parameter is set and a socket factory has not been specified, it is possible to use a 'MailSSLSocketFactory'. If this parameter is set to '*', all the hosts are trusted. If the parameter is set to a whitespace-separated list of hosts, only those hosts are trusted. Trust depends on the certificate presented by the server. Valid String mail.pop3.ssl.socketFactory If this parameter is set to a class that extends the 'javax.net.ssl.SSLSocketFactory' class, this class is used to create 'POP3 SSL' sockets. None SSL Socket Factory mail.pop3.ssl.socketFactory.class If this parameter is set, it specifies the name of a class that extends the 'javax.net.ssl.SSLSocketFactory' class. This class is used to create 'POP3 SSL' sockets. None Valid String mail.pop3.ssl.socketFactory.p This parameter pecifies the port to connect to when using the specified socket factory. 995 Valid Integer mail.pop3.ssl.protocols This parameter specifies the SSL protocols that are enabled for SSL connections. The property value is a whitespace-separated list of tokens acceptable to the 'javax.net.ssl.SSLSocket.setEnabledProtocols' method. None Valid String mail.pop3.starttls.enable If this parameter is set to 'true', it is possible to use the 'STLS' command (if supported by the server) to switch the connection to a TLS-protected connection before issuing any login commands. false true or false mail.pop3.starttls.required If this parameter is set to 'true', it is required to use the 'STLS' command. The connect method fails if the server does not support the 'STLS' command or if the command fails. false true or false mail.pop3.socks.host This parameter specifies the host name of a 'SOCKS5' proxy server that can be used to connect to the mail server. None Valid String mail.pop3.socks.port This parameter specifies the port number for the 'SOCKS5' proxy server. None Valid String mail.pop3.disabletop If this parameter is set to 'true', the 'POP3 TOP' command is not used to fetch message headers. false true or false mail.pop3.forgettopheaders If this parameter is set to 'true', the headers that might have been retrieved using the 'POP3 TOP' command is forgotten and replaced by the headers retrieved when the 'POP3 RETR' command is executed. false true or false mail.pop3.filecache.enable If this parameter is set to 'true', the 'POP3' provider caches message data in a temporary file instead of caching them in memory. Messages are only added to the cache when accessing the message content. Message headers are always cached in memory (on demand). The file cache is removed when the folder is closed or the JVM terminates. false true or false mail.pop3.filecache.dir If the file cache is enabled, this property is used to override the default directory used by the JDK for temporary files. None Valid String mail.pop3.cachewriteto This parameter controls the behavior of the 'writeTo' method on a 'POP3' message object. If the parameter is set to 'true', the message content has not been cached yet, and the 'ignoreList' is null, the message is cached before being written. If not, the message is streamed directly to the output stream without being cached. false true or false mail.pop3.keepmessagecontent If this property is set to 'true', a hard reference to the cached content is retained, preventing the memory from being reused until the folder is closed, or until the cached content is explicitly invalidated (using the 'invalidate' method). false true or false Examples EXAMPLE 1 @source(type='email', @map(type='xml'), username='receiver.account', password='account.password',)define stream inputStream (name string, age int, country string); This example illustrates how to receive events in 'xml' format via the email source. In this example, only the required parameters are defined in the stream definition. The default values are taken for the other parameters. The search term is not defined, and therefore, all the new messages in the inbox folder are polled and taken. EXAMPLE 2 @source(type='email', @map(type='xml'), username='receiver.account', password='account.password',store = 'imap',host = 'imap.gmail.com',port = '993',searchTerm = 'subject:Stream Processor, from: from.account@ , cc: cc.account',polling.interval='500',action.after.processed='DELETE',content.type='text/html,)define stream inputStream (name string, age int, country string); This example illustrates how to receive events in 'xml' format via the email source. The email source polls the mail account every 500 seconds to check whether any new mails have arrived. It processes new mails only if they satisfy the conditions specified for the email search term (the value for 'from' of the email message should be 'from.account@. host name ', and the message should contain 'cc.account' in the cc receipient list and the word 'Stream Processor' in the mail subject). in this example, the action after processing is 'DELETE'. Therefore,after processing the event, corresponding mail is deleted from the mail folder.","title":"email (Source)"},{"location":"docs/api/latest/#file-source","text":"File Source provides the functionality for user to feed data to siddhi from files. Both text and binary files are supported by file source. Origin: siddhi-io-file:2.0.3 Syntax @source(type=\"file\", dir.uri=\" STRING \", file.uri=\" STRING \", mode=\" STRING \", tailing=\" BOOL \", action.after.process=\" STRING \", action.after.failure=\" STRING \", move.after.process=\" STRING \", move.after.failure=\" STRING \", begin.regex=\" STRING \", end.regex=\" STRING \", file.polling.interval=\" STRING \", dir.polling.interval=\" STRING \", timeout=\" STRING \", file.read.wait.timeout=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic dir.uri Used to specify a directory to be processed. All the files inside this directory will be processed. Only one of 'dir.uri' and 'file.uri' should be provided. This uri MUST have the respective protocol specified. STRING No No file.uri Used to specify a file to be processed. Only one of 'dir.uri' and 'file.uri' should be provided. This uri MUST have the respective protocol specified. STRING No No mode This parameter is used to specify how files in given directory should.Possible values for this parameter are, 1. TEXT.FULL : to read a text file completely at once. 2. BINARY.FULL : to read a binary file completely at once. 3. LINE : to read a text file line by line. 4. REGEX : to read a text file and extract data using a regex. line STRING Yes No tailing This can either have value true or false. By default it will be true. This attribute allows user to specify whether the file should be tailed or not. If tailing is enabled, the first file of the directory will be tailed. Also tailing should not be enabled in 'binary.full' or 'text.full' modes. true BOOL Yes No action.after.process This parameter is used to specify the action which should be carried out after processing a file in the given directory. It can be either DELETE or MOVE and default value will be 'DELETE'. If the action.after.process is MOVE, user must specify the location to move consumed files using 'move.after.process' parameter. delete STRING Yes No action.after.failure This parameter is used to specify the action which should be carried out if a failure occurred during the process. It can be either DELETE or MOVE and default value will be 'DELETE'. If the action.after.failure is MOVE, user must specify the location to move consumed files using 'move.after.failure' parameter. delete STRING Yes No move.after.process If action.after.process is MOVE, user must specify the location to move consumed files using 'move.after.process' parameter. This should be the absolute path of the file that going to be created after moving is done. This uri MUST have the respective protocol specified. STRING No No move.after.failure If action.after.failure is MOVE, user must specify the location to move consumed files using 'move.after.failure' parameter. This should be the absolute path of the file that going to be created after moving is done. This uri MUST have the respective protocol specified. STRING No No begin.regex This will define the regex to be matched at the beginning of the retrieved content. None STRING Yes No end.regex This will define the regex to be matched at the end of the retrieved content. None STRING Yes No file.polling.interval This parameter is used to specify the time period (in milliseconds) of a polling cycle for a file. 1000 STRING Yes No dir.polling.interval This parameter is used to specify the time period (in milliseconds) of a polling cycle for a directory. 1000 STRING Yes No timeout This parameter is used to specify the maximum time period (in milliseconds) for waiting until a file is processed. 5000 STRING Yes No file.read.wait.timeout This parameter is used to specify the maximum time period (in milliseconds) till it waits before retrying to read the full file content. 1000 STRING Yes No Examples EXAMPLE 1 @source(type='file', mode='text.full', tailing='false' dir.uri='file://abc/xyz', action.after.process='delete', @map(type='json')) define stream FooStream (symbol string, price float, volume long); Under above configuration, all the files in directory will be picked and read one by one. In this case, it's assumed that all the files contains json valid json strings with keys 'symbol','price' and 'volume'. Once a file is read, its content will be converted to an event using siddhi-map-json extension and then, that event will be received to the FooStream. Finally, after reading is finished, the file will be deleted. EXAMPLE 2 @source(type='file', mode='files.repo.line', tailing='true', dir.uri='file://abc/xyz', @map(type='json')) define stream FooStream (symbol string, price float, volume long); Under above configuration, the first file in directory '/abc/xyz' will be picked and read line by line. In this case, it is assumed that the file contains lines json strings. For each line, line content will be converted to an event using siddhi-map-json extension and then, that event will be received to the FooStream. Once file content is completely read, it will keep checking whether a new entry is added to the file or not. If such entry is added, it will be immediately picked up and processed.","title":"file (Source)"},{"location":"docs/api/latest/#grpc-source","text":"This extension starts a grpc server during initialization time. The server listens to requests from grpc stubs. This source has a default mode of operation and custom user defined grpc service mode. By default this uses EventService. Please find the proto definition here . In the default mode this source will use EventService consume method. If we want to use our custom gRPC services, we have to pack auto-generated gRPC service classes and protobuf classes into a jar file and add it into the project classpath (or to the jars folder in the siddhi-tooling folder if we use it with siddhi-tooling ). Please find the custom protobuf definition that uses in examples here . This method will receive requests and injects them into stream through a mapper. Origin: siddhi-io-grpc:1.0.5 Syntax @source(type=\"grpc\", receiver.url=\" STRING \", max.inbound.message.size=\" INT \", max.inbound.metadata.size=\" INT \", server.shutdown.waiting.time=\" LONG \", truststore.file=\" STRING \", truststore.password=\" STRING \", truststore.algorithm=\" STRING \", tls.store.type=\" STRING \", keystore.file=\" STRING \", keystore.password=\" STRING \", keystore.algorithm=\" STRING \", enable.ssl=\" BOOL \", threadpool.size=\" INT \", threadpool.buffer.size=\" INT \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic receiver.url The url which can be used by a client to access the grpc server in this extension. This url should consist the host hostPort, port, fully qualified service name, method name in the following format. grpc://0.0.0.0:9763/ serviceName / methodName For example: grpc://0.0.0.0:9763/org.wso2.grpc.EventService/consume STRING No No max.inbound.message.size Sets the maximum message size in bytes allowed to be received on the server. 4194304 INT Yes No max.inbound.metadata.size Sets the maximum size of metadata in bytes allowed to be received. 8192 INT Yes No server.shutdown.waiting.time The time in seconds to wait for the server to shutdown, giving up if the timeout is reached. 5 LONG Yes No truststore.file the file path of truststore. If this is provided then server authentication is enabled - STRING Yes No truststore.password the password of truststore. If this is provided then the integrity of the keystore is checked - STRING Yes No truststore.algorithm the encryption algorithm to be used for server authentication - STRING Yes No tls.store.type TLS store type - STRING Yes No keystore.file the file path of keystore. If this is provided then client authentication is enabled - STRING Yes No keystore.password the password of keystore - STRING Yes No keystore.algorithm the encryption algorithm to be used for client authentication - STRING Yes No enable.ssl to enable ssl. If set to true and truststore.file is not given then it will be set to default carbon jks by default FALSE BOOL Yes No threadpool.size Sets the maximum size of threadpool dedicated to serve requests at the gRPC server 100 INT Yes No threadpool.buffer.size Sets the maximum size of threadpool buffer server 100 INT Yes No System Parameters Name Description Default Value Possible Parameters keyStoreFile Path of the key store file src/main/resources/security/wso2carbon.jks valid path for a key store file keyStorePassword This is the password used with key store file wso2carbon valid password for the key store file keyStoreAlgorithm The encryption algorithm to be used for client authentication SunX509 - trustStoreFile This is the trust store file with the path src/main/resources/security/client-truststore.jks - trustStorePassword This is the password used with trust store file wso2carbon valid password for the trust store file trustStoreAlgorithm the encryption algorithm to be used for server authentication SunX509 - Examples EXAMPLE 1 @source(type='grpc', receiver.url='grpc://localhost:8888/org.wso2.grpc.EventService/consume', @map(type='json')) define stream BarStream (message String); Here the port is given as 8888. So a grpc server will be started on port 8888 and the server will expose EventService. This is the default service packed with the source. In EventService the consume method is EXAMPLE 2 @source(type='grpc', receiver.url='grpc://localhost:8888/org.wso2.grpc.EventService/consume', @map(type='json', @attributes(name='trp:name', age='trp:age', message='message'))) define stream BarStream (message String, name String, age int); Here we are getting headers sent with the request as transport properties and injecting them into the stream. With each request a header will be sent in MetaData in the following format: 'Name:John', 'Age:23' EXAMPLE 3 @source(type='grpc', receiver.url='grpc://localhost:8888/org.wso2.grpc.MyService/send', @map(type='protobuf')) define stream BarStream (stringValue string, intValue int,longValue long,booleanValue bool,floatValue float,doubleValue double); Here the port is given as 8888. So a grpc server will be started on port 8888 and sever will keep listening to the 'send' RPC method in the 'MyService' service. EXAMPLE 4 @source(type='grpc', receiver.url='grpc://localhost:8888/org.wso2.grpc.MyService/send', @map(type='protobuf', @attributes(a = 'stringValue', b = 'intValue', c = 'longValue',d = 'booleanValue', e ='floatValue', f ='doubleValue'))) define stream BarStream (a string ,c long,b int, d bool,e float,f double); Here the port is given as 8888. So a grpc server will be started on port 8888 and sever will keep listening to the 'send' method in the 'MyService' service. Since we provide mapping in the stream we can use any names for stream attributes, but we have to map those names with correct protobuf message attributes' names. If we want to send metadata, we should map the attributes. EXAMPLE 5 @source(type='grpc', receiver.url='grpc://localhost:8888/org.wso2.grpc.StreamService/clientStream', @map(type='protobuf')) define stream BarStream (stringValue string, intValue int,longValue long,booleanValue bool,floatValue float,doubleValue double); Here we receive a stream of requests to the grpc source. Whenever we want to use streaming with grpc source, we have to define the RPC method as client streaming method (look at the sample proto file provided in the resource folder here ), when we define a stream method siddhi will identify it as a stream RPC method and ready to accept stream of request from the client.","title":"grpc (Source)"},{"location":"docs/api/latest/#grpc-call-response-source","text":"This grpc source receives responses received from gRPC server for requests sent from a grpc-call sink. The source will receive responses for sink with the same sink.id. For example if you have a gRPC sink with sink.id 15 then we need to set the sink.id as 15 in the source to receives responses. Sinks and sources have 1:1 mapping Origin: siddhi-io-grpc:1.0.5 Syntax @source(type=\"grpc-call-response\", sink.id=\" INT \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic sink.id a unique ID that should be set for each grpc-call source. There is a 1:1 mapping between grpc-call sinks and grpc-call-response sources. Each sink has one particular source listening to the responses to requests published from that sink. So the same sink.id should be given when writing the sink also. INT No No Examples EXAMPLE 1 @source(type='grpc-call-response', sink.id= '1') define stream BarStream (message String);@sink(type='grpc-call', publisher.url = 'grpc://194.23.98.100:8080/EventService/process', sink.id= '1', @map(type='json')) define stream FooStream (message String); Here we are listening to responses for requests sent from the sink with sink.id 1 will be received here. The results will be injected into BarStream","title":"grpc-call-response (Source)"},{"location":"docs/api/latest/#grpc-service-source","text":"This extension implements a grpc server for receiving and responding to requests. During initialization time a grpc server is started on the user specified port exposing the required service as given in the url. This source also has a default mode and a user defined grpc service mode. By default this uses EventService. Please find the proto definition here In the default mode this will use the EventService process method. If we want to use our custom gRPC services, we have to pack auto-generated gRPC service classes and protobuf classes into a jar file and add it into the project classpath (or to the jars folder in the siddhi-tooling folder if we use it with siddhi-tooling ). Please find the custom protobuf definition that uses in examples here . This accepts grpc message class Event as defined in the EventService proto. This uses GrpcServiceResponse sink to send reponses back in the same Event message format. Origin: siddhi-io-grpc:1.0.5 Syntax @source(type=\"grpc-service\", receiver.url=\" STRING \", max.inbound.message.size=\" INT \", max.inbound.metadata.size=\" INT \", service.timeout=\" INT \", server.shutdown.waiting.time=\" LONG \", truststore.file=\" STRING \", truststore.password=\" STRING \", truststore.algorithm=\" STRING \", tls.store.type=\" STRING \", keystore.file=\" STRING \", keystore.password=\" STRING \", keystore.algorithm=\" STRING \", enable.ssl=\" BOOL \", threadpool.size=\" INT \", threadpool.buffer.size=\" INT \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic receiver.url The url which can be used by a client to access the grpc server in this extension. This url should consist the host hostPort, port, fully qualified service name, method name in the following format. grpc://0.0.0.0:9763/ serviceName / methodName For example: grpc://0.0.0.0:9763/org.wso2.grpc.EventService/consume STRING No No max.inbound.message.size Sets the maximum message size in bytes allowed to be received on the server. 4194304 INT Yes No max.inbound.metadata.size Sets the maximum size of metadata in bytes allowed to be received. 8192 INT Yes No service.timeout The period of time in milliseconds to wait for siddhi to respond to a request received. After this time period of receiving a request it will be closed with an error message. 10000 INT Yes No server.shutdown.waiting.time The time in seconds to wait for the server to shutdown, giving up if the timeout is reached. 5 LONG Yes No truststore.file the file path of truststore. If this is provided then server authentication is enabled - STRING Yes No truststore.password the password of truststore. If this is provided then the integrity of the keystore is checked - STRING Yes No truststore.algorithm the encryption algorithm to be used for server authentication - STRING Yes No tls.store.type TLS store type - STRING Yes No keystore.file the file path of keystore. If this is provided then client authentication is enabled - STRING Yes No keystore.password the password of keystore - STRING Yes No keystore.algorithm the encryption algorithm to be used for client authentication - STRING Yes No enable.ssl to enable ssl. If set to true and truststore.file is not given then it will be set to default carbon jks by default FALSE BOOL Yes No threadpool.size Sets the maximum size of threadpool dedicated to serve requests at the gRPC server 100 INT Yes No threadpool.buffer.size Sets the maximum size of threadpool buffer server 100 INT Yes No System Parameters Name Description Default Value Possible Parameters keyStoreFile This is the key store file with the path src/main/resources/security/wso2carbon.jks valid path for a key store file keyStorePassword This is the password used with key store file wso2carbon valid password for the key store file keyStoreAlgorithm The encryption algorithm to be used for client authentication SunX509 - trustStoreFile This is the trust store file with the path src/main/resources/security/client-truststore.jks - trustStorePassword This is the password used with trust store file wso2carbon valid password for the trust store file trustStoreAlgorithm the encryption algorithm to be used for server authentication SunX509 - Examples EXAMPLE 1 @source(type='grpc-service', receiver.url='grpc://localhost:8888/org.wso2.grpc.EventService/process', source.id='1', @map(type='json', @attributes(messageId='trp:messageId', message='message'))) define stream FooStream (messageId String, message String); Here a grpc server will be started at port 8888. The process method of EventService will be exposed for clients. source.id is set as 1. So a grpc-service-response sink with source.id = 1 will send responses back for requests received to this source. Note that it is required to specify the transport property messageId since we need to correlate the request message with the response. EXAMPLE 2 @sink(type='grpc-service-response', source.id='1', @map(type='json')) define stream BarStream (messageId String, message String); @source(type='grpc-service', receiver.url='grpc://134.23.43.35:8080/org.wso2.grpc.EventService/process', source.id='1', @map(type='json', @attributes(messageId='trp:messageId', message='message'))) define stream FooStream (messageId String, message String); from FooStream select * insert into BarStream; The grpc requests are received through the grpc-service sink. Each received event is sent back through grpc-service-source. This is just a passthrough through Siddhi as we are selecting everything from FooStream and inserting into BarStream. EXAMPLE 3 @source(type='grpc-service', source.id='1' receiver.url='grpc://locanhost:8888/org.wso2.grpc.EventService/consume', @map(type='json', @attributes(name='trp:name', age='trp:age', message='message'))) define stream BarStream (message String, name String, age int); Here we are getting headers sent with the request as transport properties and injecting them into the stream. With each request a header will be sent in MetaData in the following format: 'Name:John', 'Age:23' EXAMPLE 4 @sink(type='grpc-service-response', source.id='1', message.id='{{messageId}}', @map(type='protobuf', @payload(stringValue='a',intValue='b',longValue='c',booleanValue='d',floatValue = 'e', doubleValue ='f'))) define stream BarStream (a string,messageId string, b int,c long,d bool,e float,f double); @source(type='grpc-service', receiver.url='grpc://134.23.43.35:8888/org.wso2.grpc.test.MyService/process', source.id='1', @map(type='protobuf', @attributes(messageId='trp:message.id', a = 'stringValue', b = 'intValue', c = 'longValue',d = 'booleanValue', e = 'floatValue', f ='doubleValue'))) define stream FooStream (a string,messageId string, b int,c long,d bool,e float,f double); from FooStream select * insert into BarStream; Here a grpc server will be started at port 8888. The process method of the MyService will be exposed to the clients. 'source.id' is set as 1. So a grpc-service-response sink with source.id = 1 will send responses back for requests received to this source. Note that it is required to specify the transport property messageId since we need to correlate the request message with the response and also we should map stream attributes with correct protobuf message attributes even they define using the same name as protobuf message attributes.","title":"grpc-service (Source)"},{"location":"docs/api/latest/#http-source","text":"HTTP source receives POST requests via HTTP and HTTPS protocols in format such as text , XML and JSON . It also supports basic authentication to ensure events are received from authorized users/systems. The request headers and properties can be accessed via transport properties in the format trp: header . Origin: siddhi-io-http:2.2.0 Syntax @source(type=\"http\", receiver.url=\" STRING \", basic.auth.enabled=\" STRING \", worker.count=\" INT \", socket.idle.timeout=\" INT \", ssl.verify.client=\" STRING \", ssl.protocol=\" STRING \", tls.store.type=\" STRING \", ssl.configurations=\" STRING \", request.size.validation.configurations=\" STRING \", header.validation.configurations=\" STRING \", server.bootstrap.configurations=\" STRING \", trace.log.enabled=\" BOOL \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic receiver.url The URL on which events should be received. To enable SSL use https protocol in the url. http://0.0.0.0:9763/ appNAme / streamName STRING Yes No basic.auth.enabled This only works in VM, Docker and Kubernetes. Where when enabled it authenticates each request using the Authorization:'Basic encodeBase64(username:Password)' header. false STRING Yes No worker.count The number of active worker threads to serve the incoming events. By default the value is set to 1 to ensure events are processed in the same order they arrived. By increasing this value, higher performance can be achieved in the expense of loosing event ordering. 1 INT Yes No socket.idle.timeout Idle timeout for HTTP connection in millis. 120000 INT Yes No ssl.verify.client The type of client certificate verification. Supported values are require , optional . - STRING Yes No ssl.protocol SSL/TLS protocol. TLS STRING Yes No tls.store.type TLS store type. JKS STRING Yes No ssl.configurations SSL/TSL configurations in format \"' key : value ',' key : value '\" . Some supported parameters: - SSL/TLS protocols: 'sslEnabledProtocols:TLSv1.1,TLSv1.2' - List of ciphers: 'ciphers:TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256' - Enable session creation: 'client.enable.session.creation:true' - Supported server names: 'server.suported.server.names:server' - Add HTTP SNIMatcher: 'server.supported.snimatchers:SNIMatcher' - STRING Yes No request.size.validation.configurations Configurations to validate the HTTP request size. Expected format \"' key : value ',' key : value '\" . Some supported configurations : - Enable request size validation: 'request.size.validation:true' If request size is validated - Maximum request size: 'request.size.validation.maximum.value:2048' - Response status code when request size validation fails: 'request.size.validation.reject.status.code:401' - Response message when request size validation fails: 'request.size.validation.reject.message:Message is bigger than the valid size' - Response Content-Type when request size validation fails: 'request.size.validation.reject.message.content.type:plain/text' - STRING Yes No header.validation.configurations Configurations to validate HTTP headers. Expected format \"' key : value ',' key : value '\" . Some supported configurations : - Enable header size validation: 'header.size.validation:true' If header size is validated - Maximum length of initial line: 'header.validation.maximum.request.line:4096' - Maximum length of all headers: 'header.validation.maximum.size:8192' - Maximum length of the content or each chunk: 'header.validation.maximum.chunk.size:8192' - Response status code when header validation fails: 'header.validation.reject.status.code:401' - Response message when header validation fails: 'header.validation.reject.message:Message header is bigger than the valid size' - Response Content-Type when header validation fails: 'header.validation.reject.message.content.type:plain/text' - STRING Yes No server.bootstrap.configurations Server bootstrap configurations in format \"' key : value ',' key : value '\" . Some supported configurations : - Server connect timeout in millis: 'server.bootstrap.connect.timeout:15000' - Server socket timeout in seconds: 'server.bootstrap.socket.timeout:15' - Enable TCP no delay: 'server.bootstrap.nodelay:true' - Enable server keep alive: 'server.bootstrap.keepalive:true' - Send buffer size: 'server.bootstrap.sendbuffersize:1048576' - Receive buffer size: 'server.bootstrap.recievebuffersize:1048576' - Number of connections queued: 'server.bootstrap.socket.backlog:100' - STRING Yes No trace.log.enabled Enable trace log for traffic monitoring. false BOOL Yes No System Parameters Name Description Default Value Possible Parameters serverBootstrapBossGroupSize Number of boss threads to accept incoming connections. Number of available processors Any positive integer serverBootstrapWorkerGroupSize Number of worker threads to accept the connections from boss threads and perform non-blocking read and write from one or more channels. (Number of available processors) * 2 Any positive integer serverBootstrapClientGroupSize Number of client threads to perform non-blocking read and write to one or more channels. (Number of available processors) * 2 Any positive integer defaultHost The default host of the transport. 0.0.0.0 Any valid host defaultScheme The default protocol. http http https defaultHttpPort The default HTTP port when default scheme is http . 8280 Any valid port defaultHttpsPort The default HTTPS port when default scheme is https . 8243 Any valid port keyStoreLocation The default keystore file path. ${carbon.home}/resources/security/wso2carbon.jks Path to .jks file keyStorePassword The default keystore password. wso2carbon Keystore password as string Examples EXAMPLE 1 @app.name('StockProcessor') @source(type='http', @map(type = 'json')) define stream StockStream (symbol string, price float, volume long); Above HTTP source listeners on url http://0.0.0.0:9763/StockProcessor/StockStream for JSON messages on the format: { \"event\": { \"symbol\": \"FB\", \"price\": 24.5, \"volume\": 5000 } } It maps the incoming messages and sends them to StockStream for processing. EXAMPLE 2 @source(type='http', receiver.url='http://localhost:5005/stocks', @map(type = 'xml')) define stream StockStream (symbol string, price float, volume long); Above HTTP source listeners on url http://localhost:5005/stocks for JSON messages on the format: events event symbol Fb /symbol price 55.6 /price volume 100 /volume /event /events It maps the incoming messages and sends them to StockStream for processing.","title":"http (Source)"},{"location":"docs/api/latest/#http-call-response-source","text":"The http-call-response source receives the responses for the calls made by its corresponding http-call sink, and maps them from formats such as text , XML and JSON . To handle messages with different http status codes having different formats, multiple http-call-response sources are allowed to associate with a single http-call sink. It allows accessing the attributes of the event that initiated the call, and the response headers and properties via transport properties in the format trp: attribute name and trp: header/property respectively. Origin: siddhi-io-http:2.2.0 Syntax @source(type=\"http-call-response\", sink.id=\" STRING \", http.status.code=\" STRING \", allow.streaming.responses=\" BOOL \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic sink.id Identifier to correlate the http-call-response source with its corresponding http-call sink that published the messages. STRING No No http.status.code The matching http responses status code regex, that is used to filter the the messages which will be processed by the source.Eg: http.status.code = '200' , http.status.code = '4\\d+' 200 STRING Yes No allow.streaming.responses Enable consuming responses on a streaming manner. false BOOL Yes No Examples EXAMPLE 1 @sink(type='http-call', method='POST', publisher.url='http://localhost:8005/registry/employee', sink.id='employee-info', @map(type='json')) define stream EmployeeRequestStream (name string, id int); @source(type='http-call-response', sink.id='employee-info', http.status.code='2\\\\d+', @map(type='json', @attributes(name='trp:name', id='trp:id', location='$.town', age='$.age'))) define stream EmployeeResponseStream(name string, id int, location string, age int); @source(type='http-call-response', sink.id='employee-info', http.status.code='4\\\\d+', @map(type='text', regex.A='((.|\\n)*)', @attributes(error='A[1]'))) define stream EmployeeErrorStream(error string); When events arrive in EmployeeRequestStream , http-call sink makes calls to endpoint on url http://localhost:8005/registry/employee with POST method and Content-Type application/json . If the arriving event has attributes name : John and id : 1423 it will send a message with default JSON mapping as follows: { \"event\": { \"name\": \"John\", \"id\": 1423 } } When the endpoint responds with status code in the range of 200 the message will be received by the http-call-response source associated with the EmployeeResponseStream stream, because it is correlated with the sink by the same sink.id employee-info and as that expects messages with http.status.code in regex format 2\\d+ . If the response message is in the format { \"town\": \"NY\", \"age\": 24 } the source maps the location and age attributes by executing JSON path on the message and maps the name and id attributes by extracting them from the request event via as transport properties. If the response status code is in the range of 400 then the message will be received by the http-call-response source associated with the EmployeeErrorStream stream, because it is correlated with the sink by the same sink.id employee-info and it expects messages with http.status.code in regex format 4\\d+ , and maps the error response to the error attribute of the event.","title":"http-call-response (Source)"},{"location":"docs/api/latest/#http-request-source","text":"Deprecated (Use http-service source instead). The http-request source receives POST requests via HTTP and HTTPS protocols in format such as text , XML and JSON and sends responses via its corresponding http-response sink correlated through a unique source.id . For request and response correlation, it generates a messageId upon each incoming request and expose it via transport properties in the format trp:messageId to correlate them with the responses at the http-response sink. The request headers and properties can be accessed via transport properties in the format trp: header . It also supports basic authentication to ensure events are received from authorized users/systems. Origin: siddhi-io-http:2.2.0 Syntax @source(type=\"http-request\", receiver.url=\" STRING \", source.id=\" STRING \", connection.timeout=\" INT \", basic.auth.enabled=\" STRING \", worker.count=\" INT \", socket.idle.timeout=\" INT \", ssl.verify.client=\" STRING \", ssl.protocol=\" STRING \", tls.store.type=\" STRING \", ssl.configurations=\" STRING \", request.size.validation.configurations=\" STRING \", header.validation.configurations=\" STRING \", server.bootstrap.configurations=\" STRING \", trace.log.enabled=\" BOOL \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic receiver.url The URL on which events should be received. To enable SSL use https protocol in the url. http://0.0.0.0:9763/ appNAme / streamName STRING Yes No source.id Identifier to correlate the http-request source to its corresponding http-response sinks to send responses. STRING No No connection.timeout Connection timeout in millis. The system will send a timeout, if a corresponding response is not sent by an associated http-response sink within the given time. 120000 INT Yes No basic.auth.enabled This only works in VM, Docker and Kubernetes. Where when enabled it authenticates each request using the Authorization:'Basic encodeBase64(username:Password)' header. false STRING Yes No worker.count The number of active worker threads to serve the incoming events. By default the value is set to 1 to ensure events are processed in the same order they arrived. By increasing this value, higher performance can be achieved in the expense of loosing event ordering. 1 INT Yes No socket.idle.timeout Idle timeout for HTTP connection in millis. 120000 INT Yes No ssl.verify.client The type of client certificate verification. Supported values are require , optional . - STRING Yes No ssl.protocol SSL/TLS protocol. TLS STRING Yes No tls.store.type TLS store type. JKS STRING Yes No ssl.configurations SSL/TSL configurations in format \"' key : value ',' key : value '\" . Some supported parameters: - SSL/TLS protocols: 'sslEnabledProtocols:TLSv1.1,TLSv1.2' - List of ciphers: 'ciphers:TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256' - Enable session creation: 'client.enable.session.creation:true' - Supported server names: 'server.suported.server.names:server' - Add HTTP SNIMatcher: 'server.supported.snimatchers:SNIMatcher' - STRING Yes No request.size.validation.configurations Configurations to validate the HTTP request size. Expected format \"' key : value ',' key : value '\" . Some supported configurations : - Enable request size validation: 'request.size.validation:true' If request size is validated - Maximum request size: 'request.size.validation.maximum.value:2048' - Response status code when request size validation fails: 'request.size.validation.reject.status.code:401' - Response message when request size validation fails: 'request.size.validation.reject.message:Message is bigger than the valid size' - Response Content-Type when request size validation fails: 'request.size.validation.reject.message.content.type:plain/text' - STRING Yes No header.validation.configurations Configurations to validate HTTP headers. Expected format \"' key : value ',' key : value '\" . Some supported configurations : - Enable header size validation: 'header.size.validation:true' If header size is validated - Maximum length of initial line: 'header.validation.maximum.request.line:4096' - Maximum length of all headers: 'header.validation.maximum.size:8192' - Maximum length of the content or each chunk: 'header.validation.maximum.chunk.size:8192' - Response status code when header validation fails: 'header.validation.reject.status.code:401' - Response message when header validation fails: 'header.validation.reject.message:Message header is bigger than the valid size' - Response Content-Type when header validation fails: 'header.validation.reject.message.content.type:plain/text' - STRING Yes No server.bootstrap.configurations Server bootstrap configurations in format \"' key : value ',' key : value '\" . Some supported configurations : - Server connect timeout in millis: 'server.bootstrap.connect.timeout:15000' - Server socket timeout in seconds: 'server.bootstrap.socket.timeout:15' - Enable TCP no delay: 'server.bootstrap.nodelay:true' - Enable server keep alive: 'server.bootstrap.keepalive:true' - Send buffer size: 'server.bootstrap.sendbuffersize:1048576' - Receive buffer size: 'server.bootstrap.recievebuffersize:1048576' - Number of connections queued: 'server.bootstrap.socket.backlog:100' - STRING Yes No trace.log.enabled Enable trace log for traffic monitoring. false BOOL Yes No System Parameters Name Description Default Value Possible Parameters serverBootstrapBossGroupSize Number of boss threads to accept incoming connections. Number of available processors Any positive integer serverBootstrapWorkerGroupSize Number of worker threads to accept the connections from boss threads and perform non-blocking read and write from one or more channels. (Number of available processors) * 2 Any positive integer serverBootstrapClientGroupSize Number of client threads to perform non-blocking read and write to one or more channels. (Number of available processors) * 2 Any positive integer defaultHost The default host of the transport. 0.0.0.0 Any valid host defaultScheme The default protocol. http http https defaultHttpPort The default HTTP port when default scheme is http . 8280 Any valid port defaultHttpsPort The default HTTPS port when default scheme is https . 8243 Any valid port keyStoreLocation The default keystore file path. ${carbon.home}/resources/security/wso2carbon.jks Path to .jks file keyStorePassword The default keystore password. wso2carbon Keystore password as string Examples EXAMPLE 1 @source(type='http-request', receiver.url='http://localhost:5005/add', source.id='adder', @map(type='json, @attributes(messageId='trp:messageId', value1='$.event.value1', value2='$.event.value2'))) define stream AddStream (messageId string, value1 long, value2 long); @sink(type='http-response', source.id='adder', message.id='{{messageId}}', @map(type = 'json')) define stream ResultStream (messageId string, results long); @info(name = 'query1') from AddStream select messageId, value1 + value2 as results insert into ResultStream; Above sample listens events on http://localhost:5005/stocks url for JSON messages on the format: { \"event\": { \"value1\": 3, \"value2\": 4 } } Map the vents into AddStream, process the events through query query1 , and sends the results produced on ResultStream via http-response sink on the message format: { \"event\": { \"results\": 7 } }","title":"http-request (Source)"},{"location":"docs/api/latest/#http-response-source","text":"Deprecated (Use http-call-response source instead). The http-response source receives the responses for the calls made by its corresponding http-request sink, and maps them from formats such as text , XML and JSON . To handle messages with different http status codes having different formats, multiple http-response sources are allowed to associate with a single http-request sink. It allows accessing the attributes of the event that initiated the call, and the response headers and properties via transport properties in the format trp: attribute name and trp: header/property respectively. Origin: siddhi-io-http:2.2.0 Syntax @source(type=\"http-response\", sink.id=\" STRING \", http.status.code=\" STRING \", allow.streaming.responses=\" BOOL \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic sink.id Identifier to correlate the http-response source with its corresponding http-request sink that published the messages. STRING No No http.status.code The matching http responses status code regex, that is used to filter the the messages which will be processed by the source.Eg: http.status.code = '200' , http.status.code = '4\\d+' 200 STRING Yes No allow.streaming.responses Enable consuming responses on a streaming manner. false BOOL Yes No Examples EXAMPLE 1 @sink(type='http-request', method='POST', publisher.url='http://localhost:8005/registry/employee', sink.id='employee-info', @map(type='json')) define stream EmployeeRequestStream (name string, id int); @source(type='http-response', sink.id='employee-info', http.status.code='2\\\\d+', @map(type='json', @attributes(name='trp:name', id='trp:id', location='$.town', age='$.age'))) define stream EmployeeResponseStream(name string, id int, location string, age int); @source(type='http-response', sink.id='employee-info', http.status.code='4\\\\d+', @map(type='text', regex.A='((.|\\n)*)', @attributes(error='A[1]'))) define stream EmployeeErrorStream(error string); When events arrive in EmployeeRequestStream , http-request sink makes calls to endpoint on url http://localhost:8005/registry/employee with POST method and Content-Type application/json . If the arriving event has attributes name : John and id : 1423 it will send a message with default JSON mapping as follows: { \"event\": { \"name\": \"John\", \"id\": 1423 } } When the endpoint responds with status code in the range of 200 the message will be received by the http-response source associated with the EmployeeResponseStream stream, because it is correlated with the sink by the same sink.id employee-info and as that expects messages with http.status.code in regex format 2\\d+ . If the response message is in the format { \"town\": \"NY\", \"age\": 24 } the source maps the location and age attributes by executing JSON path on the message and maps the name and id attributes by extracting them from the request event via as transport properties. If the response status code is in the range of 400 then the message will be received by the http-response source associated with the EmployeeErrorStream stream, because it is correlated with the sink by the same sink.id employee-info and it expects messages with http.status.code in regex format 4\\d+ , and maps the error response to the error attribute of the event.","title":"http-response (Source)"},{"location":"docs/api/latest/#http-service-source","text":"The http-service source receives POST requests via HTTP and HTTPS protocols in format such as text , XML and JSON and sends responses via its corresponding http-service-response sink correlated through a unique source.id . For request and response correlation, it generates a messageId upon each incoming request and expose it via transport properties in the format trp:messageId to correlate them with the responses at the http-service-response sink. The request headers and properties can be accessed via transport properties in the format trp: header . It also supports basic authentication to ensure events are received from authorized users/systems. Origin: siddhi-io-http:2.2.0 Syntax @source(type=\"http-service\", receiver.url=\" STRING \", source.id=\" STRING \", connection.timeout=\" INT \", basic.auth.enabled=\" STRING \", worker.count=\" INT \", socket.idle.timeout=\" INT \", ssl.verify.client=\" STRING \", ssl.protocol=\" STRING \", tls.store.type=\" STRING \", ssl.configurations=\" STRING \", request.size.validation.configurations=\" STRING \", header.validation.configurations=\" STRING \", server.bootstrap.configurations=\" STRING \", trace.log.enabled=\" BOOL \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic receiver.url The URL on which events should be received. To enable SSL use https protocol in the url. http://0.0.0.0:9763/ appNAme / streamName STRING Yes No source.id Identifier to correlate the http-service source to its corresponding http-service-response sinks to send responses. STRING No No connection.timeout Connection timeout in millis. The system will send a timeout, if a corresponding response is not sent by an associated http-service-response sink within the given time. 120000 INT Yes No basic.auth.enabled This only works in VM, Docker and Kubernetes. Where when enabled it authenticates each request using the Authorization:'Basic encodeBase64(username:Password)' header. false STRING Yes No worker.count The number of active worker threads to serve the incoming events. By default the value is set to 1 to ensure events are processed in the same order they arrived. By increasing this value, higher performance can be achieved in the expense of loosing event ordering. 1 INT Yes No socket.idle.timeout Idle timeout for HTTP connection in millis. 120000 INT Yes No ssl.verify.client The type of client certificate verification. Supported values are require , optional . - STRING Yes No ssl.protocol SSL/TLS protocol. TLS STRING Yes No tls.store.type TLS store type. JKS STRING Yes No ssl.configurations SSL/TSL configurations in format \"' key : value ',' key : value '\" . Some supported parameters: - SSL/TLS protocols: 'sslEnabledProtocols:TLSv1.1,TLSv1.2' - List of ciphers: 'ciphers:TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256' - Enable session creation: 'client.enable.session.creation:true' - Supported server names: 'server.suported.server.names:server' - Add HTTP SNIMatcher: 'server.supported.snimatchers:SNIMatcher' - STRING Yes No request.size.validation.configurations Configurations to validate the HTTP request size. Expected format \"' key : value ',' key : value '\" . Some supported configurations : - Enable request size validation: 'request.size.validation:true' If request size is validated - Maximum request size: 'request.size.validation.maximum.value:2048' - Response status code when request size validation fails: 'request.size.validation.reject.status.code:401' - Response message when request size validation fails: 'request.size.validation.reject.message:Message is bigger than the valid size' - Response Content-Type when request size validation fails: 'request.size.validation.reject.message.content.type:plain/text' - STRING Yes No header.validation.configurations Configurations to validate HTTP headers. Expected format \"' key : value ',' key : value '\" . Some supported configurations : - Enable header size validation: 'header.size.validation:true' If header size is validated - Maximum length of initial line: 'header.validation.maximum.request.line:4096' - Maximum length of all headers: 'header.validation.maximum.size:8192' - Maximum length of the content or each chunk: 'header.validation.maximum.chunk.size:8192' - Response status code when header validation fails: 'header.validation.reject.status.code:401' - Response message when header validation fails: 'header.validation.reject.message:Message header is bigger than the valid size' - Response Content-Type when header validation fails: 'header.validation.reject.message.content.type:plain/text' - STRING Yes No server.bootstrap.configurations Server bootstrap configurations in format \"' key : value ',' key : value '\" . Some supported configurations : - Server connect timeout in millis: 'server.bootstrap.connect.timeout:15000' - Server socket timeout in seconds: 'server.bootstrap.socket.timeout:15' - Enable TCP no delay: 'server.bootstrap.nodelay:true' - Enable server keep alive: 'server.bootstrap.keepalive:true' - Send buffer size: 'server.bootstrap.sendbuffersize:1048576' - Receive buffer size: 'server.bootstrap.recievebuffersize:1048576' - Number of connections queued: 'server.bootstrap.socket.backlog:100' - STRING Yes No trace.log.enabled Enable trace log for traffic monitoring. false BOOL Yes No System Parameters Name Description Default Value Possible Parameters serverBootstrapBossGroupSize Number of boss threads to accept incoming connections. Number of available processors Any positive integer serverBootstrapWorkerGroupSize Number of worker threads to accept the connections from boss threads and perform non-blocking read and write from one or more channels. (Number of available processors) * 2 Any positive integer serverBootstrapClientGroupSize Number of client threads to perform non-blocking read and write to one or more channels. (Number of available processors) * 2 Any positive integer defaultHost The default host of the transport. 0.0.0.0 Any valid host defaultScheme The default protocol. http http https defaultHttpPort The default HTTP port when default scheme is http . 8280 Any valid port defaultHttpsPort The default HTTPS port when default scheme is https . 8243 Any valid port keyStoreLocation The default keystore file path. ${carbon.home}/resources/security/wso2carbon.jks Path to .jks file keyStorePassword The default keystore password. wso2carbon Keystore password as string Examples EXAMPLE 1 @source(type='http-service', receiver.url='http://localhost:5005/add', source.id='adder', @map(type='json, @attributes(messageId='trp:messageId', value1='$.event.value1', value2='$.event.value2'))) define stream AddStream (messageId string, value1 long, value2 long); @sink(type='http-service-response', source.id='adder', message.id='{{messageId}}', @map(type = 'json')) define stream ResultStream (messageId string, results long); @info(name = 'query1') from AddStream select messageId, value1 + value2 as results insert into ResultStream; Above sample listens events on http://localhost:5005/stocks url for JSON messages on the format: { \"event\": { \"value1\": 3, \"value2\": 4 } } Map the vents into AddStream, process the events through query query1 , and sends the results produced on ResultStream via http-service-response sink on the message format: { \"event\": { \"results\": 7 } }","title":"http-service (Source)"},{"location":"docs/api/latest/#inmemory-source","text":"In-memory source subscribes to a topic to consume events which are published on the same topic by In-memory sinks. This provides a way to connect multiple Siddhi Apps deployed under the same Siddhi Manager (JVM). Here both the publisher and subscriber should have the same event schema (stream definition) for successful data transfer. Origin: siddhi-core:5.1.8 Syntax @source(type=\"inMemory\", topic=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic topic Subscribes to the events sent on the given topic. STRING No No Examples EXAMPLE 1 @source(type='inMemory', topic='Stocks', @map(type='passThrough')) define stream StocksStream (symbol string, price float, volume long); Here the StocksStream uses inMemory source to consume events published on the topic Stocks by the inMemory sinks deployed in the same JVM.","title":"inMemory (Source)"},{"location":"docs/api/latest/#jms-source","text":"JMS Source allows users to subscribe to a JMS broker and receive JMS messages. It has the ability to receive Map messages and Text messages. Origin: siddhi-io-jms:2.0.3 Syntax @source(type=\"jms\", destination=\" STRING \", connection.factory.jndi.name=\" STRING \", factory.initial=\" STRING \", provider.url=\" STRING \", connection.factory.type=\" STRING \", worker.count=\" INT \", connection.username=\" STRING \", connection.password=\" STRING \", retry.interval=\" INT \", retry.count=\" INT \", use.receiver=\" BOOL \", subscription.durable=\" BOOL \", connection.factory.nature=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic destination Queue/Topic name which JMS Source should subscribe to STRING No No connection.factory.jndi.name JMS Connection Factory JNDI name. This value will be used for the JNDI lookup to find the JMS Connection Factory. QueueConnectionFactory STRING Yes No factory.initial Naming factory initial value STRING No No provider.url Java naming provider URL. Property for specifying configuration information for the service provider to use. The value of the property should contain a URL string (e.g. \"ldap://somehost:389\") STRING No No connection.factory.type Type of the connection connection factory. This can be either queue or topic. queue STRING Yes No worker.count Number of worker threads listening on the given queue/topic. 1 INT Yes No connection.username username for the broker. None STRING Yes No connection.password Password for the broker None STRING Yes No retry.interval Interval between each retry attempt in case of connection failure in milliseconds. 10000 INT Yes No retry.count Number of maximum reties that will be attempted in case of connection failure with broker. 5 INT Yes No use.receiver Implementation to be used when consuming JMS messages. By default transport will use MessageListener and tweaking this property will make make use of MessageReceiver false BOOL Yes No subscription.durable Property to enable durable subscription. false BOOL Yes No connection.factory.nature Connection factory nature for the broker. default STRING Yes No Examples EXAMPLE 1 @source(type='jms', @map(type='json'), factory.initial='org.apache.activemq.jndi.ActiveMQInitialContextFactory', provider.url='tcp://localhost:61616',destination='DAS_JMS_TEST', connection.factory.type='topic',connection.factory.jndi.name='TopicConnectionFactory') define stream inputStream (name string, age int, country string); This example shows how to connect to an ActiveMQ topic and receive messages. EXAMPLE 2 @source(type='jms', @map(type='json'), factory.initial='org.apache.activemq.jndi.ActiveMQInitialContextFactory', provider.url='tcp://localhost:61616',destination='DAS_JMS_TEST' ) define stream inputStream (name string, age int, country string); This example shows how to connect to an ActiveMQ queue and receive messages. Note that we are not providing properties like connection factory type","title":"jms (Source)"},{"location":"docs/api/latest/#kafka-source","text":"A Kafka source receives events to be processed by WSO2 SP from a topic with a partition for a Kafka cluster. The events received can be in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. Origin: siddhi-io-kafka:5.0.5 Syntax @source(type=\"kafka\", bootstrap.servers=\" STRING \", topic.list=\" STRING \", group.id=\" STRING \", threading.option=\" STRING \", partition.no.list=\" STRING \", seq.enabled=\" BOOL \", is.binary.message=\" BOOL \", topic.offsets.map=\" STRING \", enable.auto.commit=\" BOOL \", optional.configuration=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This specifies the list of Kafka servers to which the Kafka source must listen. This list can be provided as a set of comma-separated values. e.g., localhost:9092,localhost:9093 STRING No No topic.list This specifies the list of topics to which the source must listen. This list can be provided as a set of comma-separated values. e.g., topic_one,topic_two STRING No No group.id This is an ID to identify the Kafka source group. The group ID ensures that sources with the same topic and partition that are in the same group do not receive the same event. STRING No No threading.option This specifies whether the Kafka source is to be run on a single thread, or in multiple threads based on a condition. Possible values are as follows: single.thread : To run the Kafka source on a single thread. topic.wise : To use a separate thread per topic. partition.wise : To use a separate thread per partition. STRING No No partition.no.list The partition number list for the given topic. This is provided as a list of comma-separated values. e.g., 0,1,2, . 0 STRING Yes No seq.enabled If this parameter is set to true , the sequence of the events received via the source is taken into account. Therefore, each event should contain a sequence number as an attribute value to indicate the sequence. false BOOL Yes No is.binary.message In order to receive binary events via the Kafka source,it is required to setthis parameter to 'True'. false BOOL Yes No topic.offsets.map This parameter specifies reading offsets for each topic and partition. The value for this parameter is specified in the following format: topic = offset , topic = offset , When an offset is defined for a topic, the Kafka source skips reading the message with the number specified as the offset as well as all the messages sent previous to that message. If the offset is not defined for a specific topic it reads messages from the beginning. e.g., stocks=100,trades=50 reads from the 101th message of the stocks topic, and from the 51 st message of the trades topic. null STRING Yes No enable.auto.commit This parameter specifies whether to commit offsets automatically. By default, as the Siddhi Kafka source reads messages from Kafka, it will periodically(Default value is set to 1000ms. You can configure it with auto.commit.interval.ms property as an optional.configuration ) commit its current offset (defined as the offset of the next message to be read) for the partitions it is reading from back to Kafka. To guarantee at-least-once processing, we recommend you to enable Siddhi Periodic State Persistence when enable.auto.commit property is set to true . When you would like more control over exactly when offsets are committed, you can set enable.auto.commit to false and Siddhi will commit the offset once the records are successfully processed at the Source. When enable.auto.commit is set to false , manual committing would introduce a latency during consumption. true BOOL Yes No optional.configuration This parameter contains all the other possible configurations that the consumer is created with. e.g., ssl.keystore.type:JKS,batch.size:200 . null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic,kafka_topic2', group.id='test', threading.option='partition.wise', bootstrap.servers='localhost:9092', partition.no.list='0,1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This kafka source configuration listens to the kafka_topic and kafka_topic2 topics with 0 and 1 partitions. A thread is created for each topic and partition combination. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . EXAMPLE 2 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic', group.id='test', threading.option='single.thread', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka source configuration listens to the kafka_topic topic for the default partition because no partition.no.list is defined. Only one thread is created for the topic. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream .","title":"kafka (Source)"},{"location":"docs/api/latest/#kafkamultidc-source","text":"The Kafka Multi-Datacenter(DC) source receives records from the same topic in brokers deployed in two different kafka clusters. It filters out all the duplicate messages and ensuresthat the events are received in the correct order using sequential numbering. It receives events in formats such as TEXT , XML JSON and Binary`.The Kafka Source creates the default partition '0' for a given topic, if the topic has not yet been created in the Kafka cluster. Origin: siddhi-io-kafka:5.0.5 Syntax @source(type=\"kafkaMultiDC\", bootstrap.servers=\" STRING \", topic=\" STRING \", partition.no=\" INT \", is.binary.message=\" BOOL \", optional.configuration=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This contains the kafka server list which the kafka source listens to. This is given using comma-separated values. eg: 'localhost:9092,localhost:9093' STRING No No topic This is the topic that the source listens to. eg: 'topic_one' STRING No No partition.no This is the partition number of the given topic. 0 INT Yes No is.binary.message In order to receive the binary events via the Kafka Multi-DC source, the value of this parameter needs to be set to 'True'. false BOOL Yes No optional.configuration This contains all the other possible configurations with which the consumer can be created.eg: producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source(type='kafkaMultiDC', topic='kafka_topic', bootstrap.servers='host1:9092,host1:9093', partition.no='1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; The following query listens to 'kafka_topic' topic, deployed in the broker host1:9092 and host1:9093, with partition 1. A thread is created for each broker. The receiving xml events are mapped to a siddhi event and sent to the FooStream.","title":"kafkaMultiDC (Source)"},{"location":"docs/api/latest/#nats-source","text":"NATS Source allows users to subscribe to a NATS broker and receive messages. It has the ability to receive all the message types supported by NATS. Origin: siddhi-io-nats:2.0.8 Syntax @source(type=\"nats\", destination=\" STRING \", bootstrap.servers=\" STRING \", client.id=\" STRING \", cluster.id=\" STRING \", queue.group.name=\" STRING \", durable.name=\" STRING \", subscription.sequence=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic destination Subject name which NATS Source should subscribe to. STRING No No bootstrap.servers The NATS based url of the NATS server. nats://localhost:4222 STRING Yes No client.id The identifier of the client subscribing/connecting to the NATS broker. None STRING Yes No cluster.id The identifier of the NATS server/cluster. test-cluster STRING Yes No queue.group.name This can be used when there is a requirement to share the load of a NATS subject. Clients belongs to the same queue group share the subscription load. None STRING Yes No durable.name This can be used to subscribe to a subject from the last acknowledged message when a client or connection failure happens. The client can be uniquely identified using the tuple (client.id, durable.name). None STRING Yes No subscription.sequence This can be used to subscribe to a subject from a given number of message sequence. All the messages from the given point of sequence number will be passed to the client. If not provided then the either the persisted value or 0 will be used. None STRING Yes No Examples EXAMPLE 1 @source(type='nats', @map(type='text'), destination='SP_NATS_INPUT_TEST', bootstrap.servers='nats://localhost:4222',client.id='nats_client',server.id='test-cluster',queue.group.name = 'group_nats',durable.name = 'nats-durable',subscription.sequence = '100') define stream inputStream (name string, age int, country string); This example shows how to subscribe to a NATS subject with all supporting configurations.With the following configuration the source identified as 'nats-client' will subscribes to a subject named as 'SP_NATS_INPUT_TEST' which resides in a nats instance with a cluster id of 'test-cluster', running in localhost and listening to the port 4222 for client connection. This subscription will receive all the messages from 100 th in the subject. EXAMPLE 2 @source(type='nats', @map(type='text'), destination='SP_NATS_INPUT_TEST', ) define stream inputStream (name string, age int, country string); This example shows how to subscribe to a NATS subject with mandatory configurations.With the following configuration the source identified with an auto generated client id will subscribes to a subject named as 'SP_NATS_INTPUT_TEST' which resides in a nats instance with a cluster id of 'test-cluster', running in localhost and listening to the port 4222 for client connection. This will receive all available messages in the subject EXAMPLE 3 @source(type='nats', @map(type='json', @attributes(name='$.name', age='$.age', country='$.country', sequenceNum='trp:sequenceNumber')), destination='SIDDHI_NATS_SOURCE_TEST_DEST', client.id='nats_client', bootstrap.servers='nats://localhost:4222', cluster.id='test-cluster') define stream inputStream (name string, age int, country string, sequenceNum string); This example shows how to pass NATS Streaming sequence number to the event.","title":"nats (Source)"},{"location":"docs/api/latest/#prometheus-source","text":"This source consumes Prometheus metrics that are exported from a specified URL as Siddhi events by sending HTTP requests to the URL. Based on the source configuration, it analyzes metrics from the text response and sends them as Siddhi events through key-value mapping.The user can retrieve metrics of the 'including', 'counter', 'gauge', 'histogram', and 'summary' types. The source retrieves the metrics from a text response of the target. Therefore, it is you need to use 'string' as the attribute type for the attributes that correspond with the Prometheus metric labels. Further, the Prometheus metric value is passed through the event as 'value'. This requires you to include an attribute named 'value' in the stream definition. The supported types for the 'value' attribute are 'INT', 'LONG', 'FLOAT', and 'DOUBLE'. Origin: siddhi-io-prometheus:2.1.0 Syntax @source(type=\"prometheus\", target.url=\" STRING \", scrape.interval=\" INT \", scrape.timeout=\" INT \", scheme=\" STRING \", metric.name=\" STRING \", metric.type=\" STRING \", username=\" STRING \", password=\" STRING \", client.truststore.file=\" STRING \", client.truststore.password=\" STRING \", headers=\" STRING \", job=\" STRING \", instance=\" STRING \", grouping.key=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic target.url This property specifies the target URL to which the Prometheus metrics are exported in the 'TEXT' format. STRING No No scrape.interval This property specifies the time interval in seconds within which the source should send an HTTP request to the specified target URL. 60 INT Yes No scrape.timeout This property is the time duration in seconds for a scrape request to get timed-out if the server at the URL does not respond. 10 INT Yes No scheme This property specifies the scheme of the target URL. The supported schemes are 'HTTP' and 'HTTPS'. HTTP STRING Yes No metric.name This property specifies the name of the metrics that are to be fetched. The metric name must match the regex format, i.e., '[a-zA-Z_:][a-zA-Z0-9_:]* '. Stream name STRING Yes No metric.type This property specifies the type of the Prometheus metric that is required to be fetched. The supported metric types are 'counter', 'gauge',\" 'histogram', and 'summary'. STRING No No username This property specifies the username that needs to be added in the authorization header of the HTTP request if basic authentication is enabled at the target. It is required to specify both the username and the password to enable basic authentication. If you do not provide a value for one or both of these parameters, an error is logged in the console. STRING Yes No password This property specifies the password that needs to be added in the authorization header of the HTTP request if basic authentication is enabled at the target. It is required to specify both the username and the password to enable basic authentication. If you do not provide a value for one or both of these parameters, an error is logged in the console. STRING Yes No client.truststore.file The file path to the location of the truststore to which the client needs to send HTTPS requests via the 'HTTPS' protocol. STRING Yes No client.truststore.password The password for the client-truststore. This is required to send HTTPS requests. A custom password can be specified if required. STRING Yes No headers Headers that need to be included as HTTP request headers in the request. The format of the supported input is as follows, \"'header1:value1','header2:value2'\" STRING Yes No job This property defines the job name of the exported Prometheus metrics that needs to be fetched. STRING Yes No instance This property defines the instance of the exported Prometheus metrics that needs to be fetched. STRING Yes No grouping.key This parameter specifies the grouping key of the required metrics in key-value pairs. The grouping key is used if the metrics are exported by Prometheus 'pushGateway' in order to distinguish those metrics from already existing metrics. The expected format of the grouping key is as follows: \"'key1:value1','key2:value2'\" STRING Yes No System Parameters Name Description Default Value Possible Parameters scrapeInterval The default time interval in seconds for the Prometheus source to send HTTP requests to the target URL. 60 Any integer value scrapeTimeout The default time duration (in seconds) for an HTTP request to time-out if the server at the URL does not respond. 10 Any integer value scheme The scheme of the target for the Prometheus source to send HTTP requests. The supported schemes are 'HTTP' and 'HTTPS'. HTTP HTTP or HTTPS username The username that needs to be added in the authorization header of the HTTP request if basic authentication is enabled at the target. It is required to specify both the username and password to enable basic authentication. If you do not specify a value for one or both of these parameters, an error is logged in the console. Any string password The password that needs to be added in the authorization header of the HTTP request if basic authentication is enabled at the target. It is required to specify both the username and password to enable basic authentication. If you do not specify a value for one or both of these parameters, an error is logged in the console. Any string trustStoreFile The default file path to the location of truststore that the client needs to access in order to send HTTPS requests through 'HTTPS' protocol. ${carbon.home}/resources/security/client-truststore.jks Any valid path for the truststore file trustStorePassword The default password for the client-truststore that the client needs to access in order to send HTTPS requests through 'HTTPS' protocol. wso2carbon Any string headers The headers that need to be included as HTTP request headers in the scrape request. The format of the supported input is as follows, \"'header1:value1','header2:value2'\" Any valid http headers job The default job name of the exported Prometheus metrics that needs to be fetched. Any valid job name instance The default instance of the exported Prometheus metrics that needs to be fetched. Any valid instance name groupingKey The default grouping key of the required Prometheus metrics in key-value pairs. The grouping key is used if the metrics are exported by the Prometheus pushGateway in order to distinguish these metrics from already existing metrics. The expected format of the grouping key is as follows: \"'key1:value1','key2:value2'\" Any valid grouping key pairs Examples EXAMPLE 1 @source(type= 'prometheus', target.url= 'http://localhost:9080/metrics', metric.type= 'counter', metric.name= 'sweet_production_counter', @map(type= 'keyvalue')) define stream FooStream1(metric_name string, metric_type string, help string, subtype string, name string, quantity string, value double); In this example, the Prometheus source sends an HTTP request to the 'target.url' and analyzes the response. From the analyzed response, the source retrieves the Prometheus counter metrics with the 'sweet_production_counter' nameand converts the filtered metrics into Siddhi events using the key-value mapper. The generated maps have keys and values as follows: metric_name - sweet_production_counter metric_type - counter help - help_string_of_metric subtype - null name - value_of_label_name quantity - value_of_label_quantity value - value_of_metric EXAMPLE 2 @source(type= 'prometheus', target.url= 'http://localhost:9080/metrics', metric.type= 'summary', metric.name= 'sweet_production_summary', @map(type= 'keyvalue')) define stream FooStream2(metric_name string, metric_type string, help string, subtype string, name string, quantity string, quantile string, value double); In this example, the Prometheus source sends an HTTP request to the 'target.url' and analyzes the response. From the analysed response, the source retrieves the Prometheus summary metrics with the 'sweet_production_summary' nameand converts the filtered metrics into Siddhi events using the key-value mapper. The generated maps have keys and values as follows: metric_name - sweet_production_summary metric_type - summary help - help_string_of_metric subtype - 'sum'/'count'/'null' name - value_of_label_name quantity - value_of_label_quantity quantile - value of the quantile value - value_of_metric EXAMPLE 3 @source(type= 'prometheus', target.url= 'http://localhost:9080/metrics', metric.type= 'histogram', metric.name= 'sweet_production_histogram', @map(type= 'keyvalue')) define stream FooStream3(metric_name string, metric_type string, help string, subtype string, name string, quantity string, le string, value double); In this example, the prometheus source sends an HTTP request to the 'target.url' and analyzes the response. From the analyzed response, the source retrieves the Prometheus histogram metrics with the 'sweet_production_histogram' name and converts the filtered metrics into Siddhi events using the key-value mapper. The generated maps have keys and values as follows, metric_name - sweet_production_histogram metric_type - histogram help - help_string_of_metric subtype - 'sum'/'count'/'bucket' name - value_of_label_name quantity - value_of_label_quantity le - value of the bucket value - value_of_metric","title":"prometheus (Source)"},{"location":"docs/api/latest/#rabbitmq-source","text":"The rabbitmq source receives the events from the rabbitmq broker via the AMQP protocol. Origin: siddhi-io-rabbitmq:3.0.2 Syntax @source(type=\"rabbitmq\", uri=\" STRING \", heartbeat=\" INT \", exchange.name=\" STRING \", exchange.type=\" STRING \", exchange.durable.enabled=\" BOOL \", exchange.autodelete.enabled=\" BOOL \", routing.key=\" STRING \", headers=\" STRING \", queue.name=\" STRING \", queue.durable.enabled=\" BOOL \", queue.exclusive.enabled=\" BOOL \", queue.autodelete.enabled=\" BOOL \", tls.enabled=\" BOOL \", tls.truststore.path=\" STRING \", tls.truststore.password=\" STRING \", tls.truststore.type=\" STRING \", tls.version=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic uri The URI that is used to connect to an AMQP server. If no URI is specified,an error is logged in the CLI.e.g., amqp://guest:guest , amqp://guest:guest@localhost:5672 STRING No No heartbeat The period of time (in seconds) after which the peer TCP connection should be considered unreachable (down) by RabbitMQ and client libraries. 60 INT Yes No exchange.name The name of the exchange that decides what to do with a message it receives.If the exchange.name already exists in the RabbitMQ server, then the system uses that exchange.name instead of redeclaring. STRING No No exchange.type The type of the exchange name. The exchange types available are direct , fanout , topic and headers . For a detailed description of each type, see RabbitMQ - AMQP Concepts . direct STRING Yes No exchange.durable.enabled If this is set to true , the exchange remains declared even if the broker restarts. false BOOL Yes No exchange.autodelete.enabled If this is set to true , the exchange is automatically deleted when it is not used anymore. false BOOL Yes No routing.key The key based on which the exchange determines how to route the message to queues. The routing key is like an address for the message. The routing.key must be initialized when the value for the exchange.type parameter is direct or topic . empty STRING Yes No headers The headers of the message. The attributes used for routing are taken from the this paremeter. A message is considered matching if the value of the header equals the value specified upon binding. null STRING Yes No queue.name A queue is a buffer that stores messages. If the queue name already exists in the RabbitMQ server, then the system usees that queue name instead of redeclaring it. If no value is specified for this parameter, the system uses the unique queue name that is automatically generated by the RabbitMQ server. system generated queue name STRING Yes No queue.durable.enabled If this parameter is set to true , the queue remains declared even if the broker restarts false BOOL Yes No queue.exclusive.enabled If this parameter is set to true , the queue is exclusive for the current connection. If it is set to false , it is also consumable by other connections. false BOOL Yes No queue.autodelete.enabled If this parameter is set to true , the queue is automatically deleted when it is not used anymore. false BOOL Yes No tls.enabled This parameter specifies whether an encrypted communication channel should be established or not. When this parameter is set to true , the tls.truststore.path and tls.truststore.password parameters are initialized. false BOOL Yes No tls.truststore.path The file path to the location of the truststore of the client that receives the RabbitMQ events via the AMQP protocol. A custom client-truststore can be specified if required. If a custom truststore is not specified, then the system uses the default client-trustore in the {carbon.home}/resources/security /code directory. /p /td td style=\"vertical-align: top\" {carbon.home}/resources/security</code> directory.</p></td> <td style=\"vertical-align: top\"> /resources/security/client-truststore.jks STRING Yes No tls.truststore.password The password for the client-truststore. A custom password can be specified if required. If no custom password is specified, then the system uses wso2carbon as the default password. wso2carbon STRING Yes No tls.truststore.type The type of the truststore. JKS STRING Yes No tls.version The version of the tls/ssl. SSL STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @source(type ='rabbitmq', uri = 'amqp://guest:guest@localhost:5672', exchange.name = 'direct', routing.key= 'direct', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query receives events from the direct exchange with the direct exchange type, and the directTest routing key.","title":"rabbitmq (Source)"},{"location":"docs/api/latest/#tcp-source","text":"A Siddhi application can be configured to receive events via the TCP transport by adding the @Source(type = 'tcp') annotation at the top of an event stream definition. When this is defined the associated stream will receive events from the TCP transport on the host and port defined in the system. Origin: siddhi-io-tcp:3.0.4 Syntax @source(type=\"tcp\", context=\" STRING \", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic context The URL 'context' that should be used to receive the events. / STRING Yes No System Parameters Name Description Default Value Possible Parameters host Tcp server host. 0.0.0.0 Any valid host or IP port Tcp server port. 9892 Any integer representing valid port receiver.threads Number of threads to receive connections. 10 Any positive integer worker.threads Number of threads to serve events. 10 Any positive integer tcp.no.delay This is to specify whether to disable Nagle algorithm during message passing. If tcp.no.delay = 'true', the execution of Nagle algorithm will be disabled in the underlying TCP logic. Hence there will be no delay between two successive writes to the TCP connection. Else there can be a constant ack delay. true true false keep.alive This property defines whether the server should be kept alive when there are no connections available. true true false Examples EXAMPLE 1 @Source(type = 'tcp', context='abc', @map(type='binary')) define stream Foo (attribute1 string, attribute2 int ); Under this configuration, events are received via the TCP transport on default host,port, abc context, and they are passed to Foo stream for processing.","title":"tcp (Source)"},{"location":"docs/api/latest/#sourcemapper","text":"","title":"Sourcemapper"},{"location":"docs/api/latest/#avro-source-mapper","text":"This extension is an Avro to Event input mapper. Transports that accept Avro messages can utilize this extension to convert the incoming Avro messages to Siddhi events. The Avro schema to be used for creating Avro messages can be specified as a parameter in the stream definition. If no Avro schema is specified, a flat avro schema of the 'record' type is generated with the stream attributes as schema fields. The generated/specified Avro schema is used to convert Avro messages to Siddhi events. Origin: siddhi-map-avro:2.0.6 Syntax @source(..., @map(type=\"avro\", schema.def=\" STRING \", schema.registry=\" STRING \", schema.id=\" STRING \", fail.on.missing.attribute=\" BOOL \") QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic schema.def This specifies the schema of the Avro message. The full schema used to create the Avro message needs to be specified as a quoted JSON string. STRING No No schema.registry This specifies the URL of the schema registry. STRING No No schema.id This specifies the ID of the Avro schema. This ID is the global ID that is returned from the schema registry when posting the schema to the registry. The schema is retrieved from the schema registry via the specified ID. STRING No No fail.on.missing.attribute If this parameter is set to 'true', a JSON execution failing or returning a null value results in that message being dropped by the system. If this parameter is set to 'false', a JSON execution failing or returning a null value results in the system being prompted to send the event with a null value to Siddhi so that the user can handle it as required (i.e., by assigning a default value. true BOOL Yes No Examples EXAMPLE 1 @source(type='inMemory', topic='user', @map(type='avro', schema .def = \"\"\"{\"type\":\"record\",\"name\":\"userInfo\",\"namespace\":\"user.example\",\"fields\":[{\"name\":\"name\",\"type\":\"string\"}, {\"name\":\"age\",\"type\":\"int\"}]}\"\"\")) define stream UserStream (name string, age int ); The above Siddhi query performs a default Avro input mapping. The input Avro message that contains user information is converted to a Siddhi event. The expected input is a byte array or ByteBuffer. EXAMPLE 2 @source(type='inMemory', topic='user', @map(type='avro', schema .def = \"\"\"{\"type\":\"record\",\"name\":\"userInfo\",\"namespace\":\"avro.userInfo\",\"fields\":[{\"name\":\"username\",\"type\":\"string\"}, {\"name\":\"age\",\"type\":\"int\"}]}\"\"\",@attributes(name=\"username\",age=\"age\"))) define stream userStream (name string, age int ); The above Siddhi query performs a custom Avro input mapping. The input Avro message that contains user information is converted to a Siddhi event. The expected input is a byte array or ByteBuffer. EXAMPLE 3 @source(type='inMemory', topic='user', @map(type='avro',schema.registry='http://192.168.2.5:9090', schema.id='1',@attributes(name=\"username\",age=\"age\"))) define stream UserStream (name string, age int ); The above Siddhi query performs a custom Avro input mapping. The input Avro message that contains user information is converted to a Siddhi event via the schema retrieved from the given schema registry(localhost:8081). The expected input is a byte array or ByteBuffer.","title":"avro (Source Mapper)"},{"location":"docs/api/latest/#binary-source-mapper","text":"This extension is a binary input mapper that converts events received in binary format to Siddhi events before they are processed. Origin: siddhi-map-binary:2.0.4 Syntax @source(..., @map(type=\"binary\") Examples EXAMPLE 1 @source(type='inMemory', topic='WSO2', @map(type='binary'))define stream FooStream (symbol string, price float, volume long); This query performs a mapping to convert an event of the binary format to a Siddhi event.","title":"binary (Source Mapper)"},{"location":"docs/api/latest/#csv-source-mapper","text":"This extension is used to convert CSV message to Siddhi event input mapper. You can either receive pre-defined CSV message where event conversion takes place without extra configurations,or receive custom CSV message where a custom place order to map from custom CSV message. Origin: siddhi-map-csv:2.0.3 Syntax @source(..., @map(type=\"csv\", delimiter=\" STRING \", header.present=\" BOOL \", fail.on.unknown.attribute=\" BOOL \", event.grouping.enabled=\" BOOL \") QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic delimiter When converting a CSV format message to Siddhi event, this parameter indicatesinput CSV message's data should be split by this parameter , STRING Yes No header.present When converting a CSV format message to Siddhi event, this parameter indicates whether CSV message has header or not. This can either have value true or false.If it's set to false then it indicates that CSV message has't header. false BOOL Yes No fail.on.unknown.attribute This parameter specifies how unknown attributes should be handled. If it's set to true and one or more attributes don't havevalues, then SP will drop that message. If this parameter is set to false , the Stream Processor adds the required attribute's values to such events with a null value and the event is converted to a Siddhi event. true BOOL Yes No event.grouping.enabled This parameter specifies whether event grouping is enabled or not. To receive a group of events together and generate multiple events, this parameter must be set to true . false BOOL Yes No Examples EXAMPLE 1 @source(type='inMemory', topic='stock', @map(type='csv')) define stream FooStream (symbol string, price float, volume int); Above configuration will do a default CSV input mapping. Expected input will look like below: WSO2 ,55.6 , 100OR \"WSO2,No10,Palam Groove Rd,Col-03\" ,55.6 , 100If header.present is true and delimiter is \"-\", then the input is as follows: symbol-price-volumeWSO2-55.6-100 EXAMPLE 2 @source(type='inMemory', topic='stock', @map(type='csv',header='true', @attributes(symbol = \"2\", price = \"0\", volume = \"1\"))) define stream FooStream (symbol string, price float, volume long); Above configuration will perform a custom CSV mapping. Here, user can add place order of each attribute in the @attribute. The place order indicates where the attribute name's value has appeared in the input.Expected input will look like below: 55.6,100,WSO2 OR55.6,100,\"WSO2,No10,Palm Groove Rd,Col-03\" If header is true and delimiter is \"-\", then the output is as follows: price-volume-symbol 55.6-100-WSO2 If group events is enabled then input should be as follows: price-volume-symbol 55.6-100-WSO2System.lineSeparator() 55.6-100-IBMSystem.lineSeparator() 55.6-100-IFSSystem.lineSeparator()","title":"csv (Source Mapper)"},{"location":"docs/api/latest/#json-source-mapper","text":"This extension is a JSON-to-Event input mapper. Transports that accept JSON messages can utilize this extension to convert an incoming JSON message into a Siddhi event. Users can either send a pre-defined JSON format, where event conversion happens without any configurations, or use the JSON path to map from a custom JSON message. In default mapping, the JSON string of the event can be enclosed by the element \"event\", though optional. Origin: siddhi-map-json:5.0.5 Syntax @source(..., @map(type=\"json\", enclosing.element=\" STRING \", fail.on.missing.attribute=\" BOOL \") QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic enclosing.element This is used to specify the enclosing element when sending multiple events in the same JSON message. Mapper treats the child elements of a given enclosing element as events and executes the JSON path expressions on these child elements. If the enclosing.element is not provided then the multiple-event scenario is disregarded and the JSON path is evaluated based on the root element. $ STRING Yes No fail.on.missing.attribute This parameter allows users to handle unknown attributes.The value of this can either be true or false. By default it is true. If a JSON execution fails or returns null, mapper drops that message. However, setting this property to false prompts mapper to send an event with a null value to Siddhi, where users can handle it as required, ie., assign a default value.) true BOOL Yes No Examples EXAMPLE 1 @source(type='inMemory', topic='stock', @map(type='json')) define stream FooStream (symbol string, price float, volume long); This configuration performs a default JSON input mapping. For a single event, the input is required to be in one of the following formats: { \"event\":{ \"symbol\":\"WSO2\", \"price\":55.6, \"volume\":100 } } or { \"symbol\":\"WSO2\", \"price\":55.6, \"volume\":100 } EXAMPLE 2 @source(type='inMemory', topic='stock', @map(type='json')) define stream FooStream (symbol string, price float, volume long); This configuration performs a default JSON input mapping. For multiple events, the input is required to be in one of the following formats: [ {\"event\":{\"symbol\":\"WSO2\",\"price\":55.6,\"volume\":100}}, {\"event\":{\"symbol\":\"WSO2\",\"price\":56.6,\"volume\":99}}, {\"event\":{\"symbol\":\"WSO2\",\"price\":57.6,\"volume\":80}} ] or [ {\"symbol\":\"WSO2\",\"price\":55.6,\"volume\":100}, {\"symbol\":\"WSO2\",\"price\":56.6,\"volume\":99}, {\"symbol\":\"WSO2\",\"price\":57.6,\"volume\":80} ] EXAMPLE 3 @source(type='inMemory', topic='stock', @map(type='json', enclosing.element=\"$.portfolio\", @attributes(symbol = \"company.symbol\", price = \"price\", volume = \"volume\"))) This configuration performs a custom JSON mapping. For a single event, the expected input is similar to the one shown below: { \"portfolio\":{ \"stock\":{ \"volume\":100, \"company\":{ \"symbol\":\"WSO2\" }, \"price\":55.6 } } } EXAMPLE 4 @source(type='inMemory', topic='stock', @map(type='json', enclosing.element=\"$.portfolio\", @attributes(symbol = \"stock.company.symbol\", price = \"stock.price\", volume = \"stock.volume\"))) define stream FooStream (symbol string, price float, volume long); The configuration performs a custom JSON mapping. For multiple events, expected input looks as follows. .{\"portfolio\": [ {\"stock\":{\"volume\":100,\"company\":{\"symbol\":\"wso2\"},\"price\":56.6}}, {\"stock\":{\"volume\":200,\"company\":{\"symbol\":\"wso2\"},\"price\":57.6}} ] }","title":"json (Source Mapper)"},{"location":"docs/api/latest/#keyvalue-source-mapper","text":"Key-Value Map to Event input mapper extension allows transports that accept events as key value maps to convert those events to Siddhi events. You can either receive pre-defined keys where conversion takes place without extra configurations, or use custom keys to map from the message. Origin: siddhi-map-keyvalue:2.0.5 Syntax @source(..., @map(type=\"keyvalue\", fail.on.missing.attribute=\" BOOL \") QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic fail.on.missing.attribute If this parameter is set to true , if an event arrives without a matching key for a specific attribute in the connected stream, it is dropped and not processed by the Stream Processor. If this parameter is set to false the Stream Processor adds the required key to such events with a null value, and the event is converted to a Siddhi event so that you could handle them as required before they are further processed. true BOOL Yes No Examples EXAMPLE 1 @source(type='inMemory', topic='stock', @map(type='keyvalue')) define stream FooStream (symbol string, price float, volume long); This query performs a default key value input mapping. The expected input is a map similar to the following: symbol: 'WSO2' price: 55.6f volume: 100 EXAMPLE 2 @source(type='inMemory', topic='stock', @map(type='keyvalue', fail.on.missing.attribute='true', @attributes(symbol = 's', price = 'p', volume = 'v')))define stream FooStream (symbol string, price float, volume long); This query performs a custom key value input mapping. The matching keys for the symbol , price and volume attributes are be s , p , and v respectively. The expected input is a map similar to the following: s: 'WSO2' p: 55.6 v: 100","title":"keyvalue (Source Mapper)"},{"location":"docs/api/latest/#passthrough-source-mapper","text":"Pass-through mapper passed events (Event[]) through without any mapping or modifications. Origin: siddhi-core:5.1.8 Syntax @source(..., @map(type=\"passThrough\") Examples EXAMPLE 1 @source(type='tcp', @map(type='passThrough')) define stream BarStream (symbol string, price float, volume long); In this example BarStream uses passThrough inputmapper which passes the received Siddhi event directly without any transformation into source.","title":"passThrough (Source Mapper)"},{"location":"docs/api/latest/#protobuf-source-mapper","text":"This input mapper allows you to convert protobuf messages into Events. To work with this input mapper you have to add auto-generated protobuf classes to the project classpath. When you use this input mapper, you can either define stream attributes as the same names as the protobuf message attributes or you can use custom mapping to map stream definition attributes with the protobuf attributes..Please find the sample proto definition here Origin: siddhi-map-protobuf:1.0.2 Syntax @source(..., @map(type=\"protobuf\", class=\" STRING \") QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic class This specifies the class name of the protobuf message class, If sink type is grpc then it's not necessary to provide this field. - STRING Yes No Examples EXAMPLE 1 source(type='grpc', receiver.url = 'grpc://localhost: 2000/org.wso2.grpc.test.MyService/process', @map(type='protobuf')) define stream FooStream (stringValue string, intValue int,longValue long,booleanValue bool,floatValue float,doubleValue double); Above definition will convert the protobuf messages that are received to this source into siddhi events. EXAMPLE 2 source(type='grpc', receiver.url = 'grpc://localhost: 2000/org.wso2.grpc.test.MyService/process', @map(type='protobuf', @attributes(a = 'stringValue', b = 'intValue', c = 'longValue',d = 'booleanValue',' e = floatValue', f ='doubleValue'))) define stream FooStream (a string ,c long,b int, d bool,e float,f double); Above definition will convert the protobuf messages that are received to this source into siddhi events. since there's a mapping available for the stream, protobuf message object will be map like this, -'stringValue' of the protobuf message will be assign to the 'a' attribute of the stream - 'intValue' of the protobuf message will be assign to the 'b' attribute of the stream - 'longValue' of the protobuf message will be assign to the 'c' attribute of the stream - 'booleanValue' of the protobuf message will be assign to the 'd' attribute of the stream - 'floatValue' of the protobuf message will be assign to the 'e' attribute of the stream - 'doubleValue' of the protobuf message will be assign to the 'f' attribute of the stream EXAMPLE 3 source(type='grpc', receiver.url = 'grpc://localhost: 2000/org.wso2.grpc.test.MyService/testMap', @map(type='protobuf')) define stream FooStream (stringValue string ,intValue int,map object); Above definition will convert the protobuf messages that are received to this source into siddhi events. since there's an object type attribute available in the stream (map object), mapper will assume that object is an instance of 'java.util.Map' class. otherwise mapper will throws an exception EXAMPLE 4 @source(type='inMemory', topic='test01', @map(type='protobuf', class='org.wso2.grpc.test.Request')) define stream FooStream (stringValue string, intValue int,longValue long,booleanValue bool,floatValue float,doubleValue double); The above definition will convert the 'org.wso2.grpc.test.Request' type protobuf messages into siddhi events. If we did not provide the 'receiver.url' in the stream definition we have to provide the protobuf class name in the 'class' parameter inside @map.","title":"protobuf (Source Mapper)"},{"location":"docs/api/latest/#text-source-mapper","text":"This extension is a text to Siddhi event input mapper. Transports that accept text messages can utilize this extension to convert the incoming text message to Siddhi event. Users can either use a pre-defined text format where event conversion happens without any additional configurations, or specify a regex to map a text message using custom configurations. Origin: siddhi-map-text:2.0.4 Syntax @source(..., @map(type=\"text\", regex.groupid=\" STRING \", fail.on.missing.attribute=\" BOOL \", event.grouping.enabled=\" BOOL \", delimiter=\" STRING \", new.line.character=\" STRING \") QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic regex.groupid This parameter specifies a regular expression group. The groupid can be any capital letter (e.g., regex.A,regex.B .. etc). You can specify any number of regular expression groups. In the attribute annotation, you need to map all attributes to the regular expression group with the matching group index. If you need to to enable custom mapping, it is required to specifythe matching group for each and every attribute. STRING No No fail.on.missing.attribute This parameter specifies how unknown attributes should be handled. If it is set to true a message is dropped if its execution fails, or if one or more attributes do not have values. If this parameter is set to false , null values are assigned to attributes with missing values, and messages with such attributes are not dropped. true BOOL Yes No event.grouping.enabled This parameter specifies whether event grouping is enabled or not. To receive a group of events together and generate multiple events, this parameter must be set to true . false BOOL Yes No delimiter This parameter specifies how events must be separated when multiple events are received. This must be whole line and not a single character. ~ ~ ~ ~ STRING Yes No new.line.character This attribute indicates the new line character of the event that is expected to be received. This is used mostly when communication between 2 types of operating systems is expected. For example, Linux uses \\n as the end of line character whereas windows uses \\r\\n . \\n STRING Yes No Examples EXAMPLE 1 @source(type='inMemory', topic='stock', @map(type='text')) define stream FooStream (symbol string, price float, volume long); This query performs a default text input mapping. The expected input is as follows: symbol:\"WSO2\", price:55.6, volume:100 OR symbol:'WSO2', price:55.6, volume:100 If group events is enabled then input should be as follows: symbol:\"WSO2\", price:55.6, volume:100 ~ ~ ~ ~ symbol:\"WSO2\", price:55.6, volume:100 EXAMPLE 2 @source(type='inMemory', topic='stock', @map(type='text', fail.on.missing.attribute = 'true', regex.A='(\\w+)\\s([-0-9]+)',regex.B='volume\\s([-0-9]+)', @attributes(symbol = 'A[1]',price = 'A[2]',volume = 'B'))) define stream FooStream (symbol string, price float, volume long); This query performs a custom text mapping. The expected input is as follows: wos2 550 volume 100 If group events is enabled then input should be as follows: wos2 550 volume 100 ~ ~ ~ ~ wos2 550 volume 100 ~ ~ ~ ~ wos2 550 volume 100","title":"text (Source Mapper)"},{"location":"docs/api/latest/#xml-source-mapper","text":"This mapper converts XML input to Siddhi event. Transports which accepts XML messages can utilize this extension to convert the incoming XML message to Siddhi event. Users can either send a pre-defined XML format where event conversion will happen without any configs or can use xpath to map from a custom XML message. Origin: siddhi-map-xml:5.0.3 Syntax @source(..., @map(type=\"xml\", namespaces=\" STRING \", enclosing.element=\" STRING \", fail.on.missing.attribute=\" BOOL \") QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic namespaces Used to provide namespaces used in the incoming XML message beforehand to configure xpath expressions. User can provide a comma separated list. If these are not provided xpath evaluations will fail None STRING Yes No enclosing.element Used to specify the enclosing element in case of sending multiple events in same XML message. WSO2 DAS will treat the child element of given enclosing element as events and execute xpath expressions on child elements. If enclosing.element is not provided multiple event scenario is disregarded and xpaths will be evaluated with respect to root element. Root element STRING Yes No fail.on.missing.attribute This can either have value true or false. By default it will be true. This attribute allows user to handle unknown attributes. By default if an xpath execution fails or returns null DAS will drop that message. However setting this property to false will prompt DAS to send and event with null value to Siddhi where user can handle it accordingly(ie. Assign a default value) True BOOL Yes No Examples EXAMPLE 1 @source(type='inMemory', topic='stock', @map(type='xml')) define stream FooStream (symbol string, price float, volume long); Above configuration will do a default XML input mapping. Expected input will look like below. events event symbol WSO2 /symbol price 55.6 /price volume 100 /volume /event /events EXAMPLE 2 @source(type='inMemory', topic='stock', @map(type='xml', namespaces = \"dt=urn:schemas-microsoft-com:datatypes\", enclosing.element=\"//portfolio\", @attributes(symbol = \"company/symbol\", price = \"price\", volume = \"volume\"))) define stream FooStream (symbol string, price float, volume long); Above configuration will perform a custom XML mapping. In the custom mapping user can add xpath expressions representing each event attribute using @attribute annotation. Expected input will look like below. portfolio xmlns:dt=\"urn:schemas-microsoft-com:datatypes\" stock exchange=\"nasdaq\" volume 100 /volume company symbol WSO2 /symbol /company price dt:type=\"number\" 55.6 /price /stock /portfolio","title":"xml (Source Mapper)"},{"location":"docs/api/latest/#store","text":"","title":"Store"},{"location":"docs/api/latest/#mongodb-store","text":"Using this extension a MongoDB Event Table can be configured to persist events in a MongoDB of user's choice. Origin: siddhi-store-mongodb:2.0.3 Syntax @Store(type=\"mongodb\", mongodb.uri=\" STRING \", collection.name=\" STRING \", secure.connection=\" STRING \", trust.store=\" STRING \", trust.store.password=\" STRING \", key.store=\" STRING \", key.store.password=\" STRING \") @PrimaryKey(\"PRIMARY_KEY\") @Index(\"INDEX\") QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic mongodb.uri The MongoDB URI for the MongoDB data store. The uri must be of the format mongodb://[username:password@]host1[:port1][,hostN[:portN]][/[database][?options]] The options specified in the uri will override any connection options specified in the deployment yaml file. Note: The user should have read permissions to the admindb as well as read/write permissions to the database accessed. STRING No No collection.name The name of the collection in the store this Event Table should be persisted as. Name of the siddhi event table. STRING Yes No secure.connection Describes enabling the SSL for the mongodb connection false STRING Yes No trust.store File path to the trust store. {carbon.home}/resources/security/client-truststore.jks /td td style=\"vertical-align: top\" STRING /td td style=\"vertical-align: top\" Yes /td td style=\"vertical-align: top\" No /td /tr tr td style=\"vertical-align: top\" trust.store.password /td td style=\"vertical-align: top; word-wrap: break-word\" p style=\"word-wrap: break-word;margin: 0;\" Password to access the trust store /p /td td style=\"vertical-align: top\" wso2carbon /td td style=\"vertical-align: top\" STRING /td td style=\"vertical-align: top\" Yes /td td style=\"vertical-align: top\" No /td /tr tr td style=\"vertical-align: top\" key.store /td td style=\"vertical-align: top; word-wrap: break-word\" p style=\"word-wrap: break-word;margin: 0;\" File path to the keystore. /p /td td style=\"vertical-align: top\" {carbon.home}/resources/security/client-truststore.jks</td> <td style=\"vertical-align: top\">STRING</td> <td style=\"vertical-align: top\">Yes</td> <td style=\"vertical-align: top\">No</td> </tr> <tr> <td style=\"vertical-align: top\">trust.store.password</td> <td style=\"vertical-align: top; word-wrap: break-word\"><p style=\"word-wrap: break-word;margin: 0;\">Password to access the trust store</p></td> <td style=\"vertical-align: top\">wso2carbon</td> <td style=\"vertical-align: top\">STRING</td> <td style=\"vertical-align: top\">Yes</td> <td style=\"vertical-align: top\">No</td> </tr> <tr> <td style=\"vertical-align: top\">key.store</td> <td style=\"vertical-align: top; word-wrap: break-word\"><p style=\"word-wrap: break-word;margin: 0;\">File path to the keystore.</p></td> <td style=\"vertical-align: top\"> /resources/security/client-truststore.jks STRING Yes No key.store.password Password to access the keystore wso2carbon STRING Yes No System Parameters Name Description Default Value Possible Parameters applicationName Sets the logical name of the application using this MongoClient. The application name may be used by the client to identify the application to the server, for use in server logs, slow query logs, and profile collection. null the logical name of the application using this MongoClient. The UTF-8 encoding may not exceed 128 bytes. cursorFinalizerEnabled Sets whether cursor finalizers are enabled. true true false requiredReplicaSetName The name of the replica set null the logical name of the replica set sslEnabled Sets whether to initiate connection with TSL/SSL enabled. true: Initiate the connection with TLS/SSL. false: Initiate the connection without TLS/SSL. false true false trustStore File path to the trust store. {carbon.home}/resources/security/client-truststore.jks /td td style=\"vertical-align: top\" Any valid file path. /td /tr tr td style=\"vertical-align: top\" trustStorePassword /td td style=\"vertical-align: top;\" p style=\"word-wrap: break-word;margin: 0;\" Password to access the trust store /p /td td style=\"vertical-align: top\" wso2carbon /td td style=\"vertical-align: top\" Any valid password. /td /tr tr td style=\"vertical-align: top\" keyStore /td td style=\"vertical-align: top;\" p style=\"word-wrap: break-word;margin: 0;\" File path to the keystore. /p /td td style=\"vertical-align: top\" {carbon.home}/resources/security/client-truststore.jks</td> <td style=\"vertical-align: top\">Any valid file path.</td> </tr> <tr> <td style=\"vertical-align: top\">trustStorePassword</td> <td style=\"vertical-align: top;\"><p style=\"word-wrap: break-word;margin: 0;\">Password to access the trust store</p></td> <td style=\"vertical-align: top\">wso2carbon</td> <td style=\"vertical-align: top\">Any valid password.</td> </tr> <tr> <td style=\"vertical-align: top\">keyStore</td> <td style=\"vertical-align: top;\"><p style=\"word-wrap: break-word;margin: 0;\">File path to the keystore.</p></td> <td style=\"vertical-align: top\"> /resources/security/client-truststore.jks Any valid file path. keyStorePassword Password to access the keystore wso2carbon Any valid password. connectTimeout The time in milliseconds to attempt a connection before timing out. 10000 Any positive integer connectionsPerHost The maximum number of connections in the connection pool. 100 Any positive integer minConnectionsPerHost The minimum number of connections in the connection pool. 0 Any natural number maxConnectionIdleTime The maximum number of milliseconds that a connection can remain idle in the pool before being removed and closed. A zero value indicates no limit to the idle time. A pooled connection that has exceeded its idle time will be closed and replaced when necessary by a new connection. 0 Any positive integer maxWaitTime The maximum wait time in milliseconds that a thread may wait for a connection to become available. A value of 0 means that it will not wait. A negative value means to wait indefinitely 120000 Any integer threadsAllowedToBlockForConnectionMultiplier The maximum number of connections allowed per host for this MongoClient instance. Those connections will be kept in a pool when idle. Once the pool is exhausted, any operation requiring a connection will block waiting for an available connection. 100 Any natural number maxConnectionLifeTime The maximum life time of a pooled connection. A zero value indicates no limit to the life time. A pooled connection that has exceeded its life time will be closed and replaced when necessary by a new connection. 0 Any positive integer socketKeepAlive Sets whether to keep a connection alive through firewalls false true false socketTimeout The time in milliseconds to attempt a send or receive on a socket before the attempt times out. Default 0 means never to timeout. 0 Any natural integer writeConcern The write concern to use. acknowledged acknowledged w1 w2 w3 unacknowledged fsynced journaled replica_acknowledged normal safe majority fsync_safe journal_safe replicas_safe readConcern The level of isolation for the reads from replica sets. default local majority linearizable readPreference Specifies the replica set read preference for the connection. primary primary secondary secondarypreferred primarypreferred nearest localThreshold The size (in milliseconds) of the latency window for selecting among multiple suitable MongoDB instances. 15 Any natural number serverSelectionTimeout Specifies how long (in milliseconds) to block for server selection before throwing an exception. A value of 0 means that it will timeout immediately if no server is available. A negative value means to wait indefinitely. 30000 Any integer heartbeatSocketTimeout The socket timeout for connections used for the cluster heartbeat. A value of 0 means that it will timeout immediately if no cluster member is available. A negative value means to wait indefinitely. 20000 Any integer heartbeatConnectTimeout The connect timeout for connections used for the cluster heartbeat. A value of 0 means that it will timeout immediately if no cluster member is available. A negative value means to wait indefinitely. 20000 Any integer heartbeatFrequency Specify the interval (in milliseconds) between checks, counted from the end of the previous check until the beginning of the next one. 10000 Any positive integer minHeartbeatFrequency Sets the minimum heartbeat frequency. In the event that the driver has to frequently re-check a server's availability, it will wait at least this long since the previous check to avoid wasted effort. 500 Any positive integer Examples EXAMPLE 1 @Store(type=\"mongodb\",mongodb.uri=\"mongodb://admin:admin@localhost/Foo\") @PrimaryKey(\"symbol\") @Index(\"volume:1\", {background:true,unique:true}\") define table FooTable (symbol string, price float, volume long); This will create a collection called FooTable for the events to be saved with symbol as Primary Key(unique index at mongoDB level) and index for the field volume will be created in ascending order with the index option to create the index in the background. Note: @PrimaryKey: This specifies a list of comma-separated values to be treated as unique fields in the table. Each record in the table must have a unique combination of values for the fields specified here. @Index: This specifies the fields that must be indexed at the database level. You can specify multiple values as a come-separated list. A single value to be in the format, FieldName : SortOrder . The last element is optional through which a valid index options can be passed. SortOrder : 1 for Ascending -1 for Descending. Optional, with default value as 1. IndexOptions : Index Options must be defined inside curly brackets. Options must follow the standard mongodb index options format. https://docs.mongodb.com/manual/reference/method/db.collection.createIndex/ Example 1: @Index( 'symbol:1' , '{\"unique\":true}' ) Example 2: @Index( 'symbol' , '{\"unique\":true}' ) Example 3: @Index( 'symbol:1' , 'volume:-1' , '{\"unique\":true}' )","title":"mongodb (Store)"},{"location":"docs/api/latest/#rdbms-store","text":"This extension assigns data sources and connection instructions to event tables. It also implements read-write operations on connected data sources. Origin: siddhi-store-rdbms:7.0.2 Syntax @Store(type=\"rdbms\", jdbc.url=\" STRING \", username=\" STRING \", password=\" STRING \", jdbc.driver.name=\" STRING \", pool.properties=\" STRING \", jndi.resource=\" STRING \", datasource=\" STRING \", table.name=\" STRING \", field.length=\" STRING \", table.check.query=\" STRING \", use.collation=\" BOOL \") @PrimaryKey(\"PRIMARY_KEY\") @Index(\"INDEX\") QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic jdbc.url The JDBC URL via which the RDBMS data store is accessed. STRING No No username The username to be used to access the RDBMS data store. STRING No No password The password to be used to access the RDBMS data store. STRING No No jdbc.driver.name The driver class name for connecting the RDBMS data store. STRING No No pool.properties Any pool parameters for the database connection must be specified as key-value pairs. null STRING Yes No jndi.resource The name of the JNDI resource through which the connection is attempted. If this is found, the pool properties described above are not taken into account and the connection is attempted via JNDI lookup instead. null STRING Yes No datasource The name of the Carbon datasource that should be used for creating the connection with the database. If this is found, neither the pool properties nor the JNDI resource name described above are taken into account and the connection is attempted via Carbon datasources instead. Only works in Siddhi Distribution null STRING Yes No table.name The name with which the event table should be persisted in the store. If no name is specified via this parameter, the event table is persisted with the same name as the Siddhi table. The table name defined in the Siddhi App query. STRING Yes No field.length The number of characters that the values for fields of the 'STRING' type in the table definition must contain. Each required field must be provided as a comma-separated list of key-value pairs in the ' field.name : length ' format. If this is not specified, the default number of characters specific to the database type is considered. null STRING Yes No table.check.query This query will be used to check whether the table is exist in the given database. But the provided query should return an SQLException if the table does not exist in the database. Furthermore if the provided table is a database view, and it is not exists in the database a table from given name will be created in the database The tableCheckQuery which define in store rdbms configs STRING Yes No use.collation This property allows users to use collation for string attirbutes. By default it's false and binary collation is not used. Currently 'latin1_bin' and 'SQL_Latin1_General_CP1_CS_AS' are used as collations for MySQL and Microsoft SQL database types respectively. false BOOL Yes No System Parameters Name Description Default Value Possible Parameters {{RDBMS-Name}}.maxVersion The latest version supported for {{RDBMS-Name}}. 0 N/A {{RDBMS-Name}}.minVersion The earliest version supported for {{RDBMS-Name}}. 0 N/A {{RDBMS-Name}}.tableCheckQuery The template query for the 'check table' operation in {{RDBMS-Name}}. H2 : CREATE TABLE {{TABLE_NAME}} ({{COLUMNS, PRIMARY_KEYS}}) MySQL : CREATE TABLE {{TABLE_NAME}} ({{COLUMNS, PRIMARY_KEYS}}) Oracle : CREATE TABLE {{TABLE_NAME}} ({{COLUMNS, PRIMARY_KEYS}}) Microsoft SQL Server : CREATE TABLE {{TABLE_NAME}} ({{COLUMNS, PRIMARY_KEYS}}) PostgreSQL : CREATE TABLE {{TABLE_NAME}} ({{COLUMNS, PRIMARY_KEYS}}) DB2. : CREATE TABLE {{TABLE_NAME}} ({{COLUMNS, PRIMARY_KEYS}}) N/A {{RDBMS-Name}}.tableCreateQuery The template query for the 'create table' operation in {{RDBMS-Name}}. H2 : SELECT 1 FROM {{TABLE_NAME}} LIMIT 1 MySQL : SELECT 1 FROM {{TABLE_NAME}} LIMIT 1 Oracle : SELECT 1 FROM {{TABLE_NAME}} WHERE rownum=1 Microsoft SQL Server : SELECT TOP 1 1 from {{TABLE_NAME}} PostgreSQL : SELECT 1 FROM {{TABLE_NAME}} LIMIT 1 DB2. : SELECT 1 FROM {{TABLE_NAME}} FETCH FIRST 1 ROWS ONLY N/A {{RDBMS-Name}}.indexCreateQuery The template query for the 'create index' operation in {{RDBMS-Name}}. H2 : CREATE INDEX {{TABLE_NAME}}_INDEX ON {{TABLE_NAME}} ({{INDEX_COLUMNS}}) MySQL : CREATE INDEX {{TABLE_NAME}}_INDEX ON {{TABLE_NAME}} ({{INDEX_COLUMNS}}) Oracle : CREATE INDEX {{TABLE_NAME}}_INDEX ON {{TABLE_NAME}} ({{INDEX_COLUMNS}}) Microsoft SQL Server : CREATE INDEX {{TABLE_NAME}}_INDEX ON {{TABLE_NAME}} ({{INDEX_COLUMNS}}) {{TABLE_NAME}} ({{INDEX_COLUMNS}}) PostgreSQL : CREATE INDEX {{TABLE_NAME}}_INDEX ON {{TABLE_NAME}} ({{INDEX_COLUMNS}}) DB2. : CREATE INDEX {{TABLE_NAME}}_INDEX ON {{TABLE_NAME}} ({{INDEX_COLUMNS}}) N/A {{RDBMS-Name}}.recordInsertQuery The template query for the 'insert record' operation in {{RDBMS-Name}}. H2 : INSERT INTO {{TABLE_NAME}} ({{COLUMNS}}) VALUES ({{Q}}) MySQL : INSERT INTO {{TABLE_NAME}} ({{COLUMNS}}) VALUES ({{Q}}) Oracle : INSERT INTO {{TABLE_NAME}} ({{COLUMNS}}) VALUES ({{Q}}) Microsoft SQL Server : INSERT INTO {{TABLE_NAME}} ({{COLUMNS}}) VALUES ({{Q}}) PostgreSQL : INSERT INTO {{TABLE_NAME}} ({{COLUMNS}}) VALUES ({{Q}}) DB2. : INSERT INTO {{TABLE_NAME}} ({{COLUMNS}}) VALUES ({{Q}}) N/A {{RDBMS-Name}}.recordUpdateQuery The template query for the 'update record' operation in {{RDBMS-Name}}. H2 : UPDATE {{TABLE_NAME}} SET {{COLUMNS_AND_VALUES}} {{CONDITION}} MySQL : UPDATE {{TABLE_NAME}} SET {{COLUMNS_AND_VALUES}} {{CONDITION}} Oracle : UPDATE {{TABLE_NAME}} SET {{COLUMNS_AND_VALUES}} {{CONDITION}} Microsoft SQL Server : UPDATE {{TABLE_NAME}} SET {{COLUMNS_AND_VALUES}} {{CONDITION}} PostgreSQL : UPDATE {{TABLE_NAME}} SET {{COLUMNS_AND_VALUES}} {{CONDITION}} DB2. : UPDATE {{TABLE_NAME}} SET {{COLUMNS_AND_VALUES}} {{CONDITION}} N/A {{RDBMS-Name}}.recordSelectQuery The template query for the 'select record' operation in {{RDBMS-Name}}. H2 : SELECT * FROM {{TABLE_NAME}} {{CONDITION}} MySQL : SELECT * FROM {{TABLE_NAME}} {{CONDITION}} Oracle : SELECT * FROM {{TABLE_NAME}} {{CONDITION}} Microsoft SQL Server : SELECT * FROM {{TABLE_NAME}} {{CONDITION}} PostgreSQL : SELECT * FROM {{TABLE_NAME}} {{CONDITION}} DB2. : SELECT * FROM {{TABLE_NAME}} {{CONDITION}} N/A {{RDBMS-Name}}.recordExistsQuery The template query for the 'check record existence' operation in {{RDBMS-Name}}. H2 : SELECT TOP 1 1 FROM {{TABLE_NAME}} {{CONDITION}} MySQL : SELECT 1 FROM {{TABLE_NAME}} {{CONDITION}} Oracle : SELECT COUNT(1) INTO existence FROM {{TABLE_NAME}} {{CONDITION}} Microsoft SQL Server : SELECT TOP 1 FROM {{TABLE_NAME}} {{CONDITION}} PostgreSQL : SELECT 1 FROM {{TABLE_NAME}} {{CONDITION}} LIMIT 1 DB2. : SELECT 1 FROM {{TABLE_NAME}} {{CONDITION}} FETCH FIRST 1 ROWS ONLY N/A {{RDBMS-Name}}.recordDeleteQuery The query for the 'delete record' operation in {{RDBMS-Name}}. H2 : DELETE FROM {{TABLE_NAME}} {{CONDITION}} MySQL : DELETE FROM {{TABLE_NAME}} {{CONDITION}} Oracle : DELETE FROM {{TABLE_NAME}} {{CONDITION}} Microsoft SQL Server : DELETE FROM {{TABLE_NAME}} {{CONDITION}} PostgreSQL : DELETE FROM {{TABLE_NAME}} {{CONDITION}} DB2. : DELETE FROM {{TABLE_NAME}} {{CONDITION}} N/A {{RDBMS-Name}}.stringSize This defines the length for the string fields in {{RDBMS-Name}}. H2 : 254 MySQL : 254 Oracle : 254 Microsoft SQL Server : 254 PostgreSQL : 254 DB2. : 254 N/A {{RDBMS-Name}}.fieldSizeLimit This defines the field size limit for select/switch to big string type from the default string type if the 'bigStringType' is available in field type list. H2 : N/A MySQL : N/A Oracle : 2000 Microsoft SQL Server : N/A PostgreSQL : N/A DB2. : N/A 0 = n = INT_MAX {{RDBMS-Name}}.batchSize This defines the batch size when operations are performed for batches of events. H2 : 1000 MySQL : 1000 Oracle : 1000 Microsoft SQL Server : 1000 PostgreSQL : 1000 DB2. : 1000 N/A {{RDBMS-Name}}.batchEnable This specifies whether 'Update' and 'Insert' operations can be performed for batches of events or not. H2 : true MySQL : true Oracle (versions 12.0 and less) : false Oracle (versions 12.1 and above) : true Microsoft SQL Server : true PostgreSQL : true DB2. : true N/A {{RDBMS-Name}}.transactionSupported This is used to specify whether the JDBC connection that is used supports JDBC transactions or not. H2 : true MySQL : true Oracle : true Microsoft SQL Server : true PostgreSQL : true DB2. : true N/A {{RDBMS-Name}}.typeMapping.binaryType This is used to specify the binary data type. An attribute defines as 'object' type in Siddhi stream will be stored into RDBMS with this type. H2 : BLOB MySQL : BLOB Oracle : BLOB Microsoft SQL Server : VARBINARY(max) PostgreSQL : BYTEA DB2. : BLOB(64000) N/A {{RDBMS-Name}}.typeMapping.booleanType This is used to specify the boolean data type. An attribute defines as 'bool' type in Siddhi stream will be stored into RDBMS with this type. H2 : TINYINT(1) MySQL : TINYINT(1) Oracle : NUMBER(1) Microsoft SQL Server : BIT PostgreSQL : BOOLEAN DB2. : SMALLINT N/A {{RDBMS-Name}}.typeMapping.doubleType This is used to specify the double data type. An attribute defines as 'double' type in Siddhi stream will be stored into RDBMS with this type. H2 : DOUBLE MySQL : DOUBLE Oracle : NUMBER(19,4) Microsoft SQL Server : FLOAT(32) PostgreSQL : DOUBLE PRECISION DB2. : DOUBLE N/A {{RDBMS-Name}}.typeMapping.floatType This is used to specify the float data type. An attribute defines as 'float' type in Siddhi stream will be stored into RDBMS with this type. H2 : FLOAT MySQL : FLOAT Oracle : NUMBER(19,4) Microsoft SQL Server : REAL PostgreSQL : REAL DB2. : REAL N/A {{RDBMS-Name}}.typeMapping.integerType This is used to specify the integer data type. An attribute defines as 'int' type in Siddhi stream will be stored into RDBMS with this type. H2 : INTEGER MySQL : INTEGER Oracle : NUMBER(10) Microsoft SQL Server : INTEGER PostgreSQL : INTEGER DB2. : INTEGER N/A {{RDBMS-Name}}.typeMapping.longType This is used to specify the long data type. An attribute defines as 'long' type in Siddhi stream will be stored into RDBMS with this type. H2 : BIGINT MySQL : BIGINT Oracle : NUMBER(19) Microsoft SQL Server : BIGINT PostgreSQL : BIGINT DB2. : BIGINT N/A {{RDBMS-Name}}.typeMapping.stringType This is used to specify the string data type. An attribute defines as 'string' type in Siddhi stream will be stored into RDBMS with this type. H2 : VARCHAR(stringSize) MySQL : VARCHAR(stringSize) Oracle : VARCHAR(stringSize) Microsoft SQL Server : VARCHAR(stringSize) PostgreSQL : VARCHAR(stringSize) DB2. : VARCHAR(stringSize) N/A {{RDBMS-Name}}.typeMapping.bigStringType This is used to specify the big string data type. An attribute defines as 'string' type in Siddhi stream and field.length define in the annotation is greater than the fieldSizeLimit, will be stored into RDBMS with this type. H2 : N/A MySQL : N/A Oracle : CLOB Microsoft SQL Server : N/A PostgreSQL : N/A DB2.* : N/A N/A Examples EXAMPLE 1 @Store(type=\"rdbms\", jdbc.url=\"jdbc:mysql://localhost:3306/stocks\", username=\"root\", password=\"root\", jdbc.driver.name=\"com.mysql.jdbc.Driver\",field.length=\"symbol:100\") @PrimaryKey(\"id\", \"symbol\") @Index(\"volume\") define table StockTable (id string, symbol string, price float, volume long); The above example creates an event table named 'StockTable' in the database if it does not already exist (with four attributes named id , symbol , price , and volume of the types 'string', 'string', 'float', and 'long' respectively). The connection is made as specified by the parameters configured for the '@Store' annotation. The @PrimaryKey() and @Index() annotations can be used to define primary keys or indexes for the table and they follow Siddhi query syntax. RDBMS store supports having more than one attributes in the @PrimaryKey or @Index annotations. In this example a composite Primary key of both attributes id and symbol will be created. EXAMPLE 2 @Store(type=\"rdbms\", jdbc.url=\"jdbc:mysql://localhost:3306/das\", username=\"root\", password=\"root\" , jdbc.driver.name=\"org.h2.Driver\",field.length=\"symbol:100\") @PrimaryKey(\"symbol\") @Index(\"symbol\") define table StockTable (symbol string, price float, volume long); define stream InputStream (symbol string, volume long); from InputStream as a join StockTable as b on str:contains(b.symbol, a.symbol) select a.symbol as symbol, b.volume as volume insert into FooStream; The above example creates an event table named 'StockTable' in the database if it does not already exist (with three attributes named 'symbol', 'price', and 'volume' of the types 'string', 'float' and 'long' respectively). Then the table is joined with a stream named 'InputStream' based on a condition. The following operations are included in the condition: [ AND, OR, Comparisons( = = == !=), IS NULL, NOT, str:contains(Table Column , Stream Attribute or Search.String)] EXAMPLE 3 @Store(type=\"rdbms\", jdbc.url=\"jdbc:mysql://localhost:3306/das\", table.name=\"StockTable\", username=\"root\", password=\"root\" , jdbc.driver.name=\"org.h2.Driver\", field.length=\"symbol:100\", table.check.query=\"SELECT 1 FROM StockTable LIMIT 1\") @PrimaryKey(\"symbol\") @Index(\"symbol\") define table StockTable (symbol string, price float, volume long); define stream InputStream (symbol string, volume long); from InputStream as a join StockTable as b on str:contains(b.symbol, a.symbol) select a.symbol as symbol, b.volume as volume insert into FooStream; The above example creates an event table named 'StockTable' in the database if it does not already exist (with three attributes named 'symbol', 'price', and 'volume' of the types 'string', 'float' and 'long' respectively). Then the table is joined with a stream named 'InputStream' based on a condition. The following operations are included in the condition: [ AND, OR, Comparisons( = = == !=), IS NULL, NOT, str:contains(Table Column , Stream Attribute or Search.String)]","title":"rdbms (Store)"},{"location":"docs/api/latest/#redis-store","text":"This extension assigns data source and connection instructions to event tables. It also implements read write operations on connected datasource. This extension only can be used to read the data which persisted using the same extension since unique implementation has been used to map the relational data in to redis's key and value representation Origin: siddhi-store-redis:3.1.1 Syntax @Store(type=\"redis\", table.name=\" STRING \", cluster.mode=\" BOOL \", nodes=\" STRING \", ttl.seconds=\" LONG \", ttl.on.update=\" BOOL \", ttl.on.read=\" BOOL \") @PrimaryKey(\"PRIMARY_KEY\") @Index(\"INDEX\") QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic table.name The name with which the event table should be persisted in the store. If noname is specified via this parameter, the event table is persisted with the same name as the Siddhi table. The tale name defined in the siddhi app STRING Yes No cluster.mode This will decide the redis mode. if this is false, client will connect to a single redis node. false BOOL No No nodes host, port and the password of the node(s).In single node mode node details can be provided as follows- \"node='hosts:port@password'\" In clustered mode host and port of all the master nodes should be provided separated by a comma(,). As an example \"nodes = 'localhost:30001,localhost:30002'\". localhost:6379@root STRING Yes No ttl.seconds Time to live in seconds for each record -1 LONG Yes No ttl.on.update Set ttl on row update false BOOL Yes No ttl.on.read Set ttl on read rows false BOOL Yes No Examples EXAMPLE 1 @store(type='redis',nodes='localhost:6379@root',table.name='fooTable',cluster.mode=false)define table fooTable(time long, date String) Above example will create a redis table with the name fooTable and work on asingle redis node. EXAMPLE 2 @Store(type='redis', table.name='SweetProductionTable', nodes='localhost:30001,localhost:30002,localhost:30003', cluster.mode='true') @primaryKey('symbol') @index('price') define table SweetProductionTable (symbol string, price float, volume long); Above example demonstrate how to use the redis extension to connect in to redis cluster. Please note that, as nodes all the master node's host and port should be provided in order to work correctly. In clustered node password will not besupported EXAMPLE 3 @store(type='redis',nodes='localhost:6379@root',table.name='fooTable', ttl.seconds='30', ttl.onUpdate='true', ttl.onRead='true')define table fooTable(time long, date String) Above example will create a redis table with the name fooTable and work on asingle redis node. All rows inserted, updated or read will have its ttl set to 30 seconds","title":"redis (Store)"},{"location":"docs/api/latest/#str","text":"","title":"Str"},{"location":"docs/api/latest/#groupconcat-aggregate-function","text":"This function aggregates the received events by concatenating the keys in those events using a separator, e.g.,a comma (,) or a hyphen (-), and returns the concatenated key string. Origin: siddhi-execution-string:5.0.7 Syntax STRING str:groupConcat( STRING key) STRING str:groupConcat( STRING key, STRING ...) STRING str:groupConcat( STRING key, STRING separator, BOOL distinct) STRING str:groupConcat( STRING key, STRING separator, BOOL distinct, STRING order) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic key The string that needs to be aggregated. STRING No Yes separator The separator that separates each string key after concatenating the keys. , STRING Yes Yes distinct This is used to only have distinct values in the concatenated string that is returned. false BOOL Yes Yes order This parameter accepts 'ASC' or 'DESC' strings to sort the string keys in either ascending or descending order respectively. No order STRING Yes Yes Examples EXAMPLE 1 from InputStream#window.time(5 min) select str:groupConcat(\"key\") as groupedKeys input OutputStream; When we input events having values for the key as 'A' , 'B' , 'S' , 'C' , 'A' , it returns \"A,B,S,C,A\" to the 'OutputStream'. EXAMPLE 2 from InputStream#window.time(5 min) select groupConcat(\"key\",\"-\",true,\"ASC\") as groupedKeys input OutputStream; When we input events having values for the key as 'A' , 'B' , 'S' , 'C' , 'A' , specify the seperator as hyphen and choose the order to be ascending, the function returns \"A-B-C-S\" to the 'OutputStream'.","title":"groupConcat (Aggregate Function)"},{"location":"docs/api/latest/#charat-function","text":"This function returns the 'char' value that is present at the given index position. of the input string. Origin: siddhi-execution-string:5.0.7 Syntax STRING str:charAt( STRING input.value, INT index) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.value The input string of which the char value at the given position needs to be returned. STRING No Yes index The variable that specifies the index of the char value that needs to be returned. INT No Yes Examples EXAMPLE 1 charAt(\"WSO2\", 1) In this case, the functiion returns the character that exists at index 1. Hence, it returns 'S'.","title":"charAt (Function)"},{"location":"docs/api/latest/#coalesce-function_1","text":"This returns the first input parameter value of the given argument, that is not null. Origin: siddhi-execution-string:5.0.7 Syntax INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT str:coalesce( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg, INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT ...) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg It can have one or more input parameters in any data type. However, all the specified parameters are required to be of the same type. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No Yes Examples EXAMPLE 1 coalesce(null, \"BBB\", \"CCC\") This returns the first input parameter that is not null. In this example, it returns \"BBB\".","title":"coalesce (Function)"},{"location":"docs/api/latest/#concat-function","text":"This function returns a string value that is obtained as a result of concatenating two or more input string values. Origin: siddhi-execution-string:5.0.7 Syntax STRING str:concat( STRING arg, STRING ...) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg This can have two or more string type input parameters. STRING No Yes Examples EXAMPLE 1 concat(\"D533\", \"8JU^\", \"XYZ\") This returns a string value by concatenating two or more given arguments. In the example shown above, it returns \"D5338JU^XYZ\".","title":"concat (Function)"},{"location":"docs/api/latest/#contains-function_1","text":"This function returns true if the input.string contains the specified sequence of char values in the search.string . Origin: siddhi-execution-string:5.0.7 Syntax BOOL str:contains( STRING input.string, STRING search.string) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string Input string value. STRING No Yes search.string The string value to be searched for in the input.string . STRING No Yes Examples EXAMPLE 1 contains(\"21 products are produced by WSO2 currently\", \"WSO2\") This returns a boolean value as the output. In this case, it returns true .","title":"contains (Function)"},{"location":"docs/api/latest/#equalsignorecase-function","text":"This returns a boolean value by comparing two strings lexicographically without considering the letter case. Origin: siddhi-execution-string:5.0.7 Syntax BOOL str:equalsIgnoreCase( STRING arg1, STRING arg2) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg1 The first input string argument. STRING No Yes arg2 The second input string argument. This is compared with the first argument. STRING No Yes Examples EXAMPLE 1 equalsIgnoreCase(\"WSO2\", \"wso2\") This returns a boolean value as the output. In this scenario, it returns \"true\".","title":"equalsIgnoreCase (Function)"},{"location":"docs/api/latest/#filltemplate-function","text":"fillTemplate(string, map) will replace all the keys in the string using values in the map. fillTemplate(string, r1, r2 ..) replace all the entries {{1}}, {{2}}, {{3}} with r1 , r2, r3. Origin: siddhi-execution-string:5.0.7 Syntax STRING str:fillTemplate( STRING template, STRING|INT|LONG|DOUBLE|FLOAT|BOOL replacement.type, STRING|INT|LONG|DOUBLE|FLOAT|BOOL ...) STRING str:fillTemplate( STRING template, OBJECT map) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic template The string with templated fields that needs to be filled with the given strings. The format of the templated fields should be as follows: {{KEY}} where 'KEY' is a STRING if you are using fillTemplate(string, map) {{KEY}} where 'KEY' is an INT if you are using fillTemplate(string, r1, r2 ..) This KEY is used to map the values STRING No Yes replacement.type A set of arguments with any type string|int|long|double|float|bool. - STRING INT LONG DOUBLE FLOAT BOOL Yes Yes map A map with key-value pairs to be replaced. - OBJECT Yes Yes Examples EXAMPLE 1 str:fillTemplate(\"{{prize}} 100 {{salary}} 10000\", map:create('prize', 300, 'salary', 10000)) In this example, the template is '{{prize}} 100 {{salary}} 10000'.Here, the templated string {{prize}} is replaced with the value corresponding to the 'prize' key in the given map. Likewise salary replace with the salary value of the map EXAMPLE 2 str:fillTemplate(\"{{1}} 100 {{2}} 10000\", 200, 300) In this example, the template is '{{1}} 100 {{2}} 10000'.Here, the templated string {{1}} is replaced with the corresponding 1 st value 200. Likewise {{2}} replace with the 300","title":"fillTemplate (Function)"},{"location":"docs/api/latest/#hex-function_1","text":"This function returns a hexadecimal string by converting each byte of each character in the input string to two hexadecimal digits. Origin: siddhi-execution-string:5.0.7 Syntax STRING str:hex( STRING input.string) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string The input string to derive the hexadecimal value. STRING No Yes Examples EXAMPLE 1 hex(\"MySQL\") This returns the hexadecimal value of the input.string. In this scenario, the output is \"4d7953514c\".","title":"hex (Function)"},{"location":"docs/api/latest/#length-function","text":"Returns the length of the input string. Origin: siddhi-execution-string:5.0.7 Syntax INT str:length( STRING input.string) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string The input string to derive the length. STRING No Yes Examples EXAMPLE 1 length(\"Hello World\") This outputs the length of the provided string. In this scenario, the, output is 11 .","title":"length (Function)"},{"location":"docs/api/latest/#lower-function","text":"Converts the capital letters in the input string to the equivalent simple letters. Origin: siddhi-execution-string:5.0.7 Syntax STRING str:lower( STRING input.string) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string The input string to convert to the lower case (i.e., equivalent simple letters). STRING No Yes Examples EXAMPLE 1 lower(\"WSO2 cep \") This converts the capital letters in the input.string to the equivalent simple letters. In this scenario, the output is \"wso2 cep \".","title":"lower (Function)"},{"location":"docs/api/latest/#regexp-function","text":"Returns a boolean value based on the matchability of the input string and the given regular expression. Origin: siddhi-execution-string:5.0.7 Syntax BOOL str:regexp( STRING input.string, STRING regex) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string The input string to match with the given regular expression. STRING No Yes regex The regular expression to be matched with the input string. STRING No Yes Examples EXAMPLE 1 regexp(\"WSO2 abcdh\", \"WSO(.*h)\") This returns a boolean value after matching regular expression with the given string. In this scenario, it returns \"true\" as the output.","title":"regexp (Function)"},{"location":"docs/api/latest/#repeat-function","text":"Repeats the input string for a specified number of times. Origin: siddhi-execution-string:5.0.7 Syntax STRING str:repeat( STRING input.string, INT times) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string The input string that is repeated the number of times as defined by the user. STRING No Yes times The number of times the input.string needs to be repeated . INT No Yes Examples EXAMPLE 1 repeat(\"StRing 1\", 3) This returns a string value by repeating the string for a specified number of times. In this scenario, the output is \"StRing 1StRing 1StRing 1\".","title":"repeat (Function)"},{"location":"docs/api/latest/#replaceall-function_1","text":"Finds all the substrings of the input string that matches with the given expression, and replaces them with the given replacement string. Origin: siddhi-execution-string:5.0.7 Syntax STRING str:replaceAll( STRING input.string, STRING regex, STRING replacement.string) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string The input string to be replaced. STRING No Yes regex The regular expression to be matched with the input string. STRING No Yes replacement.string The string with which each substring that matches the given expression should be replaced. STRING No Yes Examples EXAMPLE 1 replaceAll(\"hello hi hello\", 'hello', 'test') This returns a string after replacing the substrings of the input string with the replacement string. In this scenario, the output is \"test hi test\" .","title":"replaceAll (Function)"},{"location":"docs/api/latest/#replacefirst-function","text":"Finds the first substring of the input string that matches with the given regular expression, and replaces itwith the given replacement string. Origin: siddhi-execution-string:5.0.7 Syntax STRING str:replaceFirst( STRING input.string, STRING regex, STRING replacement.string) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string The input string that should be replaced. STRING No Yes regex The regular expression with which the input string should be matched. STRING No Yes replacement.string The string with which the first substring of input string that matches the regular expression should be replaced. STRING No Yes Examples EXAMPLE 1 replaceFirst(\"hello WSO2 A hello\", 'WSO2(.*)A', 'XXXX') This returns a string after replacing the first substring with the given replacement string. In this scenario, the output is \"hello XXXX hello\".","title":"replaceFirst (Function)"},{"location":"docs/api/latest/#reverse-function","text":"Returns the input string in the reverse order character-wise and string-wise. Origin: siddhi-execution-string:5.0.7 Syntax STRING str:reverse( STRING input.string) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string The input string to be reversed. STRING No Yes Examples EXAMPLE 1 reverse(\"Hello World\") This outputs a string value by reversing the incoming input.string . In this scenario, the output is \"dlroW olleH\".","title":"reverse (Function)"},{"location":"docs/api/latest/#split-function","text":"Splits the input.string into substrings using the value parsed in the split.string and returns the substring at the position specified in the group.number . Origin: siddhi-execution-string:5.0.7 Syntax STRING str:split( STRING input.string, STRING split.string, INT group.number) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string The input string to be replaced. STRING No Yes split.string The string value to be used to split the input.string . STRING No Yes group.number The index of the split group INT No Yes Examples EXAMPLE 1 split(\"WSO2,ABM,NSFT\", \",\", 0) This splits the given input.string by given split.string and returns the string in the index given by group.number. In this scenario, the output will is \"WSO2\".","title":"split (Function)"},{"location":"docs/api/latest/#strcmp-function","text":"Compares two strings lexicographically and returns an integer value. If both strings are equal, 0 is returned. If the first string is lexicographically greater than the second string, a positive value is returned. If the first string is lexicographically greater than the second string, a negative value is returned. Origin: siddhi-execution-string:5.0.7 Syntax INT str:strcmp( STRING arg1, STRING arg2) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg1 The first input string argument. STRING No Yes arg2 The second input string argument that should be compared with the first argument lexicographically. STRING No Yes Examples EXAMPLE 1 strcmp(\"AbCDefghiJ KLMN\", 'Hello') This compares two strings lexicographically and outputs an integer value.","title":"strcmp (Function)"},{"location":"docs/api/latest/#substr-function","text":"Returns a substring of the input string by considering a subset or all of the following factors: starting index, length, regular expression, and regex group number. Origin: siddhi-execution-string:5.0.7 Syntax STRING str:substr( STRING input.string, INT begin.index) STRING str:substr( STRING input.string, INT begin.index, INT length) STRING str:substr( STRING input.string, STRING regex) STRING str:substr( STRING input.string, STRING regex, INT group.number) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string The input string to be processed. STRING No Yes begin.index Starting index to consider for the substring. - INT Yes Yes length The length of the substring. input.string .length - begin.index INT Yes Yes regex The regular expression that should be matched with the input string. - STRING Yes Yes group.number The regex group number 0 INT Yes Yes Examples EXAMPLE 1 substr(\"AbCDefghiJ KLMN\", 4) This outputs the substring based on the given begin.index . In this scenario, the output is \"efghiJ KLMN\". EXAMPLE 2 substr(\"AbCDefghiJ KLMN\", 2, 4) This outputs the substring based on the given begin.index and length. In this scenario, the output is \"CDef\". EXAMPLE 3 substr(\"WSO2D efghiJ KLMN\", '^WSO2(.*)') This outputs the substring by applying the regex. In this scenario, the output is \"WSO2D efghiJ KLMN\". EXAMPLE 4 substr(\"WSO2 cep WSO2 XX E hi hA WSO2 heAllo\", 'WSO2(.*)A(.*)', 2) This outputs the substring by applying the regex and considering the group.number . In this scenario, the output is \" ello\".","title":"substr (Function)"},{"location":"docs/api/latest/#trim-function","text":"Returns a copy of the input string without the leading and trailing whitespace (if any). Origin: siddhi-execution-string:5.0.7 Syntax STRING str:trim( STRING input.string) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string The input string that needs to be trimmed. STRING No Yes Examples EXAMPLE 1 trim(\" AbCDefghiJ KLMN \") This returns a copy of the input.string with the leading and/or trailing white-spaces omitted. In this scenario, the output is \"AbCDefghiJ KLMN\".","title":"trim (Function)"},{"location":"docs/api/latest/#unhex-function","text":"Returns a string by converting the hexadecimal characters in the input string. Origin: siddhi-execution-string:5.0.7 Syntax STRING str:unhex( STRING input.string) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string The hexadecimal input string that needs to be converted to string. STRING No Yes Examples EXAMPLE 1 unhex(\"4d7953514c\") This converts the hexadecimal value to string.","title":"unhex (Function)"},{"location":"docs/api/latest/#upper-function","text":"Converts the simple letters in the input string to the equivalent capital/block letters. Origin: siddhi-execution-string:5.0.7 Syntax STRING str:upper( STRING input.string) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string The input string that should be converted to the upper case (equivalent capital/block letters). STRING No Yes Examples EXAMPLE 1 upper(\"Hello World\") This converts the simple letters in the input.string to theequivalent capital letters. In this scenario, the output is \"HELLO WORLD\".","title":"upper (Function)"},{"location":"docs/api/latest/#tokenize-stream-processor_3","text":"This function splits the input string into tokens using a given regular expression and returns the split tokens. Origin: siddhi-execution-string:5.0.7 Syntax str:tokenize( STRING input.string, STRING regex) str:tokenize( STRING input.string, STRING regex, BOOL distinct) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input.string The input string which needs to be split. STRING No Yes regex The string value which is used to tokenize the 'input.string'. STRING No Yes distinct This flag is used to return only distinct values. false BOOL Yes Yes Extra Return Attributes Name Description Possible Types token The attribute which contains a single token. STRING Examples EXAMPLE 1 define stream inputStream (str string); @info(name = 'query1') from inputStream#str:tokenize(str , ',') select token insert into outputStream; This query performs tokenization on the given string. If the str is \"Android,Windows8,iOS\", then the string is split into 3 events containing the token attribute values, i.e., Android , Windows8 and iOS .","title":"tokenize (Stream Processor)"},{"location":"docs/api/latest/#time","text":"","title":"Time"},{"location":"docs/api/latest/#currentdate-function","text":"Function returns the system time in yyyy-MM-dd format. Origin: siddhi-execution-time:5.0.4 Syntax STRING time:currentDate() Examples EXAMPLE 1 time:currentDate() Returns the current date in the yyyy-MM-dd format, such as 2019-06-21 .","title":"currentDate (Function)"},{"location":"docs/api/latest/#currenttime-function","text":"Function returns system time in the HH ss format. Origin: siddhi-execution-time:5.0.4 Syntax STRING time:currentTime() Examples EXAMPLE 1 time:currentTime() Returns the current date in the HH ss format, such as 15:23:24 .","title":"currentTime (Function)"},{"location":"docs/api/latest/#currenttimestamp-function","text":"When no argument is provided, function returns the system current timestamp in yyyy-MM-dd HH ss format, and when a timezone is provided as an argument, it converts and return the current system time to the given timezone format. Origin: siddhi-execution-time:5.0.4 Syntax STRING time:currentTimestamp() STRING time:currentTimestamp( STRING timezone) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic timezone The timezone to which the current time need to be converted. For example, Asia/Kolkata , PST . Get the supported timezone IDs from here System timezone STRING Yes No Examples EXAMPLE 1 time:currentTimestamp() Returns current system time in yyyy-MM-dd HH ss format, such as 2019-03-31 14:07:00 . EXAMPLE 2 time:currentTimestamp('Asia/Kolkata') Returns current system time converted to 'Asia/Kolkata' timezone yyyy-MM-dd HH ss format, such as 2019-03-31 19:07:00 . Get the supported timezone IDs from here EXAMPLE 3 time:currentTimestamp('CST') Returns current system time converted to 'CST' timezone yyyy-MM-dd HH ss format, such as 2019-03-31 02:07:00 . Get the supported timezone IDs from here","title":"currentTimestamp (Function)"},{"location":"docs/api/latest/#date-function","text":"Extracts the date part of a date or date-time and return it in yyyy-MM-dd format. Origin: siddhi-execution-time:5.0.4 Syntax STRING time:date( STRING date.value, STRING date.format) STRING time:date( STRING date.value) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic date.value The value of the date. For example, 2014-11-11 13:23:44.657 , 2014-11-11 , 13:23:44.657 . STRING No Yes date.format The format of the date value provided. For example, yyyy/MM/dd HH ss.SSS . yyyy-MM-dd HH:mm:ss.SSS STRING Yes Yes Examples EXAMPLE 1 time:date('2014/11/11 13:23:44', 'yyyy/MM/dd HH:mm:ss') Extracts the date and returns 2014-11-11 . EXAMPLE 2 time:date('2014-11-23 13:23:44.345') Extracts the date and returns 2014-11-13 . EXAMPLE 3 time:date('13:23:44', 'HH:mm:ss') Extracts the date and returns 1970-01-01 .","title":"date (Function)"},{"location":"docs/api/latest/#dateadd-function","text":"Adds the specified time interval to a date. Origin: siddhi-execution-time:5.0.4 Syntax STRING time:dateAdd( STRING date.value, INT expr, STRING unit) STRING time:dateAdd( LONG timestamp.in.milliseconds, INT expr, STRING unit) STRING time:dateAdd( STRING date.value, INT expr, STRING unit, STRING date.format) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic date.value The value of the date. For example, 2014-11-11 13:23:44.657 , 2014-11-11 , 13:23:44.657 . - STRING Yes Yes expr The amount by which the selected part of the date should be incremented. For example 2 , 5 , 10 , etc. INT No Yes unit This is the part of the date that needs to be modified. For example, MINUTE , HOUR , MONTH , YEAR , QUARTER , WEEK , DAY , SECOND . STRING No No date.format The format of the date value provided. For example, yyyy-MM-dd HH ss.SSS . yyyy-MM-dd HH:mm:ss.SSS STRING Yes Yes timestamp.in.milliseconds The date value in milliseconds. For example, 1415712224000L . - LONG Yes Yes Examples EXAMPLE 1 time:dateAdd('2014-11-11 13:23:44.657', 5, 'YEAR', 'yyyy-MM-dd HH:mm:ss.SSS') Adds five years to the given date value and returns 2019-11-11 13:23:44.657 . EXAMPLE 2 time:dateAdd('2014-11-11 13:23:44.657', 5, 'YEAR') Adds five years to the given date value and returns 2019-11-11 13:23:44.657 using the default date.format yyyy-MM-dd HH ss.SSS . EXAMPLE 3 time:dateAdd( 1415712224000L, 1, 'HOUR') Adds one hour and 1415715824000 as a string .","title":"dateAdd (Function)"},{"location":"docs/api/latest/#datediff-function","text":"Returns difference between two dates in days. Origin: siddhi-execution-time:5.0.4 Syntax INT time:dateDiff( STRING date.value1, STRING date.value2, STRING date.format1, STRING date.format2) INT time:dateDiff( STRING date.value1, STRING date.value2) INT time:dateDiff( LONG timestamp.in.milliseconds1, LONG timestamp.in.milliseconds2) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic date.value1 The value of the first date parameter. For example, 2014-11-11 13:23:44.657 , 2014-11-11 , 13:23:44.657 . - STRING Yes Yes date.value2 The value of the second date parameter. For example, 2014-11-11 13:23:44.657 , 2014-11-11 , 13:23:44.657 . - STRING Yes Yes date.format1 The format of the first date value provided. For example, yyyy-MM-dd HH ss.SSS . yyyy-MM-dd HH:mm:ss.SSS STRING Yes Yes date.format2 The format of the second date value provided. For example, yyyy-MM-dd HH ss.SSS . yyyy-MM-dd HH:mm:ss.SSS STRING Yes Yes timestamp.in.milliseconds1 The first date value in milliseconds from the epoch. For example, 1415712224000L . - LONG Yes Yes timestamp.in.milliseconds2 The second date value in milliseconds from the epoch. For example, 1415712224000L . - LONG Yes Yes Examples EXAMPLE 1 time:dateDiff('2014-11-11 13:23:44', '2014-11-9 13:23:44', 'yyyy-MM-dd HH:mm:ss', 'yyyy-MM-dd HH:mm:ss') Returns the date difference between the two given dates as 2 . EXAMPLE 2 time:dateDiff('2014-11-13 13:23:44', '2014-11-9 13:23:44') Returns the date difference between the two given dates as 4 . EXAMPLE 3 time:dateDiff(1415692424000L, 1412841224000L) Returns the date difference between the two given dates as 33 .","title":"dateDiff (Function)"},{"location":"docs/api/latest/#dateformat-function","text":"Formats the data in string or milliseconds format to the given date format. Origin: siddhi-execution-time:5.0.4 Syntax STRING time:dateFormat( STRING date.value, STRING date.target.format, STRING date.source.format) STRING time:dateFormat( STRING date.value, STRING date.target.format) STRING time:dateFormat( LONG timestamp.in.milliseconds, STRING date.target.format) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic date.value The value of the date. For example, 2014-11-11 13:23:44.657 , 2014-11-11 , 13:23:44.657 . - STRING Yes Yes date.target.format The format of the date into which the date value needs to be converted. For example, yyyy/MM/dd HH ss . STRING No Yes date.source.format The format input date.value.For example, yyyy-MM-dd HH ss.SSS . yyyy-MM-dd HH:mm:ss.SSS STRING Yes Yes timestamp.in.milliseconds The date value in milliseconds from the epoch. For example, 1415712224000L . - LONG Yes Yes Examples EXAMPLE 1 time:dateFormat('2014/11/11 13:23:44', 'mm:ss', 'yyyy/MM/dd HH:mm:ss') Converts date based on the target date format mm:ss and returns 23:44 . EXAMPLE 2 time:dateFormat('2014-11-11 13:23:44', 'HH:mm:ss') Converts date based on the target date format HH ss and returns 13:23:44 . EXAMPLE 3 time:dateFormat(1415692424000L, 'yyyy-MM-dd') Converts date in millisecond based on the target date format yyyy-MM-dd and returns 2014-11-11 .","title":"dateFormat (Function)"},{"location":"docs/api/latest/#datesub-function","text":"Subtracts the specified time interval from the given date. Origin: siddhi-execution-time:5.0.4 Syntax STRING time:dateSub( STRING date.value, INT expr, STRING unit) STRING time:dateSub( STRING date.value, INT expr, STRING unit, STRING date.format) STRING time:dateSub( LONG timestamp.in.milliseconds, INT expr, STRING unit) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic date.value The value of the date. For example, 2014-11-11 13:23:44.657 , 2014-11-11 , 13:23:44.657 . - STRING Yes Yes expr The amount by which the selected part of the date should be decremented. For example 2 , 5 , 10 , etc. INT No Yes unit This is the part of the date that needs to be modified. For example, MINUTE , HOUR , MONTH , YEAR , QUARTER , WEEK , DAY , SECOND . STRING No No date.format The format of the date value provided. For example, yyyy-MM-dd HH ss.SSS . yyyy-MM-dd HH:mm:ss.SSS STRING Yes Yes timestamp.in.milliseconds The date value in milliseconds. For example, 1415712224000L . - LONG Yes Yes Examples EXAMPLE 1 time:dateSub('2019-11-11 13:23:44.657', 5, 'YEAR', 'yyyy-MM-dd HH:mm:ss.SSS') Subtracts five years to the given date value and returns 2014-11-11 13:23:44.657 . EXAMPLE 2 time:dateSub('2019-11-11 13:23:44.657', 5, 'YEAR') Subtracts five years to the given date value and returns 2014-11-11 13:23:44.657 using the default date.format yyyy-MM-dd HH ss.SSS . EXAMPLE 3 time:dateSub( 1415715824000L, 1, 'HOUR') Subtracts one hour and 1415712224000 as a string .","title":"dateSub (Function)"},{"location":"docs/api/latest/#dayofweek-function","text":"Extracts the day on which a given date falls. Origin: siddhi-execution-time:5.0.4 Syntax STRING time:dayOfWeek( STRING date.value, STRING date.format) STRING time:dayOfWeek( STRING date.value) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic date.value The value of the date. For example, 2014-11-11 13:23:44.657 , 2014-11-11 , 13:23:44.657 . STRING No Yes date.format The format of the date value provided. For example, yyyy/MM/dd HH ss.SSS . yyyy-MM-dd HH:mm:ss.SSS STRING Yes Yes Examples EXAMPLE 1 time:date('2014/12/11 13:23:44', 'yyyy/MM/dd HH:mm:ss') Extracts the date and returns Thursday . EXAMPLE 2 time:date('2014-11-11 13:23:44.345') Extracts the date and returns Tuesday .","title":"dayOfWeek (Function)"},{"location":"docs/api/latest/#extract-function","text":"Function extracts a date unit from the date. Origin: siddhi-execution-time:5.0.4 Syntax INT time:extract( STRING unit, STRING date.value) INT time:extract( STRING unit, STRING date.value, STRING date.format) INT time:extract( STRING unit, STRING date.value, STRING date.format, STRING locale) INT time:extract( LONG timestamp.in.milliseconds, STRING unit) INT time:extract( LONG timestamp.in.milliseconds, STRING unit, STRING locale) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic unit This is the part of the date that needs to be modified. For example, MINUTE , HOUR , MONTH , YEAR , QUARTER , WEEK , DAY , SECOND . STRING No No date.value The value of the date. For example, 2014-11-11 13:23:44.657 , 2014-11-11 , 13:23:44.657 . - STRING Yes Yes date.format The format of the date value provided. For example, yyyy-MM-dd HH ss.SSS . yyyy-MM-dd HH:mm:ss.SSS STRING Yes Yes timestamp.in.milliseconds The date value in milliseconds. For example, 1415712224000L . - LONG Yes Yes locale Represents a specific geographical, political or cultural region. For example en_US and fr_FR Current default locale set in the Java Virtual Machine. STRING Yes No Examples EXAMPLE 1 time:extract('YEAR', '2019/11/11 13:23:44.657', 'yyyy/MM/dd HH:mm:ss.SSS') Extracts the year amount and returns 2019 . EXAMPLE 2 time:extract('DAY', '2019-11-12 13:23:44.657') Extracts the day amount and returns 12 . EXAMPLE 3 time:extract(1394556804000L, 'HOUR') Extracts the hour amount and returns 22 .","title":"extract (Function)"},{"location":"docs/api/latest/#timestampinmilliseconds-function","text":"Returns the system time or the given time in milliseconds. Origin: siddhi-execution-time:5.0.4 Syntax LONG time:timestampInMilliseconds() LONG time:timestampInMilliseconds( STRING date.value, STRING date.format) LONG time:timestampInMilliseconds( STRING date.value) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic date.value The value of the date. For example, 2014-11-11 13:23:44.657 , 2014-11-11 , 13:23:44.657 . Current system time STRING Yes Yes date.format The format of the date value provided. For example, yyyy/MM/dd HH ss.SSS . yyyy-MM-dd HH:mm:ss.SSS STRING Yes Yes Examples EXAMPLE 1 time:timestampInMilliseconds() Returns the system current time in milliseconds. EXAMPLE 2 time:timestampInMilliseconds('2007-11-30 10:30:19', 'yyyy-MM-DD HH:MM:SS') Converts 2007-11-30 10:30:19 in yyyy-MM-DD HH:MM:SS format to milliseconds as 1170131400019 . EXAMPLE 3 time:timestampInMilliseconds('2007-11-30 10:30:19.000') Converts 2007-11-30 10:30:19 in yyyy-MM-DD HH:MM:ss.SSS format to milliseconds as 1196398819000 .","title":"timestampInMilliseconds (Function)"},{"location":"docs/api/latest/#utctimestamp-function","text":"Function returns the system current time in UTC timezone with yyyy-MM-dd HH ss format. Origin: siddhi-execution-time:5.0.4 Syntax STRING time:utcTimestamp() Examples EXAMPLE 1 time:utcTimestamp() Returns the system current time in UTC timezone with yyyy-MM-dd HH ss format, and a sample output will be like 2019-07-03 09:58:34 .","title":"utcTimestamp (Function)"},{"location":"docs/api/latest/#unique","text":"","title":"Unique"},{"location":"docs/api/latest/#deduplicate-stream-processor","text":"Removes duplicate events based on the unique.key parameter that arrive within the time.interval gap from one another. Origin: siddhi-execution-unique:5.0.5 Syntax unique:deduplicate( INT|LONG|FLOAT|BOOL|DOUBLE|STRING unique.key, INT|LONG time.interval) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic unique.key Parameter to uniquely identify events. INT LONG FLOAT BOOL DOUBLE STRING No Yes time.interval The sliding time period within which the duplicate events are dropped. INT LONG No No Examples EXAMPLE 1 define stream TemperatureStream (sensorId string, temperature double) from TemperatureStream#unique:deduplicate(sensorId, 30 sec) select * insert into UniqueTemperatureStream; Query that removes duplicate events of TemperatureStream stream based on sensorId attribute when they arrive within 30 seconds.","title":"deduplicate (Stream Processor)"},{"location":"docs/api/latest/#ever-window","text":"Window that retains the latest events based on a given unique keys. When a new event arrives with the same key it replaces the one that exist in the window. b This function is not recommended to be used when the maximum number of unique attributes are undefined, as there is a risk of system going out to memory /b . Origin: siddhi-execution-unique:5.0.5 Syntax unique:ever( INT|LONG|FLOAT|BOOL|DOUBLE|STRING unique.key) unique:ever( INT|LONG|FLOAT|BOOL|DOUBLE|STRING unique.key, INT|LONG|FLOAT|BOOL|DOUBLE|STRING ...) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic unique.key The attribute used to checked for uniqueness. INT LONG FLOAT BOOL DOUBLE STRING No Yes Examples EXAMPLE 1 define stream LoginEvents (timestamp long, ip string); from LoginEvents#window.unique:ever(ip) select count(ip) as ipCount insert events into UniqueIps; Query collects all unique events based on the ip attribute by retaining the latest unique events from the LoginEvents stream. Then the query counts the unique ip s arrived so far and outputs the ipCount via the UniqueIps stream. EXAMPLE 2 define stream DriverChangeStream (trainID string, driver string); from DriverChangeStream#window.unique:ever(trainID) select trainID, driver insert expired events into PreviousDriverChangeStream; Query collects all unique events based on the trainID attribute by retaining the latest unique events from the DriverChangeStream stream. The query outputs the previous unique event stored in the window as the expired events are emitted via PreviousDriverChangeStream stream. EXAMPLE 3 define stream StockStream (symbol string, price float); define stream PriceRequestStream(symbol string); from StockStream#window.unique:ever(symbol) as s join PriceRequestStream as p on s.symbol == p.symbol select s.symbol as symbol, s.price as price insert events into PriceResponseStream; Query stores the last unique event for each symbol attribute of StockStream stream, and joins them with events arriving on the PriceRequestStream for equal symbol attributes to fetch the latest price for each requested symbol and output via PriceResponseStream stream.","title":"ever (Window)"},{"location":"docs/api/latest/#externaltimebatch-window_1","text":"This is a batch (tumbling) time window that is determined based on an external time, i.e., time stamps that are specified via an attribute in the events. It holds the latest unique events that arrived during the last window time period. The unique events are determined based on the value for a specified unique key parameter. When a new event arrives within the time window with a value for the unique key parameter that is the same as that of an existing event in the window, the existing event expires and it is replaced by the new event. Origin: siddhi-execution-unique:5.0.5 Syntax unique:externalTimeBatch( INT|LONG|FLOAT|BOOL|DOUBLE|STRING unique.key, LONG time.stamp, INT|LONG window.time) unique:externalTimeBatch( INT|LONG|FLOAT|BOOL|DOUBLE|STRING unique.key, LONG time.stamp, INT|LONG window.time, INT start.time) unique:externalTimeBatch( INT|LONG|FLOAT|BOOL|DOUBLE|STRING unique.key, LONG time.stamp, INT|LONG window.time, INT start.time, INT|LONG time.out) unique:externalTimeBatch( INT|LONG|FLOAT|BOOL|DOUBLE|STRING unique.key, LONG time.stamp, INT|LONG window.time, INT start.time, INT|LONG time.out, BOOL replace.time.stamp.with.batch.end.time) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic unique.key The attribute that should be checked for uniqueness. INT LONG FLOAT BOOL DOUBLE STRING No Yes time.stamp The time which the window determines as the current time and acts upon. The value of this parameter should be monotonically increasing. LONG No Yes window.time The sliding time period for which the window should hold events. INT LONG No No start.time This specifies an offset in milliseconds in order to start the window at a time different to the standard time. Timestamp of first event INT Yes No time.out Time to wait for arrival of a new event, before flushing and returning the output for events belonging to a specific batch. The system waits till an event from the next batch arrives to flush the current batch INT LONG Yes No replace.time.stamp.with.batch.end.time Replaces the 'timestamp' value with the corresponding batch end time stamp. false BOOL Yes No Examples EXAMPLE 1 define stream LoginEvents (timestamp long, ip string); from LoginEvents#window.unique:externalTimeBatch(ip, timestamp, 1 sec, 0, 2 sec) select timestamp, ip, count() as total insert into UniqueIps ; In this query, the window holds the latest unique events that arrive from the 'LoginEvent' stream during each second. The latest events are determined based on the external time stamp. At a given time, all the events held in the window have unique values for the 'ip' and monotonically increasing values for 'timestamp' attributes. The events in the window are inserted into the 'UniqueIps' output stream. The system waits for 2 seconds for the arrival of a new event before flushing the current batch.","title":"externalTimeBatch (Window)"},{"location":"docs/api/latest/#first-window","text":"This is a window that holds only the first set of unique events according to the unique key parameter. When a new event arrives with a key that is already in the window, that event is not processed by the window. Origin: siddhi-execution-unique:5.0.5 Syntax unique:first( INT|LONG|FLOAT|BOOL|DOUBLE|STRING unique.key) unique:first( INT|LONG|FLOAT|BOOL|DOUBLE|STRING unique.key, INT|LONG|FLOAT|BOOL|DOUBLE|STRING ...) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic unique.key The attribute that should be checked for uniqueness. If there is more than one parameter to check for uniqueness, it can be specified as an array separated by commas. INT LONG FLOAT BOOL DOUBLE STRING No Yes Examples EXAMPLE 1 define stream LoginEvents (timeStamp long, ip string); from LoginEvents#window.unique:first(ip) insert into UniqueIps ; This returns the first set of unique items that arrive from the 'LoginEvents' stream, and returns them to the 'UniqueIps' stream. The unique events are only those with a unique value for the 'ip' attribute.","title":"first (Window)"},{"location":"docs/api/latest/#firstlengthbatch-window","text":"This is a batch (tumbling) window that holds a specific number of unique events (depending on which events arrive first). The unique events are selected based on a specific parameter that is considered as the unique key. When a new event arrives with a value for the unique key parameter that matches the same of an existing event in the window, that event is not processed by the window. Origin: siddhi-execution-unique:5.0.5 Syntax unique:firstLengthBatch( INT|LONG|FLOAT|BOOL|DOUBLE|STRING unique.key, INT window.length) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic unique.key The attribute that should be checked for uniqueness. INT LONG FLOAT BOOL DOUBLE STRING No Yes window.length The number of events the window should tumble. INT No No Examples EXAMPLE 1 define window CseEventWindow (symbol string, price float, volume int) from CseEventStream#window.unique:firstLengthBatch(symbol, 10) select symbol, price, volume insert all events into OutputStream ; The window in this configuration holds the first unique events from the 'CseEventStream' stream every second, and outputs them all into the the 'OutputStream' stream. All the events in a window during a given second should have a unique value for the 'symbol' attribute.","title":"firstLengthBatch (Window)"},{"location":"docs/api/latest/#firsttimebatch-window","text":"A batch-time or tumbling window that holds the unique events according to the unique key parameters that have arrived within the time period of that window and gets updated for each such time window. When a new event arrives with a key which is already in the window, that event is not processed by the window. Origin: siddhi-execution-unique:5.0.5 Syntax unique:firstTimeBatch( INT|LONG|FLOAT|BOOL|DOUBLE|STRING unique.key, INT|LONG window.time) unique:firstTimeBatch( INT|LONG|FLOAT|BOOL|DOUBLE|STRING unique.key, INT|LONG window.time, INT|LONG start.time) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic unique.key The attribute that should be checked for uniqueness. INT LONG FLOAT BOOL DOUBLE STRING No Yes window.time The sliding time period for which the window should hold events. INT LONG No No start.time This specifies an offset in milliseconds in order to start the window at a time different to the standard time. Timestamp of the first event. INT LONG Yes No Examples EXAMPLE 1 define stream CseEventStream (symbol string, price float, volume int) from CseEventStream#window.unique:firstTimeBatch(symbol,1 sec) select symbol, price, volume insert all events into OutputStream ; This holds the first unique events that arrive from the 'cseEventStream' input stream during each second, based on the symbol,as a batch, and returns all the events to the 'OutputStream'.","title":"firstTimeBatch (Window)"},{"location":"docs/api/latest/#length-window_1","text":"This is a sliding length window that holds the events of the latest window length with the unique key and gets updated for the expiry and arrival of each event. When a new event arrives with the key that is already there in the window, then the previous event expires and new event is kept within the window. Origin: siddhi-execution-unique:5.0.5 Syntax unique:length( INT|LONG|FLOAT|BOOL|DOUBLE|STRING unique.key, INT window.length) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic unique.key The attribute that should be checked for uniqueness. INT LONG FLOAT BOOL DOUBLE STRING No Yes window.length The number of events that should be included in a sliding length window. INT No No Examples EXAMPLE 1 define stream CseEventStream (symbol string, price float, volume int) from CseEventStream#window.unique:length(symbol,10) select symbol, price, volume insert all events into OutputStream; In this configuration, the window holds the latest 10 unique events. The latest events are selected based on the symbol attribute. If the 'CseEventStream' receives an event for which the value for the symbol attribute is the same as that of an existing event in the window, the existing event is replaced by the new event. All the events are returned to the 'OutputStream' event stream once an event expires or is added to the window.","title":"length (Window)"},{"location":"docs/api/latest/#lengthbatch-window_1","text":"This is a batch (tumbling) window that holds a specified number of latest unique events. The unique events are determined based on the value for a specified unique key parameter. The window is updated for every window length, i.e., for the last set of events of the specified number in a tumbling manner. When a new event arrives within the window length having the same value for the unique key parameter as an existing event in the window, the previous event is replaced by the new event. Origin: siddhi-execution-unique:5.0.5 Syntax unique:lengthBatch( INT|LONG|FLOAT|BOOL|DOUBLE|STRING unique.key, INT window.length) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic unique.key The attribute that should be checked for uniqueness. INT LONG FLOAT BOOL DOUBLE STRING No Yes window.length The number of events the window should tumble. INT No No Examples EXAMPLE 1 define window CseEventWindow (symbol string, price float, volume int) from CseEventStream#window.unique:lengthBatch(symbol, 10) select symbol, price, volume insert expired events into OutputStream ; In this query, the window at any give time holds the last 10 unique events from the 'CseEventStream' stream. Each of the 10 events within the window at a given time has a unique value for the symbol attribute. If a new event has the same value for the symbol attribute as an existing event within the window length, the existing event expires and it is replaced by the new event. The query returns expired individual events as well as expired batches of events to the 'OutputStream' stream.","title":"lengthBatch (Window)"},{"location":"docs/api/latest/#time-window_1","text":"This is a sliding time window that holds the latest unique events that arrived during the previous time window. The unique events are determined based on the value for a specified unique key parameter. The window is updated with the arrival and expiry of each event. When a new event that arrives within a window time period has the same value for the unique key parameter as an existing event in the window, the previous event is replaced by the new event. Origin: siddhi-execution-unique:5.0.5 Syntax unique:time( INT|LONG|FLOAT|BOOL|DOUBLE|STRING unique.key, INT|LONG window.time) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic unique.key The attribute that should be checked for uniqueness. INT LONG FLOAT BOOL DOUBLE STRING No Yes window.time The sliding time period for which the window should hold events. INT LONG No No Examples EXAMPLE 1 define stream CseEventStream (symbol string, price float, volume int) from CseEventStream#window.unique:time(symbol, 1 sec) select symbol, price, volume insert expired events into OutputStream ; In this query, the window holds the latest unique events that arrived within the last second from the 'CseEventStream', and returns the expired events to the 'OutputStream' stream. During any given second, each event in the window should have a unique value for the 'symbol' attribute. If a new event that arrives within the same second has the same value for the symbol attribute as an existing event in the window, the existing event expires.","title":"time (Window)"},{"location":"docs/api/latest/#timebatch-window_1","text":"This is a batch (tumbling) time window that is updated with the latest events based on a unique key parameter. If a new event that arrives within the time period of a windowhas a value for the key parameter which matches that of an existing event, the existing event expires and it is replaced by the latest event. Origin: siddhi-execution-unique:5.0.5 Syntax unique:timeBatch( INT|LONG|FLOAT|BOOL|DOUBLE|STRING unique.key, INT|LONG window.time) unique:timeBatch( INT|LONG|FLOAT|BOOL|DOUBLE|STRING unique.key, INT|LONG window.time, INT|LONG start.time) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic unique.key The attribute that should be checked for uniqueness. INT LONG FLOAT BOOL DOUBLE STRING No Yes window.time The tumbling time period for which the window should hold events. INT LONG No No start.time This specifies an offset in milliseconds in order to start the window at a time different to the standard time. Timestamp of first event INT LONG Yes No Examples EXAMPLE 1 define stream CseEventStream (symbol string, price float, volume int) from CseEventStream#window.unique:timeBatch(symbol, 1 sec) select symbol, price, volume insert all events into OutputStream ; This window holds the latest unique events that arrive from the 'CseEventStream' at a given time, and returns all the events to the 'OutputStream' stream. It is updated every second based on the latest values for the 'symbol' attribute.","title":"timeBatch (Window)"},{"location":"docs/api/latest/#timelengthbatch-window","text":"This is a batch or tumbling time length window that is updated with the latest events based on a unique key parameter. The window tumbles upon the elapse of the time window, or when a number of unique events have arrived. If a new event that arrives within the period of the window has a value for the key parameter which matches the value of an existing event, the existing event expires and it is replaced by the new event. Origin: siddhi-execution-unique:5.0.5 Syntax unique:timeLengthBatch( INT|LONG|FLOAT|BOOL|DOUBLE|STRING unique.key, INT|LONG window.time, INT window.length) unique:timeLengthBatch( INT|LONG|FLOAT|BOOL|DOUBLE|STRING unique.key, INT|LONG window.time, INT|LONG start.time, INT window.length) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic unique.key The attribute that should be checked for uniqueness. INT LONG FLOAT BOOL DOUBLE STRING No Yes window.time The sliding time period for which the window should hold the events. INT LONG No No start.time This specifies an offset in milliseconds in order to start the window at a time different to the standard time. Timestamp of first event INT LONG Yes No window.length The number of events the window should tumble. INT No No Examples EXAMPLE 1 define stream CseEventStream (symbol string, price float, volume int) from CseEventStream#window.unique:timeLengthBatch(symbol, 1 sec, 20) select symbol, price, volume insert all events into OutputStream; This window holds the latest unique events that arrive from the 'CseEventStream' at a given time, and returns all the events to the 'OutputStream' stream. It is updated every second based on the latest values for the 'symbol' attribute.","title":"timeLengthBatch (Window)"},{"location":"docs/api/latest/#unitconversion","text":"","title":"Unitconversion"},{"location":"docs/api/latest/#mmtokm-function","text":"This converts the input given in megameters into kilometers. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:MmTokm( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from megameters into kilometers. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:MmTokm(1) The megameter value '1' is converted into kilometers as '1000.0' .","title":"MmTokm (Function)"},{"location":"docs/api/latest/#cmtoft-function","text":"This converts the input given in centimeters into feet. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:cmToft( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from centimeters into feet. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:cmToft(100) The centimeters value '100' is converted into feet as '3.280' .","title":"cmToft (Function)"},{"location":"docs/api/latest/#cmtoin-function","text":"This converts the input given in centimeters into inches. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:cmToin( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from centimeters into inches. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:cmToin(100) Input centimeters value '100' is converted into inches as '39.37'.","title":"cmToin (Function)"},{"location":"docs/api/latest/#cmtokm-function","text":"This converts the input value given in centimeters into kilometers. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:cmTokm( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from centimeters into kilometers. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:cmTokm(100) The centimeters value '100' is converted into kilometers as '0.001'.","title":"cmTokm (Function)"},{"location":"docs/api/latest/#cmtom-function","text":"This converts the input given in centimeters into meters. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:cmTom( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from centimeters into meters. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:cmTom(100) The centimeters value '100' is converted into meters as '1.0' .","title":"cmTom (Function)"},{"location":"docs/api/latest/#cmtomi-function","text":"This converts the input given in centimeters into miles. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:cmTomi( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from centimeters into miles. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:cmTomi(10000) The centimeters value '10000' is converted into miles as '0.062' .","title":"cmTomi (Function)"},{"location":"docs/api/latest/#cmtomm-function","text":"This converts the input given in centimeters into millimeters. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:cmTomm( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from centimeters into millimeters. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:cmTomm(1) The centimeter value '1' is converted into millimeters as '10.0' .","title":"cmTomm (Function)"},{"location":"docs/api/latest/#cmtonm-function","text":"This converts the input given in centimeters into nanometers. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:cmTonm( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from centimeters into nanometers. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:cmTonm(1) The centimeter value '1' is converted into nanometers as '10000000' .","title":"cmTonm (Function)"},{"location":"docs/api/latest/#cmtoum-function","text":"This converts the input in centimeters into micrometers. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:cmToum( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from centimeters into micrometers. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:cmToum(100) The centimeters value '100' is converted into micrometers as '1000000.0' .","title":"cmToum (Function)"},{"location":"docs/api/latest/#cmtoyd-function","text":"This converts the input given in centimeters into yards. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:cmToyd( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from centimeters into yards. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:cmToyd(1) The centimeter value '1' is converted into yards as '0.01' .","title":"cmToyd (Function)"},{"location":"docs/api/latest/#dtoh-function","text":"This converts the input given in days into hours. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:dToh( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from days into hours. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:dToh(1) The day value '1' is converted into hours as '24.0'.","title":"dToh (Function)"},{"location":"docs/api/latest/#gtokg-function","text":"This converts the input given in grams into kilograms. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:gTokg( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from grams into kilograms. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:gTokg(1000) The grams value '1000' is converted into kilogram as '1.0' .","title":"gTokg (Function)"},{"location":"docs/api/latest/#gtomg-function","text":"This converts the input given in grams into milligrams. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:gTomg( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from grams into milligrams. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:gTomg(1) The gram value '1' is converted into milligrams as '1000.0' .","title":"gTomg (Function)"},{"location":"docs/api/latest/#gtoug-function","text":"This converts the input given in grams into micrograms. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:gToug( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from grams into micrograms. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:gToug(1) The gram value '1' is converted into micrograms as '1000000.0' .","title":"gToug (Function)"},{"location":"docs/api/latest/#htom-function","text":"This converts the input given in hours into minutes. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:hTom( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from hours into minutes. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:hTom(1) The hour value '1' is converted into minutes as '60.0' .","title":"hTom (Function)"},{"location":"docs/api/latest/#htos-function","text":"This converts the input given in hours into seconds. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:hTos( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from hours into seconds. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:hTos(1) The hour value '1' is converted into seconds as '3600.0'.","title":"hTos (Function)"},{"location":"docs/api/latest/#kgtolt-function","text":"This converts the input given in kilograms into imperial tons. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:kgToLT( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from kilograms into imperial tons. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:kgToLT(1000) The kilograms value '1000' is converted into imperial tons as '0.9842' .","title":"kgToLT (Function)"},{"location":"docs/api/latest/#kgtost-function","text":"This converts the input given in kilograms into US tons. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:kgToST( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from kilograms into US tons. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:kgToST(1000) The kilograms value '1000 is converted into US tons as '1.10' .","title":"kgToST (Function)"},{"location":"docs/api/latest/#kgtog-function","text":"This converts the input given in kilograms into grams. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:kgTog( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from kilograms into grams. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:kgTog(1) The kilogram value '1' is converted into grams as '1000'.","title":"kgTog (Function)"},{"location":"docs/api/latest/#kgtolb-function","text":"This converts the input given in kilograms into pounds. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:kgTolb( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from kilograms into pounds. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:kgTolb(1) The kilogram value '1' is converted into pounds as '2.2' .","title":"kgTolb (Function)"},{"location":"docs/api/latest/#kgtooz-function","text":"This converts the input given in kilograms into ounces. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:kgTooz( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from kilograms into ounces. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:kgTooz(1) The kilogram value '1' is converted into ounces as ' 35.274' .","title":"kgTooz (Function)"},{"location":"docs/api/latest/#kgtost-function_1","text":"This converts the input given in kilograms into imperial stones. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:kgTost( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from kilograms into imperial stones. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:kgTost(1) The kilogram value '1' is converted into imperial stones as '0.157' .","title":"kgTost (Function)"},{"location":"docs/api/latest/#kgtot-function","text":"This converts the input given in kilograms into tonnes. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:kgTot( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from kilograms into tonnes. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:kgTot(1) The kilogram value '1' is converted into tonnes as '0.001' .","title":"kgTot (Function)"},{"location":"docs/api/latest/#kmtocm-function","text":"This converts the input given in kilometers into centimeters. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:kmTocm( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from kilometers into centimeters. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:kmTocm(1) The kilometer value '1' is converted into centimeters as '100000.0' .","title":"kmTocm (Function)"},{"location":"docs/api/latest/#kmtoft-function","text":"This converts the input given in kilometers into feet. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:kmToft( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from kilometers into feet. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:kmToft(1) The kilometer value '1' is converted into feet as '3280.8' .","title":"kmToft (Function)"},{"location":"docs/api/latest/#kmtoin-function","text":"This converts the input given in kilometers into inches. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:kmToin( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from kilometers into inches. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:kmToin(1) The kilometer value '1' is converted into inches as '39370.08' .","title":"kmToin (Function)"},{"location":"docs/api/latest/#kmtom-function","text":"This converts the input given in kilometers into meters. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:kmTom( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from kilometers into meters. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:kmTom(1) The kilometer value '1' is converted into meters as '1000.0' .","title":"kmTom (Function)"},{"location":"docs/api/latest/#kmtomi-function","text":"This converts the input given in kilometers into miles. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:kmTomi( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from kilometers into miles. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:kmTomi(1) The kilometer value '1' is converted into miles as '0.621' .","title":"kmTomi (Function)"},{"location":"docs/api/latest/#kmtomm-function","text":"This converts the input given in kilometers into millimeters. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:kmTomm( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from kilometers into millimeters. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:kmTomm(1) The kilometer value '1' is converted into millimeters as '1000000.0' .","title":"kmTomm (Function)"},{"location":"docs/api/latest/#kmtonm-function","text":"This converts the input given in kilometers into nanometers. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:kmTonm( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from kilometers into nanometers. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:kmTonm(1) The kilometer value '1' is converted into nanometers as '1000000000000.0' .","title":"kmTonm (Function)"},{"location":"docs/api/latest/#kmtoum-function","text":"This converts the input given in kilometers into micrometers. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:kmToum( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from kilometers into micrometers. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:kmToum(1) The kilometer value '1' is converted into micrometers as '1000000000.0' .","title":"kmToum (Function)"},{"location":"docs/api/latest/#kmtoyd-function","text":"This converts the input given in kilometers into yards. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:kmToyd( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from kilometers into yards. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:kmToyd(1) The kilometer value '1' is converted into yards as '1093.6' .","title":"kmToyd (Function)"},{"location":"docs/api/latest/#ltom3-function","text":"This converts the input given in liters into cubic meters. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:lTom3( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from liters into cubic meters. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:lTom3(1000) The liters value '1000' is converted into cubic meters as '1' .","title":"lTom3 (Function)"},{"location":"docs/api/latest/#ltoml-function","text":"This converts the input given in liters into milliliters. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:lToml( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from liters into milliliters. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:lToml(1) The liter value '1' is converted into milliliters as '1000.0' .","title":"lToml (Function)"},{"location":"docs/api/latest/#m3tol-function","text":"This converts the input given in cubic meters into liters. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:m3Tol( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from meters into liters. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:m3Tol(1) The cubic meter value '1' is converted into liters as '1000.0' .","title":"m3Tol (Function)"},{"location":"docs/api/latest/#mtocm-function","text":"This converts the input given in meters into centimeters. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:mTocm( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from meters into centimeters. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:mTocm(1) The meter value '1' is converted to centimeters as '100.0' .","title":"mTocm (Function)"},{"location":"docs/api/latest/#mtoft-function","text":"This converts the input given in meters into feet. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:mToft( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from meters into feet. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:mToft(1) The meter value '1' is converted into feet as '3.280' .","title":"mToft (Function)"},{"location":"docs/api/latest/#mtomm-function","text":"This converts the input given in meters into millimeters. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:mTomm( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from meters into millimeters. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:mTomm(1) The meter value '1' is converted into millimeters as '1000.0' .","title":"mTomm (Function)"},{"location":"docs/api/latest/#mtos-function","text":"This converts the input given in minutes into seconds. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:mTos( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from minutes into seconds. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:mTos(1) The minute value '1' is converted into seconds as '60.0' .","title":"mTos (Function)"},{"location":"docs/api/latest/#mtoyd-function","text":"This converts the input given in meters into yards. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:mToyd( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from meters into yards. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:mToyd(1) The meter value '1' is converted into yards as '1.093' .","title":"mToyd (Function)"},{"location":"docs/api/latest/#mitokm-function","text":"This converts the input given in miles into kilometers. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:miTokm( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from miles into kilometers. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:miTokm(1) The mile value '1' is converted into kilometers as '1.6' .","title":"miTokm (Function)"},{"location":"docs/api/latest/#mltol-function","text":"This converts the input given in milliliters into liters. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:mlTol( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from milliliters into liters. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:mlTol(1000) The milliliters value '1000' is converted into liters as '1'.","title":"mlTol (Function)"},{"location":"docs/api/latest/#stoms-function","text":"This converts the input given in seconds into milliseconds. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:sToms( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from seconds into milliseconds. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:sToms(1) The second value '1' is converted into milliseconds as '1000.0' .","title":"sToms (Function)"},{"location":"docs/api/latest/#stons-function","text":"This converts the input given in seconds into nanoseconds. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:sTons( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from seconds into nanoseconds. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:sTons(1) The second value '1' is converted into nanoseconds as '1000000000.0' .","title":"sTons (Function)"},{"location":"docs/api/latest/#stous-function","text":"This converts the input given in seconds into microseconds. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:sTous( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from seconds into microseconds. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:sTous(1) The second value '1' is converted into microseconds as '1000000.0' .","title":"sTous (Function)"},{"location":"docs/api/latest/#ttog-function","text":"This converts the input given in tonnes into grams. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:tTog( INT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from Tonnes into grams. INT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:tTog(1) The tonne value '1' is converted into grams as '1000000.0' .","title":"tTog (Function)"},{"location":"docs/api/latest/#ttokg-function","text":"This converts the input given in tonnes into kilograms. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:tTokg( INT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from tonnes into kilograms. INT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:tTokg(inValue) The tonne value is converted into kilograms as '1000.0' .","title":"tTokg (Function)"},{"location":"docs/api/latest/#ytod-function","text":"This converts the given input in years into days. Origin: siddhi-execution-unitconversion:2.0.2 Syntax DOUBLE unitconversion:yTod( INT|LONG|FLOAT|DOUBLE p1) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic p1 The value that needs to be converted from years into days. INT LONG FLOAT DOUBLE No Yes Examples EXAMPLE 1 unitconversion:yTod(1) The year value '1' is converted into days as '365.2525' .","title":"yTod (Function)"},{"location":"docs/examples/","text":"var base_url = \"\"; Siddhi By Example Siddhi by Example enables you to have complete coverage over the Siddhi query language and some of it's key extenstions, while emphasizing incremental learning. This is a series of commented example programs. $.getJSON(\"all-bbes.json\", function (all_stuff) { console.log(\"all stuff : \", all_stuff); var i = 0; var div_content; $.getJSON(\"built-bbes.json\", function (important_stuff) { console.log(\"important stuff : \", important_stuff); $.each(all_stuff, function (key, value) { var category, categoryName; if (!(value['category'])) { category = 'other'; categoryName = 'Other'; } else { category = (value['category'].toLowerCase()).replace(/./g, '_'); categoryName = (value['category']); } console.log(\"for each:\", category, categoryName); var categoryElem = $('[data-category=' + category + ']'); if (categoryElem.length <= 0) { var newCategoryElem = '<div class=\"clearfix\" data-category=\"' + category + '\">' + '<div class=\"col-md-12\"><h3>' + categoryName + '</h3><hr></div>' + '<div class=\"col-xs-12 col-sm-16 col-md-3 col-lg-3 cLanguageFeatures featureSet0\"></div>' + '<div class=\"col-xs-12 col-sm-16 col-md-3 col-lg-3 cLanguageFeatures featureSet1\"></div>' + '<div class=\"col-xs-12 col-sm-16 col-md-3 col-lg-3 cLanguageFeatures featureSet2\"></div>' + '<div class=\"col-xs-12 col-sm-16 col-md-3 col-lg-3 cLanguageFeatures featureSet3\"></div>' + '</div>'; } $(\"#example\").append(newCategoryElem); div_content = \"\"; div_content += '<ul>'; div_content += '<li class=\"cTableTitle\">' + value['title'] + '</li>'; $.each(value['samples'], function (exkey, example) { var link = example['url']; //Filtering out the failed BBEs var is_exist = $.inArray(link, important_stuff); if (is_exist == -1) { return true; } else { div_content += '<li><a href=\"/en/v5.1/docs/examples/' + link + '\">' + example['name'] + '</a></li>'; } }); div_content += '</ul>'; $('[data-category=' + category + '] .featureSet' + value['column']).append(div_content); i++; }); }); });","title":"Home"},{"location":"docs/examples/if-then-else/","text":"var base_url = \"\"; If-Then-Else This application demonstrates how to enrich events based on a simple if-then-else conditions. define stream TemperatureStream ( sensorId string , temperature double ); @ sink ( type = log , prefix = ValidTemperatureStream , @ map ( type = json )) define stream ValidTemperatureStream ( isValid string ); @ sink ( type = log , prefix = ProcessedTemperatureStream , @ map ( type = json )) define stream ProcessedTemperatureStream ( tempStatus string ); @ info ( name = SimpleIfElseQuery ) from TemperatureStream select ifThenElse ( temperature - 2 , Valid , InValid ) as isValid insert into ValidTemperatureStream ; @ info ( name = ComplexIfElseQuery ) from TemperatureStream select ifThenElse ( temperature - 2 , ifThenElse ( temperature 40 , High , Normal ), InValid ) as tempStatusinsert into ProcessedTemperatureStream ; define stream TemperatureStream (sensorId string, temperature double); Define the input stream which has input event with temperature in Fahrenheit @sink(type = log , prefix = ValidTemperatureStream , @map(type = json )) define stream ValidTemperatureStream (isValid string); @sink(type = log , prefix = ProcessedTemperatureStream , @map(type = json )) define stream ProcessedTemperatureStream(tempStatus string); Output streams and sinks to log the events to the console @info(name = SimpleIfElseQuery ) from TemperatureStream select ifThenElse(temperature -2, Valid , InValid ) as isValid insert into ValidTemperatureStream; @info(name = ComplexIfElseQuery ) from TemperatureStream If the temperature is greater than -2, it is valid event select ifThenElse(temperature -2, ifThenElse(temperature 40, High , Normal ), InValid ) as tempStatus If the temperature is more than 40 degree the status is set to High, 0-40 as Normal insert into ProcessedTemperatureStream; Following logs can be seen for the temperature attribute Values Temperature Logs -5 ValidTemperatureStream : { event :{ isValid : InValid }} ProcessedTemperatureStream : { event :{ tempStatus : InValid }} 38 ValidTemperatureStream : { event :{ isValid : Valid }} ProcessedTemperatureStream : { event :{ tempStatus : Normal }} 45 ValidTemperatureStream : { event :{ isValid : Valid }} ProcessedTemperatureStream : { event :{ tempStatus : High }}","title":"If then else"},{"location":"docs/examples/simple-aggregation/","text":"var base_url = \"\"; Simple Aggregation This application demonstrates how to simulate random events via Feed Simulation and calculate running aggregates such as avg, min, max, etc. The aggregation is executed on events within a time window. A sliding time window of 10 seconds is used in this sample. define stream TradesStream ( trader string , quantity int ); @ sink ( type = log ) define stream SummarizedTradingInformation ( trader string , noOfTrades long , totalTradingQuantity long , minTradingQuantity int , maxTradingQuantity int , avgTradingQuantity double ); @ info ( name = query1 ) from TradesStream # window . time ( 10 sec ) select trader , count () as noOfTrades , sum ( quantity ) as totalTradingQuantity , min ( quantity ) as minTradingQuantity , max ( quantity ) as maxTradingQuantity , avg ( quantity ) as avgTradingQuantity group by trader insert into SummarizedTradingInformation ; define stream TradesStream(trader string, quantity int); Define the input stream definition called TradesStream @sink(type= log ) define stream SummarizedTradingInformation(trader string, noOfTrades long, totalTradingQuantity long, minTradingQuantity int, maxTradingQuantity int, avgTradingQuantity double); Define the sink to print the output events in the log file @info(name= query1 ) from TradesStream#window.time(10 sec) select trader, count() as noOfTrades, sum(quantity) as totalTradingQuantity, min(quantity) as minTradingQuantity, max(quantity) as maxTradingQuantity, avg(quantity) as avgTradingQuantity group by trader insert into SummarizedTradingInformation; Find count, sum, min, max and avg of quantity per trader, during the last 10 seconds Input Find count, sum, min, max and avg of quantity per trader, during the last 10 seconds Output Find count, sum, min, max and avg of quantity per trader, during the last 10 seconds","title":"Simple aggregation"},{"location":"docs/guides/overview/","text":"Overview of Use case Guides Siddhi Cloud Native Stream Processor contains various set of features to cater the use cases in the stream processing and complex event processing domain. We have taken a few of such commonly used use cases and discussed in this section. These use case guides are written end to end from the development to deployment. Please find the high level overview details of the guides below. Generating Alerts Based on Static and Dynamic Thresholds In this guide, you will understand one of the common requirements of a Stream Processing which is generating alerts based on static and dynamic thresholds. To understand this requirement, we'll discuss how to implement the Throttling requirement with Siddhi. Throttling has become one of the unavoidable needs with the evolution of APIs and API management. Throttling is a process that is used to control the usage of APIs by consumers during a given period. Refer more details in the guide Data Preprocessing, Fault Tolerance, and Error Handling In this guide, we are going to understand some interesting topics around streaming data integration; they are data preprocessing, fault tolerance and error handling. To understand these capabilities, we have considered a health care use case. In this scenario, Glucose reading events are received from sensors that mounted on patients. These events are received to the Stream Processing engine, get preprocessed, unrelated attributes are removed and send them to another processing layer to process if there are any abnormal behavior observed. Refer more details in the guide Analyze Event Occurrence Patterns and Trends Over Time In this guide, we are going to discuss a unique and appealing feature of a complex event processing system which is Patterns and Trends . Patterns and Trends are highly utilized in various business domains for the day to day business activities and growth. To understand these capabilities, we have considered a Taxi service use case. In this scenario, we have identified the increasing trend of rider requests over time and direct required riders to that specific geographical area to increase the chance of getting more rides. Refer more details in the guide Static Rule Processing via Predefined and Database Based Rules In this guide, we are going to explore how to process static rules stored in a database and make a decision according to those stored rules. By following this guide you will be able to implement a system capable of making decisions according to a set of static rules. The static rules will be stored in a relational database(MySQL). You can dynamically change the rules in the database according to your business requirements without touching the deployed system. All the dynamic values need for each rule can be templated and pump into the rule at runtime. Refer more details in the guide Retrieve and Publish Data from/to Various Enterprise Systems This guide illustrates how you can receive data from various enterprise systems, process it, and then send processed data into other systems. To understand this requirement, we'll implement a traffic management system which notifies the subscribers regarding the transport delays, etc... After completing this guide you will understand how to receive various inputs from heterogeneous systems (like NATS, RDBMS, MongoDB), process it, and send back processed outputs in various formats like Email notification. Refer more details in the guide Realtime predictions with pre-trained ML models In this guide, we are going to understand how we can use Siddhi\u2019s capability to perform real time predictions with pre-trained machine learning models. This guide demonstrates how we can build a recommendation system that recommends movies based on the user\u2019s review comments. Within this guide, we focus on using machine learning capabilities integrated with Siddhi to perform Sentiment Analysis and generate movie recommendations. We'll use a pre-trained TensorFlow model to predict whether a movie review is positive or negative using BERT in Tensorflow and then a PMML model trained with the MovieLense dataset to generate recommendations for a positively reviewed movie. Refer more details in the guide Long-running time based Aggregations In this guide, you will understand one of the common requirements of Analytics which is aggregating data. Aggregation is a mandatory need in any system as it allows users to spot trends and anomalies easily which can lead to actions that will benefit an organization or the business. Aggregated data can also be processed easily to get the information needed for a business requirement decision. In this guide, we'll consider the sales in a shopping mall and discusses how Siddhi aggregations queries can be used to implement the long running aggregations to provide better insights about the business Refer more details in the guide","title":"Overview"},{"location":"docs/guides/overview/#overview-of-use-case-guides","text":"Siddhi Cloud Native Stream Processor contains various set of features to cater the use cases in the stream processing and complex event processing domain. We have taken a few of such commonly used use cases and discussed in this section. These use case guides are written end to end from the development to deployment. Please find the high level overview details of the guides below.","title":"Overview of Use case Guides"},{"location":"docs/guides/overview/#generating-alerts-based-on-static-and-dynamic-thresholds","text":"In this guide, you will understand one of the common requirements of a Stream Processing which is generating alerts based on static and dynamic thresholds. To understand this requirement, we'll discuss how to implement the Throttling requirement with Siddhi. Throttling has become one of the unavoidable needs with the evolution of APIs and API management. Throttling is a process that is used to control the usage of APIs by consumers during a given period. Refer more details in the guide","title":"Generating Alerts Based on Static and Dynamic Thresholds"},{"location":"docs/guides/overview/#data-preprocessing-fault-tolerance-and-error-handling","text":"In this guide, we are going to understand some interesting topics around streaming data integration; they are data preprocessing, fault tolerance and error handling. To understand these capabilities, we have considered a health care use case. In this scenario, Glucose reading events are received from sensors that mounted on patients. These events are received to the Stream Processing engine, get preprocessed, unrelated attributes are removed and send them to another processing layer to process if there are any abnormal behavior observed. Refer more details in the guide","title":"Data Preprocessing, Fault Tolerance, and Error Handling"},{"location":"docs/guides/overview/#analyze-event-occurrence-patterns-and-trends-over-time","text":"In this guide, we are going to discuss a unique and appealing feature of a complex event processing system which is Patterns and Trends . Patterns and Trends are highly utilized in various business domains for the day to day business activities and growth. To understand these capabilities, we have considered a Taxi service use case. In this scenario, we have identified the increasing trend of rider requests over time and direct required riders to that specific geographical area to increase the chance of getting more rides. Refer more details in the guide","title":"Analyze Event Occurrence Patterns and Trends Over Time"},{"location":"docs/guides/overview/#static-rule-processing-via-predefined-and-database-based-rules","text":"In this guide, we are going to explore how to process static rules stored in a database and make a decision according to those stored rules. By following this guide you will be able to implement a system capable of making decisions according to a set of static rules. The static rules will be stored in a relational database(MySQL). You can dynamically change the rules in the database according to your business requirements without touching the deployed system. All the dynamic values need for each rule can be templated and pump into the rule at runtime. Refer more details in the guide","title":"Static Rule Processing via Predefined and Database Based Rules"},{"location":"docs/guides/overview/#retrieve-and-publish-data-fromto-various-enterprise-systems","text":"This guide illustrates how you can receive data from various enterprise systems, process it, and then send processed data into other systems. To understand this requirement, we'll implement a traffic management system which notifies the subscribers regarding the transport delays, etc... After completing this guide you will understand how to receive various inputs from heterogeneous systems (like NATS, RDBMS, MongoDB), process it, and send back processed outputs in various formats like Email notification. Refer more details in the guide","title":"Retrieve and Publish Data from/to Various Enterprise Systems"},{"location":"docs/guides/overview/#realtime-predictions-with-pre-trained-ml-models","text":"In this guide, we are going to understand how we can use Siddhi\u2019s capability to perform real time predictions with pre-trained machine learning models. This guide demonstrates how we can build a recommendation system that recommends movies based on the user\u2019s review comments. Within this guide, we focus on using machine learning capabilities integrated with Siddhi to perform Sentiment Analysis and generate movie recommendations. We'll use a pre-trained TensorFlow model to predict whether a movie review is positive or negative using BERT in Tensorflow and then a PMML model trained with the MovieLense dataset to generate recommendations for a positively reviewed movie. Refer more details in the guide","title":"Realtime predictions with pre-trained ML models"},{"location":"docs/guides/overview/#long-running-time-based-aggregations","text":"In this guide, you will understand one of the common requirements of Analytics which is aggregating data. Aggregation is a mandatory need in any system as it allows users to spot trends and anomalies easily which can lead to actions that will benefit an organization or the business. Aggregated data can also be processed easily to get the information needed for a business requirement decision. In this guide, we'll consider the sales in a shopping mall and discusses how Siddhi aggregations queries can be used to implement the long running aggregations to provide better insights about the business Refer more details in the guide","title":"Long-running time based Aggregations"},{"location":"docs/guides/alerts-for-thresholds/guide/","text":"Generating Alerts Based on Static and Dynamic Thresholds In this guide, you will understand one of the common requirements of a Stream Processing which is generating alerts based on static and dynamic thresholds. To understand this requirement, let\u2019s consider the throttling use case in API management solutions. Scenario - Throttling for API Requests Throttling has become as one of the unavoidable needs with the evolution of APIs and API management. Throttling is a process that is used to control the usage of APIs by consumers during a given period. The following sections are available in this guide. What you'll build Prerequisites Implementation Testing Deployment Output What you'll build Let's consider a real world use case to implement the throttling requirement. This will help you to understand some Siddhi Stream Processing constructs such as windows, aggregations, source, and etc. Let\u2019s jump into the use case directly. Let's assume that you are an API developer and you have published a few APIs to the API store and there are subscribers who have subscribed to these APIs in different tiers which are categorized based on the number of requests per min/sec. If any subscriber is consuming an API more than the allowed quota within a time frame then that specific user will be throttled until that time frame passes. Also if a subscriber is getting throttled often then the system sends a notification to that user requesting to upgrade the tier. For example, let\u2019s assume that user \u201cJohn\u201d has subscribed to an API with the tier 'Silver'; silver tier allows a user to make 10 API requests per minute. If John, made more than 10 requests within a minute then his subsequent requests get throttled until the end of the minute, and if he has got throttled more than 10 times in an hour, then he will be notified to upgrade his tier via email. Now, let\u2019s understand how this could be implemented in Siddhi engine. Prerequisites Below are the prerequisites that should be considered to implement the above use case. Mandatory Requirements Siddhi tooling VM/Local distribution One of the Siddhi runner distributions VM/Local Runtime Docker Image K8S Operator (commands are given in deployment section) Java 8 or higher Requirements needed to deploy Siddhi in Docker/Kubernetes Docker Minikube or Google Kubernetes Engine(GKE) Cluster or Docker for Mac Implementation When a subscriber made an API call to order-mgt-v1 API it sends an event with the API request information to Siddhi runtime through HTTP transport. Siddhi runtime, keep track of each API request and make decisions to throttle subscribers. Again, once the corresponding time frame passed Siddhi release those throttle users. Throttling decisions are informed to API management solution through an API call. If a subscriber is getting throttled more than 10 times in an hour then sends a notification mail once every 15 minutes to that user requesting to upgrade the tier. Implement Streaming Queries Start the Siddhi tooling runtime and go to the editor UI in http://localhost:9390/editor Follow below steps to start the Siddhi tooling runtime. * Extract the downloaded zip and navigate to /bin. (TOOLING_HOME refers to the extracted folder) * Issue the following command in the command prompt (Windows) / terminal (Linux/Mac) For Windows: tooling.bat For Linux/Mac: ./tooling.sh Select File - New option, then you could either use the source view or design view to write/build the Siddhi Application. You can find the Siddhi Application bellow, that implements the requirements mentioned above. Let\u2019s write (develop) the Siddhi Application, as given below. Once the Siddhi app is created, you can use the Event Simulator option in the editor to simulate events to streams and perform developer testing. @App:name('API-Request-Throttler') @App:description('Enforces throttling on API requests') -- HTTP endpoint which listens for api request related events @source(type = 'http', receiver.url = \"http://0.0.0.0:8006/apiRequest\", basic.auth.enabled = \"false\", @map(type = 'json')) define stream APIRequestStream (apiName string, version string, tier string, user string, userEmail string); -- HTTP sink to publich throttle decisions. For testing purpose, there is a mock logger service provided @sink(type = 'http', publisher.url = \"http://${LOGGER_SERVICE_HOST}:8080/logger\", method = \"POST\", @map(type = 'json')) @sink(type = 'log', @map(type = 'text')) define stream ThrottleOutputStream (apiName string, version string, user string, tier string, userEmail string, isThrottled bool); -- Email sink to send alerts @sink(type = 'log', @map(type = 'text')) @sink(type = 'email', username = \"${EMAIL_USERNAME}\", address = \"${SENDER_EMAIL_ADDRESS}\", password = \"${EMAIL_PASSWORD}\", subject = \"Upgrade API Subscription Tier\", to = \"{{userEmail}}\", host = \"smtp.gmail.com\", port = \"465\", ssl.enable = \"true\", auth = \"true\", @map(type = 'text', @payload(\"\"\" Hi {{user}} You have subscribed to API called {{apiName}}:{{version}} with {{tier}} tier. Based on our records, it seems you are hitting the upper limit of the API requests in a frequent manner. We kindly request you to consider upgrading to next API subscription tier to avoid this in the future. Thanks, API Team\"\"\"))) define stream UserNotificationStream (user string, apiName string, version string, tier string, userEmail string, throttledCount long); @info(name = 'Query to find users who needs to be throttled based on tier `silver`') from APIRequestStream[tier == \"silver\"]#window.timeBatch(1 min, 0, true) select apiName, version, user, tier, userEmail, count() as totalRequestCount group by apiName, version, user having totalRequestCount == 10 or totalRequestCount == 0 insert all events into ThrottledStream; @info(name = 'Query to find users who needs to be throttled based on tier `gold`') from APIRequestStream[tier == \"gold\"]#window.timeBatch(1 min, 0, true) select apiName, version, user, tier, userEmail, count() as totalRequestCount group by apiName, version, user having totalRequestCount == 100 or totalRequestCount == 0 insert all events into ThrottledStream; @info(name = 'Query to add a flag for throttled request') from ThrottledStream select apiName, version, user, tier, userEmail, ifThenElse(totalRequestCount == 0, false, true) as isThrottled insert into ThrottleOutputStream; @info(name = 'Query to find frequently throttled users - who have throttled more than 10 times in the last hour') from ThrottleOutputStream[isThrottled]#window.time(1 hour) select user, apiName, version, tier, userEmail, count() as throttledCount group by user, apiName, version, tier having throttledCount 10 output first every 15 min insert into UserNotificationStream; Source view of the Siddhi app. Below is the flow diagram of the above Siddhi App. Testing NOTE: In the provided Siddhi app, there are some environmental variables (EMAIL_PASSWORD, EMAIL_USERNAME, and SENDER_EMAIL_ADDRESS) used which are required to be set to send an email alert based on the Siddhi queries defined. Again, there is a mock service configured to receive the throttle decisions, and its host is configured via LOGGER_SERVICE_HOST environment property. Hence, make sure to set the environmental variables with the proper values in the system EMAIL_USERNAME: Username of the email account which used to send email alerts. (eg: 'siddhi.gke.user') EMAIL_PASSWORD: Password of the email account which used to send email alerts. (eg: 'siddhi123') SENDER_EMAIL_ADDRESS: Email address of the account used to send email alerts. (eg: 'siddhi.gke.user@gmail.com') LOGGER_SERVICE_HOST: IP address of the host where logger service is running. (eg: 'localhost') When you run the Siddhi app in the editor, you will see below logs getting printed in the editor console. You could simply simulate some events directly into the stream and test your Siddhi app in the editor itself. Then, you can also simulate some events through HTTP to test the application. The following sections explain how you can test the Siddhi app via HTTP using cURL. Run Mock Logger service In the provided Siddhi app, there is an HTTP sink configured to push output events to an HTTP endpoint of the API Manager. For simplicity, you will be mocking this service. Please download the mock server jar and run that mock service by executing the following command. java -jar logservice-1.0.0.jar Invoking the Siddhi App As per the Siddhi app that you wrote in the 'Implementation' section, there is an HTTP service running in Siddhi which is listening for events related to API requests. The respective service can be accessed via the URL http://localhost:9090/ThotttleService . As per the app, the API request will get throttled if there are more than 10 requests by the same user, to the same API (for 'silver\u2019 tier). curl -v -X POST -d '{ \"event\": { \"apiName\": \"order-mgt-v1\", \"version\": \"1.0.0\", \"tier\":\"silver\",\"user\":\"mohan\", \"userEmail\":\"example@wso2.com\"}}' \"http://localhost:8006/apiRequest\" -H \"Content-Type:application/json\" If you invoke the above cURL request for more than 10 times within a minute, then Siddhi starts throttling the request, and sends an alert to the API Manager (logservice), while logging the alert as below. INFO {io.siddhi.core.stream.output.sink.LogSink} - API-Request-Throttler : ThrottleOutputStream : Event{timestamp=1564056341280, data=[order-mgt-v1, 1.0.0, mohan, silver, true], isExpired=false} You can validate that the alert has reached the API Manager (logservice) from its console logs. If a user gets throttled more than 10 times within an hour then Siddhi sends an email to the respective user. Note: The configurations provided in the email sink along with the environment properties will work for Gmail, but if you use other mail servers, please make sure to change the config values accordingly. Deployment Once you are done with the development, export the Siddhi app that you have developed with 'File' - 'Export File' option. You can deploy the Siddhi app using any of the methods listed below. Deploy on VM/ Bare Metal Download the latest Siddhi Runner distribution . Unzip the siddhi-runner-x.x.x.zip . Configure the necessary environmental variables In the above provided Siddhi app, there are some environmental variables (EMAIL_USERNAME, EMAIL_PASSWORD, and SENDER_EMAIL_ADDRESS) which are required to be set to send email alerts based on the Siddhi queries defined. Again, there is a mock service configured to receive the throttle decisions (instructions given below), and its host is configured via LOGGER_SERVICE_HOST environment property. Hence, make sure to set the environmental variables with the proper values in the system (make sure to follow necessary steps based on the underneath operating system). Start Siddhi app with the runner config by executing the following commands from the distribution directory. Linux/Mac : ./bin/runner.sh -Dapps= siddhi-file-path Windows : bin\\runner.bat -Dapps= siddhi-file-path Eg: If exported siddhi app in Siddhi home directory, ./bin/runner.sh -Dapps=API-Request-Throttler.siddhi Download the mock logging service which is used to demonstrate the capability of Siddhi HTTP sink. Execute the below command to run the mock server. java -jar logservice-1.0.0.jar Invoke the apiRequest service with the following cURL request for more than 10 times within a minute time period. Please make sure to change the userEmail property value to an email address that you could use to test the email alerting purposes. curl -v -X POST -d '{ event : { apiName : order-mgt-v1 , version : 1.0.0 , tier : silver , user : mohan , userEmail : example@wso2.com }}' http://localhost:8006/apiRequest -H Content-Type:application/json You can see the output log in the console. Here, you will be able to see the alert log printed as shown below. At the same time, you could also see the events received to HTTP mock service endpoint (started in step #5) via its log as below. Deploy on Docker Create a folder locally on your host machine (eg: /home/siddhi-apps ) and copy the Siddhi app into it. Pull the latest Siddhi Runner image from [Siddhiio Docker Hub] (https://hub.docker.com/u/siddhiio). docker pull siddhiio/siddhi-runner-alpine:5.1.0-alpha Start SiddhiApp by executing the following docker command. docker run -it -p 8006:8006 -v /home/siddhi-apps:/apps -e EMAIL_PASSWORD=siddhi123 -e EMAIL_USERNAME=siddhi.gke.user -e SENDER_EMAIL_ADDRESS=siddhi.gke.user@gmail.com -e LOGGER_SERVICE_HOST=10.100.0.99 siddhiio/siddhi-runner-alpine:5.1.0-alpha -Dapps=/apps/API-Request-Throttler.siddhi NOTE: In the above provided Siddhi app, there are some environmental variables (EMAIL_PASSWORD, EMAIL_USERNAME, and SENDER_EMAIL_ADDRESS) which are required to be set to send email alerts based on the Siddhi queries defined. Again, there is a mock service configured to receive the throttle decisions (instructions given below), and its host is configured via LOGGER_SERVICE_HOST environment property. Hence, make sure to add proper values for the environmental variables in the above command. Download the mock logging service which is used to demonstrate the capability of Siddhi HTTP sink. Execute the below command to run the mock server. java -jar logservice-1.0.0.jar Invoke the apiRequest service with the following cURL request for more than 10 times within a minute time period. Please make sure to change the userEmail property value to an email address that you could use to test the email alerting purposes. curl -v -X POST -d '{ \"event\": { \"apiName\": \"order-mgt-v1\", \"version\": \"1.0.0\", \"tier\": \"silver\", \"user\":\"mohan\", \"userEmail\":\"example@wso2.com\"}}' \"http://localhost:8006/apiRequest\" -H \"Content-Type:application/json\" Since you have started the docker in interactive mode you can see the output in its console as below. (If it is not started in the interactive mode then you can run docker exec -it docker-container-id sh command, go into the container and check the log file in home/siddhi_user/siddhi-runner/wso2/runner/logs/carbon.log file) At the same time, you could also see the events received to HTTP mock service endpoint (started in step #4) via its log as below. If there are more than 10 requests get throttled within 1 hour then the API invoker will receive an email (as shown in the 'Testing' section). Deploy on Kubernetes Install Siddhi Operator To install the Siddhi Kubernetes operator run the following commands. kubectl apply -f https://github.com/siddhi-io/siddhi-operator/releases/download/v0.2.0-alpha/00-prereqs.yaml kubectl apply -f https://github.com/siddhi-io/siddhi-operator/releases/download/v0.2.0-alpha/01-siddhi-operator.yaml You can verify the installation by making sure the following deployments are running in your Kubernetes cluster. Download the mock logging service which is used to demonstrate the capability of Siddhi HTTP sink. Execute the below command to run the mock server. java -jar logservice-1.0.0.jar Siddhi applications can be deployed on Kubernetes using the Siddhi operator. Before deploying the apps you have to define an Ingress , this is because there is an HTTP endpoint in the Siddhi app you have written and you will be sending events to that. To deploy the above created Siddhi app, you have to create a custom resource object YAML file (with the kind as SiddhiProcess) as following apiVersion: siddhi.io/v1alpha2 kind: SiddhiProcess metadata: name: api-throttler-app spec: apps: - script: | @App:name('API-Request-Throttler') @App:description('Enforcesthrottling to API requests ') -- HTTP endpoint which listens for api request related events @source(type = 'http', receiver.url = \"http://0.0.0.0:8006/apiRequest\", basic.auth.enabled = \"false\", @map(type = 'json')) define stream APIRequestStream (apiName string, version string, tier string, user string, userEmail string); -- HTTP sink to publich throttle decisions. For testing purpose, there is a mock logger service provided @sink(type = 'http', publisher.url = \"http://${LOGGER_SERVICE_HOST}:8080/logger\", method = \"POST\", @map(type = 'json')) @sink(type = 'log', @map(type = 'text')) define stream ThrottleOutputStream (apiName string, version string, user string, tier string, userEmail string, isThrottled bool); -- Email sink to send alerts @sink(type = 'log', @map(type = 'text')) @sink(type = 'email', username = \"${EMAIL_USERNAME}\", address = \"${SENDER_EMAIL_ADDRESS}\", password = \"${EMAIL_PASSWORD}\", subject = \"Upgrade API Subscription Tier\", to = \"{{userEmail}}\", host = \"smtp.gmail.com\", port = \"465\", ssl.enable = \"true\", auth = \"true\", @map(type = 'text', @payload(\"\"\" Hi {{user}} You have subscribed to API called {{apiName}}:{{version}} with {{tier}} tier. Based on our records, it seems you are hitting the upper limit of the API requests in a frequent manner. We kindly request you to consider upgrading to next API subscription tier to avoid this in the future. Thanks, API Team\"\"\"))) define stream UserNotificationStream (user string, apiName string, version string, tier string, userEmail string, throttledCount long); @info(name = 'Query to find users who needs to be throttled based on tier `silver`') from APIRequestStream[tier == \"silver\"]#window.timeBatch(1 min, 0, true) select apiName, version, user, tier, userEmail, count() as totalRequestCount group by apiName, version, user having totalRequestCount == 10 or totalRequestCount == 0 insert all events into ThrottledStream; @info(name = 'Query to find users who needs to be throttled based on tier `gold`') from APIRequestStream[tier == \"gold\"]#window.timeBatch(1 min, 0, true) select apiName, version, user, tier, userEmail, count() as totalRequestCount group by apiName, version, user having totalRequestCount == 100 or totalRequestCount == 0 insert all events into ThrottledStream; @info(name = 'Query to add a flag for throttled request') from ThrottledStream select apiName, version, user, tier, userEmail, ifThenElse(totalRequestCount == 0, false, true) as isThrottled insert into ThrottleOutputStream; @info(name = 'Query to find frequently throttled users - who have throttled more than 10 times in the last hour') from ThrottleOutputStream[isThrottled]#window.time(1 hour) select user, apiName, version, tier, userEmail, count() as throttledCount group by user, apiName, version, tier having throttledCount 10 output first every 15 min insert into UserNotificationStream; container: env: - name: EMAIL_PASSWORD value: \"siddhi123\" - name: EMAIL_USERNAME value: \"siddhi.gke.user\" - name: SENDER_EMAIL_ADDRESS value: \"siddhi.gke.user@gmail.com\" - name: LOGGER_SERVICE_HOST value: \"10.100.0.99\" image: \"siddhiio/siddhi-runner-ubuntu:5.1.0-alpha\" NOTE: In the above provided Siddhi app, there are some environmental variables (EMAIL_PASSWORD, EMAIL_USERNAME, and SENDER_EMAIL_ADDRESS) which are required to be set to send email alerts based on the Siddhi queries defined. Again, there is a mock service configured to receive the throttle decisions (instructions given below), and its host is configured via LOGGER_SERVICE_HOST environment property. Hence, make sure to add proper values for the environmental variables in the above YAML file (check the env section of the YAML file). Now, let\u2019s create the above resource in the Kubernetes cluster with the following command. kubectl create -f absolute-yaml-file-path /API-Request-Throttler.yaml Once, Siddhi app is successfully deployed. You can verify its health using the following commands Then, add the host siddhi and related external IP (ADDRESS) to the /etc/hosts file in your machine. For Docker for Mac , external IP is 0.0.0.0 . For Minikube, you have to use Minikube IP as the external IP. Hence, run minikube ip command to get the IP of the Minikube cluster. You can find the alert logs in the Siddhi runner log file. To see the Siddhi runner log file, first, invoke below command to get the pods. kubectl get pods Then, find the pod name of the Siddhi app deployed, and invoke below command to view the logs. kubectl logs siddhi-app-pod-name -f Eg: as shown below image, Invoke the apiRequest service with below cURL request for more than 10 times within a minute. Please make sure to change the userEmail property value to an email address that you could use to test the email alerting purposes. curl -v -X POST -d '{ \"event\": { \"apiName\": \"order-mgt-v1\", \"version\": \"1.0.0\", \"tier\": \"silver\", \"user\":\"mohan\", \"userEmail\":\"example@wso2.com\"}}' \"http://siddhi/api-throttler-app-0/8006/apiRequest\" -H \"Content-Type:application/json\" Then, you will be able to see the throttle decisions as console logs (as given below). At the same time, you could also see the events received to HTTP mock service endpoint (started in step #2) via its log as below. If there are more than 10 requests get throttled within 1 hour then the API invoker will receive an email (as shown in the 'Testing' section). Refer here to get more details about running Siddhi on Kubernetes.","title":"Alerts Based on Thresholds"},{"location":"docs/guides/alerts-for-thresholds/guide/#generating-alerts-based-on-static-and-dynamic-thresholds","text":"In this guide, you will understand one of the common requirements of a Stream Processing which is generating alerts based on static and dynamic thresholds. To understand this requirement, let\u2019s consider the throttling use case in API management solutions.","title":"Generating Alerts Based on Static and Dynamic Thresholds"},{"location":"docs/guides/alerts-for-thresholds/guide/#scenario-throttling-for-api-requests","text":"Throttling has become as one of the unavoidable needs with the evolution of APIs and API management. Throttling is a process that is used to control the usage of APIs by consumers during a given period. The following sections are available in this guide. What you'll build Prerequisites Implementation Testing Deployment Output","title":"Scenario - Throttling for API Requests"},{"location":"docs/guides/alerts-for-thresholds/guide/#what-youll-build","text":"Let's consider a real world use case to implement the throttling requirement. This will help you to understand some Siddhi Stream Processing constructs such as windows, aggregations, source, and etc. Let\u2019s jump into the use case directly. Let's assume that you are an API developer and you have published a few APIs to the API store and there are subscribers who have subscribed to these APIs in different tiers which are categorized based on the number of requests per min/sec. If any subscriber is consuming an API more than the allowed quota within a time frame then that specific user will be throttled until that time frame passes. Also if a subscriber is getting throttled often then the system sends a notification to that user requesting to upgrade the tier. For example, let\u2019s assume that user \u201cJohn\u201d has subscribed to an API with the tier 'Silver'; silver tier allows a user to make 10 API requests per minute. If John, made more than 10 requests within a minute then his subsequent requests get throttled until the end of the minute, and if he has got throttled more than 10 times in an hour, then he will be notified to upgrade his tier via email. Now, let\u2019s understand how this could be implemented in Siddhi engine.","title":"What you'll build"},{"location":"docs/guides/alerts-for-thresholds/guide/#prerequisites","text":"Below are the prerequisites that should be considered to implement the above use case.","title":"Prerequisites"},{"location":"docs/guides/alerts-for-thresholds/guide/#mandatory-requirements","text":"Siddhi tooling VM/Local distribution One of the Siddhi runner distributions VM/Local Runtime Docker Image K8S Operator (commands are given in deployment section) Java 8 or higher","title":"Mandatory Requirements"},{"location":"docs/guides/alerts-for-thresholds/guide/#requirements-needed-to-deploy-siddhi-in-dockerkubernetes","text":"Docker Minikube or Google Kubernetes Engine(GKE) Cluster or Docker for Mac","title":"Requirements needed to deploy Siddhi in Docker/Kubernetes"},{"location":"docs/guides/alerts-for-thresholds/guide/#implementation","text":"When a subscriber made an API call to order-mgt-v1 API it sends an event with the API request information to Siddhi runtime through HTTP transport. Siddhi runtime, keep track of each API request and make decisions to throttle subscribers. Again, once the corresponding time frame passed Siddhi release those throttle users. Throttling decisions are informed to API management solution through an API call. If a subscriber is getting throttled more than 10 times in an hour then sends a notification mail once every 15 minutes to that user requesting to upgrade the tier.","title":"Implementation"},{"location":"docs/guides/alerts-for-thresholds/guide/#implement-streaming-queries","text":"Start the Siddhi tooling runtime and go to the editor UI in http://localhost:9390/editor Follow below steps to start the Siddhi tooling runtime. * Extract the downloaded zip and navigate to /bin. (TOOLING_HOME refers to the extracted folder) * Issue the following command in the command prompt (Windows) / terminal (Linux/Mac) For Windows: tooling.bat For Linux/Mac: ./tooling.sh Select File - New option, then you could either use the source view or design view to write/build the Siddhi Application. You can find the Siddhi Application bellow, that implements the requirements mentioned above. Let\u2019s write (develop) the Siddhi Application, as given below. Once the Siddhi app is created, you can use the Event Simulator option in the editor to simulate events to streams and perform developer testing. @App:name('API-Request-Throttler') @App:description('Enforces throttling on API requests') -- HTTP endpoint which listens for api request related events @source(type = 'http', receiver.url = \"http://0.0.0.0:8006/apiRequest\", basic.auth.enabled = \"false\", @map(type = 'json')) define stream APIRequestStream (apiName string, version string, tier string, user string, userEmail string); -- HTTP sink to publich throttle decisions. For testing purpose, there is a mock logger service provided @sink(type = 'http', publisher.url = \"http://${LOGGER_SERVICE_HOST}:8080/logger\", method = \"POST\", @map(type = 'json')) @sink(type = 'log', @map(type = 'text')) define stream ThrottleOutputStream (apiName string, version string, user string, tier string, userEmail string, isThrottled bool); -- Email sink to send alerts @sink(type = 'log', @map(type = 'text')) @sink(type = 'email', username = \"${EMAIL_USERNAME}\", address = \"${SENDER_EMAIL_ADDRESS}\", password = \"${EMAIL_PASSWORD}\", subject = \"Upgrade API Subscription Tier\", to = \"{{userEmail}}\", host = \"smtp.gmail.com\", port = \"465\", ssl.enable = \"true\", auth = \"true\", @map(type = 'text', @payload(\"\"\" Hi {{user}} You have subscribed to API called {{apiName}}:{{version}} with {{tier}} tier. Based on our records, it seems you are hitting the upper limit of the API requests in a frequent manner. We kindly request you to consider upgrading to next API subscription tier to avoid this in the future. Thanks, API Team\"\"\"))) define stream UserNotificationStream (user string, apiName string, version string, tier string, userEmail string, throttledCount long); @info(name = 'Query to find users who needs to be throttled based on tier `silver`') from APIRequestStream[tier == \"silver\"]#window.timeBatch(1 min, 0, true) select apiName, version, user, tier, userEmail, count() as totalRequestCount group by apiName, version, user having totalRequestCount == 10 or totalRequestCount == 0 insert all events into ThrottledStream; @info(name = 'Query to find users who needs to be throttled based on tier `gold`') from APIRequestStream[tier == \"gold\"]#window.timeBatch(1 min, 0, true) select apiName, version, user, tier, userEmail, count() as totalRequestCount group by apiName, version, user having totalRequestCount == 100 or totalRequestCount == 0 insert all events into ThrottledStream; @info(name = 'Query to add a flag for throttled request') from ThrottledStream select apiName, version, user, tier, userEmail, ifThenElse(totalRequestCount == 0, false, true) as isThrottled insert into ThrottleOutputStream; @info(name = 'Query to find frequently throttled users - who have throttled more than 10 times in the last hour') from ThrottleOutputStream[isThrottled]#window.time(1 hour) select user, apiName, version, tier, userEmail, count() as throttledCount group by user, apiName, version, tier having throttledCount 10 output first every 15 min insert into UserNotificationStream; Source view of the Siddhi app. Below is the flow diagram of the above Siddhi App.","title":"Implement Streaming Queries"},{"location":"docs/guides/alerts-for-thresholds/guide/#testing","text":"NOTE: In the provided Siddhi app, there are some environmental variables (EMAIL_PASSWORD, EMAIL_USERNAME, and SENDER_EMAIL_ADDRESS) used which are required to be set to send an email alert based on the Siddhi queries defined. Again, there is a mock service configured to receive the throttle decisions, and its host is configured via LOGGER_SERVICE_HOST environment property. Hence, make sure to set the environmental variables with the proper values in the system EMAIL_USERNAME: Username of the email account which used to send email alerts. (eg: 'siddhi.gke.user') EMAIL_PASSWORD: Password of the email account which used to send email alerts. (eg: 'siddhi123') SENDER_EMAIL_ADDRESS: Email address of the account used to send email alerts. (eg: 'siddhi.gke.user@gmail.com') LOGGER_SERVICE_HOST: IP address of the host where logger service is running. (eg: 'localhost') When you run the Siddhi app in the editor, you will see below logs getting printed in the editor console. You could simply simulate some events directly into the stream and test your Siddhi app in the editor itself. Then, you can also simulate some events through HTTP to test the application. The following sections explain how you can test the Siddhi app via HTTP using cURL.","title":"Testing"},{"location":"docs/guides/alerts-for-thresholds/guide/#run-mock-logger-service","text":"In the provided Siddhi app, there is an HTTP sink configured to push output events to an HTTP endpoint of the API Manager. For simplicity, you will be mocking this service. Please download the mock server jar and run that mock service by executing the following command. java -jar logservice-1.0.0.jar","title":"Run Mock Logger service"},{"location":"docs/guides/alerts-for-thresholds/guide/#invoking-the-siddhi-app","text":"As per the Siddhi app that you wrote in the 'Implementation' section, there is an HTTP service running in Siddhi which is listening for events related to API requests. The respective service can be accessed via the URL http://localhost:9090/ThotttleService . As per the app, the API request will get throttled if there are more than 10 requests by the same user, to the same API (for 'silver\u2019 tier). curl -v -X POST -d '{ \"event\": { \"apiName\": \"order-mgt-v1\", \"version\": \"1.0.0\", \"tier\":\"silver\",\"user\":\"mohan\", \"userEmail\":\"example@wso2.com\"}}' \"http://localhost:8006/apiRequest\" -H \"Content-Type:application/json\" If you invoke the above cURL request for more than 10 times within a minute, then Siddhi starts throttling the request, and sends an alert to the API Manager (logservice), while logging the alert as below. INFO {io.siddhi.core.stream.output.sink.LogSink} - API-Request-Throttler : ThrottleOutputStream : Event{timestamp=1564056341280, data=[order-mgt-v1, 1.0.0, mohan, silver, true], isExpired=false} You can validate that the alert has reached the API Manager (logservice) from its console logs. If a user gets throttled more than 10 times within an hour then Siddhi sends an email to the respective user. Note: The configurations provided in the email sink along with the environment properties will work for Gmail, but if you use other mail servers, please make sure to change the config values accordingly.","title":"Invoking the Siddhi App"},{"location":"docs/guides/alerts-for-thresholds/guide/#deployment","text":"Once you are done with the development, export the Siddhi app that you have developed with 'File' - 'Export File' option. You can deploy the Siddhi app using any of the methods listed below.","title":"Deployment"},{"location":"docs/guides/alerts-for-thresholds/guide/#deploy-on-vm-bare-metal","text":"Download the latest Siddhi Runner distribution . Unzip the siddhi-runner-x.x.x.zip . Configure the necessary environmental variables In the above provided Siddhi app, there are some environmental variables (EMAIL_USERNAME, EMAIL_PASSWORD, and SENDER_EMAIL_ADDRESS) which are required to be set to send email alerts based on the Siddhi queries defined. Again, there is a mock service configured to receive the throttle decisions (instructions given below), and its host is configured via LOGGER_SERVICE_HOST environment property. Hence, make sure to set the environmental variables with the proper values in the system (make sure to follow necessary steps based on the underneath operating system). Start Siddhi app with the runner config by executing the following commands from the distribution directory. Linux/Mac : ./bin/runner.sh -Dapps= siddhi-file-path Windows : bin\\runner.bat -Dapps= siddhi-file-path Eg: If exported siddhi app in Siddhi home directory, ./bin/runner.sh -Dapps=API-Request-Throttler.siddhi Download the mock logging service which is used to demonstrate the capability of Siddhi HTTP sink. Execute the below command to run the mock server. java -jar logservice-1.0.0.jar Invoke the apiRequest service with the following cURL request for more than 10 times within a minute time period. Please make sure to change the userEmail property value to an email address that you could use to test the email alerting purposes. curl -v -X POST -d '{ event : { apiName : order-mgt-v1 , version : 1.0.0 , tier : silver , user : mohan , userEmail : example@wso2.com }}' http://localhost:8006/apiRequest -H Content-Type:application/json You can see the output log in the console. Here, you will be able to see the alert log printed as shown below. At the same time, you could also see the events received to HTTP mock service endpoint (started in step #5) via its log as below.","title":"Deploy on VM/ Bare Metal"},{"location":"docs/guides/alerts-for-thresholds/guide/#deploy-on-docker","text":"Create a folder locally on your host machine (eg: /home/siddhi-apps ) and copy the Siddhi app into it. Pull the latest Siddhi Runner image from [Siddhiio Docker Hub] (https://hub.docker.com/u/siddhiio). docker pull siddhiio/siddhi-runner-alpine:5.1.0-alpha Start SiddhiApp by executing the following docker command. docker run -it -p 8006:8006 -v /home/siddhi-apps:/apps -e EMAIL_PASSWORD=siddhi123 -e EMAIL_USERNAME=siddhi.gke.user -e SENDER_EMAIL_ADDRESS=siddhi.gke.user@gmail.com -e LOGGER_SERVICE_HOST=10.100.0.99 siddhiio/siddhi-runner-alpine:5.1.0-alpha -Dapps=/apps/API-Request-Throttler.siddhi NOTE: In the above provided Siddhi app, there are some environmental variables (EMAIL_PASSWORD, EMAIL_USERNAME, and SENDER_EMAIL_ADDRESS) which are required to be set to send email alerts based on the Siddhi queries defined. Again, there is a mock service configured to receive the throttle decisions (instructions given below), and its host is configured via LOGGER_SERVICE_HOST environment property. Hence, make sure to add proper values for the environmental variables in the above command. Download the mock logging service which is used to demonstrate the capability of Siddhi HTTP sink. Execute the below command to run the mock server. java -jar logservice-1.0.0.jar Invoke the apiRequest service with the following cURL request for more than 10 times within a minute time period. Please make sure to change the userEmail property value to an email address that you could use to test the email alerting purposes. curl -v -X POST -d '{ \"event\": { \"apiName\": \"order-mgt-v1\", \"version\": \"1.0.0\", \"tier\": \"silver\", \"user\":\"mohan\", \"userEmail\":\"example@wso2.com\"}}' \"http://localhost:8006/apiRequest\" -H \"Content-Type:application/json\" Since you have started the docker in interactive mode you can see the output in its console as below. (If it is not started in the interactive mode then you can run docker exec -it docker-container-id sh command, go into the container and check the log file in home/siddhi_user/siddhi-runner/wso2/runner/logs/carbon.log file) At the same time, you could also see the events received to HTTP mock service endpoint (started in step #4) via its log as below. If there are more than 10 requests get throttled within 1 hour then the API invoker will receive an email (as shown in the 'Testing' section).","title":"Deploy on Docker"},{"location":"docs/guides/alerts-for-thresholds/guide/#deploy-on-kubernetes","text":"Install Siddhi Operator To install the Siddhi Kubernetes operator run the following commands. kubectl apply -f https://github.com/siddhi-io/siddhi-operator/releases/download/v0.2.0-alpha/00-prereqs.yaml kubectl apply -f https://github.com/siddhi-io/siddhi-operator/releases/download/v0.2.0-alpha/01-siddhi-operator.yaml You can verify the installation by making sure the following deployments are running in your Kubernetes cluster. Download the mock logging service which is used to demonstrate the capability of Siddhi HTTP sink. Execute the below command to run the mock server. java -jar logservice-1.0.0.jar Siddhi applications can be deployed on Kubernetes using the Siddhi operator. Before deploying the apps you have to define an Ingress , this is because there is an HTTP endpoint in the Siddhi app you have written and you will be sending events to that. To deploy the above created Siddhi app, you have to create a custom resource object YAML file (with the kind as SiddhiProcess) as following apiVersion: siddhi.io/v1alpha2 kind: SiddhiProcess metadata: name: api-throttler-app spec: apps: - script: | @App:name('API-Request-Throttler') @App:description('Enforcesthrottling to API requests ') -- HTTP endpoint which listens for api request related events @source(type = 'http', receiver.url = \"http://0.0.0.0:8006/apiRequest\", basic.auth.enabled = \"false\", @map(type = 'json')) define stream APIRequestStream (apiName string, version string, tier string, user string, userEmail string); -- HTTP sink to publich throttle decisions. For testing purpose, there is a mock logger service provided @sink(type = 'http', publisher.url = \"http://${LOGGER_SERVICE_HOST}:8080/logger\", method = \"POST\", @map(type = 'json')) @sink(type = 'log', @map(type = 'text')) define stream ThrottleOutputStream (apiName string, version string, user string, tier string, userEmail string, isThrottled bool); -- Email sink to send alerts @sink(type = 'log', @map(type = 'text')) @sink(type = 'email', username = \"${EMAIL_USERNAME}\", address = \"${SENDER_EMAIL_ADDRESS}\", password = \"${EMAIL_PASSWORD}\", subject = \"Upgrade API Subscription Tier\", to = \"{{userEmail}}\", host = \"smtp.gmail.com\", port = \"465\", ssl.enable = \"true\", auth = \"true\", @map(type = 'text', @payload(\"\"\" Hi {{user}} You have subscribed to API called {{apiName}}:{{version}} with {{tier}} tier. Based on our records, it seems you are hitting the upper limit of the API requests in a frequent manner. We kindly request you to consider upgrading to next API subscription tier to avoid this in the future. Thanks, API Team\"\"\"))) define stream UserNotificationStream (user string, apiName string, version string, tier string, userEmail string, throttledCount long); @info(name = 'Query to find users who needs to be throttled based on tier `silver`') from APIRequestStream[tier == \"silver\"]#window.timeBatch(1 min, 0, true) select apiName, version, user, tier, userEmail, count() as totalRequestCount group by apiName, version, user having totalRequestCount == 10 or totalRequestCount == 0 insert all events into ThrottledStream; @info(name = 'Query to find users who needs to be throttled based on tier `gold`') from APIRequestStream[tier == \"gold\"]#window.timeBatch(1 min, 0, true) select apiName, version, user, tier, userEmail, count() as totalRequestCount group by apiName, version, user having totalRequestCount == 100 or totalRequestCount == 0 insert all events into ThrottledStream; @info(name = 'Query to add a flag for throttled request') from ThrottledStream select apiName, version, user, tier, userEmail, ifThenElse(totalRequestCount == 0, false, true) as isThrottled insert into ThrottleOutputStream; @info(name = 'Query to find frequently throttled users - who have throttled more than 10 times in the last hour') from ThrottleOutputStream[isThrottled]#window.time(1 hour) select user, apiName, version, tier, userEmail, count() as throttledCount group by user, apiName, version, tier having throttledCount 10 output first every 15 min insert into UserNotificationStream; container: env: - name: EMAIL_PASSWORD value: \"siddhi123\" - name: EMAIL_USERNAME value: \"siddhi.gke.user\" - name: SENDER_EMAIL_ADDRESS value: \"siddhi.gke.user@gmail.com\" - name: LOGGER_SERVICE_HOST value: \"10.100.0.99\" image: \"siddhiio/siddhi-runner-ubuntu:5.1.0-alpha\" NOTE: In the above provided Siddhi app, there are some environmental variables (EMAIL_PASSWORD, EMAIL_USERNAME, and SENDER_EMAIL_ADDRESS) which are required to be set to send email alerts based on the Siddhi queries defined. Again, there is a mock service configured to receive the throttle decisions (instructions given below), and its host is configured via LOGGER_SERVICE_HOST environment property. Hence, make sure to add proper values for the environmental variables in the above YAML file (check the env section of the YAML file). Now, let\u2019s create the above resource in the Kubernetes cluster with the following command. kubectl create -f absolute-yaml-file-path /API-Request-Throttler.yaml Once, Siddhi app is successfully deployed. You can verify its health using the following commands Then, add the host siddhi and related external IP (ADDRESS) to the /etc/hosts file in your machine. For Docker for Mac , external IP is 0.0.0.0 . For Minikube, you have to use Minikube IP as the external IP. Hence, run minikube ip command to get the IP of the Minikube cluster. You can find the alert logs in the Siddhi runner log file. To see the Siddhi runner log file, first, invoke below command to get the pods. kubectl get pods Then, find the pod name of the Siddhi app deployed, and invoke below command to view the logs. kubectl logs siddhi-app-pod-name -f Eg: as shown below image, Invoke the apiRequest service with below cURL request for more than 10 times within a minute. Please make sure to change the userEmail property value to an email address that you could use to test the email alerting purposes. curl -v -X POST -d '{ \"event\": { \"apiName\": \"order-mgt-v1\", \"version\": \"1.0.0\", \"tier\": \"silver\", \"user\":\"mohan\", \"userEmail\":\"example@wso2.com\"}}' \"http://siddhi/api-throttler-app-0/8006/apiRequest\" -H \"Content-Type:application/json\" Then, you will be able to see the throttle decisions as console logs (as given below). At the same time, you could also see the events received to HTTP mock service endpoint (started in step #2) via its log as below. If there are more than 10 requests get throttled within 1 hour then the API invoker will receive an email (as shown in the 'Testing' section). Refer here to get more details about running Siddhi on Kubernetes.","title":"Deploy on Kubernetes"},{"location":"docs/guides/database-static-rule-processing/guide/","text":"Static Rule Processing via Predefined and Database Based Rules In this guide, we are going to explore how to process static rules stored in a database and make a decision according to those stored rules. Scenario - Static Rule Processing After completing this scenario, you will be able to implement a system capable of making decisions according to a set of static rules. The static rules will be stored in a relational database(MySQL). You can dynamically change the rules in the database according to your business requirements without touching the deployed system. All the dynamic values need for each rule can be templated and pump into the rule at runtime. What You'll Build The implementation of this use case explains using the functional requirements of the Combo supermart. Combo supermart is a supermarket that resides in our town. This Combo supermarket gives a loyalty card for their frequent customers. Each loyalty card has a type. These card types are Platinum, Gold, and Silver, etc. In each season management of the combo supermart planning to give discounts for each loyalty card. The percentage of the discount will change according to the type of loyalty card. Moreover, each loyalty card will select to have these discounts if that loyalty card fulfills several requirements. Those requirements are stored in a database as rules. These rules can be changed from time to time. Since combo supermart has to handle thousands of customer transaction per day in this season they cannot calculate these discounts in a manual process. Hence they planning to build an automatic system to calculate these discounts. As shown in the following diagram the business process has to execute using six steps. A user sends an HTTP request with a JSON payload that contained an ID of a promo card and the amount of the transaction. That JSON payload will consume by the HTTP service. Then parse the mapped values to the processing logic of the Siddhi query. Initially, processing logic inputs the promo card ID to the MySQL database. And retrieves all the rules relevant to the given promo card type. After that process these rules and send output to the HTTP service response. The HTTP service response will send back another JSON payload which indicating, whether this user allowed to have a discount or not. If this user allowed to have a discount then what is the percentage of the discount. Prerequisites Mandatory Requirements Siddhi tooling VM/Local distribution One of the Siddhi runner distributions VM/Local Runtime Docker Image K8S Operator (commands are given in Kubernetes deployment section) MySQL database Java 8 or higher Requirements needed to deploy Siddhi in Docker/Kubernetes Docker Kubernetes cluster Minikube Google Kubernetes Engine(GKE) Docker for Mac Refer to this documentation about Configuring a Google Kubernetes Engine (GKE) Cluster to deploy Siddhi apps in GKE. Implementation First of all, we assume that the Combo supermart has a MySQL database that contained the all promo card and user details. The following ER diagram describes the schema of the database. Now, you have to start the Siddhi tooling editor before implementing the Siddhi app. Siddhi tooling editor is an IDE that supports to implement Siddhi apps. First, you can download the Siddhi tooling pack as a zip file from here and extract it. After creating the above database we need to have a mechanism to connect this database to Siddhi runtime. To do that you have to update the Siddhi tooling configuration file. Siddhi tooling configuration file is TOOLING_HOME /conf/tooling/deployment.yaml . You have to add the following YAML block under the dataSources entry in the deployment.yaml. The following YAML block contained all the details that need to connect to the database. For example, database username, password, and URL, etc. - name: COMBO_SUPERMART_DB description: The datasource used for lending process of the Combo supermarket jndiConfig: name: jdbc/ComboSuperMart definition: type: RDBMS configuration: jdbcUrl: 'jdbc:mysql://127.0.0.1:3306/ComboSuperMart' username: siddhi_user password: siddhiio driverClassName: com.mysql.jdbc.Driver maxPoolSize: 10 idleTimeout: 60000 connectionTestQuery: SELECT 1 validationTimeout: 30000 isAutoCommit: false In order to connect to a MySQL server Siddhi tooling needs the MySQL connector JAR. You can download the MySQL connector JAR from here and copy that JAR into TOOLING_HOME /jars directory. Start the following binary file. Using this link http://localhost:9390/editor now you can access the Siddhi tooling editor from your web browser. For Linux/Mac: TOOLING_HOME /bin/tooling.sh For Windows: TOOLING_HOME /bin/tooling.bat Select File - New option, then you could either use the source view or design view to write/build the Siddhi Application. You can find the Siddhi Application bellow, that implements the requirements mentioned above. Siddhi Query Guide The execution steps and the logic of the Siddhi query described as comments in the following Siddhi app. Therefore here we are not going to explain in detail here. For more details about Siddhi queries please refer Siddhi query guide . @App:name(\"ComboSuperMartPromoProcess\") @App:description(\"The promotion selection process of Combo super mart.\") /* Purpose: The combo supermart has multiple promotion cards that given to their loyal users. For a particular season combo supermart plan to give discounts for each card. These discounts will change according to the card type and several rules. So, the combo supermart needed an automatic system to retrieve the discounts for a given card. To do that we implement this Siddhi app which interacts with a MySQL database and executes their functionalities. All card details and rules were stored in a MySQL database. All the rules for a particular card were in the templated format. Therefore all rules executed dynamically and give the final result. Input: HTTP POST with JSON payload { \"event\": { \"promoCardId\": \"PC001\", \"amount\": 20000 } } Output: { \"event\": { \"messageId\": \"1817d06b-22ae-438e-990d-42fedcb50607\", \"discountAmount\": 15.0, \"discountApplied\": true } } */ -- HTTP source @source( type='http-service', source.id='adder', receiver.url='http://0.0.0.0:8088/comboSuperMart/promo', basic.auth.enabled='', @map(type='json', @attributes(messageId='trp:messageId', promoCardId='$.event.promoCardId', amount='$.event.amount')) ) define stream InputTransactionStream(messageId string, promoCardId string, amount long); -- RDBMS data stores @Store(type=\"rdbms\", datasource=\"COMBO_SUPERMART_DB\") define table Customer(customerId string, promoCardId string); @Store(type=\"rdbms\", datasource=\"COMBO_SUPERMART_DB\") define table PromoCard(promoCardId string, promoCardTypeId string, promoCardIssueDate string); @Store(type=\"rdbms\", datasource=\"COMBO_SUPERMART_DB\") define table PromoRule(promoRuleId string, promoCardTypeId string, promoRule string); @Store(type=\"rdbms\", datasource=\"COMBO_SUPERMART_DB\") define table PromoCardType(promoCardTypeId string, promoCardTypeDiscount double); -- Output stream @sink(type='http-service-response', source.id='adder', message.id='{{messageId}}', @map(type = 'json')) define stream ResultStream (messageId string, discountAmount double, discountApplied bool); -- Find and execute rules @info(name='get-promocard-type') from InputTransactionStream#window.length(1) join PromoCard on InputTransactionStream.promoCardId==PromoCard.promoCardId select InputTransactionStream.messageId, InputTransactionStream.promoCardId, PromoCard.promoCardTypeId, PromoCard.promoCardIssueDate, InputTransactionStream.amount insert into GetPromoCardTypeStream; @info(name='get-promocard-type-details') from GetPromoCardTypeStream#window.length(1) join PromoCardType on GetPromoCardTypeStream.promoCardTypeId==PromoCardType.promoCardTypeId select GetPromoCardTypeStream.messageId, GetPromoCardTypeStream.promoCardId, GetPromoCardTypeStream.promoCardTypeId, GetPromoCardTypeStream.amount, PromoCardType.promoCardTypeDiscount, GetPromoCardTypeStream.promoCardIssueDate insert into GetPromoCardTypeDetailsStream; @info(name='get-promocard-rules') from GetPromoCardTypeDetailsStream#window.length(1) right outer join PromoRule on GetPromoCardTypeDetailsStream.promoCardTypeId==PromoRule.promoCardTypeId select GetPromoCardTypeDetailsStream.messageId, GetPromoCardTypeDetailsStream.promoCardId, GetPromoCardTypeDetailsStream.promoCardTypeId, GetPromoCardTypeDetailsStream.amount, GetPromoCardTypeDetailsStream.promoCardTypeDiscount, PromoRule.promoRuleId, PromoRule.promoRule, GetPromoCardTypeDetailsStream.promoCardIssueDate insert into RuleStream; @info(name='execute-promocard-rules') from RuleStream#window.length(1) select RuleStream.messageId, RuleStream.promoCardTypeDiscount, js:eval(str:fillTemplate(RuleStream.promoRule, map:create(\"amount\", RuleStream.amount, \"cardPeriod\", time:dateDiff(time:currentDate(), RuleStream.promoCardIssueDate, 'yyyy-MM-dd', 'yyyy-MM-dd'))), 'bool') as result insert into RuleReslutsStream; -- Output results @info(name='filter-true-rules') from RuleReslutsStream[result == true]#window.batch() select RuleReslutsStream.messageId, RuleReslutsStream.promoCardTypeDiscount, count(result) as result insert into TrueResultStream; @info(name='get-all-results') from RuleReslutsStream#window.batch() select RuleReslutsStream.messageId, RuleReslutsStream.promoCardTypeDiscount, count(result) as result insert into AllResultStream; @info(name='discout-reply-stream') from AllResultStream#window.length(1) unidirectional join TrueResultStream#window.length(1) on TrueResultStream.result==AllResultStream.result select AllResultStream.messageId, AllResultStream.promoCardTypeDiscount as discountAmount, true as discountApplied insert into ResultStream; @info(name='filter-true-rules-and-reply') from RuleReslutsStream[result == false]#window.batch() select RuleReslutsStream.messageId, 0.00 as discountAmount, false as discountApplied insert into ResultStream; The following flow diagram depicts the design view of the above Siddhi app. Testing Let\u2019s run and test the above Siddhi app in your local machine. If you completed the implementation process correctly you will be able to start your Siddhi application without any error. Before you run the application your database should have sample data to be processed. The following images show the sample data tables that we are using for this guide. Customer Table Promo Card Table Promo Card Rule Table Promo Card Type Table To connect to the database Siddhi app will use a user called siddhi_user identified by password siddhiio . To access the database you have to give correct permissions to the user. For testing purposes, you can grant all privileges to this user using the following command. GRANT ALL PRIVILEGES ON ComboSuperMart.* TO 'siddhi_user'@'siddhiio'; To create those tables, you can use the following SQL script. CREATE DATABASE IF NOT EXISTS `ComboSuperMart` /*!40100 DEFAULT CHARACTER SET utf8 */; USE `ComboSuperMart`; -- -- Table structure for table `PromoCardType` -- DROP TABLE IF EXISTS `PromoCardType`; CREATE TABLE `PromoCardType` ( `promoCardTypeId` varchar(45) NOT NULL, `promoCardType` varchar(45) NOT NULL, `promoCardTypeDiscount` decimal(5,2) DEFAULT NULL, PRIMARY KEY (`promoCardTypeId`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8; -- -- Dumping data for table `PromoCardType` -- LOCK TABLES `PromoCardType` WRITE; INSERT INTO `PromoCardType` VALUES ('PCT01','PLATINUM',15.00),('PCT02','GOLD',10.00),('PCT03','SILVER',5.00); UNLOCK TABLES; -- -- Table structure for table `PromoCard` -- DROP TABLE IF EXISTS `PromoCard`; CREATE TABLE `PromoCard` ( `promoCardId` varchar(40) NOT NULL, `promoCardIssueDate` date NOT NULL, `promoCardTypeId` varchar(45) NOT NULL, PRIMARY KEY (`promoCardId`), KEY `promoCardTypeIdKF_idx` (`promoCardTypeId`), CONSTRAINT `promoCardIdTypeFK` FOREIGN KEY (`promoCardTypeId`) REFERENCES `PromoCardType` (`promoCardTypeId`) ON DELETE NO ACTION ON UPDATE NO ACTION ) ENGINE=InnoDB DEFAULT CHARSET=utf8; -- -- Dumping data for table `PromoCard` -- LOCK TABLES `PromoCard` WRITE; INSERT INTO `PromoCard` VALUES ('PC001','2018-04-03','PCT01'),('PC002','2017-04-30','PCT02'),('PC003','2017-08-09','PCT03'),('PC004','2018-03-06','PCT01'); UNLOCK TABLES; -- -- Table structure for table `Customer` -- DROP TABLE IF EXISTS `Customer`; CREATE TABLE `Customer` ( `customerId` varchar(45) NOT NULL, `cutomerName` varchar(60) NOT NULL, `promoCardId` varchar(40) NOT NULL, PRIMARY KEY (`customerId`), KEY `promoCardIdFK_idx` (`promoCardId`), CONSTRAINT `promoCardIdFK` FOREIGN KEY (`promoCardId`) REFERENCES `PromoCard` (`promoCardId`) ON DELETE CASCADE ON UPDATE CASCADE ) ENGINE=InnoDB DEFAULT CHARSET=utf8; -- -- Dumping data for table `Customer` -- LOCK TABLES `Customer` WRITE; INSERT INTO `Customer` VALUES ('C001','John Doy','PC001'),('C002','Micheal Dawson','PC002'),('C003','Neil Clain','PC003'),('C004','Anne Cath','PC004'); UNLOCK TABLES; -- -- Table structure for table `PromoRule` -- DROP TABLE IF EXISTS `PromoRule`; CREATE TABLE `PromoRule` ( `promoRuleId` varchar(40) NOT NULL, `promoCardTypeId` varchar(45) NOT NULL, `promoRule` varchar(200) NOT NULL, PRIMARY KEY (`promoRuleId`), KEY `cardId_idx` (`promoCardTypeId`), CONSTRAINT `promoCardTypeIdFK` FOREIGN KEY (`promoCardTypeId`) REFERENCES `PromoCardType` (`promoCardTypeId`) ON DELETE NO ACTION ON UPDATE NO ACTION ) ENGINE=InnoDB DEFAULT CHARSET=utf8; -- -- Dumping data for table `PromoRule` -- LOCK TABLES `PromoRule` WRITE; INSERT INTO `PromoRule` VALUES ('RULE001','PCT01','{{amount}} 10000 {{amount}} 50000'),('RULE002','PCT01','{{cardPeriod}} 200'); UNLOCK TABLES; The above Siddhi app will start an HTTP service in 8088 port. Therefore, you can send a request to that service using the following CURL command. curl -X POST \\ http://0.0.0.0:8088/comboSuperMart/promo \\ -H 'Accept: */*' \\ -H 'Content-Type: application/json' \\ -H 'Host: 0.0.0.0:8088' \\ -d '{ \"event\": { \"promoCardId\": \"PC001\", \"amount\": 10000 } }' This request will response back the following JSON. { \"event\": { \"messageId\": \"8f38faea-9dbd-4387-b807-b9f170db20dd\", \"discountAmount\": 0.0, \"discountApplied\": false } } Deployment Deploy on VM/ Bare Metal First you have to setup MySQL as described earlier. Refer this link to setup MySQL. Download the Siddhi runner distribution pack from here and unzip it. Add following YAML block to the RUNNER_HOME /conf/runner/deployment.yaml . - name: COMBO_SUPERMART_DB description: The datasource used for lending process of the Combo supermarket jndiConfig: name: jdbc/ComboSuperMart definition: type: RDBMS configuration: jdbcUrl: 'jdbc:mysql://127.0.0.1:3306/ComboSuperMart' username: siddhi_user password: siddhiio driverClassName: com.mysql.jdbc.Driver maxPoolSize: 10 idleTimeout: 60000 connectionTestQuery: SELECT 1 validationTimeout: 30000 isAutoCommit: false Download the MySQL connector JAR from here and copy that JAR into RUNNER_HOME /jars directory. Copy your Siddhi file into RUNNER_HOME /wso2/runner/deployment/siddhi-files Start the following binary file. Using this link http://localhost:9390/editor now you can access the Siddhi tooling editor from your web browser. For Linux/Mac: RUNNER_HOME /bin/runner.sh For Windows: RUNNER_HOME /bin/runner.bat Execute the following CURL command. curl -X POST \\ http://0.0.0.0:8088/comboSuperMart/promo \\ -H 'Accept: */*' \\ -H 'Content-Type: application/json' \\ -H 'Host: 0.0.0.0:8088' \\ -d '{ \"event\": { \"promoCardId\": \"PC001\", \"amount\": 10000 } }' It will results the following JSON. { \"event\": { \"messageId\": \"8f38faea-9dbd-4387-b807-b9f170db20dd\", \"discountAmount\": 0.0, \"discountApplied\": false } } Deploy on Docker Prerequisite Install Docker into your machine. Siddhi Docker Configurations In the tooling editor itself, you can export your Siddhi app into a runnable docker artifact. You can go to Export- For Docker and it will give to a zip file. Database URL for Docker When you changing the deployment.yaml configuration of the data source in the Docker export process, you have to specify this URL in the configuration as below. - name: COMBO_SUPERMART_DB description: The datasource used for lending process of the Combo supermarket jndiConfig: name: jdbc/ComboSuperMart definition: type: RDBMS configuration: jdbcUrl: jdbc:mysql://mysqldb:3306/ComboSuperMart username: siddhi_user password: siddhiio driverClassName: com.mysql.jdbc.Driver maxPoolSize: 10 idleTimeout: 60000 connectionTestQuery: SELECT 1 validationTimeout: 30000 isAutoCommit: false The extracted zip file will be looks like follows. \u251c\u2500\u2500 Dockerfile \u251c\u2500\u2500 configurations.yaml \u251c\u2500\u2500 jars \u2502 \u2514\u2500\u2500 mysql-connector.jar \u2514\u2500\u2500 siddhi-files \u2514\u2500\u2500 ComboSuperMartPromoProcess.siddhi You also need to set up a MySQL docker container and connect it into the Siddhi docker runtime. You can do it very easily from writing a docker composer file like below. Now you need to have a Docker compose file like below to set up all the prerequisites. This compose file contains volume mounts to change configurations of the MySQL container. version: \"3\" services: backend: container_name: combosupermart-promo user: 802:802 build: context: . dockerfile: ./Dockerfile ports: - \"8088:8088\" links: - mysqldb networks: - default restart: on-failure mysqldb: image: 'mysql:5.7' container_name: combosupermart-db environment: MYSQL_USER: siddhi_user MYSQL_PASSWORD: siddhiio MYSQL_ROOT_PASSWORD: siddhiio ports: - \"3304:3306\" networks: - default restart: on-failure Save this file as docker-compose.yaml to the root directory of the extracted zip file. And the extracted zip file directory now looks like below: \u251c\u2500\u2500 Dockerfile \u251c\u2500\u2500 configurations.yaml \u251c\u2500\u2500 docker-compose.yaml \u251c\u2500\u2500 jars \u2502 \u2514\u2500\u2500 mysql-connector.jar \u2514\u2500\u2500 siddhi-files \u2514\u2500\u2500 ComboSuperMartPromoProcess.siddhi Now, you have to build the docker composer. $ docker-compose build Now you have to start the MySQL container. And then set up your MySQL database as described above . $ docker-compose up -d mysqldb Now you can connect to the MySQL server using the following configurations. Host: 0.0.0.0 Port: 3304 Root user: root Root password: siddhiio Custom user: siddhi_user Custom user password: siddhiio Then you can create the database schema in that MySQL container. After that, you can start the Siddhi runtime using the following command. $ docker-compose up -d backend To check the deployments up and running, send the following CURL request. curl -X POST \\ http://0.0.0.0:8088/comboSuperMart/promo \\ -H 'Accept: */*' \\ -H 'Content-Type: application/json' \\ -H 'Host: 0.0.0.0:8088' \\ -d '{ \"event\": { \"promoCardId\": \"PC001\", \"amount\": 20000 } }' It will results the following JSON. { \"event\": { \"messageId\": \"285703c1-866a-4a88-8e1f-e75543e18373\", \"discountAmount\": 15.0, \"discountApplied\": true } } Deploy on Kubernetes Prerequisites Kubernetes cluster Minikube Google Kubernetes Engine(GKE) Docker for Mac Install HELM Siddhi Kubernetes Configurations In the tooling editor itself, you can export your Siddhi app into a runnable Kubernetes artifact. You can go to Export- For Kubernetes and it will give to a zip file that contained the following files. \u251c\u2500\u2500 Dockerfile \u251c\u2500\u2500 configurations.yaml \u251c\u2500\u2500 jars \u2502 \u2514\u2500\u2500 mysql-connector.jar \u251c\u2500\u2500 siddhi-files \u2502 \u2514\u2500\u2500 ComboSuperMartPromoProcess.siddhi.siddhi \u2514\u2500\u2500 siddhi-process.yaml First, you need to install the Siddhi Kubernetes operator using following commands. For more details about the Siddhi operator refer to this documentation . $ kubectl apply -f https://github.com/siddhi-io/siddhi-operator/releases/download/v0.2.0/00-prereqs.yaml $ kubectl apply -f https://github.com/siddhi-io/siddhi-operator/releases/download/v0.2.0/01-siddhi-operator.yaml Now you have to set up MySQL in your Kubernetes cluster. To do that use the following helm command. $ helm install --name mysql-db --set mysqlRootPassword=siddhiio,mysqlUser=siddhi_user,mysqlPassword=siddhiio,mysqlDatabase=ComboSuperMart stable/mysql Then, you can set a port forwarding to the MySQL service which allows you to connect from the Host $ kubectl port-forward svc/mysql-db 3307:3306 Now you can access the MySQL cluster externally. Accessing MySQL cluster externally you can create ComboSuperMart database and the database schema. Using jdbc:mysql://mysql-db:3306/ComboSuperMart URL you can access the database. Database URL for Kubernetes When you changing the deployment.yaml configuration of the data source in the Kubernetes export process, you have to specify this URL in the configuration as below. - name: COMBO_SUPERMART_DB description: The datasource used for lending process of the Combo supermarket jndiConfig: name: jdbc/ComboSuperMart definition: type: RDBMS configuration: jdbcUrl: jdbc:mysql://mysql-db:3306/ComboSuperMart username: siddhi_user password: siddhiio driverClassName: com.mysql.jdbc.Driver maxPoolSize: 10 idleTimeout: 60000 connectionTestQuery: SELECT 1 validationTimeout: 30000 isAutoCommit: false Siddhi operator will automatically set up the external access to any HTTP service in a Siddhi app. To set up that external access Siddhi operator by default uses ingress NGINX. Set up the NGINX controller in your Kubernetes cluster as described in this document . Now you need to create your own docker image with including all the custom libraries and configuration changes that you have made. Use following command to build and push the docker image with the tag DOCKER_HUB_USER_NAME /siddhi-runner-alpine:latest . $ docker build -t DOCKER_HUB_USER_NAME /siddhi-runner-alpine:latest . $ docker push DOCKER_HUB_USER_NAME /siddhi-runner-alpine:latest After the Kubernetes export now you already have this siddhi-process.yaml file. apiVersion: siddhi.io/v1alpha2 kind: SiddhiProcess metadata: name: combo-super-mart spec: apps: - script: | @App:name(\"ComboSuperMartPromoProcess\") @App:description(\"The promotion selection process of Combo super mart.\") /* Purpose: The combo supermart has multiple promotion card that given to their loyal users. For a particular season combo supermart plan to give discounts for each card. These discounts will change according to the card type and several rules. So, combo supermart needed an automatic system to retrieve the discounts for a given card. To do that we implement this Siddhi app which interacts with a MySQL database and executes their functionalities. All card details and rules were stored in a MySQL database. All the rules for a particular card were in the templated format. Therefore all rules executed dynamically and give the final result. Input: HTTP POST with JSON payload { \"event\": { \"promoCardId\": \"PC001\", \"amount\": 20000 } } Output: { \"event\": { \"messageId\": \"1817d06b-22ae-438e-990d-42fedcb50607\", \"discountAmount\": 15.0, \"discountApplied\": true } } */ -- HTTP source @source( type='http-service', source.id='adder', receiver.url='http://0.0.0.0:8088/comboSuperMart/promo', basic.auth.enabled='', @map(type='json', @attributes(messageId='trp:messageId', promoCardId='$.event.promoCardId', amount='$.event.amount')) ) define stream InputTransactionStream(messageId string, promoCardId string, amount long); -- RDBMS data stores @Store(type=\"rdbms\", datasource=\"COMBO_SUPERMART_DB\") define table Customer(customerId string, promoCardId string); @Store(type=\"rdbms\", datasource=\"COMBO_SUPERMART_DB\") define table PromoCard(promoCardId string, promoCardTypeId string, promoCardIssueDate string); @Store(type=\"rdbms\", datasource=\"COMBO_SUPERMART_DB\") define table PromoRule(promoRuleId string, promoCardTypeId string, promoRule string); @Store(type=\"rdbms\", datasource=\"COMBO_SUPERMART_DB\") define table PromoCardType(promoCardTypeId string, promoCardTypeDiscount double); -- Output stream @sink(type='http-service-response', source.id='adder', message.id='{{messageId}}', @map(type = 'json')) define stream ResultStream (messageId string, discountAmount double, discountApplied bool); -- Find and execute rules @info(name='get-promocard-type') from InputTransactionStream#window.length(1) join PromoCard on InputTransactionStream.promoCardId==PromoCard.promoCardId select InputTransactionStream.messageId, InputTransactionStream.promoCardId, PromoCard.promoCardTypeId, PromoCard.promoCardIssueDate, InputTransactionStream.amount insert into GetPromoCardTypeStream; @info(name='get-promocard-type-details') from GetPromoCardTypeStream#window.length(1) join PromoCardType on GetPromoCardTypeStream.promoCardTypeId==PromoCardType.promoCardTypeId select GetPromoCardTypeStream.messageId, GetPromoCardTypeStream.promoCardId, GetPromoCardTypeStream.promoCardTypeId, GetPromoCardTypeStream.amount, PromoCardType.promoCardTypeDiscount, GetPromoCardTypeStream.promoCardIssueDate insert into GetPromoCardTypeDetailsStream; @info(name='get-promocard-rules') from GetPromoCardTypeDetailsStream#window.length(1) right outer join PromoRule on GetPromoCardTypeDetailsStream.promoCardTypeId==PromoRule.promoCardTypeId select GetPromoCardTypeDetailsStream.messageId, GetPromoCardTypeDetailsStream.promoCardId, GetPromoCardTypeDetailsStream.promoCardTypeId, GetPromoCardTypeDetailsStream.amount, GetPromoCardTypeDetailsStream.promoCardTypeDiscount, PromoRule.promoRuleId, PromoRule.promoRule, GetPromoCardTypeDetailsStream.promoCardIssueDate insert into RuleStream; @info(name='execute-promocard-rules') from RuleStream#window.length(1) select RuleStream.messageId, RuleStream.promoCardTypeDiscount, js:eval(str:fillTemplate(RuleStream.promoRule, map:create(\"amount\", RuleStream.amount, \"cardPeriod\", time:dateDiff(time:currentDate(), RuleStream.promoCardIssueDate, 'yyyy-MM-dd', 'yyyy-MM-dd'))), 'bool') as result insert into RuleReslutsStream; -- Output results @info(name='filter-true-rules') from RuleReslutsStream[result == true]#window.batch() select RuleReslutsStream.messageId, RuleReslutsStream.promoCardTypeDiscount, count(result) as result insert into TrueResultStream; @info(name='get-all-results') from RuleReslutsStream#window.batch() select RuleReslutsStream.messageId, RuleReslutsStream.promoCardTypeDiscount, count(result) as result insert into AllResultStream; @info(name='discout-reply-stream') from AllResultStream#window.length(1) unidirectional join TrueResultStream#window.length(1) on TrueResultStream.result==AllResultStream.result select AllResultStream.messageId, AllResultStream.promoCardTypeDiscount as discountAmount, true as discountApplied insert into ResultStream; @info(name='filter-true-rules-and-reply') from RuleReslutsStream[result == false]#window.batch() select RuleReslutsStream.messageId, 0.00 as discountAmount, false as discountApplied insert into ResultStream; runner: | wso2.carbon: id: siddhi-runner name: Siddhi Runner Distribution ports: offset: 0 transports: http: listenerConfigurations: - id: default host: 0.0.0.0 port: 9090 - id: msf4j-https host: 0.0.0.0 port: 9443 scheme: https keyStoreFile: ${carbon.home}/resources/security/wso2carbon.jks keyStorePassword: wso2carbon certPass: wso2carbon transportProperties: - name: server.bootstrap.socket.timeout value: 60 - name: client.bootstrap.socket.timeout value: 60 - name: latency.metrics.enabled value: true dataSources: - name: WSO2_CARBON_DB description: The datasource used for registry and user manager definition: type: RDBMS configuration: jdbcUrl: jdbc:h2:${sys:carbon.home}/wso2/${sys:wso2.runtime}/database/WSO2_CARBON_DB;DB_CLOSE_ON_EXIT=FALSE;LOCK_TIMEOUT=60000 username: wso2carbon password: wso2carbon driverClassName: org.h2.Driver maxPoolSize: 10 idleTimeout: 60000 connectionTestQuery: SELECT 1 validationTimeout: 30000 isAutoCommit: false - name: COMBO_SUPERMART_DB description: The datasource used for lending process of the Combo supermarket jndiConfig: name: jdbc/ComboSuperMart definition: type: RDBMS configuration: jdbcUrl: jdbc:mysql://mysql-db:3306/ComboSuperMart username: siddhi_user password: siddhiio driverClassName: com.mysql.jdbc.Driver maxPoolSize: 10 idleTimeout: 60000 connectionTestQuery: SELECT 1 validationTimeout: 30000 isAutoCommit: false persistentVolumeClaim: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi storageClassName: standard volumeMode: Filesystem container: image: DOCKER_HUB_USER_NAME /siddhi-runner-alpine:latest Now you can install the SiddhiProcess using following kubectl command. Before you install the SiddhiProcess you have to add the docker image tag in the siddhi-process.yaml file. You have to add the docker image name( DOCKER_HUB_USER_NAME /siddhi-runner-alpine:latest ) in the YAML entry spec.container.image . $ kubectl apply -f siddhi-process.yaml The ingress created by Siddhi operator will use the hostname as siddhi. Therefore you have to update your /etc/host file with siddhi hostname along with the external IP of ingress. External IP of Ingress For minikube the ingress external IP is minikube IP and in docker, for Mac, the external IP is 0.0.0.0. For more details about Siddhi, ingress setup refer to this documentation . Now you can send HTTP requests to deployed the Siddhi app in the Kubernetes cluster using following CURL command. curl -X POST \\ http://siddhi/combo-super-mart-0/8088/comboSuperMart/promo \\ -H 'Accept: */*' \\ -H 'Accept-Encoding: gzip, deflate' \\ -H 'Content-Type: application/json' \\ -H 'Host: siddhi' \\ -d '{ \"event\": { \"promoCardId\": \"PC001\", \"amount\": 20000 } }' It will results the following JSON. { \"event\": { \"messageId\": \"285703c1-866a-4a88-8e1f-e75543e18374\", \"discountAmount\": 15.0, \"discountApplied\": true } }","title":"Static Rule Processing using Database Rules"},{"location":"docs/guides/database-static-rule-processing/guide/#static-rule-processing-via-predefined-and-database-based-rules","text":"In this guide, we are going to explore how to process static rules stored in a database and make a decision according to those stored rules.","title":"Static Rule Processing via Predefined and Database Based Rules"},{"location":"docs/guides/database-static-rule-processing/guide/#scenario-static-rule-processing","text":"After completing this scenario, you will be able to implement a system capable of making decisions according to a set of static rules. The static rules will be stored in a relational database(MySQL). You can dynamically change the rules in the database according to your business requirements without touching the deployed system. All the dynamic values need for each rule can be templated and pump into the rule at runtime.","title":"Scenario - Static Rule Processing"},{"location":"docs/guides/database-static-rule-processing/guide/#what-youll-build","text":"The implementation of this use case explains using the functional requirements of the Combo supermart. Combo supermart is a supermarket that resides in our town. This Combo supermarket gives a loyalty card for their frequent customers. Each loyalty card has a type. These card types are Platinum, Gold, and Silver, etc. In each season management of the combo supermart planning to give discounts for each loyalty card. The percentage of the discount will change according to the type of loyalty card. Moreover, each loyalty card will select to have these discounts if that loyalty card fulfills several requirements. Those requirements are stored in a database as rules. These rules can be changed from time to time. Since combo supermart has to handle thousands of customer transaction per day in this season they cannot calculate these discounts in a manual process. Hence they planning to build an automatic system to calculate these discounts. As shown in the following diagram the business process has to execute using six steps. A user sends an HTTP request with a JSON payload that contained an ID of a promo card and the amount of the transaction. That JSON payload will consume by the HTTP service. Then parse the mapped values to the processing logic of the Siddhi query. Initially, processing logic inputs the promo card ID to the MySQL database. And retrieves all the rules relevant to the given promo card type. After that process these rules and send output to the HTTP service response. The HTTP service response will send back another JSON payload which indicating, whether this user allowed to have a discount or not. If this user allowed to have a discount then what is the percentage of the discount.","title":"What You'll Build"},{"location":"docs/guides/database-static-rule-processing/guide/#prerequisites","text":"","title":"Prerequisites"},{"location":"docs/guides/database-static-rule-processing/guide/#mandatory-requirements","text":"Siddhi tooling VM/Local distribution One of the Siddhi runner distributions VM/Local Runtime Docker Image K8S Operator (commands are given in Kubernetes deployment section) MySQL database Java 8 or higher","title":"Mandatory Requirements"},{"location":"docs/guides/database-static-rule-processing/guide/#requirements-needed-to-deploy-siddhi-in-dockerkubernetes","text":"Docker Kubernetes cluster Minikube Google Kubernetes Engine(GKE) Docker for Mac Refer to this documentation about Configuring a Google Kubernetes Engine (GKE) Cluster to deploy Siddhi apps in GKE.","title":"Requirements needed to deploy Siddhi in Docker/Kubernetes"},{"location":"docs/guides/database-static-rule-processing/guide/#implementation","text":"First of all, we assume that the Combo supermart has a MySQL database that contained the all promo card and user details. The following ER diagram describes the schema of the database. Now, you have to start the Siddhi tooling editor before implementing the Siddhi app. Siddhi tooling editor is an IDE that supports to implement Siddhi apps. First, you can download the Siddhi tooling pack as a zip file from here and extract it. After creating the above database we need to have a mechanism to connect this database to Siddhi runtime. To do that you have to update the Siddhi tooling configuration file. Siddhi tooling configuration file is TOOLING_HOME /conf/tooling/deployment.yaml . You have to add the following YAML block under the dataSources entry in the deployment.yaml. The following YAML block contained all the details that need to connect to the database. For example, database username, password, and URL, etc. - name: COMBO_SUPERMART_DB description: The datasource used for lending process of the Combo supermarket jndiConfig: name: jdbc/ComboSuperMart definition: type: RDBMS configuration: jdbcUrl: 'jdbc:mysql://127.0.0.1:3306/ComboSuperMart' username: siddhi_user password: siddhiio driverClassName: com.mysql.jdbc.Driver maxPoolSize: 10 idleTimeout: 60000 connectionTestQuery: SELECT 1 validationTimeout: 30000 isAutoCommit: false In order to connect to a MySQL server Siddhi tooling needs the MySQL connector JAR. You can download the MySQL connector JAR from here and copy that JAR into TOOLING_HOME /jars directory. Start the following binary file. Using this link http://localhost:9390/editor now you can access the Siddhi tooling editor from your web browser. For Linux/Mac: TOOLING_HOME /bin/tooling.sh For Windows: TOOLING_HOME /bin/tooling.bat Select File - New option, then you could either use the source view or design view to write/build the Siddhi Application. You can find the Siddhi Application bellow, that implements the requirements mentioned above. Siddhi Query Guide The execution steps and the logic of the Siddhi query described as comments in the following Siddhi app. Therefore here we are not going to explain in detail here. For more details about Siddhi queries please refer Siddhi query guide . @App:name(\"ComboSuperMartPromoProcess\") @App:description(\"The promotion selection process of Combo super mart.\") /* Purpose: The combo supermart has multiple promotion cards that given to their loyal users. For a particular season combo supermart plan to give discounts for each card. These discounts will change according to the card type and several rules. So, the combo supermart needed an automatic system to retrieve the discounts for a given card. To do that we implement this Siddhi app which interacts with a MySQL database and executes their functionalities. All card details and rules were stored in a MySQL database. All the rules for a particular card were in the templated format. Therefore all rules executed dynamically and give the final result. Input: HTTP POST with JSON payload { \"event\": { \"promoCardId\": \"PC001\", \"amount\": 20000 } } Output: { \"event\": { \"messageId\": \"1817d06b-22ae-438e-990d-42fedcb50607\", \"discountAmount\": 15.0, \"discountApplied\": true } } */ -- HTTP source @source( type='http-service', source.id='adder', receiver.url='http://0.0.0.0:8088/comboSuperMart/promo', basic.auth.enabled='', @map(type='json', @attributes(messageId='trp:messageId', promoCardId='$.event.promoCardId', amount='$.event.amount')) ) define stream InputTransactionStream(messageId string, promoCardId string, amount long); -- RDBMS data stores @Store(type=\"rdbms\", datasource=\"COMBO_SUPERMART_DB\") define table Customer(customerId string, promoCardId string); @Store(type=\"rdbms\", datasource=\"COMBO_SUPERMART_DB\") define table PromoCard(promoCardId string, promoCardTypeId string, promoCardIssueDate string); @Store(type=\"rdbms\", datasource=\"COMBO_SUPERMART_DB\") define table PromoRule(promoRuleId string, promoCardTypeId string, promoRule string); @Store(type=\"rdbms\", datasource=\"COMBO_SUPERMART_DB\") define table PromoCardType(promoCardTypeId string, promoCardTypeDiscount double); -- Output stream @sink(type='http-service-response', source.id='adder', message.id='{{messageId}}', @map(type = 'json')) define stream ResultStream (messageId string, discountAmount double, discountApplied bool); -- Find and execute rules @info(name='get-promocard-type') from InputTransactionStream#window.length(1) join PromoCard on InputTransactionStream.promoCardId==PromoCard.promoCardId select InputTransactionStream.messageId, InputTransactionStream.promoCardId, PromoCard.promoCardTypeId, PromoCard.promoCardIssueDate, InputTransactionStream.amount insert into GetPromoCardTypeStream; @info(name='get-promocard-type-details') from GetPromoCardTypeStream#window.length(1) join PromoCardType on GetPromoCardTypeStream.promoCardTypeId==PromoCardType.promoCardTypeId select GetPromoCardTypeStream.messageId, GetPromoCardTypeStream.promoCardId, GetPromoCardTypeStream.promoCardTypeId, GetPromoCardTypeStream.amount, PromoCardType.promoCardTypeDiscount, GetPromoCardTypeStream.promoCardIssueDate insert into GetPromoCardTypeDetailsStream; @info(name='get-promocard-rules') from GetPromoCardTypeDetailsStream#window.length(1) right outer join PromoRule on GetPromoCardTypeDetailsStream.promoCardTypeId==PromoRule.promoCardTypeId select GetPromoCardTypeDetailsStream.messageId, GetPromoCardTypeDetailsStream.promoCardId, GetPromoCardTypeDetailsStream.promoCardTypeId, GetPromoCardTypeDetailsStream.amount, GetPromoCardTypeDetailsStream.promoCardTypeDiscount, PromoRule.promoRuleId, PromoRule.promoRule, GetPromoCardTypeDetailsStream.promoCardIssueDate insert into RuleStream; @info(name='execute-promocard-rules') from RuleStream#window.length(1) select RuleStream.messageId, RuleStream.promoCardTypeDiscount, js:eval(str:fillTemplate(RuleStream.promoRule, map:create(\"amount\", RuleStream.amount, \"cardPeriod\", time:dateDiff(time:currentDate(), RuleStream.promoCardIssueDate, 'yyyy-MM-dd', 'yyyy-MM-dd'))), 'bool') as result insert into RuleReslutsStream; -- Output results @info(name='filter-true-rules') from RuleReslutsStream[result == true]#window.batch() select RuleReslutsStream.messageId, RuleReslutsStream.promoCardTypeDiscount, count(result) as result insert into TrueResultStream; @info(name='get-all-results') from RuleReslutsStream#window.batch() select RuleReslutsStream.messageId, RuleReslutsStream.promoCardTypeDiscount, count(result) as result insert into AllResultStream; @info(name='discout-reply-stream') from AllResultStream#window.length(1) unidirectional join TrueResultStream#window.length(1) on TrueResultStream.result==AllResultStream.result select AllResultStream.messageId, AllResultStream.promoCardTypeDiscount as discountAmount, true as discountApplied insert into ResultStream; @info(name='filter-true-rules-and-reply') from RuleReslutsStream[result == false]#window.batch() select RuleReslutsStream.messageId, 0.00 as discountAmount, false as discountApplied insert into ResultStream; The following flow diagram depicts the design view of the above Siddhi app.","title":"Implementation"},{"location":"docs/guides/database-static-rule-processing/guide/#testing","text":"Let\u2019s run and test the above Siddhi app in your local machine. If you completed the implementation process correctly you will be able to start your Siddhi application without any error. Before you run the application your database should have sample data to be processed. The following images show the sample data tables that we are using for this guide.","title":"Testing"},{"location":"docs/guides/database-static-rule-processing/guide/#customer-table","text":"","title":"Customer Table"},{"location":"docs/guides/database-static-rule-processing/guide/#promo-card-table","text":"","title":"Promo Card Table"},{"location":"docs/guides/database-static-rule-processing/guide/#promo-card-rule-table","text":"","title":"Promo Card Rule Table"},{"location":"docs/guides/database-static-rule-processing/guide/#promo-card-type-table","text":"To connect to the database Siddhi app will use a user called siddhi_user identified by password siddhiio . To access the database you have to give correct permissions to the user. For testing purposes, you can grant all privileges to this user using the following command. GRANT ALL PRIVILEGES ON ComboSuperMart.* TO 'siddhi_user'@'siddhiio'; To create those tables, you can use the following SQL script. CREATE DATABASE IF NOT EXISTS `ComboSuperMart` /*!40100 DEFAULT CHARACTER SET utf8 */; USE `ComboSuperMart`; -- -- Table structure for table `PromoCardType` -- DROP TABLE IF EXISTS `PromoCardType`; CREATE TABLE `PromoCardType` ( `promoCardTypeId` varchar(45) NOT NULL, `promoCardType` varchar(45) NOT NULL, `promoCardTypeDiscount` decimal(5,2) DEFAULT NULL, PRIMARY KEY (`promoCardTypeId`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8; -- -- Dumping data for table `PromoCardType` -- LOCK TABLES `PromoCardType` WRITE; INSERT INTO `PromoCardType` VALUES ('PCT01','PLATINUM',15.00),('PCT02','GOLD',10.00),('PCT03','SILVER',5.00); UNLOCK TABLES; -- -- Table structure for table `PromoCard` -- DROP TABLE IF EXISTS `PromoCard`; CREATE TABLE `PromoCard` ( `promoCardId` varchar(40) NOT NULL, `promoCardIssueDate` date NOT NULL, `promoCardTypeId` varchar(45) NOT NULL, PRIMARY KEY (`promoCardId`), KEY `promoCardTypeIdKF_idx` (`promoCardTypeId`), CONSTRAINT `promoCardIdTypeFK` FOREIGN KEY (`promoCardTypeId`) REFERENCES `PromoCardType` (`promoCardTypeId`) ON DELETE NO ACTION ON UPDATE NO ACTION ) ENGINE=InnoDB DEFAULT CHARSET=utf8; -- -- Dumping data for table `PromoCard` -- LOCK TABLES `PromoCard` WRITE; INSERT INTO `PromoCard` VALUES ('PC001','2018-04-03','PCT01'),('PC002','2017-04-30','PCT02'),('PC003','2017-08-09','PCT03'),('PC004','2018-03-06','PCT01'); UNLOCK TABLES; -- -- Table structure for table `Customer` -- DROP TABLE IF EXISTS `Customer`; CREATE TABLE `Customer` ( `customerId` varchar(45) NOT NULL, `cutomerName` varchar(60) NOT NULL, `promoCardId` varchar(40) NOT NULL, PRIMARY KEY (`customerId`), KEY `promoCardIdFK_idx` (`promoCardId`), CONSTRAINT `promoCardIdFK` FOREIGN KEY (`promoCardId`) REFERENCES `PromoCard` (`promoCardId`) ON DELETE CASCADE ON UPDATE CASCADE ) ENGINE=InnoDB DEFAULT CHARSET=utf8; -- -- Dumping data for table `Customer` -- LOCK TABLES `Customer` WRITE; INSERT INTO `Customer` VALUES ('C001','John Doy','PC001'),('C002','Micheal Dawson','PC002'),('C003','Neil Clain','PC003'),('C004','Anne Cath','PC004'); UNLOCK TABLES; -- -- Table structure for table `PromoRule` -- DROP TABLE IF EXISTS `PromoRule`; CREATE TABLE `PromoRule` ( `promoRuleId` varchar(40) NOT NULL, `promoCardTypeId` varchar(45) NOT NULL, `promoRule` varchar(200) NOT NULL, PRIMARY KEY (`promoRuleId`), KEY `cardId_idx` (`promoCardTypeId`), CONSTRAINT `promoCardTypeIdFK` FOREIGN KEY (`promoCardTypeId`) REFERENCES `PromoCardType` (`promoCardTypeId`) ON DELETE NO ACTION ON UPDATE NO ACTION ) ENGINE=InnoDB DEFAULT CHARSET=utf8; -- -- Dumping data for table `PromoRule` -- LOCK TABLES `PromoRule` WRITE; INSERT INTO `PromoRule` VALUES ('RULE001','PCT01','{{amount}} 10000 {{amount}} 50000'),('RULE002','PCT01','{{cardPeriod}} 200'); UNLOCK TABLES; The above Siddhi app will start an HTTP service in 8088 port. Therefore, you can send a request to that service using the following CURL command. curl -X POST \\ http://0.0.0.0:8088/comboSuperMart/promo \\ -H 'Accept: */*' \\ -H 'Content-Type: application/json' \\ -H 'Host: 0.0.0.0:8088' \\ -d '{ \"event\": { \"promoCardId\": \"PC001\", \"amount\": 10000 } }' This request will response back the following JSON. { \"event\": { \"messageId\": \"8f38faea-9dbd-4387-b807-b9f170db20dd\", \"discountAmount\": 0.0, \"discountApplied\": false } }","title":"Promo Card Type Table"},{"location":"docs/guides/database-static-rule-processing/guide/#deployment","text":"","title":"Deployment"},{"location":"docs/guides/database-static-rule-processing/guide/#deploy-on-vm-bare-metal","text":"First you have to setup MySQL as described earlier. Refer this link to setup MySQL. Download the Siddhi runner distribution pack from here and unzip it. Add following YAML block to the RUNNER_HOME /conf/runner/deployment.yaml . - name: COMBO_SUPERMART_DB description: The datasource used for lending process of the Combo supermarket jndiConfig: name: jdbc/ComboSuperMart definition: type: RDBMS configuration: jdbcUrl: 'jdbc:mysql://127.0.0.1:3306/ComboSuperMart' username: siddhi_user password: siddhiio driverClassName: com.mysql.jdbc.Driver maxPoolSize: 10 idleTimeout: 60000 connectionTestQuery: SELECT 1 validationTimeout: 30000 isAutoCommit: false Download the MySQL connector JAR from here and copy that JAR into RUNNER_HOME /jars directory. Copy your Siddhi file into RUNNER_HOME /wso2/runner/deployment/siddhi-files Start the following binary file. Using this link http://localhost:9390/editor now you can access the Siddhi tooling editor from your web browser. For Linux/Mac: RUNNER_HOME /bin/runner.sh For Windows: RUNNER_HOME /bin/runner.bat Execute the following CURL command. curl -X POST \\ http://0.0.0.0:8088/comboSuperMart/promo \\ -H 'Accept: */*' \\ -H 'Content-Type: application/json' \\ -H 'Host: 0.0.0.0:8088' \\ -d '{ \"event\": { \"promoCardId\": \"PC001\", \"amount\": 10000 } }' It will results the following JSON. { \"event\": { \"messageId\": \"8f38faea-9dbd-4387-b807-b9f170db20dd\", \"discountAmount\": 0.0, \"discountApplied\": false } }","title":"Deploy on VM/ Bare Metal"},{"location":"docs/guides/database-static-rule-processing/guide/#deploy-on-docker","text":"","title":"Deploy on Docker"},{"location":"docs/guides/database-static-rule-processing/guide/#prerequisite","text":"Install Docker into your machine.","title":"Prerequisite"},{"location":"docs/guides/database-static-rule-processing/guide/#siddhi-docker-configurations","text":"In the tooling editor itself, you can export your Siddhi app into a runnable docker artifact. You can go to Export- For Docker and it will give to a zip file. Database URL for Docker When you changing the deployment.yaml configuration of the data source in the Docker export process, you have to specify this URL in the configuration as below. - name: COMBO_SUPERMART_DB description: The datasource used for lending process of the Combo supermarket jndiConfig: name: jdbc/ComboSuperMart definition: type: RDBMS configuration: jdbcUrl: jdbc:mysql://mysqldb:3306/ComboSuperMart username: siddhi_user password: siddhiio driverClassName: com.mysql.jdbc.Driver maxPoolSize: 10 idleTimeout: 60000 connectionTestQuery: SELECT 1 validationTimeout: 30000 isAutoCommit: false The extracted zip file will be looks like follows. \u251c\u2500\u2500 Dockerfile \u251c\u2500\u2500 configurations.yaml \u251c\u2500\u2500 jars \u2502 \u2514\u2500\u2500 mysql-connector.jar \u2514\u2500\u2500 siddhi-files \u2514\u2500\u2500 ComboSuperMartPromoProcess.siddhi You also need to set up a MySQL docker container and connect it into the Siddhi docker runtime. You can do it very easily from writing a docker composer file like below. Now you need to have a Docker compose file like below to set up all the prerequisites. This compose file contains volume mounts to change configurations of the MySQL container. version: \"3\" services: backend: container_name: combosupermart-promo user: 802:802 build: context: . dockerfile: ./Dockerfile ports: - \"8088:8088\" links: - mysqldb networks: - default restart: on-failure mysqldb: image: 'mysql:5.7' container_name: combosupermart-db environment: MYSQL_USER: siddhi_user MYSQL_PASSWORD: siddhiio MYSQL_ROOT_PASSWORD: siddhiio ports: - \"3304:3306\" networks: - default restart: on-failure Save this file as docker-compose.yaml to the root directory of the extracted zip file. And the extracted zip file directory now looks like below: \u251c\u2500\u2500 Dockerfile \u251c\u2500\u2500 configurations.yaml \u251c\u2500\u2500 docker-compose.yaml \u251c\u2500\u2500 jars \u2502 \u2514\u2500\u2500 mysql-connector.jar \u2514\u2500\u2500 siddhi-files \u2514\u2500\u2500 ComboSuperMartPromoProcess.siddhi Now, you have to build the docker composer. $ docker-compose build Now you have to start the MySQL container. And then set up your MySQL database as described above . $ docker-compose up -d mysqldb Now you can connect to the MySQL server using the following configurations. Host: 0.0.0.0 Port: 3304 Root user: root Root password: siddhiio Custom user: siddhi_user Custom user password: siddhiio Then you can create the database schema in that MySQL container. After that, you can start the Siddhi runtime using the following command. $ docker-compose up -d backend To check the deployments up and running, send the following CURL request. curl -X POST \\ http://0.0.0.0:8088/comboSuperMart/promo \\ -H 'Accept: */*' \\ -H 'Content-Type: application/json' \\ -H 'Host: 0.0.0.0:8088' \\ -d '{ \"event\": { \"promoCardId\": \"PC001\", \"amount\": 20000 } }' It will results the following JSON. { \"event\": { \"messageId\": \"285703c1-866a-4a88-8e1f-e75543e18373\", \"discountAmount\": 15.0, \"discountApplied\": true } }","title":"Siddhi Docker Configurations"},{"location":"docs/guides/database-static-rule-processing/guide/#deploy-on-kubernetes","text":"","title":"Deploy on Kubernetes"},{"location":"docs/guides/database-static-rule-processing/guide/#prerequisites_1","text":"Kubernetes cluster Minikube Google Kubernetes Engine(GKE) Docker for Mac Install HELM","title":"Prerequisites"},{"location":"docs/guides/database-static-rule-processing/guide/#siddhi-kubernetes-configurations","text":"In the tooling editor itself, you can export your Siddhi app into a runnable Kubernetes artifact. You can go to Export- For Kubernetes and it will give to a zip file that contained the following files. \u251c\u2500\u2500 Dockerfile \u251c\u2500\u2500 configurations.yaml \u251c\u2500\u2500 jars \u2502 \u2514\u2500\u2500 mysql-connector.jar \u251c\u2500\u2500 siddhi-files \u2502 \u2514\u2500\u2500 ComboSuperMartPromoProcess.siddhi.siddhi \u2514\u2500\u2500 siddhi-process.yaml First, you need to install the Siddhi Kubernetes operator using following commands. For more details about the Siddhi operator refer to this documentation . $ kubectl apply -f https://github.com/siddhi-io/siddhi-operator/releases/download/v0.2.0/00-prereqs.yaml $ kubectl apply -f https://github.com/siddhi-io/siddhi-operator/releases/download/v0.2.0/01-siddhi-operator.yaml Now you have to set up MySQL in your Kubernetes cluster. To do that use the following helm command. $ helm install --name mysql-db --set mysqlRootPassword=siddhiio,mysqlUser=siddhi_user,mysqlPassword=siddhiio,mysqlDatabase=ComboSuperMart stable/mysql Then, you can set a port forwarding to the MySQL service which allows you to connect from the Host $ kubectl port-forward svc/mysql-db 3307:3306 Now you can access the MySQL cluster externally. Accessing MySQL cluster externally you can create ComboSuperMart database and the database schema. Using jdbc:mysql://mysql-db:3306/ComboSuperMart URL you can access the database. Database URL for Kubernetes When you changing the deployment.yaml configuration of the data source in the Kubernetes export process, you have to specify this URL in the configuration as below. - name: COMBO_SUPERMART_DB description: The datasource used for lending process of the Combo supermarket jndiConfig: name: jdbc/ComboSuperMart definition: type: RDBMS configuration: jdbcUrl: jdbc:mysql://mysql-db:3306/ComboSuperMart username: siddhi_user password: siddhiio driverClassName: com.mysql.jdbc.Driver maxPoolSize: 10 idleTimeout: 60000 connectionTestQuery: SELECT 1 validationTimeout: 30000 isAutoCommit: false Siddhi operator will automatically set up the external access to any HTTP service in a Siddhi app. To set up that external access Siddhi operator by default uses ingress NGINX. Set up the NGINX controller in your Kubernetes cluster as described in this document . Now you need to create your own docker image with including all the custom libraries and configuration changes that you have made. Use following command to build and push the docker image with the tag DOCKER_HUB_USER_NAME /siddhi-runner-alpine:latest . $ docker build -t DOCKER_HUB_USER_NAME /siddhi-runner-alpine:latest . $ docker push DOCKER_HUB_USER_NAME /siddhi-runner-alpine:latest After the Kubernetes export now you already have this siddhi-process.yaml file. apiVersion: siddhi.io/v1alpha2 kind: SiddhiProcess metadata: name: combo-super-mart spec: apps: - script: | @App:name(\"ComboSuperMartPromoProcess\") @App:description(\"The promotion selection process of Combo super mart.\") /* Purpose: The combo supermart has multiple promotion card that given to their loyal users. For a particular season combo supermart plan to give discounts for each card. These discounts will change according to the card type and several rules. So, combo supermart needed an automatic system to retrieve the discounts for a given card. To do that we implement this Siddhi app which interacts with a MySQL database and executes their functionalities. All card details and rules were stored in a MySQL database. All the rules for a particular card were in the templated format. Therefore all rules executed dynamically and give the final result. Input: HTTP POST with JSON payload { \"event\": { \"promoCardId\": \"PC001\", \"amount\": 20000 } } Output: { \"event\": { \"messageId\": \"1817d06b-22ae-438e-990d-42fedcb50607\", \"discountAmount\": 15.0, \"discountApplied\": true } } */ -- HTTP source @source( type='http-service', source.id='adder', receiver.url='http://0.0.0.0:8088/comboSuperMart/promo', basic.auth.enabled='', @map(type='json', @attributes(messageId='trp:messageId', promoCardId='$.event.promoCardId', amount='$.event.amount')) ) define stream InputTransactionStream(messageId string, promoCardId string, amount long); -- RDBMS data stores @Store(type=\"rdbms\", datasource=\"COMBO_SUPERMART_DB\") define table Customer(customerId string, promoCardId string); @Store(type=\"rdbms\", datasource=\"COMBO_SUPERMART_DB\") define table PromoCard(promoCardId string, promoCardTypeId string, promoCardIssueDate string); @Store(type=\"rdbms\", datasource=\"COMBO_SUPERMART_DB\") define table PromoRule(promoRuleId string, promoCardTypeId string, promoRule string); @Store(type=\"rdbms\", datasource=\"COMBO_SUPERMART_DB\") define table PromoCardType(promoCardTypeId string, promoCardTypeDiscount double); -- Output stream @sink(type='http-service-response', source.id='adder', message.id='{{messageId}}', @map(type = 'json')) define stream ResultStream (messageId string, discountAmount double, discountApplied bool); -- Find and execute rules @info(name='get-promocard-type') from InputTransactionStream#window.length(1) join PromoCard on InputTransactionStream.promoCardId==PromoCard.promoCardId select InputTransactionStream.messageId, InputTransactionStream.promoCardId, PromoCard.promoCardTypeId, PromoCard.promoCardIssueDate, InputTransactionStream.amount insert into GetPromoCardTypeStream; @info(name='get-promocard-type-details') from GetPromoCardTypeStream#window.length(1) join PromoCardType on GetPromoCardTypeStream.promoCardTypeId==PromoCardType.promoCardTypeId select GetPromoCardTypeStream.messageId, GetPromoCardTypeStream.promoCardId, GetPromoCardTypeStream.promoCardTypeId, GetPromoCardTypeStream.amount, PromoCardType.promoCardTypeDiscount, GetPromoCardTypeStream.promoCardIssueDate insert into GetPromoCardTypeDetailsStream; @info(name='get-promocard-rules') from GetPromoCardTypeDetailsStream#window.length(1) right outer join PromoRule on GetPromoCardTypeDetailsStream.promoCardTypeId==PromoRule.promoCardTypeId select GetPromoCardTypeDetailsStream.messageId, GetPromoCardTypeDetailsStream.promoCardId, GetPromoCardTypeDetailsStream.promoCardTypeId, GetPromoCardTypeDetailsStream.amount, GetPromoCardTypeDetailsStream.promoCardTypeDiscount, PromoRule.promoRuleId, PromoRule.promoRule, GetPromoCardTypeDetailsStream.promoCardIssueDate insert into RuleStream; @info(name='execute-promocard-rules') from RuleStream#window.length(1) select RuleStream.messageId, RuleStream.promoCardTypeDiscount, js:eval(str:fillTemplate(RuleStream.promoRule, map:create(\"amount\", RuleStream.amount, \"cardPeriod\", time:dateDiff(time:currentDate(), RuleStream.promoCardIssueDate, 'yyyy-MM-dd', 'yyyy-MM-dd'))), 'bool') as result insert into RuleReslutsStream; -- Output results @info(name='filter-true-rules') from RuleReslutsStream[result == true]#window.batch() select RuleReslutsStream.messageId, RuleReslutsStream.promoCardTypeDiscount, count(result) as result insert into TrueResultStream; @info(name='get-all-results') from RuleReslutsStream#window.batch() select RuleReslutsStream.messageId, RuleReslutsStream.promoCardTypeDiscount, count(result) as result insert into AllResultStream; @info(name='discout-reply-stream') from AllResultStream#window.length(1) unidirectional join TrueResultStream#window.length(1) on TrueResultStream.result==AllResultStream.result select AllResultStream.messageId, AllResultStream.promoCardTypeDiscount as discountAmount, true as discountApplied insert into ResultStream; @info(name='filter-true-rules-and-reply') from RuleReslutsStream[result == false]#window.batch() select RuleReslutsStream.messageId, 0.00 as discountAmount, false as discountApplied insert into ResultStream; runner: | wso2.carbon: id: siddhi-runner name: Siddhi Runner Distribution ports: offset: 0 transports: http: listenerConfigurations: - id: default host: 0.0.0.0 port: 9090 - id: msf4j-https host: 0.0.0.0 port: 9443 scheme: https keyStoreFile: ${carbon.home}/resources/security/wso2carbon.jks keyStorePassword: wso2carbon certPass: wso2carbon transportProperties: - name: server.bootstrap.socket.timeout value: 60 - name: client.bootstrap.socket.timeout value: 60 - name: latency.metrics.enabled value: true dataSources: - name: WSO2_CARBON_DB description: The datasource used for registry and user manager definition: type: RDBMS configuration: jdbcUrl: jdbc:h2:${sys:carbon.home}/wso2/${sys:wso2.runtime}/database/WSO2_CARBON_DB;DB_CLOSE_ON_EXIT=FALSE;LOCK_TIMEOUT=60000 username: wso2carbon password: wso2carbon driverClassName: org.h2.Driver maxPoolSize: 10 idleTimeout: 60000 connectionTestQuery: SELECT 1 validationTimeout: 30000 isAutoCommit: false - name: COMBO_SUPERMART_DB description: The datasource used for lending process of the Combo supermarket jndiConfig: name: jdbc/ComboSuperMart definition: type: RDBMS configuration: jdbcUrl: jdbc:mysql://mysql-db:3306/ComboSuperMart username: siddhi_user password: siddhiio driverClassName: com.mysql.jdbc.Driver maxPoolSize: 10 idleTimeout: 60000 connectionTestQuery: SELECT 1 validationTimeout: 30000 isAutoCommit: false persistentVolumeClaim: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi storageClassName: standard volumeMode: Filesystem container: image: DOCKER_HUB_USER_NAME /siddhi-runner-alpine:latest Now you can install the SiddhiProcess using following kubectl command. Before you install the SiddhiProcess you have to add the docker image tag in the siddhi-process.yaml file. You have to add the docker image name( DOCKER_HUB_USER_NAME /siddhi-runner-alpine:latest ) in the YAML entry spec.container.image . $ kubectl apply -f siddhi-process.yaml The ingress created by Siddhi operator will use the hostname as siddhi. Therefore you have to update your /etc/host file with siddhi hostname along with the external IP of ingress. External IP of Ingress For minikube the ingress external IP is minikube IP and in docker, for Mac, the external IP is 0.0.0.0. For more details about Siddhi, ingress setup refer to this documentation . Now you can send HTTP requests to deployed the Siddhi app in the Kubernetes cluster using following CURL command. curl -X POST \\ http://siddhi/combo-super-mart-0/8088/comboSuperMart/promo \\ -H 'Accept: */*' \\ -H 'Accept-Encoding: gzip, deflate' \\ -H 'Content-Type: application/json' \\ -H 'Host: siddhi' \\ -d '{ \"event\": { \"promoCardId\": \"PC001\", \"amount\": 20000 } }' It will results the following JSON. { \"event\": { \"messageId\": \"285703c1-866a-4a88-8e1f-e75543e18374\", \"discountAmount\": 15.0, \"discountApplied\": true } }","title":"Siddhi Kubernetes Configurations"},{"location":"docs/guides/fault-tolerance/guide/","text":"Data Preprocessing, Fault Tolerance, and Error Handling In this guide, we are going to understand some interesting topics around streaming data integration; they are data preprocessing, fault tolerance and error handling. To understand these capabilities, we are going to consider a health care use case. Scenario - Processing Health Care Events (Glucose readings from sensors) In this scenario, Glucose reading events are received from sensors that mounted on patients. These events are received to the Stream Processing engine, get preprocessed, unrelated attributes are removed and send them to another processing layer to process if there are any abnormal behavior observed. The following sections are available in this guide. What you'll build Prerequisites Implementation Testing Deployment Output What you'll build By following this guide, you will understand the capabilities of Siddhi streaming engine related to data preprocessing, fault tolerance and error handling. To understand better, let\u2019s consider a real-world use case in the health care sector. Let\u2019s jump into the use case directly. Let\u2019s consider a hospital (or healthcare institution) which primarily provide treatment to diabetic patients. Then, as you are aware it is important to keep close monitoring about the Glucose level of the patients and act accordingly. Then, there is a sensor mounted to each patient to track the Glucose level of the patients. These Glucose reading events are pushed to the central data collection hub, then it pushes those events to Kafka message broker to allow respective interested parties(systems) to consume those events for further processing. These events are published as Avro type messages to Kafka then the Siddhi Stream processing engine consume those events from Kafka message broker, perform some preprocessing, identify abnormal events and push them to an HTTP endpoint. In this complete flow, reliable data processing is a mandatory requirement and cannot lose any events due to failures then there should be proper error handling and fault tolerance features are activated and in place to avoid it. Now, let\u2019s understand how this could be implemented in Siddhi engine. Prerequisites Below are the prerequisites that should be considered to implement the above use case. Mandatory Requirements Siddhi tooling VM/Local distribution One of the Siddhi runner distributions VM/Local Runtime Docker Image K8S Operator (commands are given in deployment section) Kafka Distribution MySQL Database Java 8 or higher Requirements needed to deploy Siddhi in Docker/Kubernetes Docker Minikube or Google Kubernetes Engine(GKE) Cluster or Docker for Mac Implementation Events are consumed by Siddhi engine from Kafka message broker. These events are AVRO type. Siddhi performs preprocessing for received events and checks for abnormal events. If there are any abnormal Glucose reading found then it is forwarded to another processing layer through HTTP. To cater to reliable event messaging, necessary fault tolerance and error handling mechanisms are enabled in Siddhi Stream Processor. Implement Streaming Queries Start the Siddhi tooling runtime and go to the editor UI in http://localhost:9390/editor Follow below steps to start the Siddhi tooling runtime. * Extract the downloaded zip and navigate to /bin. (TOOLING_HOME refers to the extracted folder) * Issue the following command in the command prompt (Windows) / terminal (Linux/Mac) For Windows: tooling.bat For Linux/Mac: ./tooling.sh Select File - New option, then you could either use the source view or design view to write/build the Siddhi Application. You can find the Siddhi Application bellow, that implements the requirements mentioned above. Let\u2019s write (develop) the Siddhi Application, as given below. Once the Siddhi app is created, you can use the Event Simulator option in the editor to simulate events to streams and perform developer testing. @App:name(\"Glucose-Reading-PreProcessing-App\") @App:description('Process Glucose Readings received from patients') -- Kakfka source which consumes Glucose reading events @source(type='kafka', topic.list='glucose-readings', partition.no.list='0', threading.option='single.thread', group.id=\"group\", is.binary.message='true', bootstrap.servers='${KAFKA_BOOTSTRAP_SERVER_URL}', @map(type='avro',schema.def=\"\"\" { \"type\": \"record\", \"name\": \"glucose_reading\", \"namespace\": \"glucose\", \"fields\": [{ \"name\": \"locationRoom\", \"type\": \"string\" }, { \"name\": \"locationBed\", \"type\": \"string\" }, { \"name\": \"timeStamp\", \"type\": \"string\" }, { \"name\": \"sensorID\", \"type\": \"long\" }, { \"name\": \"patientGroup\", \"type\": \"string\" }, { \"name\": \"patientFirstName\", \"type\": \"string\" }, { \"name\": \"patientLastName\", \"type\": \"string\" }, { \"name\": \"sensorValue\", \"type\": \"double\" }, { \"name\": \"unitOfMeasure\", \"type\": \"string\" }] }\"\"\")) define stream GlucoseReadingStream (locationRoom string, locationBed string, timeStamp string, sensorID long, patientGroup string, patientFirstName string, patientLastName string, sensorValue double, unitOfMeasure string); -- HTTP sink which publishes abnormal Glucose reading related events. -- If there is any errors occurred when publishing events to HTTP endpoint then respective events are sent to error stream @OnError(action='STREAM') @sink(type = 'http', blocking.io='true', publisher.url = \"http://localhost:8080/logger\", method = \"POST\", @map(type = 'json')) define stream AbnormalGlucoseReadingStream (timeStampInLong long, locationRoom string, locationBed string, sensorID long, patientGroup string, patientFullName string, sensorReadingValue double, unitOfMeasure string, abnormalReadingCount long); -- RDBMS event table which stores the failed events when publishing to HTTP endpoint @Store(type=\"rdbms\", jdbc.url=\"${MYSQL_DB_URL}\", username=\"${MYSQL_USERNAME}\", password=\"${MYSQL_PASSWORD}\" , jdbc.driver.name=\"com.mysql.jdbc.Driver\") @PrimaryKey(\"locationRoom\", \"locationBed\", \"sensorID\", \"failedEventTime\") define table FailedAbnormalReadingTable (failedEventTime string, originTime long, locationRoom string, locationBed string, sensorID long, patientFullName string, sensorReadingValue double, rootCause string); -- Javascript function which converts the Glucose sensor reading to mg/dl define function sensorReadingInMGDL[JavaScript] return double { var sensorReading = data[0]; var metricUnit = data[1]; if(metricUnit === 'MGDL') { return metricUnit; } return sensorReading * 18; }; @info(name='Glucose-preprocessing') from GlucoseReadingStream select math:parseLong(timeStamp) as timeStampInLong, locationRoom, locationBed, sensorID, patientGroup, str:concat(patientFirstName, \" \", patientLastName) as patientFullName, sensorReadingInMGDL(sensorValue, unitOfMeasure) as sensorReadingValue, 'MGDL' as unitOfMeasure insert into PreProcessedGlucoseReadingStream; @info(name='Abnormal-Glucose-reading-identifier') from PreProcessedGlucoseReadingStream[sensorReadingValue 100]#window.time(15 min) select timeStampInLong, locationRoom, locationBed, sensorID, patientGroup, patientFullName, sensorReadingValue, unitOfMeasure, count() as abnormalReadingCount group by locationRoom, locationBed, sensorID having abnormalReadingCount 3 output first every 15 min insert into AbnormalGlucoseReadingStream; @info(name = 'Error-handler') from !AbnormalGlucoseReadingStream#log(\"Error Occurred!\") select time:currentTimestamp() as failedEventTime, timeStampInLong as originTime, locationRoom, locationBed, sensorID, patientFullName, sensorReadingValue, convert(_error, 'string') as rootCause insert into FailedAbnormalReadingStream; @info(name='Dump-to-event-store') from FailedAbnormalReadingStream select * insert into FailedAbnormalReadingTable; Source view of the Siddhi app. Below is the flow diagram of the above Siddhi App. Testing NOTE: In the provided Siddhi app, there are some environmental variables (MYSQL_DB_URL, MYSQL_USERNAME, MYSQL_PASSWORD, and KAFKA_BOOTSTRAP_SERVER_URL) are used. These values are required to be set to try out the scenario end to end. MYSQL related environmental variables are required to store the events which are failed to publish to the HTTP endpoint. Environmental variable KAFKA_BOOTSTRAP_SERVER_URL is the Kafka endpoint URL where Siddhi listens and consume events from. MYSQL_DB_URL: MySQL database jdbc url to persist failed events. (eg: 'jdbc:mysql://localhost:3306/HCD') MYSQL_USERNAME: Username of the user account to connect MySQL database. (eg: 'root') MYSQL_PASSWORD: Password of the user account to connect MySQL database. (eg: 'root') KAFKA_BOOTSTRAP_SERVER_URL: List of Kafka servers to which the Siddhi Kafka source must listen. (eg: 'localhost:9092') Setup Kafka As a prerequisite, you have to start the Kafka message broker. Please follow better steps. 1. Download the Kafka distribution 2. Unzip the above distribution and go to the \u2018bin\u2019 directory 3. Start the zookeeper by executing below command, zookeeper-server-start.sh config/zookeeper.properties 4. Start the Kafka broker by executing below command, kafka-server-start.sh config/server.properties Refer the Kafka documentation for more details, https://kafka.apache.org/quickstart Then, you have to add necessary client jars (from /libs directory) to Siddhi distribution as given below. Copy below client libs to /bundles directory scala-library-2.12.8.jar zkclient-0.11.jar zookeeper-3.4.14.jar Copy below client libs to jars directory kafka-clients-2.3.0.jar kafka_2.12-2.3.0.jar metrics-core-2.2.0.jar bundles directory to add OSGI bundles and jars directory to add non-OSGI jars. Setup MySQL Download and Install MySQL database as per the guidelines (https://www.mysql.com/downloads/) Log in to the MySQL server and create a database called \u201cHCD\u201d Download the MySQL client connector jar and add it to jars (if it is non OSGI) or bundles (if it is OSGI bundle) directory of Siddhi distribution Tryout There are multiple options available to test the developed Siddhi App. As mentioned in the previous step you could simply simulate some events directly into the stream and test your queries. But, if you are willing to test the end to end flow (from an input source to sink) then you can start the Siddhi app in the editor itself. In this guide, we are going to run the Siddhi App in the editor itself. Once the server is started, you will see below logs get printed in the editor console. In the above provided Siddhi App, last 15 minutes events are kept in memory for processing (because 15 minutes time window is defined in the query). As mentioned in the use case these events (Glucose reading of the patients) are very sensitive hence durable messaging is a requirement; losing a single event might cause a huge impact. In this situation, if the Siddhi server goes down while processing events then there is a possibility that events kept in the memory get lost. Then, to avoid this you should enable state persistence to Siddhi distribution. State persistence store in-memory state into the file system or database in a time interval. If the Siddhi runtime failed due to some reasons then Siddhi can recover to the state which is stored and continue processing. But, even though state persistence is enabled still there is a possibility that some intermediate events get lost due to runtime failures (or machine failures). For example, let\u2019s assume you have enabled snapshot persistence interval as 5 minutes (means, in-memory state get persisted to file system/DB each 5 minutes) then, if Siddhi runtime failed at 4 th minute after consuming events for last 4 minutes then you will be anyway losing last 4 minutes events even though you could recover to the previous state. To overcome this, we could leverage the \u201cOffset\u201d support provided by Kafka. Kafka is a publish-subscribe architecture that can handle multiple subscribers. To keep track of the consumers and how many events they have consumed Kafka uses something called \u201coffsets\u201d. An offset is a unique identifier which shows the sequential ID of a record within a Kafka topic partition. The offset is controlled by the subscriber. When the subscriber reads a record it advances its offset by one. In the snapshot persistence, Kafka offset value is also get persisted then when the specific snapshot is restored then Siddhi can request events from Kafka with the restored offset then Kafka can send those events of that specific offset. You can configure the snapshot state persistence with the below configuration. Save below configuration as YAML file (for example snapshot.yaml) state.persistence: enabled: true intervalInMin: 1 revisionsToKeep: 2 persistenceStore: io.siddhi.distribution.core.persistence.FileSystemPersistenceStore config: location: siddhi-app-persistence Then start the Siddhi runtime with below command. ./tooling.sh -Dconfig= File-Path /snapshot.yaml Invoking the Siddhi App Now, you can publish events to Kafka message broker using a Kafka publisher. In this case, you can use the sample Kafka publisher implementation given in here When you send more than 3 events (where sensorReading value is greater than 100) to Kafka within 15 minutes then it is identified as the abnormal situation and there will be an alert sent to HTTP endpoint as defined in the above provided Siddhi application. In this scenario, to demonstrate the error handling scenario, Siddhi app is configured to send the events to an unavailable endpoint. In this situation, since Siddhi cannot publish those events to HTTP endpoint then respective error handling flow gets activated as per the configuration (@onError (action=\u2019STREAM\u2019)) and respective failed events are pushed to the error stream as per the configuration (in this case, those failed events are pushed to the stream !AbnormalGlucoseReadingStream). Then, those events are dumped to a database for further reference for respective authorities. In this flow, Siddhi guarantees that there is no events get lost and ensure the durability of the system. You can see, as per the below figure you can find the respective failed events in the database table. Deployment Once you are done with the development, export the Siddhi app that you have developed with 'File' - 'Export File' option. You can deploy the Siddhi app using any of the methods listed below. Deploy on VM/ Bare Metal Prerequisites First, please make sure that necessary prerequisites are met as given the Testing section . Apache Kafka and MySQL are required to try out the use case. Then, as given in Setup Kafka and Setup MySQL section. Download the Kafka distribution and start it up and download the MySQL database and install it. Then create a database called \u201cHCD\u201d in the MySQL database. Siddhi Runtime Configuration Make sure to set the necessary environmental variables as given above. Note: In the above provided Siddhi app, there are some environmental variables (MYSQL_DB_URL, MYSQL_USERNAME, and MYSQL_PASSWORD) are used. These values are required to be set to try out the scenario end to end. MYSQL related environmental variables are required to store the events which are failed to publish to the HTTP endpoint. Environmental variable KAFKA_BOOTSTRAP_SERVER_URL is the Kafka endpoint URL where Siddhi listens and consume events from. Hence, make sure to set the environmental variables with the proper values in the system (make sure to follow necessary steps based on the underneath operating system). Download the latest Siddhi Runner distribution . Unzip the siddhi-runner-x.x.x.zip . You have to copy necessary Kafka and Mysql client jars to Siddhi runner distribution to connect with Kafka and MySQL database. Download the Kafka distribution and copy below Kafka client jars from /lib folder Copy below client libs to /bundles directory scala-library-2.12.8.jar zkclient-0.11.jar zookeeper-3.4.14.jar Copy below client libs to jars directory kafka-clients-2.3.0.jar kafka_2.12-2.3.0.jar metrics-core-2.2.0.jar Then, copy the MySQL client connector jar to jars (if it is non OSGI) or bundles (if it is OSGI bundle) directory of Siddhi runner. Please refer this . Start Siddhi app with the runner config by executing the following commands from the distribution directory. Linux/Mac : ./bin/runner.sh -Dapps= siddhi-file-path -Dconfig= config-yaml-path Windows : bin\\runner.bat -Dapps= siddhi-file-path -Dconfig= config-yaml-path Eg: If exported siddhi app in Siddhi home directory, ./bin/runner.sh -Dapps=Glucose-Reading-PreProcessing-App.siddhi -Dconfig=snapshot.yaml Note: snapshot.yaml file contains the configuration to enable state snapshot persistence Once server is started, download the sample Kafka Avro event generator from here and execute below command. java -jar kafka-avro-producer-1.0.0-jar-with-dependencies.jar Above event publishes send 4 Avro events to generate an abnormal behavior as defined in the Siddhi application. You can change the kafka endpoint and topic by passing them as java arguments. If not, sample client consider \u201clocalhost:9092\u201d as the kafka bootstrap server endpoint and \u201cglucose-readings\u201d as the topic. You can find the sample client source code in here In this situation, you can find a log gets printed in the Siddhi runner console/log and respective failed event is added to the database with the error cause. Deploy on Docker Prerequisites Apache Kafka and MySQL are the external dependencies for this use case. Hence, you could use the corresponding docker artifacts to test the requirement. First, you can create a docker network for the deployment as shown below docker network create siddhi-tier --driver bridge Then, you can get the MySQL docker image from here and run it with below command. We are going to use mysql version 5.7.27. Start the MySQL docker images with below command, docker run --name mysql-server --network siddhi-tier -e MYSQL_ROOT_PASSWORD=root e1e1680ac726 e1e1680ac726 is the MySQL docker image id in this case Login to the MySQL docker instance and create a database called \u201cHCD\u201d. Then, you can pull the Kafka docker image and deploy it. There is no any official Apache Kafka image is available in docker hub hence you can use the Kafka docker image provided by bitnami in here Launch the Zookeeper server instance with below provided command, docker run -d --name zookeeper-server --network siddhi-tier -e ALLOW_ANONYMOUS_LOGIN=yes bitnami/zookeeper:latest Launch the Kafka server instance with below provide command, docker run -d --name kafka-server --network siddhi-tier -e ALLOW_PLAINTEXT_LISTENER=yes -e KAFKA_CFG_ZOOKEEPER_CONNECT=zookeeper-server:2181 bitnami/kafka:latest Now, you have configured necessary prerequisites that required to run the use case. Siddhi Docker Configuration Since, there are some external client jars (Kafka MySQL) are required for the Siddhi runner. You have to create the docker image accordingly. Below is the sample Docker file created FROM siddhiio/siddhi-runner-base-alpine:5.1.0-alpha MAINTAINER Siddhi IO Docker Maintainers \"siddhi-dev@googlegroups.com\" ARG HOST_BUNDLES_DIR=./files/bundles ARG HOST_JARS_DIR=./files/jars ARG JARS=${RUNTIME_SERVER_HOME}/jars ARG BUNDLES=${RUNTIME_SERVER_HOME}/bundles # copy bundles jars to the siddhi-runner distribution COPY --chown=siddhi_user:siddhi_io ${HOST_BUNDLES_DIR}/ ${BUNDLES} COPY --chown=siddhi_user:siddhi_io ${HOST_JARS_DIR}/ ${JARS} # expose ports EXPOSE 9090 9443 9712 9612 7711 7611 7070 7443 RUN bash ${RUNTIME_SERVER_HOME}/bin/install-jars.sh STOPSIGNAL SIGINT ENTRYPOINT [\"/home/siddhi_user/siddhi-runner/bin/runner.sh\", \"--\"] Here, you have to create two folders called bundles and jars to add necessary external client dependencies to the docker image. You can refer the official Siddhi documentation reference for this purpose. Once, Dockerfile is created you can create the docker image with below command. docker build -t siddhi_for_kafka . Create a folder locally (eg: /home/siddhi-artifacts) and copy the Siddhi app in to it. Also, create a YAML file (snapshot.yaml) with below configuration to enable state snapshot persistence in the created folder. state.persistence: enabled: true intervalInMin: 1 revisionsToKeep: 2 persistenceStore: io.siddhi.distribution.core.persistence.FileSystemPersistenceStore config: location: /siddhi-app-persistence Then, you can run the Siddhi docker image that you created with necessary external dependencies to work with Kafka and MySQL. docker run --network siddhi-tier -it -v /Users/mohan/scratch/siddhi-artifacts:/artifacts -v /Users/mohan/scratch/local-mount:/siddhi-app-persistence -e MYSQL_DB_URL=jdbc:mysql://mysql-server:3306/HCD -e MYSQL_USERNAME=root -e MYSQL_PASSWORD=root -e KAFKA_BOOTSTRAP_SERVER_URL=kafka-server:9092 siddhi_for_kafka:latest -Dapps=/artifacts/Glucose-Reading-PreProcessing-App.siddhi -Dconfig=/artifacts/snapshot.yaml Note: In the above provided Siddhi app, there are some environmental variables (MYSQL_DB_URL, MYSQL_USERNAME and MYSQL_PASSWORD) are used. These values are required to be set to tryout the scenario end to end. MYSQL related environmental variables are required to store the events which are failed to publish to the HTTP endpoint. Environmental variable KAFKA_BOOTSTRAP_SERVER_URL is the Kafka endpoint url where Siddhi listens and consume events from. You can use the sample Kafka publisher client available in docker hub to simulate required events. Use the below command to use the sample docker Kafka publisher client. docker run --network siddhi-tier -it mohanvive/kafka-event-publisher:latest Then, you could see below log gets printed in the Siddhi runner console and failed events are stored in the database table. Deploy on Kubernetes It is advisable to create a namespace in Kubernetes to follow below steps. kubectl create ns siddhi-kafka-test There are some prerequisites that you should meet to tryout below SiddhiProcess. Such as configure MySQL database and Kafka messaging system in Kubernetes. First, configure the MySQL server within the above created namespace. You can use the official helm chart provided for MySQL. First, install the MySQL helm chart as shown below, helm install --name mysql-db --namespace=siddhi-kafka-test --set mysqlRootPassword=root,mysqlDatabase=HCD stable/mysql Here, you can define the root password to connect to the MYSQL database and also define the database name. BTW, make sure to do helm init if it is not done yet. Then, you can set a port forwarding to the MySQL service which allows you to connect from the Host. kubectl port-forward svc/mysql-db 13306:3306 --namespace=siddhi-kafka-test Then, you can login to the MySQL server from your host machine as shown below. Next, you can configure Kafka messaging system in the Kubernetes (in the above created namespace). You can use the Kafka Helm chart for this purpose. First, install Kafka helm chart as shown below. helm install --name my-kafka incubator/kafka --namespace=siddhi-kafka-test Then, you could find required Kafka broker pods and Zookeeper pods are getting created. Then, you can install Siddhi Operator To install the Siddhi Kubernetes operator run the following commands. kubectl apply -f https://github.com/siddhi-io/siddhi-operator/releases/download/v0.2.0-alpha/00-prereqs.yaml --namespace=siddhi-kafka-test kubectl apply -f https://github.com/siddhi-io/siddhi-operator/releases/download/v0.2.0-alpha/01-siddhi-operator.yaml --namespace=siddhi-kafka-test You can verify the installation by making sure the following deployments are running in your Kubernetes cluster. Siddhi applications can be deployed on Kubernetes using the Siddhi operator. To deploy the above created Siddhi app, we have to create custom resource object yaml file (with the kind as SiddhiProcess) as given below apiVersion: siddhi.io/v1alpha2 kind: SiddhiProcess metadata: name: glucose-reading-preprocessing-app spec: apps: - script: | @App:name(\"Glucose-Reading-PreProcessing-App\") @App:description('Process Glucose Readings received from patients') -- Kakfka source which consumes Glucose reading events @source(type='kafka', topic.list='glucose-readings', partition.no.list='0', threading.option='single.thread', group.id=\"group\", is.binary.message='true', bootstrap.servers='${KAFKA_BOOTSTRAP_SERVER_URL}', @map(type='avro',schema.def=\"\"\" { \"type\": \"record\", \"name\": \"glucose_reading\", \"namespace\": \"glucose\", \"fields\": [{ \"name\": \"locationRoom\", \"type\": \"string\" }, { \"name\": \"locationBed\", \"type\": \"string\" }, { \"name\": \"timeStamp\", \"type\": \"string\" }, { \"name\": \"sensorID\", \"type\": \"long\" }, { \"name\": \"patientGroup\", \"type\": \"string\" }, { \"name\": \"patientFirstName\", \"type\": \"string\" }, { \"name\": \"patientLastName\", \"type\": \"string\" }, { \"name\": \"sensorValue\", \"type\": \"double\" }, { \"name\": \"unitOfMeasure\", \"type\": \"string\" }] }\"\"\")) define stream GlucoseReadingStream (locationRoom string, locationBed string, timeStamp string, sensorID long, patientGroup string, patientFirstName string, patientLastName string, sensorValue double, unitOfMeasure string); -- HTTP sink which publishes abnormal Glucose reading related events. -- If there is any errors occurred when publishing events to HTTP endpoint then respective events are sent to error stream @OnError(action='STREAM') @sink(type = 'http', blocking.io='true', publisher.url = \"http://localhost:8080/logger\", method = \"POST\", @map(type = 'json')) define stream AbnormalGlucoseReadingStream (timeStampInLong long, locationRoom string, locationBed string, sensorID long, patientGroup string, patientFullName string, sensorReadingValue double, unitOfMeasure string, abnormalReadingCount long); -- RDBMS event table which stores the failed events when publishing to HTTP endpoint @Store(type=\"rdbms\", jdbc.url=\"${MYSQL_DB_URL}\", username=\"${MYSQL_USERNAME}\", password=\"${MYSQL_PASSWORD}\" , jdbc.driver.name=\"com.mysql.jdbc.Driver\") @PrimaryKey(\"locationRoom\", \"locationBed\", \"sensorID\", \"failedEventTime\") define table FailedAbnormalReadingTable (failedEventTime string, originTime long, locationRoom string, locationBed string, sensorID long, patientFullName string, sensorReadingValue double, rootCause string); -- Javascript function which converts the Glucose sensor reading to mg/dl define function sensorReadingInMGDL[JavaScript] return double { var sensorReading = data[0]; var metricUnit = data[1]; if(metricUnit === 'MGDL') { return metricUnit; } return sensorReading * 18; }; @info(name='Glucose-preprocessing') from GlucoseReadingStream select math:parseLong(timeStamp) as timeStampInLong, locationRoom, locationBed, sensorID, patientGroup, str:concat(patientFirstName, \" \", patientLastName) as patientFullName, sensorReadingInMGDL(sensorValue, unitOfMeasure) as sensorReadingValue, 'MGDL' as unitOfMeasure insert into PreProcessedGlucoseReadingStream; @info(name='Abnormal-Glucose-reading-identifier') from PreProcessedGlucoseReadingStream[sensorReadingValue 100]#window.time(15 min) select timeStampInLong, locationRoom, locationBed, sensorID, patientGroup, patientFullName, sensorReadingValue, unitOfMeasure, count() as abnormalReadingCount group by locationRoom, locationBed, sensorID having abnormalReadingCount 3 output first every 15 min insert into AbnormalGlucoseReadingStream; @info(name = 'Error-handler') from !AbnormalGlucoseReadingStream#log(\"Error Occurred!\") select time:currentTimestamp() as failedEventTime, timeStampInLong as originTime, locationRoom, locationBed, sensorID, patientFullName, sensorReadingValue, convert(_error, 'string') as rootCause insert into FailedAbnormalReadingStream; @info(name='Dump-to-event-store') from FailedAbnormalReadingStream select * insert into FailedAbnormalReadingTable; persistentVolumeClaim: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi storageClassName: hostpath volumeMode: Filesystem runner: | state.persistence: enabled: true intervalInMin: 1 revisionsToKeep: 2 persistenceStore: io.siddhi.distribution.core.persistence.FileSystemPersistenceStore config: location: /siddhi-app-persistence container: env: - name: MYSQL_DB_URL value: \"jdbc:mysql://mysql-db:3306/HCD\" - name: MYSQL_USERNAME value: \"root\" - name: MYSQL_PASSWORD value: \"root\" - name: KAFKA_BOOTSTRAP_SERVER_URL value: \"my-kafka-headless:9092\" image: \"mohanvive/siddhi_for_kafka:latest\" Note: In the above provided Siddhi app, there are some environmental variables (MYSQL_DB_URL, MYSQL_USERNAME and MYSQL_PASSWORD) are used. These values are required to be set to tryout the scenario end to end. MYSQL related environmental variables are required to store the events which are failed to publish to the HTTP endpoint. Environmental variable KAFKA_BOOTSTRAP_SERVER_URL is the Kafka endpoint url where Siddhi listens and consume events from. Hence, make sure to add proper values for the environmental variables in the above yaml file (check the env section of the yaml file). Here, you can use the docker image that created in the Deploy on Docker section since you need a docker images with required extensions and client jars to test it in Kubernetes. Other than that, Siddhi runtime is configured to enable state snapshot persistence under the runner entry as shown above. Now, let\u2019s create the above resource in the Kubernetes cluster with below command. kubectl --namespace=siddhi-kafka-test create -f absolute-yaml-file-path /glucose-reading-preprocessing-app.yaml Once, siddhi app is successfully deployed. You can verify its health with below Kubernetes commands Now, you can send some events to Kafka messaging server and test the use case. If you are planning to push events to Kafka messaging server from your host machine then you have to enable few external listeners in Kafka. Please check the documentation provided for the Kafka helm chart. For testing purposes, you could use the test-client pod provided by us. Create a pod with below definition. apiVersion: v1 kind: Pod metadata: name: testclient namespace: siddhi-kafka-test spec: containers: - name: kafka image: mohanvive/test-client command: - sh - -c - \"exec tail -f /dev/null\" Create a YAML file called test-client.yaml, add above pod definition in it and run below command. kubectl apply -f test-client.yaml Then, go into the above created pod using below command. kubectl exec -it testclient sh --namespace=siddhi-kafka-test Download the sample Kafka client which could publish events related to above use case. wget https://github.com/mohanvive/siddhi-sample-clients/releases/download/v1.0.0/kafka-avro-producer-1.0.0-jar-with-dependencies.jar Then execute below command to push events to Kafka messaging system. java -jar kafka-avro-producer-1.0.0-jar-with-dependencies.jar my-kafka-headless:9092 Then, as defined in the SIddhi application abnormal events get logged since it tries to publish to an unavailable endpoint. Refer here to get more details about running Siddhi on Kubernetes.","title":"Fault Tolerance & Error Handling"},{"location":"docs/guides/fault-tolerance/guide/#data-preprocessing-fault-tolerance-and-error-handling","text":"In this guide, we are going to understand some interesting topics around streaming data integration; they are data preprocessing, fault tolerance and error handling. To understand these capabilities, we are going to consider a health care use case.","title":"Data Preprocessing, Fault Tolerance, and Error Handling"},{"location":"docs/guides/fault-tolerance/guide/#scenario-processing-health-care-events-glucose-readings-from-sensors","text":"In this scenario, Glucose reading events are received from sensors that mounted on patients. These events are received to the Stream Processing engine, get preprocessed, unrelated attributes are removed and send them to another processing layer to process if there are any abnormal behavior observed. The following sections are available in this guide. What you'll build Prerequisites Implementation Testing Deployment Output","title":"Scenario - Processing Health Care Events (Glucose readings from sensors)"},{"location":"docs/guides/fault-tolerance/guide/#what-youll-build","text":"By following this guide, you will understand the capabilities of Siddhi streaming engine related to data preprocessing, fault tolerance and error handling. To understand better, let\u2019s consider a real-world use case in the health care sector. Let\u2019s jump into the use case directly. Let\u2019s consider a hospital (or healthcare institution) which primarily provide treatment to diabetic patients. Then, as you are aware it is important to keep close monitoring about the Glucose level of the patients and act accordingly. Then, there is a sensor mounted to each patient to track the Glucose level of the patients. These Glucose reading events are pushed to the central data collection hub, then it pushes those events to Kafka message broker to allow respective interested parties(systems) to consume those events for further processing. These events are published as Avro type messages to Kafka then the Siddhi Stream processing engine consume those events from Kafka message broker, perform some preprocessing, identify abnormal events and push them to an HTTP endpoint. In this complete flow, reliable data processing is a mandatory requirement and cannot lose any events due to failures then there should be proper error handling and fault tolerance features are activated and in place to avoid it. Now, let\u2019s understand how this could be implemented in Siddhi engine.","title":"What you'll build"},{"location":"docs/guides/fault-tolerance/guide/#prerequisites","text":"Below are the prerequisites that should be considered to implement the above use case.","title":"Prerequisites"},{"location":"docs/guides/fault-tolerance/guide/#mandatory-requirements","text":"Siddhi tooling VM/Local distribution One of the Siddhi runner distributions VM/Local Runtime Docker Image K8S Operator (commands are given in deployment section) Kafka Distribution MySQL Database Java 8 or higher","title":"Mandatory Requirements"},{"location":"docs/guides/fault-tolerance/guide/#requirements-needed-to-deploy-siddhi-in-dockerkubernetes","text":"Docker Minikube or Google Kubernetes Engine(GKE) Cluster or Docker for Mac","title":"Requirements needed to deploy Siddhi in Docker/Kubernetes"},{"location":"docs/guides/fault-tolerance/guide/#implementation","text":"Events are consumed by Siddhi engine from Kafka message broker. These events are AVRO type. Siddhi performs preprocessing for received events and checks for abnormal events. If there are any abnormal Glucose reading found then it is forwarded to another processing layer through HTTP. To cater to reliable event messaging, necessary fault tolerance and error handling mechanisms are enabled in Siddhi Stream Processor.","title":"Implementation"},{"location":"docs/guides/fault-tolerance/guide/#implement-streaming-queries","text":"Start the Siddhi tooling runtime and go to the editor UI in http://localhost:9390/editor Follow below steps to start the Siddhi tooling runtime. * Extract the downloaded zip and navigate to /bin. (TOOLING_HOME refers to the extracted folder) * Issue the following command in the command prompt (Windows) / terminal (Linux/Mac) For Windows: tooling.bat For Linux/Mac: ./tooling.sh Select File - New option, then you could either use the source view or design view to write/build the Siddhi Application. You can find the Siddhi Application bellow, that implements the requirements mentioned above. Let\u2019s write (develop) the Siddhi Application, as given below. Once the Siddhi app is created, you can use the Event Simulator option in the editor to simulate events to streams and perform developer testing. @App:name(\"Glucose-Reading-PreProcessing-App\") @App:description('Process Glucose Readings received from patients') -- Kakfka source which consumes Glucose reading events @source(type='kafka', topic.list='glucose-readings', partition.no.list='0', threading.option='single.thread', group.id=\"group\", is.binary.message='true', bootstrap.servers='${KAFKA_BOOTSTRAP_SERVER_URL}', @map(type='avro',schema.def=\"\"\" { \"type\": \"record\", \"name\": \"glucose_reading\", \"namespace\": \"glucose\", \"fields\": [{ \"name\": \"locationRoom\", \"type\": \"string\" }, { \"name\": \"locationBed\", \"type\": \"string\" }, { \"name\": \"timeStamp\", \"type\": \"string\" }, { \"name\": \"sensorID\", \"type\": \"long\" }, { \"name\": \"patientGroup\", \"type\": \"string\" }, { \"name\": \"patientFirstName\", \"type\": \"string\" }, { \"name\": \"patientLastName\", \"type\": \"string\" }, { \"name\": \"sensorValue\", \"type\": \"double\" }, { \"name\": \"unitOfMeasure\", \"type\": \"string\" }] }\"\"\")) define stream GlucoseReadingStream (locationRoom string, locationBed string, timeStamp string, sensorID long, patientGroup string, patientFirstName string, patientLastName string, sensorValue double, unitOfMeasure string); -- HTTP sink which publishes abnormal Glucose reading related events. -- If there is any errors occurred when publishing events to HTTP endpoint then respective events are sent to error stream @OnError(action='STREAM') @sink(type = 'http', blocking.io='true', publisher.url = \"http://localhost:8080/logger\", method = \"POST\", @map(type = 'json')) define stream AbnormalGlucoseReadingStream (timeStampInLong long, locationRoom string, locationBed string, sensorID long, patientGroup string, patientFullName string, sensorReadingValue double, unitOfMeasure string, abnormalReadingCount long); -- RDBMS event table which stores the failed events when publishing to HTTP endpoint @Store(type=\"rdbms\", jdbc.url=\"${MYSQL_DB_URL}\", username=\"${MYSQL_USERNAME}\", password=\"${MYSQL_PASSWORD}\" , jdbc.driver.name=\"com.mysql.jdbc.Driver\") @PrimaryKey(\"locationRoom\", \"locationBed\", \"sensorID\", \"failedEventTime\") define table FailedAbnormalReadingTable (failedEventTime string, originTime long, locationRoom string, locationBed string, sensorID long, patientFullName string, sensorReadingValue double, rootCause string); -- Javascript function which converts the Glucose sensor reading to mg/dl define function sensorReadingInMGDL[JavaScript] return double { var sensorReading = data[0]; var metricUnit = data[1]; if(metricUnit === 'MGDL') { return metricUnit; } return sensorReading * 18; }; @info(name='Glucose-preprocessing') from GlucoseReadingStream select math:parseLong(timeStamp) as timeStampInLong, locationRoom, locationBed, sensorID, patientGroup, str:concat(patientFirstName, \" \", patientLastName) as patientFullName, sensorReadingInMGDL(sensorValue, unitOfMeasure) as sensorReadingValue, 'MGDL' as unitOfMeasure insert into PreProcessedGlucoseReadingStream; @info(name='Abnormal-Glucose-reading-identifier') from PreProcessedGlucoseReadingStream[sensorReadingValue 100]#window.time(15 min) select timeStampInLong, locationRoom, locationBed, sensorID, patientGroup, patientFullName, sensorReadingValue, unitOfMeasure, count() as abnormalReadingCount group by locationRoom, locationBed, sensorID having abnormalReadingCount 3 output first every 15 min insert into AbnormalGlucoseReadingStream; @info(name = 'Error-handler') from !AbnormalGlucoseReadingStream#log(\"Error Occurred!\") select time:currentTimestamp() as failedEventTime, timeStampInLong as originTime, locationRoom, locationBed, sensorID, patientFullName, sensorReadingValue, convert(_error, 'string') as rootCause insert into FailedAbnormalReadingStream; @info(name='Dump-to-event-store') from FailedAbnormalReadingStream select * insert into FailedAbnormalReadingTable; Source view of the Siddhi app. Below is the flow diagram of the above Siddhi App.","title":"Implement Streaming Queries"},{"location":"docs/guides/fault-tolerance/guide/#testing","text":"NOTE: In the provided Siddhi app, there are some environmental variables (MYSQL_DB_URL, MYSQL_USERNAME, MYSQL_PASSWORD, and KAFKA_BOOTSTRAP_SERVER_URL) are used. These values are required to be set to try out the scenario end to end. MYSQL related environmental variables are required to store the events which are failed to publish to the HTTP endpoint. Environmental variable KAFKA_BOOTSTRAP_SERVER_URL is the Kafka endpoint URL where Siddhi listens and consume events from. MYSQL_DB_URL: MySQL database jdbc url to persist failed events. (eg: 'jdbc:mysql://localhost:3306/HCD') MYSQL_USERNAME: Username of the user account to connect MySQL database. (eg: 'root') MYSQL_PASSWORD: Password of the user account to connect MySQL database. (eg: 'root') KAFKA_BOOTSTRAP_SERVER_URL: List of Kafka servers to which the Siddhi Kafka source must listen. (eg: 'localhost:9092')","title":"Testing"},{"location":"docs/guides/fault-tolerance/guide/#setup-kafka","text":"As a prerequisite, you have to start the Kafka message broker. Please follow better steps. 1. Download the Kafka distribution 2. Unzip the above distribution and go to the \u2018bin\u2019 directory 3. Start the zookeeper by executing below command, zookeeper-server-start.sh config/zookeeper.properties 4. Start the Kafka broker by executing below command, kafka-server-start.sh config/server.properties Refer the Kafka documentation for more details, https://kafka.apache.org/quickstart Then, you have to add necessary client jars (from /libs directory) to Siddhi distribution as given below. Copy below client libs to /bundles directory scala-library-2.12.8.jar zkclient-0.11.jar zookeeper-3.4.14.jar Copy below client libs to jars directory kafka-clients-2.3.0.jar kafka_2.12-2.3.0.jar metrics-core-2.2.0.jar bundles directory to add OSGI bundles and jars directory to add non-OSGI jars.","title":"Setup Kafka"},{"location":"docs/guides/fault-tolerance/guide/#setup-mysql","text":"Download and Install MySQL database as per the guidelines (https://www.mysql.com/downloads/) Log in to the MySQL server and create a database called \u201cHCD\u201d Download the MySQL client connector jar and add it to jars (if it is non OSGI) or bundles (if it is OSGI bundle) directory of Siddhi distribution","title":"Setup MySQL"},{"location":"docs/guides/fault-tolerance/guide/#tryout","text":"There are multiple options available to test the developed Siddhi App. As mentioned in the previous step you could simply simulate some events directly into the stream and test your queries. But, if you are willing to test the end to end flow (from an input source to sink) then you can start the Siddhi app in the editor itself. In this guide, we are going to run the Siddhi App in the editor itself. Once the server is started, you will see below logs get printed in the editor console. In the above provided Siddhi App, last 15 minutes events are kept in memory for processing (because 15 minutes time window is defined in the query). As mentioned in the use case these events (Glucose reading of the patients) are very sensitive hence durable messaging is a requirement; losing a single event might cause a huge impact. In this situation, if the Siddhi server goes down while processing events then there is a possibility that events kept in the memory get lost. Then, to avoid this you should enable state persistence to Siddhi distribution. State persistence store in-memory state into the file system or database in a time interval. If the Siddhi runtime failed due to some reasons then Siddhi can recover to the state which is stored and continue processing. But, even though state persistence is enabled still there is a possibility that some intermediate events get lost due to runtime failures (or machine failures). For example, let\u2019s assume you have enabled snapshot persistence interval as 5 minutes (means, in-memory state get persisted to file system/DB each 5 minutes) then, if Siddhi runtime failed at 4 th minute after consuming events for last 4 minutes then you will be anyway losing last 4 minutes events even though you could recover to the previous state. To overcome this, we could leverage the \u201cOffset\u201d support provided by Kafka. Kafka is a publish-subscribe architecture that can handle multiple subscribers. To keep track of the consumers and how many events they have consumed Kafka uses something called \u201coffsets\u201d. An offset is a unique identifier which shows the sequential ID of a record within a Kafka topic partition. The offset is controlled by the subscriber. When the subscriber reads a record it advances its offset by one. In the snapshot persistence, Kafka offset value is also get persisted then when the specific snapshot is restored then Siddhi can request events from Kafka with the restored offset then Kafka can send those events of that specific offset. You can configure the snapshot state persistence with the below configuration. Save below configuration as YAML file (for example snapshot.yaml) state.persistence: enabled: true intervalInMin: 1 revisionsToKeep: 2 persistenceStore: io.siddhi.distribution.core.persistence.FileSystemPersistenceStore config: location: siddhi-app-persistence Then start the Siddhi runtime with below command. ./tooling.sh -Dconfig= File-Path /snapshot.yaml","title":"Tryout"},{"location":"docs/guides/fault-tolerance/guide/#invoking-the-siddhi-app","text":"Now, you can publish events to Kafka message broker using a Kafka publisher. In this case, you can use the sample Kafka publisher implementation given in here When you send more than 3 events (where sensorReading value is greater than 100) to Kafka within 15 minutes then it is identified as the abnormal situation and there will be an alert sent to HTTP endpoint as defined in the above provided Siddhi application. In this scenario, to demonstrate the error handling scenario, Siddhi app is configured to send the events to an unavailable endpoint. In this situation, since Siddhi cannot publish those events to HTTP endpoint then respective error handling flow gets activated as per the configuration (@onError (action=\u2019STREAM\u2019)) and respective failed events are pushed to the error stream as per the configuration (in this case, those failed events are pushed to the stream !AbnormalGlucoseReadingStream). Then, those events are dumped to a database for further reference for respective authorities. In this flow, Siddhi guarantees that there is no events get lost and ensure the durability of the system. You can see, as per the below figure you can find the respective failed events in the database table.","title":"Invoking the Siddhi App"},{"location":"docs/guides/fault-tolerance/guide/#deployment","text":"Once you are done with the development, export the Siddhi app that you have developed with 'File' - 'Export File' option. You can deploy the Siddhi app using any of the methods listed below.","title":"Deployment"},{"location":"docs/guides/fault-tolerance/guide/#deploy-on-vm-bare-metal","text":"","title":"Deploy on VM/ Bare Metal"},{"location":"docs/guides/fault-tolerance/guide/#prerequisites_1","text":"First, please make sure that necessary prerequisites are met as given the Testing section . Apache Kafka and MySQL are required to try out the use case. Then, as given in Setup Kafka and Setup MySQL section. Download the Kafka distribution and start it up and download the MySQL database and install it. Then create a database called \u201cHCD\u201d in the MySQL database.","title":"Prerequisites"},{"location":"docs/guides/fault-tolerance/guide/#siddhi-runtime-configuration","text":"Make sure to set the necessary environmental variables as given above. Note: In the above provided Siddhi app, there are some environmental variables (MYSQL_DB_URL, MYSQL_USERNAME, and MYSQL_PASSWORD) are used. These values are required to be set to try out the scenario end to end. MYSQL related environmental variables are required to store the events which are failed to publish to the HTTP endpoint. Environmental variable KAFKA_BOOTSTRAP_SERVER_URL is the Kafka endpoint URL where Siddhi listens and consume events from. Hence, make sure to set the environmental variables with the proper values in the system (make sure to follow necessary steps based on the underneath operating system). Download the latest Siddhi Runner distribution . Unzip the siddhi-runner-x.x.x.zip . You have to copy necessary Kafka and Mysql client jars to Siddhi runner distribution to connect with Kafka and MySQL database. Download the Kafka distribution and copy below Kafka client jars from /lib folder Copy below client libs to /bundles directory scala-library-2.12.8.jar zkclient-0.11.jar zookeeper-3.4.14.jar Copy below client libs to jars directory kafka-clients-2.3.0.jar kafka_2.12-2.3.0.jar metrics-core-2.2.0.jar Then, copy the MySQL client connector jar to jars (if it is non OSGI) or bundles (if it is OSGI bundle) directory of Siddhi runner. Please refer this . Start Siddhi app with the runner config by executing the following commands from the distribution directory. Linux/Mac : ./bin/runner.sh -Dapps= siddhi-file-path -Dconfig= config-yaml-path Windows : bin\\runner.bat -Dapps= siddhi-file-path -Dconfig= config-yaml-path Eg: If exported siddhi app in Siddhi home directory, ./bin/runner.sh -Dapps=Glucose-Reading-PreProcessing-App.siddhi -Dconfig=snapshot.yaml Note: snapshot.yaml file contains the configuration to enable state snapshot persistence Once server is started, download the sample Kafka Avro event generator from here and execute below command. java -jar kafka-avro-producer-1.0.0-jar-with-dependencies.jar Above event publishes send 4 Avro events to generate an abnormal behavior as defined in the Siddhi application. You can change the kafka endpoint and topic by passing them as java arguments. If not, sample client consider \u201clocalhost:9092\u201d as the kafka bootstrap server endpoint and \u201cglucose-readings\u201d as the topic. You can find the sample client source code in here In this situation, you can find a log gets printed in the Siddhi runner console/log and respective failed event is added to the database with the error cause.","title":"Siddhi Runtime Configuration"},{"location":"docs/guides/fault-tolerance/guide/#deploy-on-docker","text":"","title":"Deploy on Docker"},{"location":"docs/guides/fault-tolerance/guide/#prerequisites_2","text":"Apache Kafka and MySQL are the external dependencies for this use case. Hence, you could use the corresponding docker artifacts to test the requirement. First, you can create a docker network for the deployment as shown below docker network create siddhi-tier --driver bridge Then, you can get the MySQL docker image from here and run it with below command. We are going to use mysql version 5.7.27. Start the MySQL docker images with below command, docker run --name mysql-server --network siddhi-tier -e MYSQL_ROOT_PASSWORD=root e1e1680ac726 e1e1680ac726 is the MySQL docker image id in this case Login to the MySQL docker instance and create a database called \u201cHCD\u201d. Then, you can pull the Kafka docker image and deploy it. There is no any official Apache Kafka image is available in docker hub hence you can use the Kafka docker image provided by bitnami in here Launch the Zookeeper server instance with below provided command, docker run -d --name zookeeper-server --network siddhi-tier -e ALLOW_ANONYMOUS_LOGIN=yes bitnami/zookeeper:latest Launch the Kafka server instance with below provide command, docker run -d --name kafka-server --network siddhi-tier -e ALLOW_PLAINTEXT_LISTENER=yes -e KAFKA_CFG_ZOOKEEPER_CONNECT=zookeeper-server:2181 bitnami/kafka:latest Now, you have configured necessary prerequisites that required to run the use case.","title":"Prerequisites"},{"location":"docs/guides/fault-tolerance/guide/#siddhi-docker-configuration","text":"Since, there are some external client jars (Kafka MySQL) are required for the Siddhi runner. You have to create the docker image accordingly. Below is the sample Docker file created FROM siddhiio/siddhi-runner-base-alpine:5.1.0-alpha MAINTAINER Siddhi IO Docker Maintainers \"siddhi-dev@googlegroups.com\" ARG HOST_BUNDLES_DIR=./files/bundles ARG HOST_JARS_DIR=./files/jars ARG JARS=${RUNTIME_SERVER_HOME}/jars ARG BUNDLES=${RUNTIME_SERVER_HOME}/bundles # copy bundles jars to the siddhi-runner distribution COPY --chown=siddhi_user:siddhi_io ${HOST_BUNDLES_DIR}/ ${BUNDLES} COPY --chown=siddhi_user:siddhi_io ${HOST_JARS_DIR}/ ${JARS} # expose ports EXPOSE 9090 9443 9712 9612 7711 7611 7070 7443 RUN bash ${RUNTIME_SERVER_HOME}/bin/install-jars.sh STOPSIGNAL SIGINT ENTRYPOINT [\"/home/siddhi_user/siddhi-runner/bin/runner.sh\", \"--\"] Here, you have to create two folders called bundles and jars to add necessary external client dependencies to the docker image. You can refer the official Siddhi documentation reference for this purpose. Once, Dockerfile is created you can create the docker image with below command. docker build -t siddhi_for_kafka . Create a folder locally (eg: /home/siddhi-artifacts) and copy the Siddhi app in to it. Also, create a YAML file (snapshot.yaml) with below configuration to enable state snapshot persistence in the created folder. state.persistence: enabled: true intervalInMin: 1 revisionsToKeep: 2 persistenceStore: io.siddhi.distribution.core.persistence.FileSystemPersistenceStore config: location: /siddhi-app-persistence Then, you can run the Siddhi docker image that you created with necessary external dependencies to work with Kafka and MySQL. docker run --network siddhi-tier -it -v /Users/mohan/scratch/siddhi-artifacts:/artifacts -v /Users/mohan/scratch/local-mount:/siddhi-app-persistence -e MYSQL_DB_URL=jdbc:mysql://mysql-server:3306/HCD -e MYSQL_USERNAME=root -e MYSQL_PASSWORD=root -e KAFKA_BOOTSTRAP_SERVER_URL=kafka-server:9092 siddhi_for_kafka:latest -Dapps=/artifacts/Glucose-Reading-PreProcessing-App.siddhi -Dconfig=/artifacts/snapshot.yaml Note: In the above provided Siddhi app, there are some environmental variables (MYSQL_DB_URL, MYSQL_USERNAME and MYSQL_PASSWORD) are used. These values are required to be set to tryout the scenario end to end. MYSQL related environmental variables are required to store the events which are failed to publish to the HTTP endpoint. Environmental variable KAFKA_BOOTSTRAP_SERVER_URL is the Kafka endpoint url where Siddhi listens and consume events from. You can use the sample Kafka publisher client available in docker hub to simulate required events. Use the below command to use the sample docker Kafka publisher client. docker run --network siddhi-tier -it mohanvive/kafka-event-publisher:latest Then, you could see below log gets printed in the Siddhi runner console and failed events are stored in the database table.","title":"Siddhi Docker Configuration"},{"location":"docs/guides/fault-tolerance/guide/#deploy-on-kubernetes","text":"It is advisable to create a namespace in Kubernetes to follow below steps. kubectl create ns siddhi-kafka-test There are some prerequisites that you should meet to tryout below SiddhiProcess. Such as configure MySQL database and Kafka messaging system in Kubernetes. First, configure the MySQL server within the above created namespace. You can use the official helm chart provided for MySQL. First, install the MySQL helm chart as shown below, helm install --name mysql-db --namespace=siddhi-kafka-test --set mysqlRootPassword=root,mysqlDatabase=HCD stable/mysql Here, you can define the root password to connect to the MYSQL database and also define the database name. BTW, make sure to do helm init if it is not done yet. Then, you can set a port forwarding to the MySQL service which allows you to connect from the Host. kubectl port-forward svc/mysql-db 13306:3306 --namespace=siddhi-kafka-test Then, you can login to the MySQL server from your host machine as shown below. Next, you can configure Kafka messaging system in the Kubernetes (in the above created namespace). You can use the Kafka Helm chart for this purpose. First, install Kafka helm chart as shown below. helm install --name my-kafka incubator/kafka --namespace=siddhi-kafka-test Then, you could find required Kafka broker pods and Zookeeper pods are getting created. Then, you can install Siddhi Operator To install the Siddhi Kubernetes operator run the following commands. kubectl apply -f https://github.com/siddhi-io/siddhi-operator/releases/download/v0.2.0-alpha/00-prereqs.yaml --namespace=siddhi-kafka-test kubectl apply -f https://github.com/siddhi-io/siddhi-operator/releases/download/v0.2.0-alpha/01-siddhi-operator.yaml --namespace=siddhi-kafka-test You can verify the installation by making sure the following deployments are running in your Kubernetes cluster. Siddhi applications can be deployed on Kubernetes using the Siddhi operator. To deploy the above created Siddhi app, we have to create custom resource object yaml file (with the kind as SiddhiProcess) as given below apiVersion: siddhi.io/v1alpha2 kind: SiddhiProcess metadata: name: glucose-reading-preprocessing-app spec: apps: - script: | @App:name(\"Glucose-Reading-PreProcessing-App\") @App:description('Process Glucose Readings received from patients') -- Kakfka source which consumes Glucose reading events @source(type='kafka', topic.list='glucose-readings', partition.no.list='0', threading.option='single.thread', group.id=\"group\", is.binary.message='true', bootstrap.servers='${KAFKA_BOOTSTRAP_SERVER_URL}', @map(type='avro',schema.def=\"\"\" { \"type\": \"record\", \"name\": \"glucose_reading\", \"namespace\": \"glucose\", \"fields\": [{ \"name\": \"locationRoom\", \"type\": \"string\" }, { \"name\": \"locationBed\", \"type\": \"string\" }, { \"name\": \"timeStamp\", \"type\": \"string\" }, { \"name\": \"sensorID\", \"type\": \"long\" }, { \"name\": \"patientGroup\", \"type\": \"string\" }, { \"name\": \"patientFirstName\", \"type\": \"string\" }, { \"name\": \"patientLastName\", \"type\": \"string\" }, { \"name\": \"sensorValue\", \"type\": \"double\" }, { \"name\": \"unitOfMeasure\", \"type\": \"string\" }] }\"\"\")) define stream GlucoseReadingStream (locationRoom string, locationBed string, timeStamp string, sensorID long, patientGroup string, patientFirstName string, patientLastName string, sensorValue double, unitOfMeasure string); -- HTTP sink which publishes abnormal Glucose reading related events. -- If there is any errors occurred when publishing events to HTTP endpoint then respective events are sent to error stream @OnError(action='STREAM') @sink(type = 'http', blocking.io='true', publisher.url = \"http://localhost:8080/logger\", method = \"POST\", @map(type = 'json')) define stream AbnormalGlucoseReadingStream (timeStampInLong long, locationRoom string, locationBed string, sensorID long, patientGroup string, patientFullName string, sensorReadingValue double, unitOfMeasure string, abnormalReadingCount long); -- RDBMS event table which stores the failed events when publishing to HTTP endpoint @Store(type=\"rdbms\", jdbc.url=\"${MYSQL_DB_URL}\", username=\"${MYSQL_USERNAME}\", password=\"${MYSQL_PASSWORD}\" , jdbc.driver.name=\"com.mysql.jdbc.Driver\") @PrimaryKey(\"locationRoom\", \"locationBed\", \"sensorID\", \"failedEventTime\") define table FailedAbnormalReadingTable (failedEventTime string, originTime long, locationRoom string, locationBed string, sensorID long, patientFullName string, sensorReadingValue double, rootCause string); -- Javascript function which converts the Glucose sensor reading to mg/dl define function sensorReadingInMGDL[JavaScript] return double { var sensorReading = data[0]; var metricUnit = data[1]; if(metricUnit === 'MGDL') { return metricUnit; } return sensorReading * 18; }; @info(name='Glucose-preprocessing') from GlucoseReadingStream select math:parseLong(timeStamp) as timeStampInLong, locationRoom, locationBed, sensorID, patientGroup, str:concat(patientFirstName, \" \", patientLastName) as patientFullName, sensorReadingInMGDL(sensorValue, unitOfMeasure) as sensorReadingValue, 'MGDL' as unitOfMeasure insert into PreProcessedGlucoseReadingStream; @info(name='Abnormal-Glucose-reading-identifier') from PreProcessedGlucoseReadingStream[sensorReadingValue 100]#window.time(15 min) select timeStampInLong, locationRoom, locationBed, sensorID, patientGroup, patientFullName, sensorReadingValue, unitOfMeasure, count() as abnormalReadingCount group by locationRoom, locationBed, sensorID having abnormalReadingCount 3 output first every 15 min insert into AbnormalGlucoseReadingStream; @info(name = 'Error-handler') from !AbnormalGlucoseReadingStream#log(\"Error Occurred!\") select time:currentTimestamp() as failedEventTime, timeStampInLong as originTime, locationRoom, locationBed, sensorID, patientFullName, sensorReadingValue, convert(_error, 'string') as rootCause insert into FailedAbnormalReadingStream; @info(name='Dump-to-event-store') from FailedAbnormalReadingStream select * insert into FailedAbnormalReadingTable; persistentVolumeClaim: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi storageClassName: hostpath volumeMode: Filesystem runner: | state.persistence: enabled: true intervalInMin: 1 revisionsToKeep: 2 persistenceStore: io.siddhi.distribution.core.persistence.FileSystemPersistenceStore config: location: /siddhi-app-persistence container: env: - name: MYSQL_DB_URL value: \"jdbc:mysql://mysql-db:3306/HCD\" - name: MYSQL_USERNAME value: \"root\" - name: MYSQL_PASSWORD value: \"root\" - name: KAFKA_BOOTSTRAP_SERVER_URL value: \"my-kafka-headless:9092\" image: \"mohanvive/siddhi_for_kafka:latest\" Note: In the above provided Siddhi app, there are some environmental variables (MYSQL_DB_URL, MYSQL_USERNAME and MYSQL_PASSWORD) are used. These values are required to be set to tryout the scenario end to end. MYSQL related environmental variables are required to store the events which are failed to publish to the HTTP endpoint. Environmental variable KAFKA_BOOTSTRAP_SERVER_URL is the Kafka endpoint url where Siddhi listens and consume events from. Hence, make sure to add proper values for the environmental variables in the above yaml file (check the env section of the yaml file). Here, you can use the docker image that created in the Deploy on Docker section since you need a docker images with required extensions and client jars to test it in Kubernetes. Other than that, Siddhi runtime is configured to enable state snapshot persistence under the runner entry as shown above. Now, let\u2019s create the above resource in the Kubernetes cluster with below command. kubectl --namespace=siddhi-kafka-test create -f absolute-yaml-file-path /glucose-reading-preprocessing-app.yaml Once, siddhi app is successfully deployed. You can verify its health with below Kubernetes commands Now, you can send some events to Kafka messaging server and test the use case. If you are planning to push events to Kafka messaging server from your host machine then you have to enable few external listeners in Kafka. Please check the documentation provided for the Kafka helm chart. For testing purposes, you could use the test-client pod provided by us. Create a pod with below definition. apiVersion: v1 kind: Pod metadata: name: testclient namespace: siddhi-kafka-test spec: containers: - name: kafka image: mohanvive/test-client command: - sh - -c - \"exec tail -f /dev/null\" Create a YAML file called test-client.yaml, add above pod definition in it and run below command. kubectl apply -f test-client.yaml Then, go into the above created pod using below command. kubectl exec -it testclient sh --namespace=siddhi-kafka-test Download the sample Kafka client which could publish events related to above use case. wget https://github.com/mohanvive/siddhi-sample-clients/releases/download/v1.0.0/kafka-avro-producer-1.0.0-jar-with-dependencies.jar Then execute below command to push events to Kafka messaging system. java -jar kafka-avro-producer-1.0.0-jar-with-dependencies.jar my-kafka-headless:9092 Then, as defined in the SIddhi application abnormal events get logged since it tries to publish to an unavailable endpoint. Refer here to get more details about running Siddhi on Kubernetes.","title":"Deploy on Kubernetes"},{"location":"docs/guides/integrate-various-enterprise-systems/guide/","text":"Retrieve and Publish Data from/to Various Enterprise Systems This guide illustrates how you can receive data from various enterprise systems, process it, and then send processed data into other systems. Scenario Paris becomes very busy in the rush hours. Most of the people trying to go to offices in the morning and children try to go to school. In the afternoon all those people trying to go back to their premises. The traffic becomes high in those hours. If there is a bad weather condition in those rush hours the traffic might increase up to a certain level. Partis Transport Management Authority (Paris TMA) is accountable for handling all the traffic in Paris. They have noticed this increase in traffic due to the bad weather conditions in several areas. To reduce this traffic, they try to build a system that notifies all users if there are bad weather conditions in a particular area. Paris TMA uses a centralized database to receive notifications about bad weather conditions. Besides, they are tracking locations of all the vehicles in Paris city. If some incident happens, this system will notify all the users that are near to that area. After completing this guide you will understand how to receive various inputs from heterogeneous systems (like NATS, RDBMS, MongoDB), process it, and send back processed outputs in various formats like Email notification. What You'll Build The implementation of the ParisTMA system represented in the following architecture diagram. The overall architecture can be expressed as the following subsystems. Vehicle Locator Subsystem The vehicle locator subsystem is responsible for receiving the location of each subscriber and update subscriber database collection with their current location. The process of this subsystem showed in step 01-03 in the architecture diagram. The location of a vehicle, input to the NATS cluster as a JSON object like below. { \"vehicleId\": \"v001\", \"currentArea\": \"a002\" } The Siddhi app called ParisTMAVehicleLocator will listen to those messages that come to the NATS cluster and periodically update the Mongo database with those current locations. This Mongo database contained all the information relevant to subscribers along with their current location. Notifier Subsystem The process of the notifier subsystem illustrated in step 04-07 in the architecture diagram. The notifier subsystem triggers the process when our system receives weather update to the RDBMS as shown in step 04. The ParisTMANotifier Siddhi app will handle all the logical execution of this notification process. There exists a CDC source which listens to the changes of the RDBMS. When some weather notification inserts into the RDBMS, the ParisTMANotifier Siddhi app retrieves this insert event. That insert event will contain the area that this weather incident happened. Using that area, ParisTMANotifier filters out all the subscribers that are in that given area using the subscriber collection in the Mongo database. To all those subscribers ParisTMANotifier will send an email notification about this incident. Prerequisites Mandatory Requirements Siddhi tooling VM/Local distribution One of the Siddhi runner distributions VM/Local Runtime Docker Image K8s Operator (commands are given in deployment section) MySQL database Mongo database NATS streaming server Java 8 or higher Requirements needed to deploy Siddhi in Docker/Kubernetes Docker Kubernetes cluster Minikube Google Kubernetes Engine(GKE) Docker for Mac Helm Refer to this documentation about Configuring a Google Kubernetes Engine (GKE) Cluster to deploy Siddhi apps in GKE. Implementation As illustrated in the previous architecture diagram you will need Siddhi apps called ParisTMAVehicleLocator and ParisTMANotifier. Start the Siddhi tooling runtime and go to the editor UI in http://localhost:9390/editor Use the following steps to start the Siddhi tooling runtime. Extract the downloaded zip and navigate to /bin. (TOOLING_HOME refers to the extracted folder). Use the following command in the command prompt (Windows) / terminal (Linux/Mac). For Windows: tooling.bat For Linux/Mac: ./tooling.sh Select File - New option, then you could either use the source view or design view to write/build the Siddhi Application. You can find the Siddhi Application bellow, that implements the requirements mentioned above. Let\u2019s write (develop) the Siddhi Application, as given below. Once the Siddhi app is created, you can use the Event Simulator option in the editor to simulate events to streams and perform developer testing. The implementation of those apps is described in the following subtopics. Siddhi Query Guide The execution steps and the logic of the Siddhi query described as comments in the following Siddhi app. Therefore here we are not going to explain in detail here. For more details about Siddhi queries please refer Siddhi query guide . Vehicle Locator @App:name(\"ParisTMAVehicleLocator\") @App:description(\"Listen to the events send by NATS about vehicle locations and periodically update the Mongo database.\") /* Purpose: The ParisTMA has subscribers that subscribe to get the service. All the subscriber details are saved in the Mongo database. In that database ParisTMA stores the current location of subscriber vehicles. The events related to current locations come as TCP requests through NATS. This app listens to those events and eventually update the database with the current location of each subscriber. Input: NATS event with JSON payload: { \"vehicleId\": \"v001\", \"currentArea\": \"a002\" } Output: Update the collection(subscriber) documents of MongoDB(parisTma). */ -- NATS source that listens to the events @source( type='nats', cluster.id='${STAN_CLUSTER_ID}', destination='VehicleInputTunnel', bootstrap.servers='${STAN_SERVER_URL}', @map(type='json') ) define stream CurrentVehicleAreaInputStream(vehicleId string, currentArea string); -- MongoDB store @store(type='mongodb', mongodb.uri='${MONGODB_URI}') define table subscriber(vehicleId string, currentArea string); -- Retrieves events from NATS source and update in MongoDB @info(name='update-or-insert-subscriber-location') from CurrentVehicleAreaInputStream update or insert into subscriber on subscriber.vehicleId==vehicleId; Notifier @App:name(\"ParisTMANotifier\") @App:description(\"Listen to the CDC events about the weather updates and send notifications to the subscribed users.\") /* Purpose: The ParisTMA has subscribers that subscribe to get the service. All the subscriber details are saved in the Mongo database. In that database ParisTMA stores the current location of subscriber vehicles. There is a separate system handle by the Paris weather forecast organization. They periodically update the MySQL database called 'paristma' with recent weather incidents. This app listens to those insertions using CDC and sends notifications to the subscribers who are in that particular area that incident happens. Input: CDC insertions. Output: Email to users that notifying about the incident. Note: Even you can change this email notification to an SMS notification calling an SMS provider. */ -- CDC source that listens to insertions of the Incident table @source( type='cdc', url='${MYSQL_URL}', username='siddhi_user', password='siddhiio', table.name='Incident', operation='insert', @map( type='keyvalue' ) ) define stream IncidentInputStream(incidentId int, incidentName string, incidentType string, incidentDetails string, incidentArea string); -- MongoDB store of subscribers @store(type='mongodb', mongodb.uri='${MONGODB_URI}') define table subscriber(vehicleId string, currentArea string, subscriberName string, subscriberEmail string); -- Email sink to send notifications @sink(type='email', username=\"${PARIS_TMA_EMAIL}\", address=\"${PARIS_TMA_EMAIL}\", password=\"${PARIS_TMA_EMAIL_PASSWORD}\", subject=\"Weather Notification in {{incidentArea}}\", to=\"{{subscriberEmail}}\", host=\"smtp.gmail.com\", port=\"465\", ssl.enable=\"true\", auth=\"true\", @map(type='text', @payload(\"\"\" Hi {{subscriberName}} Recent reports state that there is {{incidentName}} in your area {{incidentArea}}. It is in {{incidentType}} state right now. The report state that {{incidentDetails}}. Thanks, Paris Transport Management Authority\"\"\"))) define stream UserNotificationStream (subscriberName string, subscriberEmail string, incidentName string, incidentType string, incidentDetails string, incidentArea string); -- Listen to the insertions in IncidentInputStream and join it with subscriber collection in MongoDB to get user information. Send notifications using that user information. @info(name='listen-and-notify') from IncidentInputStream#window.length(1) join subscriber on IncidentInputStream.incidentArea==subscriber.currentArea select subscriber.subscriberName, subscriber.subscriberEmail, IncidentInputStream.incidentName, IncidentInputStream.incidentType, IncidentInputStream.incidentDetails, IncidentInputStream.incidentArea insert into UserNotificationStream; Testing Setup MongoDB Refer this link to install MongoDB in your local machine. The ParisTMA system uses a Mongo database called parisTma and that parisTma collection contained a collection called the subscriber . In order to connect to the Mongo database, the Siddhi runtime will use a user called siddhi_user that identified by password siddhiio . You can create those MongoDB databases, collections, users and insert data using the following MongoDB commands. use parisTma db.createCollection(\"subscriber\") db.createUser( { user: \"siddhi_user\", pwd: \"siddhiio\", roles:[{role: \"userAdmin\" , db:\"parisTma\"}] } ) db.subscriber.insert({ vehicleId: 'v001', currentArea: 'a001', subscriberName: 'John Dus', subscriberEmail: 'b.wathsala.bw@gmail.com' }) db.subscriber.insert({ vehicleId: 'v002', currentArea: 'a002', subscriberName: 'Natalie Jonson', subscriberEmail: 'b.wathsala.bw@gmail.com' }) db.subscriber.insert({ vehicleId: 'v003', currentArea: 'a003', subscriberName: 'Mark Norman', subscriberEmail: 'buddhik@wso2.com' }) Setup MySQL After that, you have to create a MySQL database called paristma . It contained a table called Incident to store all the weather incidents happened recently. To access the MySQL database you also need to create a user called siddhi_user that identified by password siddhiio . The incident table will look like the following. To connect to the MySQL database Siddhi tooling runtime needs the MySQL client library. You can download the MySQL JAR from here and then add the JAR to the ${TOOLING_HOME}/jars directory. This example uses the Siddhi CDC in the default(listening) mode. The listening mode needs to change the configurations as described here . For example, you have to add the following configurations to the my.cnf file. [mysqld] server-id = 223344 log_bin = mysql-bin binlog_format = row binlog_row_image = full expire_logs_days = 10 gtid_mode = on enforce_gtid_consistency = on binlog_rows_query_log_events = on log-error=/var/log/mysql/mysql.err log-bin = /var/log/mysql/mysql-replication.log Setup NATS Streaming Cluster Now you need to install a NATS streaming server . For example, to install the NATS streaming cluster in MacOS use the following commands. $ brew install nats-streaming-server After the installation runs the NATS streaming cluster as below. $ nats-streaming-server The Siddhi app runtime will need the following JARs to connect to the NATS streaming server. Download those JARs using the following links and add those JARs to ${TOOLING_HOME}/jars directory. Java NATS streaming JNATS It also needs the Protobuf bundle. Download the Protobuf bundle using the following link and add the bundle to ${TOOLING_HOME}/bundles directory. Protobuf Configurations The above Siddhi apps receive the MySQL, MongoDB, and NATS connections details as environment variables. For local deployments for testing these apps, you have to set up those environmental variables as follows. STAN_CLUSTER_ID= test-cluster STAN_SERVER_URL= nats://localhost:4222 MONGODB_URI= mongodb://127.0.0.1:27017/parisTma? gssapiServiceName=mongodb MYSQL_URL= jdbc:mysql://127.0.0.1:3306/paristma PARIS_TMA_EMAIL= TEST_EMAIL PARIS_TMA_EMAIL_PASSWORD= TEST_EMAIL_PASSWORD Tryout After all those configurations and setups you will be able to starts the both Siddhi apps without any error. Testing the app can be done in two levels. Update the vehicle location periodically Send notifications To test the vehicle location updates you have to send a request to the NATS streaming server. You can send the request using various NATS clients. In this use case, you can use NATS streaming Go language client to easily send requests. To do that you need to have to install Golang in your machine. Or else you can use other clients like ( Java , JS , Python , or Ruby etc). You can use following Golang commands to send a request to the NATS streaming cluster. $ go get github.com/nats-io/stan.go/ $ go run $GOPATH/src/github.com/nats-io/stan.go/examples/stan-pub/main.go -s localhost:4222 -c test-cluster VehicleInputTunnel \"{\\\"vehicleId\\\": \\\"v001\\\", \\\"currentArea\\\":\\\"a003\\\"}\" To run the notification Siddhi app you just need to insert data entry to the Incident table. When you do that it the Siddhi app will simply send an email to the subscribers like below. Deployment Deploy on VM/ Bare Metal First, you have to set up prerequisites as described in the previous sections. Setup MongoDB Setup MySQL Setup NATS Streaming Download the Siddhi runner distribution pack from here and unzip it. Download relevant JARs and add those JARs to ${RUNNER_HOME}/jars directory. MySQL Java NATS streaming JNATS Download Protobuf bundle and add the bundle to ${RUNNER_HOME}/bundles directory. Protobuf Copy your Siddhi file into RUNNER_HOME /wso2/runner/deployment/siddhi-files Start the following binary file to run the Siddhi runner server. For Linux/Mac: RUNNER_HOME /bin/runner.sh For Windows: RUNNER_HOME /bin/runner.bat Now you can try out the sample as described in this tryout section . Deploy on Docker Prerequisite Install Docker in your machine. Siddhi Docker Configurations In the tooling editor itself, you can export your Siddhi app into a runnable docker artifact. You can go to Export- For Docker and it will give to a zip file that contained the following files. \u251c\u2500\u2500 Dockerfile \u251c\u2500\u2500 bundles \u2502 \u2514\u2500\u2500 protobuf-java-3.6.1.jar \u251c\u2500\u2500 configurations.yaml \u251c\u2500\u2500 jars \u2502 \u251c\u2500\u2500 java_nats_streaming_2.1.2.jar \u2502 \u251c\u2500\u2500 jnats_2.3.0.jar \u2502 \u2514\u2500\u2500 mysql-connector-java-5.1.45-bin.jar \u2514\u2500\u2500 siddhi-files \u251c\u2500\u2500 ParisTMANotifier.siddhi \u2514\u2500\u2500 ParisTMAVehicleLocator.siddhi When you export the apps as a Docker, it will ask for the templated values to be filled. You have to use the following values for those templated values. STAN_CLUSTER_ID= test-cluster STAN_SERVER_URL= nats://nats:4222 MONGODB_URI= mongodb://mongodb:27017/parisTma? gssapiServiceName=mongodb MYSQL_URL= jdbc:mysql://mysqldb:3306/paristma PARIS_TMA_EMAIL= TEST_EMAIL PARIS_TMA_EMAIL_PASSWORD= TEST_EMAIL_PASSWORD In order to change the configurations of MySQL Docker container you have create my.cnf file in your current directory and add the following content to that file. Also create directory called mysql . [mysqld] server-id = 223344 log_bin = mysql-bin binlog_format = row binlog_row_image = full expire_logs_days = 10 gtid_mode = on enforce_gtid_consistency = on binlog_rows_query_log_events = on log-error=/var/log/mysql/mysql.err log-bin = /var/log/mysql/mysql-replication.log Now you need to have a Docker compose file like below to set up all the prerequisites. This compose file contains volume mounts to change configurations of the MySQL container. version: \"3\" services: backend: container_name: paristma-notifier user: 802:802 build: context: . dockerfile: ./Dockerfile links: - mysqldb - mongodb - nats networks: - default restart: on-failure mysqldb: image: 'mysql:5.7' container_name: paristma-mysqldb environment: MYSQL_USER: siddhi_user MYSQL_PASSWORD: siddhiio MYSQL_ROOT_PASSWORD: siddhiio ports: - \"3304:3306\" networks: - default restart: on-failure volumes: - ./my.cnf:/etc/my.cnf - ./mysql:/var/log/mysql command: --sql_mode=\"\" mongodb: image: 'mongo:4.0.4' container_name: paristma-mongodb ports: - \"27017-27019:27017-27019\" networks: - default restart: on-failure nats: image: 'nats-streaming:0.16.2-linux' container_name: paristma-nats ports: - \"4223:4223\" - \"8223:8223\" networks: - default restart: on-failure Now you can start each containers. First, you need to build the Docker composer. docker-compose build Starts the MySQL container using the following command and set up the MySQL database as described above . docker-compose up -d mysqldb Then, starts the MongoDB container using the following command and set up the MongoDB database as described above . docker-compose up -d mongodb After that, starts the NATS container using the following command. docker-compose up -d nats Finally, start the Siddhi backend runtime. docker-compose up -d backend When you insert data entry to the Incident table, the Siddhi app will simply send an email to the subscribers like below. Deploy on Kubernetes Prerequisites Kubernetes cluster Minikube Google Kubernetes Engine(GKE) Docker for Mac Install HELM Siddhi Kubernetes Configurations In the tooling editor itself, you can export your Siddhi app into a runnable Kubernetes artifact. You can go to Export- For Kubernetes and it will give to a zip file that contained the following files. \u251c\u2500\u2500 Dockerfile \u251c\u2500\u2500 bundles \u2502 \u2514\u2500\u2500 protobuf-java-3.6.1.jar \u251c\u2500\u2500 configurations.yaml \u251c\u2500\u2500 jars \u2502 \u251c\u2500\u2500 java_nats_streaming_2.1.2.jar \u2502 \u251c\u2500\u2500 jnats_2.3.0.jar \u2502 \u2514\u2500\u2500 mysql-connector-java-5.1.45-bin.jar \u251c\u2500\u2500 siddhi-files \u2502 \u251c\u2500\u2500 ParisTMANotifier.siddhi \u2502 \u2514\u2500\u2500 ParisTMAVehicleLocator.siddhi \u2514\u2500\u2500 siddhi-process.yaml When you export the apps as a Kubernetes, it will ask for the templated values to be filled. You have to use the following values for those templated values. STAN_CLUSTER_ID= siddhi-stan STAN_SERVER_URL= nats://siddhi-nats:4222 MONGODB_URI= mongodb://siddhi_user:siddhiio@paristma-mongodb:27017/parisTma MYSQL_URL= jdbc:mysql://mysqldb:3306/paristma PARIS_TMA_EMAIL= TEST_EMAIL PARIS_TMA_EMAIL_PASSWORD= TEST_EMAIL_PASSWORD Setup MongoDB Let\u2019s install MongoDB in your Kubernetes cluster using Helm. $ helm install stable/mongodb $ helm install --name paristma --set volumePermissions.enabled=true,mongodbRootPassword=siddhiio,mongodbUsername=siddhi_user,mongodbPassword=siddhiio,mongodbDatabase=parisTma stable/mongodb Now you can access the MongoDB externally using following commands and set up the Mongo database as described previously . $ export MONGODB_ROOT_PASSWORD=$(kubectl get secret --namespace default paristma-mongodb -o jsonpath=\"{.data.mongodb-root-password}\" | base64 --decode) $ kubectl run --namespace default paristma-mongodb-client --rm --tty -i --restart='Never' --image bitnami/mongodb --command -- mongo admin --host paristma-mongodb --authenticationDatabase admin -u root -p $MONGODB_ROOT_PASSWORD Setup MySQL In order to change the MySQL configurations for the Siddhi CDC extension, you need to create a file called values.yaml with including the following configurations. mysqlRootPassword: siddhiio mysqlUser: siddhi_user mysqlPassword: siddhiio mysqlDatabase: paristma configurationFiles: my.cnf: |- [mysqld] server-id = 223344 log_bin = mysql-bin binlog_format = row binlog_row_image = full expire_logs_days = 10 gtid_mode = on enforce_gtid_consistency = on binlog_rows_query_log_events = on log-error=/var/log/mysql/mysql.err log-bin = /var/log/mysql/mysql-replication.log Now you can install MySQL in you Kubernetes cluster using Helm. $ helm install --name mysqldb -f values.yaml stable/mysql Use Kubernetes port forwading to access MySQL externally using 3307 port and create nessasary tables in the MySQL database as described previously . $ kubectl port-forward svc/mysql-db 3307:3306 Setup NATS Streaming Use the following kubectl commands to set up the NATS streaming cluster. $ kubectl apply -f https://github.com/nats-io/nats-operator/releases/download/v0.5.0/00-prereqs.yaml $ kubectl apply -f https://github.com/nats-io/nats-operator/releases/download/v0.5.0/10-deployment.yaml $ kubectl apply -f https://github.com/nats-io/nats-streaming-operator/releases/download/v0.2.2/default-rbac.yaml $ kubectl apply -f https://github.com/nats-io/nats-streaming-operator/releases/download/v0.2.2/deployment.yaml $ echo ' --- apiVersion: \"nats.io/v1alpha2\" kind: \"NatsCluster\" metadata: name: \"siddhi-nats\" spec: size: 1 ' | kubectl apply -f - $ echo ' --- apiVersion: \"streaming.nats.io/v1alpha1\" kind: \"NatsStreamingCluster\" metadata: name: \"siddhi-stan\" spec: size: 1 natsSvc: \"siddhi-nats\" ' | kubectl apply -f - For more details about the NATS streaming server, refer to this documentation . Setup Siddhi Operator Use the following kubectl commands to install the Siddhi operator in your Kubernetes cluster. $ kubectl apply -f https://github.com/siddhi-io/siddhi-operator/releases/download/v0.2.0/00-prereqs.yaml $ kubectl apply -f https://github.com/siddhi-io/siddhi-operator/releases/download/v0.2.0/01-siddhi-operator.yaml Now you need to create your own docker image with including all the custom libraries and configuration changes that you have made. Use following command to build and push the docker image with the tag DOCKER_HUB_USER_NAME /siddhi-runner-alpine:latest . $ docker build -t DOCKER_HUB_USER_NAME /siddhi-runner-alpine:latest . $ docker push DOCKER_HUB_USER_NAME /siddhi-runner-alpine:latest After the Kubernetes export now you already have this siddhi-process.yaml file. apiVersion: siddhi.io/v1alpha2 kind: SiddhiProcess metadata: name: paris-tma spec: apps: - script: | @App:name(\"ParisTMANotifier\") @App:description(\"Listen to the CDC events about the weather updates and send notifications to the subscribed users.\") /* Purpose: The ParisTMA has subscribers that subscribe to get the service. All the subscriber details are saved in the Mongo database. In that database ParisTMA stores the current location of subscriber vehicles. There is a separate system handle by the Paris weather forecast organization. They periodically update the MySQL database called 'paristma' with recent weather incidents. This app listens to those insertions using CDC and sends notifications to the subscribers who are in that particular area that incident happens. Input: CDC insertions. Output: Email to users that notifying about the incident. Note: Even you can change this email notification to an SMS notification calling an SMS provider. */ -- CDC source that listens to insertions of the Incident table @source( type='cdc', url='${MYSQL_URL}', username='siddhi_user', password='siddhiio', table.name='Incident', operation='insert', @map( type='keyvalue' ) ) define stream IncidentInputStream(incidentId int, incidentName string, incidentType string, incidentDetails string, incidentArea string); -- MongoDB store of subscribers @store(type='mongodb', mongodb.uri='${MONGODB_URI}') define table subscriber(vehicleId string, currentArea string, subscriberName string, subscriberEmail string); -- Email sink to send notifications @sink(type='email', username=\"${PARIS_TMA_EMAIL}\", address=\"${PARIS_TMA_EMAIL}\", password=\"${PARIS_TMA_EMAIL_PASSWORD}\", subject=\"Weather Notification in {{incidentArea}}\", to=\"{{subscriberEmail}}\", host=\"smtp.gmail.com\", port=\"465\", ssl.enable=\"true\", auth=\"true\", @map(type='text', @payload(\"\"\" Hi {{subscriberName}} Recent reports state that there is {{incidentName}} in your area {{incidentArea}}. It is in {{incidentType}} state right now. The report state that {{incidentDetails}}. Thanks, Paris Transport Management Authority\"\"\"))) define stream UserNotificationStream (subscriberName string, subscriberEmail string, incidentName string, incidentType string, incidentDetails string, incidentArea string); -- Listen to the insertions in IncidentInputStream and join it with subscriber collection in MongoDB to get user information. Send notifications using that user information. @info(name='listen-and-notify') from IncidentInputStream#window.length(1) join subscriber on IncidentInputStream.incidentArea==subscriber.currentArea select subscriber.subscriberName, subscriber.subscriberEmail, IncidentInputStream.incidentName, IncidentInputStream.incidentType, IncidentInputStream.incidentDetails, IncidentInputStream.incidentArea insert into UserNotificationStream; - script: |- @App:name(\"ParisTMAVehicleLocator\") @App:description(\"Listen to the events send by NATS about vehicle locations and periodically update the Mongo database.\") /* Purpose: The ParisTMA has subscribers that subscribe to get the service. All the subscriber details are saved in the Mongo database. In that database ParisTMA stores the current location of subscriber vehicles. The events related to current locations come as TCP requests through NATS. This app listens to those events and eventually update the database with the current location of each subscriber. Input: NATS event with JSON payload: { \"vehicleId\": \"v001\", \"currentArea\": \"a002\" } Output: Update the collection(subscriber) documents of MongoDB(parisTma). */ -- NATS source that listens to the events @source( type='nats', cluster.id='${STAN_CLUSTER_ID}', destination='VehicleInputTunnel', bootstrap.servers='${STAN_SERVER_URL}', @map(type='json') ) define stream CurrentVehicleAreaInputStream(vehicleId string, currentArea string); -- MongoDB store @store(type='mongodb', mongodb.uri='${MONGODB_URI}') define table subscriber(vehicleId string, currentArea string); -- Retrieves events from NATS source and update in MongoDB @info(name='update-or-insert-subscriber-location') from CurrentVehicleAreaInputStream update or insert into subscriber on subscriber.vehicleId==vehicleId; container: env: - name: MYSQL_URL value: jdbc:mysql://mysqldb:3306/paristma - name: MONGODB_URI value: mongodb://siddhi_user:siddhiio@paristma-mongodb:27017/parisTma - name: PARIS_TMA_EMAIL value: TEST_EMAIL - name: PARIS_TMA_EMAIL_PASSWORD value: TEST_EMAIL_PASSWORD - name: STAN_CLUSTER_ID value: siddhi-stan - name: STAN_SERVER_URL value: nats://siddhi-nats:4222 image: DOCKER_HUB_USER_NAME /siddhi-runner-alpine:latest runner: | wso2.carbon: id: siddhi-runner name: Siddhi Runner Distribution ports: offset: 0 transports: http: listenerConfigurations: - id: default host: 0.0.0.0 port: 9090 - id: msf4j-https host: 0.0.0.0 port: 9443 scheme: https keyStoreFile: ${carbon.home}/resources/security/wso2carbon.jks keyStorePassword: wso2carbon certPass: wso2carbon transportProperties: - name: server.bootstrap.socket.timeout value: 60 - name: client.bootstrap.socket.timeout value: 60 - name: latency.metrics.enabled value: true dataSources: - name: WSO2_CARBON_DB description: The datasource used for registry and user manager definition: type: RDBMS configuration: jdbcUrl: jdbc:h2:${sys:carbon.home}/wso2/${sys:wso2.runtime}/database/WSO2_CARBON_DB;DB_CLOSE_ON_EXIT=FALSE;LOCK_TIMEOUT=60000 username: wso2carbon password: wso2carbon driverClassName: org.h2.Driver maxPoolSize: 10 idleTimeout: 60000 connectionTestQuery: SELECT 1 validationTimeout: 30000 isAutoCommit: false Now you can install the SiddhiProcess using following kubectl command. Before you install the SiddhiProcess you have to add the docker image tag in the siddhi-process.yaml file. You have to add the docker image name( /siddhi-runner-alpine:latest) in the YAML entry spec.container.image . $ kubectl apply -f siddhi-process.yaml If all the Kubernetes artifacts deployed correctly, it will show the status like below. $ kubectl get deploy NAME READY UP-TO-DATE AVAILABLE AGE bold-boxer-mongodb 1/1 1 1 2h mysqldb 1/1 1 1 2h nats-operator 1/1 1 1 2h nats-streaming-operator 1/1 1 1 2h paris-tma-0 1/1 1 1 2h paris-tma-1 1/1 1 1 2h paristma-mongodb 1/1 1 1 2h siddhi-operator 1/1 1 1 2h tiller-deploy 1/1 1 1 2h $ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE bold-boxer-mongodb ClusterIP 10.100.153.173 none 27017/TCP 2h kubernetes ClusterIP 10.96.0.1 none 443/TCP 21d mysqldb ClusterIP 10.106.112.140 none 3306/TCP 2h paristma-mongodb ClusterIP 10.102.211.214 none 27017/TCP 2h siddhi-nats ClusterIP 10.110.123.222 none 4222/TCP 2h siddhi-nats-mgmt ClusterIP None none 6222/TCP,8222/TCP,7777/TCP 2h siddhi-operator ClusterIP 10.102.157.184 none 8383/TCP 2h tiller-deploy ClusterIP 10.103.160.179 none 44134/TCP 2h $ kubectl get siddhi NAME STATUS READY AGE paris-tma Running 2/2 2h When you insert data entry to the Incident table, the Siddhi app will simply send an email to the subscribers like below.","title":"Integrate Various Enterprise Systems"},{"location":"docs/guides/integrate-various-enterprise-systems/guide/#retrieve-and-publish-data-fromto-various-enterprise-systems","text":"This guide illustrates how you can receive data from various enterprise systems, process it, and then send processed data into other systems.","title":"Retrieve and Publish Data from/to Various Enterprise Systems"},{"location":"docs/guides/integrate-various-enterprise-systems/guide/#scenario","text":"Paris becomes very busy in the rush hours. Most of the people trying to go to offices in the morning and children try to go to school. In the afternoon all those people trying to go back to their premises. The traffic becomes high in those hours. If there is a bad weather condition in those rush hours the traffic might increase up to a certain level. Partis Transport Management Authority (Paris TMA) is accountable for handling all the traffic in Paris. They have noticed this increase in traffic due to the bad weather conditions in several areas. To reduce this traffic, they try to build a system that notifies all users if there are bad weather conditions in a particular area. Paris TMA uses a centralized database to receive notifications about bad weather conditions. Besides, they are tracking locations of all the vehicles in Paris city. If some incident happens, this system will notify all the users that are near to that area. After completing this guide you will understand how to receive various inputs from heterogeneous systems (like NATS, RDBMS, MongoDB), process it, and send back processed outputs in various formats like Email notification.","title":"Scenario"},{"location":"docs/guides/integrate-various-enterprise-systems/guide/#what-youll-build","text":"The implementation of the ParisTMA system represented in the following architecture diagram. The overall architecture can be expressed as the following subsystems.","title":"What You'll Build"},{"location":"docs/guides/integrate-various-enterprise-systems/guide/#vehicle-locator-subsystem","text":"The vehicle locator subsystem is responsible for receiving the location of each subscriber and update subscriber database collection with their current location. The process of this subsystem showed in step 01-03 in the architecture diagram. The location of a vehicle, input to the NATS cluster as a JSON object like below. { \"vehicleId\": \"v001\", \"currentArea\": \"a002\" } The Siddhi app called ParisTMAVehicleLocator will listen to those messages that come to the NATS cluster and periodically update the Mongo database with those current locations. This Mongo database contained all the information relevant to subscribers along with their current location.","title":"Vehicle Locator Subsystem"},{"location":"docs/guides/integrate-various-enterprise-systems/guide/#notifier-subsystem","text":"The process of the notifier subsystem illustrated in step 04-07 in the architecture diagram. The notifier subsystem triggers the process when our system receives weather update to the RDBMS as shown in step 04. The ParisTMANotifier Siddhi app will handle all the logical execution of this notification process. There exists a CDC source which listens to the changes of the RDBMS. When some weather notification inserts into the RDBMS, the ParisTMANotifier Siddhi app retrieves this insert event. That insert event will contain the area that this weather incident happened. Using that area, ParisTMANotifier filters out all the subscribers that are in that given area using the subscriber collection in the Mongo database. To all those subscribers ParisTMANotifier will send an email notification about this incident.","title":"Notifier Subsystem"},{"location":"docs/guides/integrate-various-enterprise-systems/guide/#prerequisites","text":"","title":"Prerequisites"},{"location":"docs/guides/integrate-various-enterprise-systems/guide/#mandatory-requirements","text":"Siddhi tooling VM/Local distribution One of the Siddhi runner distributions VM/Local Runtime Docker Image K8s Operator (commands are given in deployment section) MySQL database Mongo database NATS streaming server Java 8 or higher","title":"Mandatory Requirements"},{"location":"docs/guides/integrate-various-enterprise-systems/guide/#requirements-needed-to-deploy-siddhi-in-dockerkubernetes","text":"Docker Kubernetes cluster Minikube Google Kubernetes Engine(GKE) Docker for Mac Helm Refer to this documentation about Configuring a Google Kubernetes Engine (GKE) Cluster to deploy Siddhi apps in GKE.","title":"Requirements needed to deploy Siddhi in Docker/Kubernetes"},{"location":"docs/guides/integrate-various-enterprise-systems/guide/#implementation","text":"As illustrated in the previous architecture diagram you will need Siddhi apps called ParisTMAVehicleLocator and ParisTMANotifier. Start the Siddhi tooling runtime and go to the editor UI in http://localhost:9390/editor Use the following steps to start the Siddhi tooling runtime. Extract the downloaded zip and navigate to /bin. (TOOLING_HOME refers to the extracted folder). Use the following command in the command prompt (Windows) / terminal (Linux/Mac). For Windows: tooling.bat For Linux/Mac: ./tooling.sh Select File - New option, then you could either use the source view or design view to write/build the Siddhi Application. You can find the Siddhi Application bellow, that implements the requirements mentioned above. Let\u2019s write (develop) the Siddhi Application, as given below. Once the Siddhi app is created, you can use the Event Simulator option in the editor to simulate events to streams and perform developer testing. The implementation of those apps is described in the following subtopics. Siddhi Query Guide The execution steps and the logic of the Siddhi query described as comments in the following Siddhi app. Therefore here we are not going to explain in detail here. For more details about Siddhi queries please refer Siddhi query guide .","title":"Implementation"},{"location":"docs/guides/integrate-various-enterprise-systems/guide/#vehicle-locator","text":"@App:name(\"ParisTMAVehicleLocator\") @App:description(\"Listen to the events send by NATS about vehicle locations and periodically update the Mongo database.\") /* Purpose: The ParisTMA has subscribers that subscribe to get the service. All the subscriber details are saved in the Mongo database. In that database ParisTMA stores the current location of subscriber vehicles. The events related to current locations come as TCP requests through NATS. This app listens to those events and eventually update the database with the current location of each subscriber. Input: NATS event with JSON payload: { \"vehicleId\": \"v001\", \"currentArea\": \"a002\" } Output: Update the collection(subscriber) documents of MongoDB(parisTma). */ -- NATS source that listens to the events @source( type='nats', cluster.id='${STAN_CLUSTER_ID}', destination='VehicleInputTunnel', bootstrap.servers='${STAN_SERVER_URL}', @map(type='json') ) define stream CurrentVehicleAreaInputStream(vehicleId string, currentArea string); -- MongoDB store @store(type='mongodb', mongodb.uri='${MONGODB_URI}') define table subscriber(vehicleId string, currentArea string); -- Retrieves events from NATS source and update in MongoDB @info(name='update-or-insert-subscriber-location') from CurrentVehicleAreaInputStream update or insert into subscriber on subscriber.vehicleId==vehicleId;","title":"Vehicle Locator"},{"location":"docs/guides/integrate-various-enterprise-systems/guide/#notifier","text":"@App:name(\"ParisTMANotifier\") @App:description(\"Listen to the CDC events about the weather updates and send notifications to the subscribed users.\") /* Purpose: The ParisTMA has subscribers that subscribe to get the service. All the subscriber details are saved in the Mongo database. In that database ParisTMA stores the current location of subscriber vehicles. There is a separate system handle by the Paris weather forecast organization. They periodically update the MySQL database called 'paristma' with recent weather incidents. This app listens to those insertions using CDC and sends notifications to the subscribers who are in that particular area that incident happens. Input: CDC insertions. Output: Email to users that notifying about the incident. Note: Even you can change this email notification to an SMS notification calling an SMS provider. */ -- CDC source that listens to insertions of the Incident table @source( type='cdc', url='${MYSQL_URL}', username='siddhi_user', password='siddhiio', table.name='Incident', operation='insert', @map( type='keyvalue' ) ) define stream IncidentInputStream(incidentId int, incidentName string, incidentType string, incidentDetails string, incidentArea string); -- MongoDB store of subscribers @store(type='mongodb', mongodb.uri='${MONGODB_URI}') define table subscriber(vehicleId string, currentArea string, subscriberName string, subscriberEmail string); -- Email sink to send notifications @sink(type='email', username=\"${PARIS_TMA_EMAIL}\", address=\"${PARIS_TMA_EMAIL}\", password=\"${PARIS_TMA_EMAIL_PASSWORD}\", subject=\"Weather Notification in {{incidentArea}}\", to=\"{{subscriberEmail}}\", host=\"smtp.gmail.com\", port=\"465\", ssl.enable=\"true\", auth=\"true\", @map(type='text', @payload(\"\"\" Hi {{subscriberName}} Recent reports state that there is {{incidentName}} in your area {{incidentArea}}. It is in {{incidentType}} state right now. The report state that {{incidentDetails}}. Thanks, Paris Transport Management Authority\"\"\"))) define stream UserNotificationStream (subscriberName string, subscriberEmail string, incidentName string, incidentType string, incidentDetails string, incidentArea string); -- Listen to the insertions in IncidentInputStream and join it with subscriber collection in MongoDB to get user information. Send notifications using that user information. @info(name='listen-and-notify') from IncidentInputStream#window.length(1) join subscriber on IncidentInputStream.incidentArea==subscriber.currentArea select subscriber.subscriberName, subscriber.subscriberEmail, IncidentInputStream.incidentName, IncidentInputStream.incidentType, IncidentInputStream.incidentDetails, IncidentInputStream.incidentArea insert into UserNotificationStream;","title":"Notifier"},{"location":"docs/guides/integrate-various-enterprise-systems/guide/#testing","text":"","title":"Testing"},{"location":"docs/guides/integrate-various-enterprise-systems/guide/#setup-mongodb","text":"Refer this link to install MongoDB in your local machine. The ParisTMA system uses a Mongo database called parisTma and that parisTma collection contained a collection called the subscriber . In order to connect to the Mongo database, the Siddhi runtime will use a user called siddhi_user that identified by password siddhiio . You can create those MongoDB databases, collections, users and insert data using the following MongoDB commands. use parisTma db.createCollection(\"subscriber\") db.createUser( { user: \"siddhi_user\", pwd: \"siddhiio\", roles:[{role: \"userAdmin\" , db:\"parisTma\"}] } ) db.subscriber.insert({ vehicleId: 'v001', currentArea: 'a001', subscriberName: 'John Dus', subscriberEmail: 'b.wathsala.bw@gmail.com' }) db.subscriber.insert({ vehicleId: 'v002', currentArea: 'a002', subscriberName: 'Natalie Jonson', subscriberEmail: 'b.wathsala.bw@gmail.com' }) db.subscriber.insert({ vehicleId: 'v003', currentArea: 'a003', subscriberName: 'Mark Norman', subscriberEmail: 'buddhik@wso2.com' })","title":"Setup MongoDB"},{"location":"docs/guides/integrate-various-enterprise-systems/guide/#setup-mysql","text":"After that, you have to create a MySQL database called paristma . It contained a table called Incident to store all the weather incidents happened recently. To access the MySQL database you also need to create a user called siddhi_user that identified by password siddhiio . The incident table will look like the following. To connect to the MySQL database Siddhi tooling runtime needs the MySQL client library. You can download the MySQL JAR from here and then add the JAR to the ${TOOLING_HOME}/jars directory. This example uses the Siddhi CDC in the default(listening) mode. The listening mode needs to change the configurations as described here . For example, you have to add the following configurations to the my.cnf file. [mysqld] server-id = 223344 log_bin = mysql-bin binlog_format = row binlog_row_image = full expire_logs_days = 10 gtid_mode = on enforce_gtid_consistency = on binlog_rows_query_log_events = on log-error=/var/log/mysql/mysql.err log-bin = /var/log/mysql/mysql-replication.log","title":"Setup MySQL"},{"location":"docs/guides/integrate-various-enterprise-systems/guide/#setup-nats-streaming-cluster","text":"Now you need to install a NATS streaming server . For example, to install the NATS streaming cluster in MacOS use the following commands. $ brew install nats-streaming-server After the installation runs the NATS streaming cluster as below. $ nats-streaming-server The Siddhi app runtime will need the following JARs to connect to the NATS streaming server. Download those JARs using the following links and add those JARs to ${TOOLING_HOME}/jars directory. Java NATS streaming JNATS It also needs the Protobuf bundle. Download the Protobuf bundle using the following link and add the bundle to ${TOOLING_HOME}/bundles directory. Protobuf","title":"Setup NATS Streaming Cluster"},{"location":"docs/guides/integrate-various-enterprise-systems/guide/#configurations","text":"The above Siddhi apps receive the MySQL, MongoDB, and NATS connections details as environment variables. For local deployments for testing these apps, you have to set up those environmental variables as follows. STAN_CLUSTER_ID= test-cluster STAN_SERVER_URL= nats://localhost:4222 MONGODB_URI= mongodb://127.0.0.1:27017/parisTma? gssapiServiceName=mongodb MYSQL_URL= jdbc:mysql://127.0.0.1:3306/paristma PARIS_TMA_EMAIL= TEST_EMAIL PARIS_TMA_EMAIL_PASSWORD= TEST_EMAIL_PASSWORD","title":"Configurations"},{"location":"docs/guides/integrate-various-enterprise-systems/guide/#tryout","text":"After all those configurations and setups you will be able to starts the both Siddhi apps without any error. Testing the app can be done in two levels. Update the vehicle location periodically Send notifications To test the vehicle location updates you have to send a request to the NATS streaming server. You can send the request using various NATS clients. In this use case, you can use NATS streaming Go language client to easily send requests. To do that you need to have to install Golang in your machine. Or else you can use other clients like ( Java , JS , Python , or Ruby etc). You can use following Golang commands to send a request to the NATS streaming cluster. $ go get github.com/nats-io/stan.go/ $ go run $GOPATH/src/github.com/nats-io/stan.go/examples/stan-pub/main.go -s localhost:4222 -c test-cluster VehicleInputTunnel \"{\\\"vehicleId\\\": \\\"v001\\\", \\\"currentArea\\\":\\\"a003\\\"}\" To run the notification Siddhi app you just need to insert data entry to the Incident table. When you do that it the Siddhi app will simply send an email to the subscribers like below.","title":"Tryout"},{"location":"docs/guides/integrate-various-enterprise-systems/guide/#deployment","text":"","title":"Deployment"},{"location":"docs/guides/integrate-various-enterprise-systems/guide/#deploy-on-vm-bare-metal","text":"First, you have to set up prerequisites as described in the previous sections. Setup MongoDB Setup MySQL Setup NATS Streaming Download the Siddhi runner distribution pack from here and unzip it. Download relevant JARs and add those JARs to ${RUNNER_HOME}/jars directory. MySQL Java NATS streaming JNATS Download Protobuf bundle and add the bundle to ${RUNNER_HOME}/bundles directory. Protobuf Copy your Siddhi file into RUNNER_HOME /wso2/runner/deployment/siddhi-files Start the following binary file to run the Siddhi runner server. For Linux/Mac: RUNNER_HOME /bin/runner.sh For Windows: RUNNER_HOME /bin/runner.bat Now you can try out the sample as described in this tryout section .","title":"Deploy on VM/ Bare Metal"},{"location":"docs/guides/integrate-various-enterprise-systems/guide/#deploy-on-docker","text":"","title":"Deploy on Docker"},{"location":"docs/guides/integrate-various-enterprise-systems/guide/#prerequisite","text":"Install Docker in your machine.","title":"Prerequisite"},{"location":"docs/guides/integrate-various-enterprise-systems/guide/#siddhi-docker-configurations","text":"In the tooling editor itself, you can export your Siddhi app into a runnable docker artifact. You can go to Export- For Docker and it will give to a zip file that contained the following files. \u251c\u2500\u2500 Dockerfile \u251c\u2500\u2500 bundles \u2502 \u2514\u2500\u2500 protobuf-java-3.6.1.jar \u251c\u2500\u2500 configurations.yaml \u251c\u2500\u2500 jars \u2502 \u251c\u2500\u2500 java_nats_streaming_2.1.2.jar \u2502 \u251c\u2500\u2500 jnats_2.3.0.jar \u2502 \u2514\u2500\u2500 mysql-connector-java-5.1.45-bin.jar \u2514\u2500\u2500 siddhi-files \u251c\u2500\u2500 ParisTMANotifier.siddhi \u2514\u2500\u2500 ParisTMAVehicleLocator.siddhi When you export the apps as a Docker, it will ask for the templated values to be filled. You have to use the following values for those templated values. STAN_CLUSTER_ID= test-cluster STAN_SERVER_URL= nats://nats:4222 MONGODB_URI= mongodb://mongodb:27017/parisTma? gssapiServiceName=mongodb MYSQL_URL= jdbc:mysql://mysqldb:3306/paristma PARIS_TMA_EMAIL= TEST_EMAIL PARIS_TMA_EMAIL_PASSWORD= TEST_EMAIL_PASSWORD In order to change the configurations of MySQL Docker container you have create my.cnf file in your current directory and add the following content to that file. Also create directory called mysql . [mysqld] server-id = 223344 log_bin = mysql-bin binlog_format = row binlog_row_image = full expire_logs_days = 10 gtid_mode = on enforce_gtid_consistency = on binlog_rows_query_log_events = on log-error=/var/log/mysql/mysql.err log-bin = /var/log/mysql/mysql-replication.log Now you need to have a Docker compose file like below to set up all the prerequisites. This compose file contains volume mounts to change configurations of the MySQL container. version: \"3\" services: backend: container_name: paristma-notifier user: 802:802 build: context: . dockerfile: ./Dockerfile links: - mysqldb - mongodb - nats networks: - default restart: on-failure mysqldb: image: 'mysql:5.7' container_name: paristma-mysqldb environment: MYSQL_USER: siddhi_user MYSQL_PASSWORD: siddhiio MYSQL_ROOT_PASSWORD: siddhiio ports: - \"3304:3306\" networks: - default restart: on-failure volumes: - ./my.cnf:/etc/my.cnf - ./mysql:/var/log/mysql command: --sql_mode=\"\" mongodb: image: 'mongo:4.0.4' container_name: paristma-mongodb ports: - \"27017-27019:27017-27019\" networks: - default restart: on-failure nats: image: 'nats-streaming:0.16.2-linux' container_name: paristma-nats ports: - \"4223:4223\" - \"8223:8223\" networks: - default restart: on-failure Now you can start each containers. First, you need to build the Docker composer. docker-compose build Starts the MySQL container using the following command and set up the MySQL database as described above . docker-compose up -d mysqldb Then, starts the MongoDB container using the following command and set up the MongoDB database as described above . docker-compose up -d mongodb After that, starts the NATS container using the following command. docker-compose up -d nats Finally, start the Siddhi backend runtime. docker-compose up -d backend When you insert data entry to the Incident table, the Siddhi app will simply send an email to the subscribers like below.","title":"Siddhi Docker Configurations"},{"location":"docs/guides/integrate-various-enterprise-systems/guide/#deploy-on-kubernetes","text":"","title":"Deploy on Kubernetes"},{"location":"docs/guides/integrate-various-enterprise-systems/guide/#prerequisites_1","text":"Kubernetes cluster Minikube Google Kubernetes Engine(GKE) Docker for Mac Install HELM","title":"Prerequisites"},{"location":"docs/guides/integrate-various-enterprise-systems/guide/#siddhi-kubernetes-configurations","text":"In the tooling editor itself, you can export your Siddhi app into a runnable Kubernetes artifact. You can go to Export- For Kubernetes and it will give to a zip file that contained the following files. \u251c\u2500\u2500 Dockerfile \u251c\u2500\u2500 bundles \u2502 \u2514\u2500\u2500 protobuf-java-3.6.1.jar \u251c\u2500\u2500 configurations.yaml \u251c\u2500\u2500 jars \u2502 \u251c\u2500\u2500 java_nats_streaming_2.1.2.jar \u2502 \u251c\u2500\u2500 jnats_2.3.0.jar \u2502 \u2514\u2500\u2500 mysql-connector-java-5.1.45-bin.jar \u251c\u2500\u2500 siddhi-files \u2502 \u251c\u2500\u2500 ParisTMANotifier.siddhi \u2502 \u2514\u2500\u2500 ParisTMAVehicleLocator.siddhi \u2514\u2500\u2500 siddhi-process.yaml When you export the apps as a Kubernetes, it will ask for the templated values to be filled. You have to use the following values for those templated values. STAN_CLUSTER_ID= siddhi-stan STAN_SERVER_URL= nats://siddhi-nats:4222 MONGODB_URI= mongodb://siddhi_user:siddhiio@paristma-mongodb:27017/parisTma MYSQL_URL= jdbc:mysql://mysqldb:3306/paristma PARIS_TMA_EMAIL= TEST_EMAIL PARIS_TMA_EMAIL_PASSWORD= TEST_EMAIL_PASSWORD","title":"Siddhi Kubernetes Configurations"},{"location":"docs/guides/integrate-various-enterprise-systems/guide/#setup-mongodb_1","text":"Let\u2019s install MongoDB in your Kubernetes cluster using Helm. $ helm install stable/mongodb $ helm install --name paristma --set volumePermissions.enabled=true,mongodbRootPassword=siddhiio,mongodbUsername=siddhi_user,mongodbPassword=siddhiio,mongodbDatabase=parisTma stable/mongodb Now you can access the MongoDB externally using following commands and set up the Mongo database as described previously . $ export MONGODB_ROOT_PASSWORD=$(kubectl get secret --namespace default paristma-mongodb -o jsonpath=\"{.data.mongodb-root-password}\" | base64 --decode) $ kubectl run --namespace default paristma-mongodb-client --rm --tty -i --restart='Never' --image bitnami/mongodb --command -- mongo admin --host paristma-mongodb --authenticationDatabase admin -u root -p $MONGODB_ROOT_PASSWORD","title":"Setup MongoDB"},{"location":"docs/guides/integrate-various-enterprise-systems/guide/#setup-mysql_1","text":"In order to change the MySQL configurations for the Siddhi CDC extension, you need to create a file called values.yaml with including the following configurations. mysqlRootPassword: siddhiio mysqlUser: siddhi_user mysqlPassword: siddhiio mysqlDatabase: paristma configurationFiles: my.cnf: |- [mysqld] server-id = 223344 log_bin = mysql-bin binlog_format = row binlog_row_image = full expire_logs_days = 10 gtid_mode = on enforce_gtid_consistency = on binlog_rows_query_log_events = on log-error=/var/log/mysql/mysql.err log-bin = /var/log/mysql/mysql-replication.log Now you can install MySQL in you Kubernetes cluster using Helm. $ helm install --name mysqldb -f values.yaml stable/mysql Use Kubernetes port forwading to access MySQL externally using 3307 port and create nessasary tables in the MySQL database as described previously . $ kubectl port-forward svc/mysql-db 3307:3306","title":"Setup MySQL"},{"location":"docs/guides/integrate-various-enterprise-systems/guide/#setup-nats-streaming","text":"Use the following kubectl commands to set up the NATS streaming cluster. $ kubectl apply -f https://github.com/nats-io/nats-operator/releases/download/v0.5.0/00-prereqs.yaml $ kubectl apply -f https://github.com/nats-io/nats-operator/releases/download/v0.5.0/10-deployment.yaml $ kubectl apply -f https://github.com/nats-io/nats-streaming-operator/releases/download/v0.2.2/default-rbac.yaml $ kubectl apply -f https://github.com/nats-io/nats-streaming-operator/releases/download/v0.2.2/deployment.yaml $ echo ' --- apiVersion: \"nats.io/v1alpha2\" kind: \"NatsCluster\" metadata: name: \"siddhi-nats\" spec: size: 1 ' | kubectl apply -f - $ echo ' --- apiVersion: \"streaming.nats.io/v1alpha1\" kind: \"NatsStreamingCluster\" metadata: name: \"siddhi-stan\" spec: size: 1 natsSvc: \"siddhi-nats\" ' | kubectl apply -f - For more details about the NATS streaming server, refer to this documentation .","title":"Setup NATS Streaming"},{"location":"docs/guides/integrate-various-enterprise-systems/guide/#setup-siddhi-operator","text":"Use the following kubectl commands to install the Siddhi operator in your Kubernetes cluster. $ kubectl apply -f https://github.com/siddhi-io/siddhi-operator/releases/download/v0.2.0/00-prereqs.yaml $ kubectl apply -f https://github.com/siddhi-io/siddhi-operator/releases/download/v0.2.0/01-siddhi-operator.yaml Now you need to create your own docker image with including all the custom libraries and configuration changes that you have made. Use following command to build and push the docker image with the tag DOCKER_HUB_USER_NAME /siddhi-runner-alpine:latest . $ docker build -t DOCKER_HUB_USER_NAME /siddhi-runner-alpine:latest . $ docker push DOCKER_HUB_USER_NAME /siddhi-runner-alpine:latest After the Kubernetes export now you already have this siddhi-process.yaml file. apiVersion: siddhi.io/v1alpha2 kind: SiddhiProcess metadata: name: paris-tma spec: apps: - script: | @App:name(\"ParisTMANotifier\") @App:description(\"Listen to the CDC events about the weather updates and send notifications to the subscribed users.\") /* Purpose: The ParisTMA has subscribers that subscribe to get the service. All the subscriber details are saved in the Mongo database. In that database ParisTMA stores the current location of subscriber vehicles. There is a separate system handle by the Paris weather forecast organization. They periodically update the MySQL database called 'paristma' with recent weather incidents. This app listens to those insertions using CDC and sends notifications to the subscribers who are in that particular area that incident happens. Input: CDC insertions. Output: Email to users that notifying about the incident. Note: Even you can change this email notification to an SMS notification calling an SMS provider. */ -- CDC source that listens to insertions of the Incident table @source( type='cdc', url='${MYSQL_URL}', username='siddhi_user', password='siddhiio', table.name='Incident', operation='insert', @map( type='keyvalue' ) ) define stream IncidentInputStream(incidentId int, incidentName string, incidentType string, incidentDetails string, incidentArea string); -- MongoDB store of subscribers @store(type='mongodb', mongodb.uri='${MONGODB_URI}') define table subscriber(vehicleId string, currentArea string, subscriberName string, subscriberEmail string); -- Email sink to send notifications @sink(type='email', username=\"${PARIS_TMA_EMAIL}\", address=\"${PARIS_TMA_EMAIL}\", password=\"${PARIS_TMA_EMAIL_PASSWORD}\", subject=\"Weather Notification in {{incidentArea}}\", to=\"{{subscriberEmail}}\", host=\"smtp.gmail.com\", port=\"465\", ssl.enable=\"true\", auth=\"true\", @map(type='text', @payload(\"\"\" Hi {{subscriberName}} Recent reports state that there is {{incidentName}} in your area {{incidentArea}}. It is in {{incidentType}} state right now. The report state that {{incidentDetails}}. Thanks, Paris Transport Management Authority\"\"\"))) define stream UserNotificationStream (subscriberName string, subscriberEmail string, incidentName string, incidentType string, incidentDetails string, incidentArea string); -- Listen to the insertions in IncidentInputStream and join it with subscriber collection in MongoDB to get user information. Send notifications using that user information. @info(name='listen-and-notify') from IncidentInputStream#window.length(1) join subscriber on IncidentInputStream.incidentArea==subscriber.currentArea select subscriber.subscriberName, subscriber.subscriberEmail, IncidentInputStream.incidentName, IncidentInputStream.incidentType, IncidentInputStream.incidentDetails, IncidentInputStream.incidentArea insert into UserNotificationStream; - script: |- @App:name(\"ParisTMAVehicleLocator\") @App:description(\"Listen to the events send by NATS about vehicle locations and periodically update the Mongo database.\") /* Purpose: The ParisTMA has subscribers that subscribe to get the service. All the subscriber details are saved in the Mongo database. In that database ParisTMA stores the current location of subscriber vehicles. The events related to current locations come as TCP requests through NATS. This app listens to those events and eventually update the database with the current location of each subscriber. Input: NATS event with JSON payload: { \"vehicleId\": \"v001\", \"currentArea\": \"a002\" } Output: Update the collection(subscriber) documents of MongoDB(parisTma). */ -- NATS source that listens to the events @source( type='nats', cluster.id='${STAN_CLUSTER_ID}', destination='VehicleInputTunnel', bootstrap.servers='${STAN_SERVER_URL}', @map(type='json') ) define stream CurrentVehicleAreaInputStream(vehicleId string, currentArea string); -- MongoDB store @store(type='mongodb', mongodb.uri='${MONGODB_URI}') define table subscriber(vehicleId string, currentArea string); -- Retrieves events from NATS source and update in MongoDB @info(name='update-or-insert-subscriber-location') from CurrentVehicleAreaInputStream update or insert into subscriber on subscriber.vehicleId==vehicleId; container: env: - name: MYSQL_URL value: jdbc:mysql://mysqldb:3306/paristma - name: MONGODB_URI value: mongodb://siddhi_user:siddhiio@paristma-mongodb:27017/parisTma - name: PARIS_TMA_EMAIL value: TEST_EMAIL - name: PARIS_TMA_EMAIL_PASSWORD value: TEST_EMAIL_PASSWORD - name: STAN_CLUSTER_ID value: siddhi-stan - name: STAN_SERVER_URL value: nats://siddhi-nats:4222 image: DOCKER_HUB_USER_NAME /siddhi-runner-alpine:latest runner: | wso2.carbon: id: siddhi-runner name: Siddhi Runner Distribution ports: offset: 0 transports: http: listenerConfigurations: - id: default host: 0.0.0.0 port: 9090 - id: msf4j-https host: 0.0.0.0 port: 9443 scheme: https keyStoreFile: ${carbon.home}/resources/security/wso2carbon.jks keyStorePassword: wso2carbon certPass: wso2carbon transportProperties: - name: server.bootstrap.socket.timeout value: 60 - name: client.bootstrap.socket.timeout value: 60 - name: latency.metrics.enabled value: true dataSources: - name: WSO2_CARBON_DB description: The datasource used for registry and user manager definition: type: RDBMS configuration: jdbcUrl: jdbc:h2:${sys:carbon.home}/wso2/${sys:wso2.runtime}/database/WSO2_CARBON_DB;DB_CLOSE_ON_EXIT=FALSE;LOCK_TIMEOUT=60000 username: wso2carbon password: wso2carbon driverClassName: org.h2.Driver maxPoolSize: 10 idleTimeout: 60000 connectionTestQuery: SELECT 1 validationTimeout: 30000 isAutoCommit: false Now you can install the SiddhiProcess using following kubectl command. Before you install the SiddhiProcess you have to add the docker image tag in the siddhi-process.yaml file. You have to add the docker image name( /siddhi-runner-alpine:latest) in the YAML entry spec.container.image . $ kubectl apply -f siddhi-process.yaml If all the Kubernetes artifacts deployed correctly, it will show the status like below. $ kubectl get deploy NAME READY UP-TO-DATE AVAILABLE AGE bold-boxer-mongodb 1/1 1 1 2h mysqldb 1/1 1 1 2h nats-operator 1/1 1 1 2h nats-streaming-operator 1/1 1 1 2h paris-tma-0 1/1 1 1 2h paris-tma-1 1/1 1 1 2h paristma-mongodb 1/1 1 1 2h siddhi-operator 1/1 1 1 2h tiller-deploy 1/1 1 1 2h $ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE bold-boxer-mongodb ClusterIP 10.100.153.173 none 27017/TCP 2h kubernetes ClusterIP 10.96.0.1 none 443/TCP 21d mysqldb ClusterIP 10.106.112.140 none 3306/TCP 2h paristma-mongodb ClusterIP 10.102.211.214 none 27017/TCP 2h siddhi-nats ClusterIP 10.110.123.222 none 4222/TCP 2h siddhi-nats-mgmt ClusterIP None none 6222/TCP,8222/TCP,7777/TCP 2h siddhi-operator ClusterIP 10.102.157.184 none 8383/TCP 2h tiller-deploy ClusterIP 10.103.160.179 none 44134/TCP 2h $ kubectl get siddhi NAME STATUS READY AGE paris-tma Running 2/2 2h When you insert data entry to the Incident table, the Siddhi app will simply send an email to the subscribers like below.","title":"Setup Siddhi Operator"},{"location":"docs/guides/long-term-aggregation/guide/","text":"Long-running time based Aggregations In this guide, you will understand one of the common requirements of Analytics which is aggregating data. Scenario - Generating alerts based on long-running summarization Aggregation is a mandatory need in any system as it allows users to spot trends and anomalies easily which can lead to actions that will benefit an organization or the business. Aggregated data can also be processed easily to get the information needed for a business requirement decision. The following sections are available in this guide. What you'll build Prerequisites Implementation Testing Deployment Output What you'll build Let's consider a real world use case to implement the aggregation requirement. This will help you to understand Siddhi Stream Processing construct, Named Aggregations. Let\u2019s jump into the use case directly. Let's assume you are a data analyst in the 'X' online shopping mall. 'X' shopping mall daily offers deals to customers to boost their sales. Each day past sales is analysed such that recommendation can be given on what items to be put up for Daily deals and more importantly out of all sellers, whose items to be used in Daily Deals. Let's take a simple logic to select Daily Deals, you will analyse the last 10 days of sales and get the products which has a lower amount of sales, lets say 100 units. For these products we will select sellers who has the highest sales in the past month and generate an email with top 5 candidates whose products can be offered in the Daily Deal. Now, let\u2019s understand how this could be implemented in Siddhi engine. For the aggregation use case we will be using a specific function of Siddhi engine, called Named Aggregations. Prerequisites Below are the prerequisites that should be considered to implement the above use case. Mandatory Requirements Siddhi tooling VM/Local distribution One of the Siddhi runner distributions VM/Local Runtime Docker Image K8S Operator (commands are given in deployment section) Java 8 or higher Requirements needed to deploy Siddhi in Docker/Kubernetes Docker Minikube or Google Kubernetes Engine(GKE) Cluster or Docker for Mac Implementation When a user checkouts out a sales cart in 'X' online shopping malls website, the website will send the information to Siddhi runtime through HTTP transport. Siddhi runtime will aggregate the sales data in multiple granularities from seconds to years, grouped by category, product code and seller. Daily at 11.50 pm an alert will be generated to include, Daily Deals product and seller recommendations. Implement Streaming Queries Start the Siddhi tooling runtime and go to the editor UI in http://localhost:9390/editor Follow below steps to start the Siddhi tooling runtime, * Extract the downloaded zip and navigate to /bin. (TOOLING_HOME refers to the extracted folder) * Issue the following command in the command prompt (Windows) / terminal (Linux/Mac) For Windows: tooling.bat For Linux/Mac: ./tooling.sh Select File - New option, then you could either use the source view or design view to write/build the Siddhi Application. You can find the Siddhi Applications below, that implements the requirements mentioned above. Let\u2019s write (develop) the Siddhi Applications, as given below. Once the Siddhi apps are created, you can use the Event Simulator option in the editor to simulate events to streams and perform developer testing. Siddhi App 1 : Responsible for Aggregating Data @App:name(\"SalesSummarization\") @App:description('X online shopping mall - Summarization for sales by category, product and seller') @Source(type = 'http', receiver.url = 'http://0.0.0.0:8080/sales', basic.auth.enabled = 'false', @map(type = 'json')) define stream SalesStream(timestamp long, categoryName string, productName string, sellerName string, quantity int); --Long term Summarization @store(type = 'rdbms' , jdbc.url = '${MYSQL_URL}', username = '${MYSQL_USERNAME}', password = '${MYSQL_PASSWORD}', jdbc.driver.name = 'com.mysql.jdbc.Driver', pool.properties = \"maximumPoolSize:1\") @purge(enable = 'false') define aggregation SalesAggregation from SalesStream select categoryName, productName, sellerName, sum(quantity) as totalSales group by categoryName, productName, sellerName aggregate by timestamp every sec...year; Siddhi App 2: Responsible for Sending alerts @App:name(\"DailyDealsCandidatesAlert\") @App:description(\"Alerts regarding daily deals candidates - 1 product with max 5 sellers who sold the most in past 30 days\") define trigger DailyTrigger at '0 00 23 ? * MON-FRI'; @Source(type = 'http', receiver.url = 'http://0.0.0.0:8080/dailyDealsAlert', basic.auth.enabled = 'false', @map(type = 'json')) define stream DailyDealsTriggerStream(emailToBeSent bool); define stream SalesStream(timestamp long, categoryName string, productName string, sellerName string, quantity int); @sink(type = 'email', address = \"${SENDER_EMAIL_ADDRESS}\", username = \"${EMAIL_USERNAME}\", password = \"${EMAIL_PASSWORD}\", subject = \"{{emailSubject}}\", to = \"${RECEIVER_EMAIL_ADDRESS}\", host = \"smtp.gmail.com\", port = \"465\", content.type = 'text/html', ssl.enable = \"true\", auth = \"true\", @map(type = 'text', @payload(\"\"\" Hi, br/ br/ Please find the seller candidates for Daily Deal {{dailyDealDate}} below, br/ br/ Product strong {{productName}} /strong -{{categoryName}} sold {{last10DaysSales}} units for the last 10 days br/ br/ table tr th Seller Name /th th Total Sales for the last Month /th /tr {{candidateInfo}} /table br/ br/ Thanks, br/ Analytics Team\"\"\"))) define stream EmailNotificationStream(dailyDealDate string, emailSubject string, productName string, categoryName string, last10DaysSales long, candidateInfo string); @sink(type = 'log', @map(type = 'json')) define stream DailyDealsSellerCandidates (emailToBeSent bool, categoryName string, productName string, last10DaysSales long, sellerName string, sellersSales long); --Long term Summarization @store(type = 'rdbms' , jdbc.url = '${MYSQL_URL}', username = '${MYSQL_USERNAME}', password = '${MYSQL_PASSWORD}', jdbc.driver.name = 'com.mysql.jdbc.Driver' , pool.properties = \"maximumPoolSize:1\") @purge(enable = 'false') define aggregation SalesAggregation from SalesStream select categoryName, productName, sellerName, sum(quantity) as totalSales group by categoryName, productName, sellerName aggregate by timestamp every sec...year; @info(name = 'calculateStartEndTimeForHTTPTrigger') from DailyDealsTriggerStream select emailToBeSent, time:timestampInMilliseconds(time:dateSub(time:currentDate(), 9, 'DAY', 'yyyy-MM-dd'), 'yyyy-MM-dd') as startTime, time:timestampInMilliseconds() as endTime insert into JoinStream; @info(name = 'calculateStartEndTimeForDailyTrigger') from DailyTrigger select true as emailToBeSent, time:timestampInMilliseconds(time:dateSub(time:currentDate(), 9, 'DAY', 'yyyy-MM-dd'), 'yyyy-MM-dd') as startTime, triggered_time as endTime insert into JoinStream; @info(name = 'findLowSalesProduct') from JoinStream join SalesAggregation within startTime, endTime per 'seconds' select endTime, emailToBeSent, categoryName, productName, sum(totalSales) as totalPastSales group by categoryName, productName having totalPastSales 100 order by totalPastSales desc limit 1 insert into PastSalesStream; @info(name = 'findSellersWhoSoldHighestQuantity') from PastSalesStream as PS join SalesAggregation as SA on PS.categoryName == SA.categoryName and PS.productName == SA.productName within time:timestampInMilliseconds(time:dateSub(time:currentDate(), 29, 'DAY', 'yyyy-MM-dd'), 'yyyy-MM-dd'), endTime per 'seconds' select emailToBeSent, PS.categoryName, PS.productName, totalPastSales as last10DaysSales, sellerName, sum(totalSales) as sellersSales group by categoryName, productName, sellerName order by sellersSales desc limit 5 insert into DailyDealsSellerCandidates; @info(name = 'emailFormatting') from DailyDealsSellerCandidates[emailToBeSent == true]#window.batch() select time:dateAdd(time:currentDate(), 1, 'DAY', 'yyyy-MM-dd') as dailyDealDate, str:concat(\"Daily Deals Seller Candidate for \", time:dateAdd(time:currentDate(), 1, 'DAY', 'yyyy-MM-dd')) as emailSubject, productName, categoryName, last10DaysSales, str:groupConcat(str:fillTemplate(\" tr td {{1}} /td td {{2}} /td /tr \", sellerName, sellersSales), \"\") as candidateInfo insert into EmailNotificationStream; Source view of the Siddhi apps. Siddhi App 1 Siddhi App 2 Below are the flow diagram of the above Siddhi Apps. Siddhi App 1 Siddhi App 2 Testing NOTE: In the provided Siddhi app, there are some environmental variables (EMAIL_USERNAME,EMAIL_PASSWORD, SENDER_EMAIL_ADDRESS and RECEIVER_EMAIL_ADDRESS) used which are required to be set to send an email alert based on the Siddhi queries defined. Moreover, we are using a MySQL database backend to persist aggregation data. Hence, make sure to set the environmental variables with the proper values in the system SENDER_EMAIL_ADDRESS: Email address of the account used to send email alerts. (eg: 'siddhi.gke.user@gmail.com') EMAIL_USERNAME: Username of the email account which used to send email alerts. (eg: 'siddhi.gke.user') EMAIL_PASSWORD: Password of the email account which used to send email alerts. (eg: 'siddhi123') RECEIVER_EMAIL_ADDRESS: Email address of the account used to receive email alerts. MYSQL_URL: MySQL url used to connect to the database. (eg: 'jdbc:mysql://localhost:3306/testdb?useSSL=false') MYSQL_USERNAME: Username of the MySQL database used to persist aggregated data. MYSQL_PASSWORD: Password of the MySQL database used to persist aggregated data. Setup MySQL Download and Install MySQL database as per the guidelines https://www.mysql.com/downloads/ Log in to the MySQL server and create a database called \u201ctestdb\u201d Download the MySQL client connector jar and add it to jars (if it is non OSGi) or bundles (if it is OSGi bundle) directory of Siddhi distribution Tryout When you run the Siddhi app in the editor, you will see below logs getting printed in the editor console. You could simply simulate some events directly into the stream and test your Siddhi app in the editor itself. You can also send data for the past 5 days using the siddhi mock data generator. Then, you can also simulate some events through HTTP to test the application. The following sections explain how you can test the Siddhi app via HTTP using cURL. Run Mock Data Generator In the provided Siddhi app, there is an HTTP source configured to receive sales data. For simplicity, you will be mocking this data through a generator . Please download the mock server jar and run that mock service by executing the following command. java -jar siddhi-mock-data-generator-2.0.0.jar Invoking the Siddhi App As per the Siddhi app that you wrote in the 'Implementation' section, each day at 11pm an email will be generated with the candidates for the Daily Deal tomorrow. Furthermore, this can be generated by sending a POST request to http://localhost:8080/dailyDealsAlert . curl -X POST \\ http://localhost:8080/dailyDealsAlert \\ -k \\ -H 'Content-Type: application/json' \\ -d '{ \"emailToBeSent\": true }' If you invoke the above cURL request email alert will be triggered. You can also observe the logs along with the details sent in the email. DailyDealsCandidatesAlert : DailyDealsSellerCandidates : [ {\"event\":{\"emailToBeSent\":true,\"categoryName\":\"Accessories\",\"productName\":\"Earring\",\"last10DaysSales\":80,\"sellerName\":\"Malfoy\",\"sellersSales\":20}}, {\"event\":{\"emailToBeSent\":true,\"categoryName\":\"Accessories\",\"productName\":\"Earring\",\"last10DaysSales\":80,\"sellerName\":\"George\",\"sellersSales\":20}}, {\"event\":{\"emailToBeSent\":true,\"categoryName\":\"Accessories\",\"productName\":\"Earring\",\"last10DaysSales\":80,\"sellerName\":\"Harry\",\"sellersSales\":20}}, {\"event\":{\"emailToBeSent\":true,\"categoryName\":\"Accessories\",\"productName\":\"Earring\",\"last10DaysSales\":80,\"sellerName\":\"Molly\",\"sellersSales\":10}}, {\"event\":{\"emailToBeSent\":true,\"categoryName\":\"Accessories\",\"productName\":\"Earring\",\"last10DaysSales\":80,\"sellerName\":\"Fred\",\"sellersSales\":10}} ] Note: The configurations provided in the email sink along with the environment properties will work for Gmail, but if you use other mail servers, please make sure to change the config values accordingly. Deployment Once you are done with the development, export the Siddhi app that you have developed with 'File' - 'Export File' option. You can deploy the Siddhi app using any of the methods listed below. Deploy on VM/ Bare Metal Prerequisites First, please make sure that necessary prerequisites are met as given the Testing section . MySQL is required to try out the use case. Then, as given in Setup MySQL section. Download the MySQL database and install it. Then create a database called \u201ctestdb\u201d in the MySQL database. Environmental variables related to Email and MySQL needs to be exported. Siddhi Runner Configuration Download the latest Siddhi Runner distribution . Unzip the siddhi-runner-x.x.x.zip . Configure the necessary environmental variables. Refer Testing Start Siddhi app with the runner config by executing the following commands from the distribution directory. Linux/Mac : ./bin/runner.sh -Dapps= siddhi-files-directory Windows : bin\\runner.bat -Dapps= siddhi-files-directory If pointing to different DB, run the mock data generator to add data for past 5 days sales. Download the mock data generator . Execute the below command to run the mock data generator. java -jar siddhi-mock-data-generator-2.0.0.jar Invoke the dailyDealsAlert service with the following cURL request. You can set emailToBeSent as false to not send an email but only to observe the logs. curl -X POST \\ http://localhost:8080/dailyDealsAlert \\ -k \\ -H 'Content-Type: application/json' \\ -d '{ \"emailToBeSent\": true }' You can see the output log in the console. Here, you will be able to see the alert log printed as shown below. At the same time, you will also receive the email alert if emailToBeSent is true. Deploy on Docker Prerequisites MySQL is an external dependency for this use case. Hence, you could use the corresponding docker artifacts to test the requirement. First, you can create a docker network for the deployment as shown below docker network create siddhi-tier --driver bridge Then, you can get the MySQL docker image from here and run it with below command. We are going to use mysql version 5.7.27. Start the MySQL docker images with below command, docker run -d --name mysql-server --network siddhi-tier -e MYSQL_DATABASE=testdb -e MYSQL_ROOT_PASSWORD=root mysql:5.7.27 Info Check if the image is started correctly by using docker ps . The above command will start mysql server with credentials root:root and create a database testdb Now, you have configured necessary prerequisites that required to run the use case. Siddhi Docker Configuration Siddhi docker artifacts can be exported from the Editor UI as follows, Start the Siddhi tooling runtime and go to the editor UI in http://localhost:9390/editor Follow below steps to start the Siddhi tooling runtime. * Extract the downloaded zip and navigate to /bin. (TOOLING_HOME refers to the extracted folder) * Issue the following command in the command prompt (Windows) / terminal (Linux/Mac) For Windows: tooling.bat For Linux/Mac: ./tooling.sh Go to Export - For Docker Option to export and push the docker image. Step 1: Select both siddhi apps Step 2: No need to template siddhi apps as it is already a template Step 3: Default configs can be used Step 4: Fill the template variables Setting MYSQL_URL Since we are using MySQL docker container MySQL URL should be jdbc:mysql:// mysql docker container name :3306/ created DB ?useSSL=false i.e jdbc:mysql://mysql-server:3306/testdb?useSSL=false by default Step 5: Select MySQL connector to use Step 6: Push to docker repository Then, you can run the Siddhi docker image that you created with necessary external dependencies to work with MySQL. docker run --network siddhi-tier -p 8080:8080 -it docker-image-name Info Here port 8080 is bound with localhost:8080 to run the mock-data generator You can use the sample mock data generator to add data for past 5 days sales. Download the mock data generator . Execute the below command to run the mock data generator. java -jar siddhi-mock-data-generator-2.0.0.jar Invoke the dailyDealsAlert service with the following cURL request. You can set emailToBeSent as false to not send an email but only to observe the logs. curl -X POST \\ http://localhost:8080/dailyDealsAlert \\ -k \\ -H 'Content-Type: application/json' \\ -d '{ \"emailToBeSent\": true }' You can see the output log in the console. Here, you will be able to see the alert log printed as shown below. Deploy on Kubernetes Prerequisites MySQL is an external dependency for this use case. Hence, you could use the corresponding docker artifacts to test the requirement. It is advisable to create a namespace in Kubernetes to follow below steps. kubectl create ns agg-guide There are some prerequisites that you should meet to tryout below SiddhiProcess. Such as configure MySQL database in Kubernetes. First, configure the MySQL server within the created namespace as in Step 1. You can use the official helm chart provided for MySQL. First, install the MySQL helm chart as shown below, helm install --name mysql-server --namespace=agg-guide --set mysqlRootPassword=root,mysqlUser=root,mysqlDatabase=testdb stable/mysql Here, you can define the root password to connect to the MYSQL database and also define the database name. BTW, make sure to do helm init --tiller-namespace=agg-guide if it is not done yet. Verify pods are running with kubectl get pods --namespace=agg-guide Then, you can set a port forwarding to the MySQL service which allows you to connect from the Host. kubectl port-forward svc/mysql-server 13300:3306 --namespace=agg-guide Then, you can login to the MySQL server from your host machine as shown below. Afterwards, you can install Siddhi Operator To install the Siddhi Kubernetes operator run the following commands. kubectl apply -f https://github.com/siddhi-io/siddhi-operator/releases/download/v0.2.1/00-prereqs.yaml --namespace=agg-guide kubectl apply -f https://github.com/siddhi-io/siddhi-operator/releases/download/v0.2.1/01-siddhi-operator.yaml --namespace=agg-guide You can verify the installation by making sure the following deployments are running in your Kubernetes cluster. Enable ingress Mandatory Commands for all clusters kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/static/mandatory.yaml Docker for Mac/Docker for Windows kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/static/provider/cloud-generic.yaml Minikube minikube addons enable ingress GKE kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/static/provider/cloud-generic.yaml Configure Kubernetes cluster IP as \u201csiddhi\u201d hostname in your /etc/hosts file. Minikube: add minikube IP to the \u201c/etc/hosts\u201d file as host \u201csiddhi\u201d, Run \u201cminikube ip\u201d command in terminal to get the minikube IP. Docker for Mac: use 0.0.0.0 to the /etc/hosts file as host \u201csiddhi\u201d. Docker for Windows: use IP that resolves to host.docker.internal in the /etc/hosts file as host \u201csiddhi\u201d. GKE: Obtain the external IP (EXTERNAL-IP) of the Ingress resources by listing down the Kubernetes Ingresses. Siddhi Kubernetes configuration To deploy the above created Siddhi app, we have to create custom resource object yaml file (with the kind as SiddhiProcess). Kubernetes CRD can be exported from the Siddhi tooling runtime with the editor UI in http://localhost:9390/editor Select the Export - For Kubernetes Option Steps 1- 5 is the same for both artifacts. In Step 6, push the docker image with different name to Docker deployment. Info This docker image will be different to the one pushed in Docker deployment, since only base image with additional bundles will be push to the docker in Kubernetes deployment. Info Ensure that the pushed docker image is public Step 7 : Let's create non-distributed and stateless deployment Extract siddhi-kubernetes.zip Now, let\u2019s create the above resource in the Kubernetes cluster with below command. kubectl --namespace=agg-guide apply -f absolute-path /siddhi-kubernetes/siddhi-process.yaml Once, siddhi apps are successfully deployed. You can verify its health with below Kubernetes commands You can use the sample mock data generator to add data for past 5 days sales. Download the mock data generator . Execute the below command to run the mock data generator. java -jar siddhi-mock-data-generator-2.0.0.jar siddhi/siddhi-aggregation-guide-0/8080 Hostname of the HTTP Source Here the hostname will include deployment name along with the port with syntax, IP / DEPLOYMENT_NAME / PORT . The above sample is for artifacts generated with process name, siddhi-aggregation-guide in Step 6 of the Export Dialog. Invoke the dailyDealsAlert service with the following cURL request. You can set emailToBeSent as false to not send an email but only to observe the logs. curl -X POST \\ http://siddhi/siddhi-aggregation-guide-1/8080/dailyDealsAlert \\ -k \\ -H 'Content-Type: application/json' \\ -d '{ \"emailToBeSent\": true }' Hostname of the HTTP Source Here the hostname will include deployment name along with the port with syntax, IP / DEPLOYMENT_NAME / PORT . The above sample is for artifacts generated with process name, siddhi-aggregation-guide in Step 6 of the Export Dialog. You can see the output log in the console. Here, you will be able to see the alert log printed as shown below. Refer here to get more details about running Siddhi on Kubernetes.","title":"Long-running time based Aggregations"},{"location":"docs/guides/long-term-aggregation/guide/#long-running-time-based-aggregations","text":"In this guide, you will understand one of the common requirements of Analytics which is aggregating data.","title":"Long-running time based Aggregations"},{"location":"docs/guides/long-term-aggregation/guide/#scenario-generating-alerts-based-on-long-running-summarization","text":"Aggregation is a mandatory need in any system as it allows users to spot trends and anomalies easily which can lead to actions that will benefit an organization or the business. Aggregated data can also be processed easily to get the information needed for a business requirement decision. The following sections are available in this guide. What you'll build Prerequisites Implementation Testing Deployment Output","title":"Scenario - Generating alerts based on long-running summarization"},{"location":"docs/guides/long-term-aggregation/guide/#what-youll-build","text":"Let's consider a real world use case to implement the aggregation requirement. This will help you to understand Siddhi Stream Processing construct, Named Aggregations. Let\u2019s jump into the use case directly. Let's assume you are a data analyst in the 'X' online shopping mall. 'X' shopping mall daily offers deals to customers to boost their sales. Each day past sales is analysed such that recommendation can be given on what items to be put up for Daily deals and more importantly out of all sellers, whose items to be used in Daily Deals. Let's take a simple logic to select Daily Deals, you will analyse the last 10 days of sales and get the products which has a lower amount of sales, lets say 100 units. For these products we will select sellers who has the highest sales in the past month and generate an email with top 5 candidates whose products can be offered in the Daily Deal. Now, let\u2019s understand how this could be implemented in Siddhi engine. For the aggregation use case we will be using a specific function of Siddhi engine, called Named Aggregations.","title":"What you'll build"},{"location":"docs/guides/long-term-aggregation/guide/#prerequisites","text":"Below are the prerequisites that should be considered to implement the above use case.","title":"Prerequisites"},{"location":"docs/guides/long-term-aggregation/guide/#mandatory-requirements","text":"Siddhi tooling VM/Local distribution One of the Siddhi runner distributions VM/Local Runtime Docker Image K8S Operator (commands are given in deployment section) Java 8 or higher","title":"Mandatory Requirements"},{"location":"docs/guides/long-term-aggregation/guide/#requirements-needed-to-deploy-siddhi-in-dockerkubernetes","text":"Docker Minikube or Google Kubernetes Engine(GKE) Cluster or Docker for Mac","title":"Requirements needed to deploy Siddhi in Docker/Kubernetes"},{"location":"docs/guides/long-term-aggregation/guide/#implementation","text":"When a user checkouts out a sales cart in 'X' online shopping malls website, the website will send the information to Siddhi runtime through HTTP transport. Siddhi runtime will aggregate the sales data in multiple granularities from seconds to years, grouped by category, product code and seller. Daily at 11.50 pm an alert will be generated to include, Daily Deals product and seller recommendations.","title":"Implementation"},{"location":"docs/guides/long-term-aggregation/guide/#implement-streaming-queries","text":"Start the Siddhi tooling runtime and go to the editor UI in http://localhost:9390/editor Follow below steps to start the Siddhi tooling runtime, * Extract the downloaded zip and navigate to /bin. (TOOLING_HOME refers to the extracted folder) * Issue the following command in the command prompt (Windows) / terminal (Linux/Mac) For Windows: tooling.bat For Linux/Mac: ./tooling.sh Select File - New option, then you could either use the source view or design view to write/build the Siddhi Application. You can find the Siddhi Applications below, that implements the requirements mentioned above. Let\u2019s write (develop) the Siddhi Applications, as given below. Once the Siddhi apps are created, you can use the Event Simulator option in the editor to simulate events to streams and perform developer testing. Siddhi App 1 : Responsible for Aggregating Data @App:name(\"SalesSummarization\") @App:description('X online shopping mall - Summarization for sales by category, product and seller') @Source(type = 'http', receiver.url = 'http://0.0.0.0:8080/sales', basic.auth.enabled = 'false', @map(type = 'json')) define stream SalesStream(timestamp long, categoryName string, productName string, sellerName string, quantity int); --Long term Summarization @store(type = 'rdbms' , jdbc.url = '${MYSQL_URL}', username = '${MYSQL_USERNAME}', password = '${MYSQL_PASSWORD}', jdbc.driver.name = 'com.mysql.jdbc.Driver', pool.properties = \"maximumPoolSize:1\") @purge(enable = 'false') define aggregation SalesAggregation from SalesStream select categoryName, productName, sellerName, sum(quantity) as totalSales group by categoryName, productName, sellerName aggregate by timestamp every sec...year; Siddhi App 2: Responsible for Sending alerts @App:name(\"DailyDealsCandidatesAlert\") @App:description(\"Alerts regarding daily deals candidates - 1 product with max 5 sellers who sold the most in past 30 days\") define trigger DailyTrigger at '0 00 23 ? * MON-FRI'; @Source(type = 'http', receiver.url = 'http://0.0.0.0:8080/dailyDealsAlert', basic.auth.enabled = 'false', @map(type = 'json')) define stream DailyDealsTriggerStream(emailToBeSent bool); define stream SalesStream(timestamp long, categoryName string, productName string, sellerName string, quantity int); @sink(type = 'email', address = \"${SENDER_EMAIL_ADDRESS}\", username = \"${EMAIL_USERNAME}\", password = \"${EMAIL_PASSWORD}\", subject = \"{{emailSubject}}\", to = \"${RECEIVER_EMAIL_ADDRESS}\", host = \"smtp.gmail.com\", port = \"465\", content.type = 'text/html', ssl.enable = \"true\", auth = \"true\", @map(type = 'text', @payload(\"\"\" Hi, br/ br/ Please find the seller candidates for Daily Deal {{dailyDealDate}} below, br/ br/ Product strong {{productName}} /strong -{{categoryName}} sold {{last10DaysSales}} units for the last 10 days br/ br/ table tr th Seller Name /th th Total Sales for the last Month /th /tr {{candidateInfo}} /table br/ br/ Thanks, br/ Analytics Team\"\"\"))) define stream EmailNotificationStream(dailyDealDate string, emailSubject string, productName string, categoryName string, last10DaysSales long, candidateInfo string); @sink(type = 'log', @map(type = 'json')) define stream DailyDealsSellerCandidates (emailToBeSent bool, categoryName string, productName string, last10DaysSales long, sellerName string, sellersSales long); --Long term Summarization @store(type = 'rdbms' , jdbc.url = '${MYSQL_URL}', username = '${MYSQL_USERNAME}', password = '${MYSQL_PASSWORD}', jdbc.driver.name = 'com.mysql.jdbc.Driver' , pool.properties = \"maximumPoolSize:1\") @purge(enable = 'false') define aggregation SalesAggregation from SalesStream select categoryName, productName, sellerName, sum(quantity) as totalSales group by categoryName, productName, sellerName aggregate by timestamp every sec...year; @info(name = 'calculateStartEndTimeForHTTPTrigger') from DailyDealsTriggerStream select emailToBeSent, time:timestampInMilliseconds(time:dateSub(time:currentDate(), 9, 'DAY', 'yyyy-MM-dd'), 'yyyy-MM-dd') as startTime, time:timestampInMilliseconds() as endTime insert into JoinStream; @info(name = 'calculateStartEndTimeForDailyTrigger') from DailyTrigger select true as emailToBeSent, time:timestampInMilliseconds(time:dateSub(time:currentDate(), 9, 'DAY', 'yyyy-MM-dd'), 'yyyy-MM-dd') as startTime, triggered_time as endTime insert into JoinStream; @info(name = 'findLowSalesProduct') from JoinStream join SalesAggregation within startTime, endTime per 'seconds' select endTime, emailToBeSent, categoryName, productName, sum(totalSales) as totalPastSales group by categoryName, productName having totalPastSales 100 order by totalPastSales desc limit 1 insert into PastSalesStream; @info(name = 'findSellersWhoSoldHighestQuantity') from PastSalesStream as PS join SalesAggregation as SA on PS.categoryName == SA.categoryName and PS.productName == SA.productName within time:timestampInMilliseconds(time:dateSub(time:currentDate(), 29, 'DAY', 'yyyy-MM-dd'), 'yyyy-MM-dd'), endTime per 'seconds' select emailToBeSent, PS.categoryName, PS.productName, totalPastSales as last10DaysSales, sellerName, sum(totalSales) as sellersSales group by categoryName, productName, sellerName order by sellersSales desc limit 5 insert into DailyDealsSellerCandidates; @info(name = 'emailFormatting') from DailyDealsSellerCandidates[emailToBeSent == true]#window.batch() select time:dateAdd(time:currentDate(), 1, 'DAY', 'yyyy-MM-dd') as dailyDealDate, str:concat(\"Daily Deals Seller Candidate for \", time:dateAdd(time:currentDate(), 1, 'DAY', 'yyyy-MM-dd')) as emailSubject, productName, categoryName, last10DaysSales, str:groupConcat(str:fillTemplate(\" tr td {{1}} /td td {{2}} /td /tr \", sellerName, sellersSales), \"\") as candidateInfo insert into EmailNotificationStream; Source view of the Siddhi apps. Siddhi App 1 Siddhi App 2 Below are the flow diagram of the above Siddhi Apps. Siddhi App 1 Siddhi App 2","title":"Implement Streaming Queries"},{"location":"docs/guides/long-term-aggregation/guide/#testing","text":"NOTE: In the provided Siddhi app, there are some environmental variables (EMAIL_USERNAME,EMAIL_PASSWORD, SENDER_EMAIL_ADDRESS and RECEIVER_EMAIL_ADDRESS) used which are required to be set to send an email alert based on the Siddhi queries defined. Moreover, we are using a MySQL database backend to persist aggregation data. Hence, make sure to set the environmental variables with the proper values in the system SENDER_EMAIL_ADDRESS: Email address of the account used to send email alerts. (eg: 'siddhi.gke.user@gmail.com') EMAIL_USERNAME: Username of the email account which used to send email alerts. (eg: 'siddhi.gke.user') EMAIL_PASSWORD: Password of the email account which used to send email alerts. (eg: 'siddhi123') RECEIVER_EMAIL_ADDRESS: Email address of the account used to receive email alerts. MYSQL_URL: MySQL url used to connect to the database. (eg: 'jdbc:mysql://localhost:3306/testdb?useSSL=false') MYSQL_USERNAME: Username of the MySQL database used to persist aggregated data. MYSQL_PASSWORD: Password of the MySQL database used to persist aggregated data.","title":"Testing"},{"location":"docs/guides/long-term-aggregation/guide/#setup-mysql","text":"Download and Install MySQL database as per the guidelines https://www.mysql.com/downloads/ Log in to the MySQL server and create a database called \u201ctestdb\u201d Download the MySQL client connector jar and add it to jars (if it is non OSGi) or bundles (if it is OSGi bundle) directory of Siddhi distribution","title":"Setup MySQL"},{"location":"docs/guides/long-term-aggregation/guide/#tryout","text":"When you run the Siddhi app in the editor, you will see below logs getting printed in the editor console. You could simply simulate some events directly into the stream and test your Siddhi app in the editor itself. You can also send data for the past 5 days using the siddhi mock data generator. Then, you can also simulate some events through HTTP to test the application. The following sections explain how you can test the Siddhi app via HTTP using cURL.","title":"Tryout"},{"location":"docs/guides/long-term-aggregation/guide/#run-mock-data-generator","text":"In the provided Siddhi app, there is an HTTP source configured to receive sales data. For simplicity, you will be mocking this data through a generator . Please download the mock server jar and run that mock service by executing the following command. java -jar siddhi-mock-data-generator-2.0.0.jar","title":"Run Mock Data Generator"},{"location":"docs/guides/long-term-aggregation/guide/#invoking-the-siddhi-app","text":"As per the Siddhi app that you wrote in the 'Implementation' section, each day at 11pm an email will be generated with the candidates for the Daily Deal tomorrow. Furthermore, this can be generated by sending a POST request to http://localhost:8080/dailyDealsAlert . curl -X POST \\ http://localhost:8080/dailyDealsAlert \\ -k \\ -H 'Content-Type: application/json' \\ -d '{ \"emailToBeSent\": true }' If you invoke the above cURL request email alert will be triggered. You can also observe the logs along with the details sent in the email. DailyDealsCandidatesAlert : DailyDealsSellerCandidates : [ {\"event\":{\"emailToBeSent\":true,\"categoryName\":\"Accessories\",\"productName\":\"Earring\",\"last10DaysSales\":80,\"sellerName\":\"Malfoy\",\"sellersSales\":20}}, {\"event\":{\"emailToBeSent\":true,\"categoryName\":\"Accessories\",\"productName\":\"Earring\",\"last10DaysSales\":80,\"sellerName\":\"George\",\"sellersSales\":20}}, {\"event\":{\"emailToBeSent\":true,\"categoryName\":\"Accessories\",\"productName\":\"Earring\",\"last10DaysSales\":80,\"sellerName\":\"Harry\",\"sellersSales\":20}}, {\"event\":{\"emailToBeSent\":true,\"categoryName\":\"Accessories\",\"productName\":\"Earring\",\"last10DaysSales\":80,\"sellerName\":\"Molly\",\"sellersSales\":10}}, {\"event\":{\"emailToBeSent\":true,\"categoryName\":\"Accessories\",\"productName\":\"Earring\",\"last10DaysSales\":80,\"sellerName\":\"Fred\",\"sellersSales\":10}} ] Note: The configurations provided in the email sink along with the environment properties will work for Gmail, but if you use other mail servers, please make sure to change the config values accordingly.","title":"Invoking the Siddhi App"},{"location":"docs/guides/long-term-aggregation/guide/#deployment","text":"Once you are done with the development, export the Siddhi app that you have developed with 'File' - 'Export File' option. You can deploy the Siddhi app using any of the methods listed below.","title":"Deployment"},{"location":"docs/guides/long-term-aggregation/guide/#deploy-on-vm-bare-metal","text":"","title":"Deploy on VM/ Bare Metal"},{"location":"docs/guides/long-term-aggregation/guide/#prerequisites_1","text":"First, please make sure that necessary prerequisites are met as given the Testing section . MySQL is required to try out the use case. Then, as given in Setup MySQL section. Download the MySQL database and install it. Then create a database called \u201ctestdb\u201d in the MySQL database. Environmental variables related to Email and MySQL needs to be exported.","title":"Prerequisites"},{"location":"docs/guides/long-term-aggregation/guide/#siddhi-runner-configuration","text":"Download the latest Siddhi Runner distribution . Unzip the siddhi-runner-x.x.x.zip . Configure the necessary environmental variables. Refer Testing Start Siddhi app with the runner config by executing the following commands from the distribution directory. Linux/Mac : ./bin/runner.sh -Dapps= siddhi-files-directory Windows : bin\\runner.bat -Dapps= siddhi-files-directory If pointing to different DB, run the mock data generator to add data for past 5 days sales. Download the mock data generator . Execute the below command to run the mock data generator. java -jar siddhi-mock-data-generator-2.0.0.jar Invoke the dailyDealsAlert service with the following cURL request. You can set emailToBeSent as false to not send an email but only to observe the logs. curl -X POST \\ http://localhost:8080/dailyDealsAlert \\ -k \\ -H 'Content-Type: application/json' \\ -d '{ \"emailToBeSent\": true }' You can see the output log in the console. Here, you will be able to see the alert log printed as shown below. At the same time, you will also receive the email alert if emailToBeSent is true.","title":"Siddhi Runner Configuration"},{"location":"docs/guides/long-term-aggregation/guide/#deploy-on-docker","text":"","title":"Deploy on Docker"},{"location":"docs/guides/long-term-aggregation/guide/#prerequisites_2","text":"MySQL is an external dependency for this use case. Hence, you could use the corresponding docker artifacts to test the requirement. First, you can create a docker network for the deployment as shown below docker network create siddhi-tier --driver bridge Then, you can get the MySQL docker image from here and run it with below command. We are going to use mysql version 5.7.27. Start the MySQL docker images with below command, docker run -d --name mysql-server --network siddhi-tier -e MYSQL_DATABASE=testdb -e MYSQL_ROOT_PASSWORD=root mysql:5.7.27 Info Check if the image is started correctly by using docker ps . The above command will start mysql server with credentials root:root and create a database testdb Now, you have configured necessary prerequisites that required to run the use case.","title":"Prerequisites"},{"location":"docs/guides/long-term-aggregation/guide/#siddhi-docker-configuration","text":"Siddhi docker artifacts can be exported from the Editor UI as follows, Start the Siddhi tooling runtime and go to the editor UI in http://localhost:9390/editor Follow below steps to start the Siddhi tooling runtime. * Extract the downloaded zip and navigate to /bin. (TOOLING_HOME refers to the extracted folder) * Issue the following command in the command prompt (Windows) / terminal (Linux/Mac) For Windows: tooling.bat For Linux/Mac: ./tooling.sh Go to Export - For Docker Option to export and push the docker image. Step 1: Select both siddhi apps Step 2: No need to template siddhi apps as it is already a template Step 3: Default configs can be used Step 4: Fill the template variables Setting MYSQL_URL Since we are using MySQL docker container MySQL URL should be jdbc:mysql:// mysql docker container name :3306/ created DB ?useSSL=false i.e jdbc:mysql://mysql-server:3306/testdb?useSSL=false by default Step 5: Select MySQL connector to use Step 6: Push to docker repository Then, you can run the Siddhi docker image that you created with necessary external dependencies to work with MySQL. docker run --network siddhi-tier -p 8080:8080 -it docker-image-name Info Here port 8080 is bound with localhost:8080 to run the mock-data generator You can use the sample mock data generator to add data for past 5 days sales. Download the mock data generator . Execute the below command to run the mock data generator. java -jar siddhi-mock-data-generator-2.0.0.jar Invoke the dailyDealsAlert service with the following cURL request. You can set emailToBeSent as false to not send an email but only to observe the logs. curl -X POST \\ http://localhost:8080/dailyDealsAlert \\ -k \\ -H 'Content-Type: application/json' \\ -d '{ \"emailToBeSent\": true }' You can see the output log in the console. Here, you will be able to see the alert log printed as shown below.","title":"Siddhi Docker Configuration"},{"location":"docs/guides/long-term-aggregation/guide/#deploy-on-kubernetes","text":"","title":"Deploy on Kubernetes"},{"location":"docs/guides/long-term-aggregation/guide/#prerequisites_3","text":"MySQL is an external dependency for this use case. Hence, you could use the corresponding docker artifacts to test the requirement. It is advisable to create a namespace in Kubernetes to follow below steps. kubectl create ns agg-guide There are some prerequisites that you should meet to tryout below SiddhiProcess. Such as configure MySQL database in Kubernetes. First, configure the MySQL server within the created namespace as in Step 1. You can use the official helm chart provided for MySQL. First, install the MySQL helm chart as shown below, helm install --name mysql-server --namespace=agg-guide --set mysqlRootPassword=root,mysqlUser=root,mysqlDatabase=testdb stable/mysql Here, you can define the root password to connect to the MYSQL database and also define the database name. BTW, make sure to do helm init --tiller-namespace=agg-guide if it is not done yet. Verify pods are running with kubectl get pods --namespace=agg-guide Then, you can set a port forwarding to the MySQL service which allows you to connect from the Host. kubectl port-forward svc/mysql-server 13300:3306 --namespace=agg-guide Then, you can login to the MySQL server from your host machine as shown below. Afterwards, you can install Siddhi Operator To install the Siddhi Kubernetes operator run the following commands. kubectl apply -f https://github.com/siddhi-io/siddhi-operator/releases/download/v0.2.1/00-prereqs.yaml --namespace=agg-guide kubectl apply -f https://github.com/siddhi-io/siddhi-operator/releases/download/v0.2.1/01-siddhi-operator.yaml --namespace=agg-guide You can verify the installation by making sure the following deployments are running in your Kubernetes cluster. Enable ingress Mandatory Commands for all clusters kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/static/mandatory.yaml Docker for Mac/Docker for Windows kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/static/provider/cloud-generic.yaml Minikube minikube addons enable ingress GKE kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/static/provider/cloud-generic.yaml Configure Kubernetes cluster IP as \u201csiddhi\u201d hostname in your /etc/hosts file. Minikube: add minikube IP to the \u201c/etc/hosts\u201d file as host \u201csiddhi\u201d, Run \u201cminikube ip\u201d command in terminal to get the minikube IP. Docker for Mac: use 0.0.0.0 to the /etc/hosts file as host \u201csiddhi\u201d. Docker for Windows: use IP that resolves to host.docker.internal in the /etc/hosts file as host \u201csiddhi\u201d. GKE: Obtain the external IP (EXTERNAL-IP) of the Ingress resources by listing down the Kubernetes Ingresses.","title":"Prerequisites"},{"location":"docs/guides/long-term-aggregation/guide/#siddhi-kubernetes-configuration","text":"To deploy the above created Siddhi app, we have to create custom resource object yaml file (with the kind as SiddhiProcess). Kubernetes CRD can be exported from the Siddhi tooling runtime with the editor UI in http://localhost:9390/editor Select the Export - For Kubernetes Option Steps 1- 5 is the same for both artifacts. In Step 6, push the docker image with different name to Docker deployment. Info This docker image will be different to the one pushed in Docker deployment, since only base image with additional bundles will be push to the docker in Kubernetes deployment. Info Ensure that the pushed docker image is public Step 7 : Let's create non-distributed and stateless deployment Extract siddhi-kubernetes.zip Now, let\u2019s create the above resource in the Kubernetes cluster with below command. kubectl --namespace=agg-guide apply -f absolute-path /siddhi-kubernetes/siddhi-process.yaml Once, siddhi apps are successfully deployed. You can verify its health with below Kubernetes commands You can use the sample mock data generator to add data for past 5 days sales. Download the mock data generator . Execute the below command to run the mock data generator. java -jar siddhi-mock-data-generator-2.0.0.jar siddhi/siddhi-aggregation-guide-0/8080 Hostname of the HTTP Source Here the hostname will include deployment name along with the port with syntax, IP / DEPLOYMENT_NAME / PORT . The above sample is for artifacts generated with process name, siddhi-aggregation-guide in Step 6 of the Export Dialog. Invoke the dailyDealsAlert service with the following cURL request. You can set emailToBeSent as false to not send an email but only to observe the logs. curl -X POST \\ http://siddhi/siddhi-aggregation-guide-1/8080/dailyDealsAlert \\ -k \\ -H 'Content-Type: application/json' \\ -d '{ \"emailToBeSent\": true }' Hostname of the HTTP Source Here the hostname will include deployment name along with the port with syntax, IP / DEPLOYMENT_NAME / PORT . The above sample is for artifacts generated with process name, siddhi-aggregation-guide in Step 6 of the Export Dialog. You can see the output log in the console. Here, you will be able to see the alert log printed as shown below. Refer here to get more details about running Siddhi on Kubernetes.","title":"Siddhi Kubernetes configuration"},{"location":"docs/guides/patterns-and-trends/guide/","text":"Analyze Event Occurrence Patterns and Trends Over Time In this guide, we are going to discuss a unique and appealing feature of a complex event processing system which is Patterns and Trends . Patterns and Trends are highly utilized in various business domains for the day to day business activities and growth. To understand these capabilities, we are going to consider a Taxi service use case. Scenario - Optimize Rider Requests in a Taxi Service Company Taxi service is one of the emerging businesses in metro cities. There are a lot of Taxi service companies such as UBER, LYFT, OLA, GRAB, etc.. are in the market. Due to the number of competitors in the market passengers have the freedom to select their preferred Taxi service based on cost, waiting time, etc.. As a passenger, the main requirement is to find a Taxi within a short time (less waiting time). Then, it is important to understand the rider requests and effectively use the available drivers/riders. In this, identifying the trend of passenger request will help to get more passengers and increase the business overall. The following sections are available in this guide. What you'll build Prerequisites Implementation Testing Deployment Output What you'll build You will be implementing a scenario to identify the increasing trend of rider requests over time and direct required riders to that specific geographical area to increase the chance of getting more rides. You will be using Siddhi streaming engine and related stream processing capabilities to achieve the requirement. Let\u2019s jump into the use case directly. Let\u2019s consider a Taxi service company called myTaxi . myTaxi is one of the startup Taxi service companies in the city and they have launched very recently. As per the analysis, they have found that they are a lot of ride cancellations happened over the last few months because the waiting time for the taxi is high. Even Though, myTaxi has enough riders they are not around the expected area where there is a sudden peak for rider requests. Then, they have decided to integrate a Stream Processing system to analyze the patterns and trends in real-time and act accordingly. In this solution, passengers use the myTaxi mobile application to book Taxi and those events are received to the myTaxi request processing system, it sends those events to Siddhi Stream Processor through TCP endpoint. Siddhi process those incoming events to identify predefined trends and patterns. Once a specific trend/patterns are identified then Siddhi sends those required trend/pattern specific attributes for further processing. In this case, that information is stored in a database. Now, let\u2019s understand how this could be implemented in Siddhi engine. Prerequisites Below are the prerequisites that should be considered to implement the above use case. Mandatory Requirements Siddhi tooling VM/Local distribution One of the Siddhi runner distributions VM/Local Runtime Docker Image K8S Operator (commands are given in deployment section) MySQL Database Java 8 or higher Requirements needed to deploy Siddhi in Docker/Kubernetes Docker Minikube or Google Kubernetes Engine(GKE) Cluster or Docker for Mac Implementation Implement Streaming Queries Start the Siddhi tooling runtime and go to the editor UI in http://localhost:9390/editor Follow below steps to start the Siddhi tooling runtime. * Extract the downloaded zip and navigate to /bin. (TOOLING_HOME refers to the extracted folder) * Issue the following command in the command prompt (Windows) / terminal (Linux/Mac) For Windows: tooling.bat For Linux/Mac: ./tooling.sh Select File - New option, then you could either use the source view or design view to write/build the Siddhi Application. You can find the Siddhi Application bellow, that implements the requirements mentioned above. Let\u2019s write (develop) the Siddhi Application, as given below. Once the Siddhi app is created, you can use the Event Simulator option in the editor to simulate events to streams and perform developer testing. @App:name(\"Taxi-Rider-Requests-Processing-App\") @App:description(\"Siddhi application that processes Taxi Rider request events\") -- TCP source which accepts Taxi rider requests @source(type='tcp', context='taxiRideRequests', @map(type='binary')) define stream TaxiRideEventStream(id long, time string, passengerId string, passengerName string, pickUpAddress string, pickUpZone string, dropOutAddress string, routingDetails string, expectedFare double, status string, passengerGrade string, additionalNote string); -- For testing purposes, offer messages are logged in console. -- This could be further extended to send as sms to the premium users @sink(type='log', @map(type = 'text', @payload(\"\"\" Hi {{passengerName}} Unfortunately, you couldn't travel with us Today. We apologise for the high waiting time. As a token of apology please accept {{offerAmount}} USD off from your next ride. Truly, MyTaxi Team\"\"\"))) define stream InstantOfferAlertStream(passengerName string, pickUpZone string, offerAmount double); -- RDBMS event table which stores events related to the requirement of need more riders @store(type=\"rdbms\", jdbc.url=\"${MYSQL_DB_URL}\", username=\"${MYSQL_USERNAME}\", password=\"${MYSQL_PASSWORD}\" , jdbc.driver.name=\"com.mysql.jdbc.Driver\") define table NeedMoreRidersTable (systemTime string, zone string); -- Email sink which send email alerts to the manager of MyTaxi @sink(type = 'email', username = \"${EMAIL_USERNAME}\", address = \"${SENDER_EMAIL_ADDRESS}\", password = \"${EMAIL_PASSWORD}\", subject = \"[Need Immediate Attention] High Waiting Time\", to = \"${MANAGER_EMAIL_ADDRESS}\", host = \"smtp.gmail.com\", port = \"465\", ssl.enable = \"true\", auth = \"true\", @map(type = 'text', @payload(\"\"\" Hi, There is an increasing trend of ride cancellations in the {{zone}} area due to high waiting time. Increasing trend is detected at {{systemTime}}. Please take immediate action to sort this out. Thanks...\"\"\"))) define stream AttentionRequiredCancellationStream (systemTime string, zone string, lastNoOfCancellations long); @info(name='Ride-cancellation-identifier') from every e1=TaxiRideEventStream[status == 'Assigned'] - e2=TaxiRideEventStream[e1.passengerId == passengerId and status == 'Cancelled' and additionalNote == 'WT is High'] within 30 seconds select e2.passengerName, e1.passengerGrade, e1.pickUpZone, e1.expectedFare insert into TaxiRideCancelStream; @info(name='Offer-for-premium-users') from TaxiRideCancelStream[passengerGrade == 'Premium'] select passengerName, pickUpZone, math:floor(expectedFare) / 2.0 as offerAmount insert into InstantOfferAlertStream; @info(name='Frequently-ride-cancellation-identifier') from TaxiRideCancelStream#window.timeBatch(1 min) select pickUpZone, count() as totalCancellations, time:currentTimestamp() as systemTime group by pickUpZone having totalCancellations 3 insert into NeedMoreRidersStream; @info(name='Dump-needMoreRider-events') from NeedMoreRidersStream select systemTime, pickUpZone as zone insert into NeedMoreRidersTable; @info(name='Ride-cancellation-increasing-trend-identifer') partition with (pickUpZone of NeedMoreRidersStream) begin from e1 = NeedMoreRidersStream, e2 = NeedMoreRidersStream[totalCancellations e1.totalCancellations], e3 = NeedMoreRidersStream[totalCancellations e2.totalCancellations] select e3.systemTime, e1.pickUpZone as zone, e3.totalCancellations as lastNoOfCancellations insert into AttentionRequiredCancellationStream; end; Source view of the Siddhi app. Below is the flow diagram of the above Siddhi App. Testing NOTE: In the above provided Siddhi app, there are some environmental variables (MYSQL_DB_URL, MYSQL_USERNAME, and MYSQL_PASSWORD) are used. These values are required to be set to try out the scenario end to end. MYSQL related environmental variables are required to store the events of stream NeedMoreRidersStream . Environmental variables EMAIL_PASSWORD, EMAIL_USERNAME, SENDER_EMAIL_ADDRESS and MANAGER_EMAIL_ADDRESS are used to send an email alert when there is an increasing trend of cancellation on specific area. MYSQL_DB_URL: MySQL database jdbc url to persist failed events. (eg: 'jdbc:mysql://localhost:3306/MyTaxi') MYSQL_USERNAME: Username of the user account to connect MySQL database. (eg: 'root') MYSQL_PASSWORD: Password of the user account to connect MySQL database. (eg: 'root') EMAIL_USERNAME: Username of the email account which used to send email alerts. (eg: 'siddhi.gke.user') EMAIL_PASSWORD: Password of the email account which used to send email alerts. (eg: 'siddhi123') SENDER_EMAIL_ADDRESS: Email address of the account used to send email alerts. (eg: 'siddhi.gke.user@gmail.com') MANAGER_EMAIL_ADDRESS: Destination Email address where escalation mails are sent. (eg: 'manager@mytaxi.com') Setup MySQL Download and Install MySQL database as per the guidelines (https://www.mysql.com/downloads/) Log in to the MySQL server and create a database called \u201cMyTaxi\u201d Download the MySQL client connector jar and add it to jars (if it is non OSGI) or bundles (if it is OSGI bundle) directory of Siddhi distribution Tryout There are multiple options available to test the developed Siddhi App. As mentioned in the previous step you could simply simulate some events directly into the stream and test your queries. But, if you are willing to test the end to end flow (from an input source to sink) then you can start the Siddhi app in the editor itself. In this guide, we are going to run the Siddhi App in the editor itself. Once the server is started, you will see below logs get printed in the editor console. As written in the above Siddhi application, taxi ride requests are accepted by the TCP endpoint of the Siddhi Stream Processor; those events are pushed to a stream called taxiRideEventStream . The first query is written to identify the pattern of rider cancellation after rider request within 30 seconds due to high waiting time. If such a pattern is identified then system will the user grade and grant some offers for subsequent rides for premium users. Parallelly, Stream Processor keep tracking the number of ride cancellations for each minute and if it found a situation of more than 3 ride cancellation then the system will identify that area/zone and send that details to next processing system to take necessary action. In the above query, such events are pushed to a database table. There is another query which continuously listens for the total number of cancellations for each minute and looking for increasing trend of ride cancellations and notifies accordingly. In this situation, if the Streaming system sends an email alert to the manager of the MyTaxi for his/her further consideration. Invoking the Siddhi App To try out the above use case, you have to send a set of events in a certain order to match with the query conditions. There is a sample TCP publisher could publish events in such an order. Hence, you could use the sample publisher given in here . Then you can execute below command to run the TCP client. java -jar tcp-producer-1.0.0-jar-with-dependencies.jar It will take nearly 3 minutes to publish events which required to test all the flows in the given Siddhi app. When you are publishing events to Siddhi Stream processor, you could see the logs that get printed in Siddhi Stream processor side as well. There are related to instant offer alerts. Once, TCP publisher completes publishing events then you could check the email alert which is generated. Deployment Once you are done with the development, export the Siddhi app that you have developed with 'File' - 'Export File' option. You can deploy the Siddhi app using any of the methods listed below. Deploy on VM/ Bare Metal Prerequisites First, please make sure that necessary prerequisites are met as given the Testing section . MySQL is required to try out the use case. Then, as given in the Setup MySQL section. Download the MySQL database and install it. Then create a database called \u201cMyTaxi\u201d in the MySQL database. Siddhi Runtime Configuration Make sure to set the necessary environmental variables as given above. Note: In the above provided Siddhi app, there are some environmental variables (MYSQL_DB_URL, MYSQL_USERNAME, and MYSQL_PASSWORD) are used. These values are required to be set to try out the scenario end to end. MYSQL related environmental variables are required to store the events of stream NeedMoreRidersStream . Environmental variables EMAIL_PASSWORD, EMAIL_USERNAME, SENDER_EMAIL_ADDRESS and MANAGER_EMAIL_ADDRESS are used to send an email alert when there is an increasing trend of cancellation on specific area. Hence, make sure to set the environmental variables with the proper values in the system (make sure to follow necessary steps based on the underneath operating system). Download the latest Siddhi Runner distribution . Unzip the siddhi-runner-x.x.x.zip . You have to copy necessary Mysql client jar to Siddhi runner distribution to connect with MySQL database. Copy the MySQL client connector jar to jars (if it is non OSGI) or bundles (if it is OSGI bundle) directory of Siddhi runner. Please refer this for information. Start Siddhi app with the runner config by executing the following commands from the distribution directory. Linux/Mac : ./bin/runner.sh -Dapps= siddhi-file-path Windows : bin\\runner.bat -Dapps= siddhi-file-path Eg: If exported siddhi app in Siddhi home directory, ./bin/runner.sh -Dapps=Taxi-Rider-Requests-Processing-App.siddhi Once server is started, download the sample TCP event generator from here and execute below command. java -jar tcp-producer-1.0.0-jar-with-dependencies.jar tcp://localhost:9892/taxiRideRequests Above event publishes send binary events through TCP to the TCP endpoint defined in the Siddhi application. You can change the TCP endpoint url by passing them as java arguments. If not, sample client consider tcp://localhost:9892/taxiRideRequests as the TCP endpoint url. You can find the sample client source code in here In this situation, you can find the logs printed in the Siddhi runner console/log, events related to NeedMoreRidersStream are stored in the database table and escalation email is sent to the manager when there is an increasing trend found in the cancellations. Deploy on Docker Prerequisites MySQL is an external dependency for this use case. Hence, you could use the corresponding MySQL docker artifact to test the requirement. First, you can create a docker network for the deployment as shown below docker network create siddhi-tier --driver bridge Then, you can get the MySQL docker image from here and run it with below command. We are going to use mysql version 5.7.27. Start the MySQL docker images with below command, docker run --name mysql-server --network siddhi-tier -e MYSQL_ROOT_PASSWORD=root e1e1680ac726 e1e1680ac726 is the MySQL docker image id in this case Login to the MySQL docker instance and create a database called \u201cMyTaxi\u201d. Now, you have configured necessary prerequisites that required to run the use case. Siddhi Docker Configuration Since, MySQL client jar is required for the Siddhi runner; you have to create the docker image accordingly. Below is the sample Docker file created FROM siddhiio/siddhi-runner-base-alpine:5.1.0-alpha MAINTAINER Siddhi IO Docker Maintainers \"siddhi-dev@googlegroups.com\" ARG HOST_BUNDLES_DIR=./files/bundles ARG HOST_JARS_DIR=./files/jars ARG JARS=${RUNTIME_SERVER_HOME}/jars ARG BUNDLES=${RUNTIME_SERVER_HOME}/bundles # copy bundles jars to the siddhi-runner distribution COPY --chown=siddhi_user:siddhi_io ${HOST_JARS_DIR}/ ${JARS} # expose ports EXPOSE 9090 9443 9712 9612 7711 7611 7070 7443 RUN bash ${RUNTIME_SERVER_HOME}/bin/install-jars.sh STOPSIGNAL SIGINT ENTRYPOINT [\"/home/siddhi_user/siddhi-runner/bin/runner.sh\", \"--\"] Here, you have to create a folder called jars to add necessary external client dependencies to the docker image. You can refer the official Siddhi documentation reference for this purpose. Once, Dockerfile is created you can create the docker image with below command. docker build -t siddhi_mysql . Then, you can run the Siddhi docker image that you created with necessary external dependencies to work with MySQL. docker run --network siddhi-tier -it -p 9892:9892 -v /Users/mohan/siddhi-apps/:/siddhi-apps -e MYSQL_DB_URL=jdbc:mysql://mysql-server:3306/MyTaxi -e MYSQL_USERNAME=root -e MYSQL_PASSWORD=root -e EMAIL_USERNAME=siddhi.gke.user -e EMAIL_PASSWORD=siddhi123 -e SENDER_EMAIL_ADDRESS=siddhi.gke.user@gmail.com -e MANAGER_EMAIL_ADDRESS=mohan@wso2.com siddhi_mysql:latest -Dapps=/siddhi-apps/Taxi-Rider-Requests-Processing-App.siddhi Note: In the above provided Siddhi app, there are some environmental variables (MYSQL_DB_URL, MYSQL_USERNAME, and MYSQL_PASSWORD) are used. These values are required to be set to try out the scenario end to end. MYSQL related environmental variables are required to store the events of stream NeedMoreRidersStream . Environmental variables EMAIL_PASSWORD, EMAIL_USERNAME, SENDER_EMAIL_ADDRESS and MANAGER_EMAIL_ADDRESS are used to send an email alert when there is an increasing trend of cancellation on specific area. You can use the sample TCP event publisher to simulate required events. Use the below command to publish events to TCP endpoint. java -jar tcp-producer-1.0.0-jar-with-dependencies.jar tcp://localhost:9892/taxiRideRequests Then, you could see below logs get printed in the Siddhi runner console/log, events related to NeedMoreRidersStream are stored in the database table and escalation email is sent to the manager when there is an increasing trend found in the cancellations. Deploy on Kubernetes It is advisable to create a namespace in Kubernetes to follow below steps. kubectl create ns siddhi-mysql-test There is a prerequisite that you should meet to tryout below SiddhiProcess; configuring MySQL database server within the above created namespace. You can use the official helm chart provided for MySQL. First, install the MySQL helm chart as shown below, helm install --name mysql-db --namespace=siddhi-mysql-test --set mysqlRootPassword=root,mysqlDatabase=MyTaxi stable/mysql Here, you can define the root password to connect to the MYSQL database and also define the database name. BTW, make sure to do helm init if it is not done yet. Then, you can set a port forwarding to the MySQL service which allows you to connect from the Host. kubectl port-forward svc/mysql-db 13306:3306 --namespace=siddhi-mysql-test Then, you can login to the MySQL server from your host machine as shown below. Then, you can install Siddhi Operator To install the Siddhi Kubernetes operator run the following commands. kubectl apply -f https://github.com/siddhi-io/siddhi-operator/releases/download/v0.2.0-alpha/00-prereqs.yaml --namespace=siddhi-mysql-test kubectl apply -f https://github.com/siddhi-io/siddhi-operator/releases/download/v0.2.0-alpha/01-siddhi-operator.yaml --namespace=siddhi-mysql-test You can verify the installation by making sure the following deployments are running in your Kubernetes cluster. Siddhi applications can be deployed on Kubernetes using the Siddhi operator. To deploy the above created Siddhi app, we have to create custom resource object yaml file (with the kind as SiddhiProcess) as given below apiVersion: siddhi.io/v1alpha2 kind: SiddhiProcess metadata: name: taxi-rider-requests-processing-app spec: apps: - script: | @App:name(\"Taxi-Rider-Requests-Processing-App\") @App:description(\"Siddhi application that processes Taxi Rider request events\") -- TCP source which accepts Taxi rider requests @source(type='tcp', context='taxiRideRequests', @map(type='binary')) define stream TaxiRideEventStream(id long, time string, passengerId string, passengerName string, pickUpAddress string, pickUpZone string, dropOutAddress string, routingDetails string, expectedFare double, status string, passengerGrade string, additionalNote string); -- For testing purposes, offer messages are logged in console. -- This could be further extended to send as sms to the premium users @sink(type='log', @map(type = 'text', @payload(\"\"\" Hi {{passengerName}} Unfortunately, you couldn't travel with us Today. We apologise for the high waiting time. As a token of apology please accept {{offerAmount}} USD off from your next ride. Truly, MyTaxi Team\"\"\"))) define stream InstantOfferAlertStream(passengerName string, pickUpZone string, offerAmount double); -- RDBMS event table which stores events related to the requirement of need more riders @store(type=\"rdbms\", jdbc.url=\"${MYSQL_DB_URL}\", username=\"${MYSQL_USERNAME}\", password=\"${MYSQL_PASSWORD}\" , jdbc.driver.name=\"com.mysql.jdbc.Driver\") define table NeedMoreRidersTable (systemTime string, zone string); -- Email sink which send email alerts to the manager of MyTaxi @sink(type = 'email', username = \"${EMAIL_USERNAME}\", address = \"${SENDER_EMAIL_ADDRESS}\", password = \"${EMAIL_PASSWORD}\", subject = \"[Need Immediate Attention] High Waiting Time\", to = \"${MANAGER_EMAIL_ADDRESS}\", host = \"smtp.gmail.com\", port = \"465\", ssl.enable = \"true\", auth = \"true\", @map(type = 'text', @payload(\"\"\" Hi, There is an increasing trend of ride cancellations in the {{zone}} area due to high waiting time. Increasing trend is detected at {{systemTime}}. Please take immediate action to sort this out. Thanks...\"\"\"))) define stream AttentionRequiredCancellationStream (systemTime string, zone string, lastNoOfCancellations long); @info(name='Ride-cancellation-identifier') from every e1=TaxiRideEventStream[status == 'Assigned'] - e2=TaxiRideEventStream[e1.passengerId == passengerId and status == 'Cancelled' and additionalNote == 'WT is High'] within 30 seconds select e2.passengerName, e1.passengerGrade, e1.pickUpZone, e1.expectedFare insert into TaxiRideCancelStream; @info(name='Offer-for-premium-users') from TaxiRideCancelStream[passengerGrade == 'Premium'] select passengerName, pickUpZone, math:floor(expectedFare) / 2.0 as offerAmount insert into InstantOfferAlertStream; @info(name='Frequently-ride-cancellation-identifier') from TaxiRideCancelStream#window.timeBatch(1 min) select pickUpZone, count() as totalCancellations, time:currentTimestamp() as systemTime group by pickUpZone having totalCancellations 3 insert into NeedMoreRidersStream; @info(name='Dump-needMoreRider-events') from NeedMoreRidersStream select systemTime, pickUpZone as zone insert into NeedMoreRidersTable; @info(name='Ride-cancellation-increasing-trend-identifer') partition with (pickUpZone of NeedMoreRidersStream) begin from e1 = NeedMoreRidersStream, e2 = NeedMoreRidersStream[totalCancellations e1.totalCancellations], e3 = NeedMoreRidersStream[totalCancellations e2.totalCancellations] select e3.systemTime, e1.pickUpZone as zone, e3.totalCancellations as lastNoOfCancellations insert into AttentionRequiredCancellationStream; end; persistentVolumeClaim: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi storageClassName: hostpath volumeMode: Filesystem container: env: - name: MYSQL_DB_URL value: \"jdbc:mysql://mysql-db:3306/MyTaxi\" - name: MYSQL_USERNAME value: \"root\" - name: MYSQL_PASSWORD value: \"root\" - name: EMAIL_PASSWORD value: \"siddhi123\" - name: EMAIL_USERNAME value: \"siddhi.gke.user\" - name: SENDER_EMAIL_ADDRESS value: \"siddhi.gke.user@gmail.com\" - name: MANAGER_EMAIL_ADDRESS value: \"mohan@wso2.com\" image: \"mohanvive/siddhi_mysql:latest\" Note: In the above provided Siddhi app, there are some environmental variables (MYSQL_DB_URL, MYSQL_USERNAME, and MYSQL_PASSWORD) are used. These values are required to be set to try out the scenario end to end. MYSQL related environmental variables are required to store the events of stream NeedMoreRidersStream . Environmental variables EMAIL_PASSWORD, EMAIL_USERNAME, SENDER_EMAIL_ADDRESS and MANAGER_EMAIL_ADDRESS are used to send an email alert when there is an increasing trend of cancellation on specific area. Hence, make sure to add proper values for the environmental variables in the above yaml file (check the \u2018env\u2019 section of the yaml file). Here, you can use the docker image that created in the section since you need a docker images with required extensions and client jars to test it in Kubernetes. Now, let\u2019s create the above resource in the Kubernetes cluster with below command. kubectl --namespace=siddhi-mysql-test create -f absolute-yaml-file-path /taxi-rider-requests-processing-app.yaml Once, siddhi app is successfully deployed. You can verify its health with below Kubernetes commands You can find the alert logs in the siddhi runner log file. To see the Siddhi runner log file, you can invoke below command. kubectl get pods Then, find the pod name of the siddhi app deployed. Then invoke below command, kubectl logs siddhi-app-pod-name -f Then, you can set a port forwarding to the Siddhi TCP endpoint which allows you to connect from the Host with below command. kubectl port-forward svc/taxi-rider-requests-processing-app-0 9892:9892 --namespace=siddhi-mysql-test Then execute below command to send TCP events to Siddhi Stream Processor. java -jar tcp-producer-1.0.0-jar-with-dependencies.jar tcp://0.0.0.0:9892/taxiRideRequests Then, you could see below logs get printed in the Siddhi runner console/log, events related to NeedMoreRidersStream are stored in the database table and escalation email is sent to the manager when there is an increasing trend found in the cancellations. Refer here to get more details about running Siddhi on Kubernetes.","title":"Patterns & Trends Over Time"},{"location":"docs/guides/patterns-and-trends/guide/#analyze-event-occurrence-patterns-and-trends-over-time","text":"In this guide, we are going to discuss a unique and appealing feature of a complex event processing system which is Patterns and Trends . Patterns and Trends are highly utilized in various business domains for the day to day business activities and growth. To understand these capabilities, we are going to consider a Taxi service use case.","title":"Analyze Event Occurrence Patterns and Trends Over Time"},{"location":"docs/guides/patterns-and-trends/guide/#scenario-optimize-rider-requests-in-a-taxi-service-company","text":"Taxi service is one of the emerging businesses in metro cities. There are a lot of Taxi service companies such as UBER, LYFT, OLA, GRAB, etc.. are in the market. Due to the number of competitors in the market passengers have the freedom to select their preferred Taxi service based on cost, waiting time, etc.. As a passenger, the main requirement is to find a Taxi within a short time (less waiting time). Then, it is important to understand the rider requests and effectively use the available drivers/riders. In this, identifying the trend of passenger request will help to get more passengers and increase the business overall. The following sections are available in this guide. What you'll build Prerequisites Implementation Testing Deployment Output","title":"Scenario - Optimize Rider Requests in a Taxi Service Company"},{"location":"docs/guides/patterns-and-trends/guide/#what-youll-build","text":"You will be implementing a scenario to identify the increasing trend of rider requests over time and direct required riders to that specific geographical area to increase the chance of getting more rides. You will be using Siddhi streaming engine and related stream processing capabilities to achieve the requirement. Let\u2019s jump into the use case directly. Let\u2019s consider a Taxi service company called myTaxi . myTaxi is one of the startup Taxi service companies in the city and they have launched very recently. As per the analysis, they have found that they are a lot of ride cancellations happened over the last few months because the waiting time for the taxi is high. Even Though, myTaxi has enough riders they are not around the expected area where there is a sudden peak for rider requests. Then, they have decided to integrate a Stream Processing system to analyze the patterns and trends in real-time and act accordingly. In this solution, passengers use the myTaxi mobile application to book Taxi and those events are received to the myTaxi request processing system, it sends those events to Siddhi Stream Processor through TCP endpoint. Siddhi process those incoming events to identify predefined trends and patterns. Once a specific trend/patterns are identified then Siddhi sends those required trend/pattern specific attributes for further processing. In this case, that information is stored in a database. Now, let\u2019s understand how this could be implemented in Siddhi engine.","title":"What you'll build"},{"location":"docs/guides/patterns-and-trends/guide/#prerequisites","text":"Below are the prerequisites that should be considered to implement the above use case.","title":"Prerequisites"},{"location":"docs/guides/patterns-and-trends/guide/#mandatory-requirements","text":"Siddhi tooling VM/Local distribution One of the Siddhi runner distributions VM/Local Runtime Docker Image K8S Operator (commands are given in deployment section) MySQL Database Java 8 or higher","title":"Mandatory Requirements"},{"location":"docs/guides/patterns-and-trends/guide/#requirements-needed-to-deploy-siddhi-in-dockerkubernetes","text":"Docker Minikube or Google Kubernetes Engine(GKE) Cluster or Docker for Mac","title":"Requirements needed to deploy Siddhi in Docker/Kubernetes"},{"location":"docs/guides/patterns-and-trends/guide/#implementation","text":"","title":"Implementation"},{"location":"docs/guides/patterns-and-trends/guide/#implement-streaming-queries","text":"Start the Siddhi tooling runtime and go to the editor UI in http://localhost:9390/editor Follow below steps to start the Siddhi tooling runtime. * Extract the downloaded zip and navigate to /bin. (TOOLING_HOME refers to the extracted folder) * Issue the following command in the command prompt (Windows) / terminal (Linux/Mac) For Windows: tooling.bat For Linux/Mac: ./tooling.sh Select File - New option, then you could either use the source view or design view to write/build the Siddhi Application. You can find the Siddhi Application bellow, that implements the requirements mentioned above. Let\u2019s write (develop) the Siddhi Application, as given below. Once the Siddhi app is created, you can use the Event Simulator option in the editor to simulate events to streams and perform developer testing. @App:name(\"Taxi-Rider-Requests-Processing-App\") @App:description(\"Siddhi application that processes Taxi Rider request events\") -- TCP source which accepts Taxi rider requests @source(type='tcp', context='taxiRideRequests', @map(type='binary')) define stream TaxiRideEventStream(id long, time string, passengerId string, passengerName string, pickUpAddress string, pickUpZone string, dropOutAddress string, routingDetails string, expectedFare double, status string, passengerGrade string, additionalNote string); -- For testing purposes, offer messages are logged in console. -- This could be further extended to send as sms to the premium users @sink(type='log', @map(type = 'text', @payload(\"\"\" Hi {{passengerName}} Unfortunately, you couldn't travel with us Today. We apologise for the high waiting time. As a token of apology please accept {{offerAmount}} USD off from your next ride. Truly, MyTaxi Team\"\"\"))) define stream InstantOfferAlertStream(passengerName string, pickUpZone string, offerAmount double); -- RDBMS event table which stores events related to the requirement of need more riders @store(type=\"rdbms\", jdbc.url=\"${MYSQL_DB_URL}\", username=\"${MYSQL_USERNAME}\", password=\"${MYSQL_PASSWORD}\" , jdbc.driver.name=\"com.mysql.jdbc.Driver\") define table NeedMoreRidersTable (systemTime string, zone string); -- Email sink which send email alerts to the manager of MyTaxi @sink(type = 'email', username = \"${EMAIL_USERNAME}\", address = \"${SENDER_EMAIL_ADDRESS}\", password = \"${EMAIL_PASSWORD}\", subject = \"[Need Immediate Attention] High Waiting Time\", to = \"${MANAGER_EMAIL_ADDRESS}\", host = \"smtp.gmail.com\", port = \"465\", ssl.enable = \"true\", auth = \"true\", @map(type = 'text', @payload(\"\"\" Hi, There is an increasing trend of ride cancellations in the {{zone}} area due to high waiting time. Increasing trend is detected at {{systemTime}}. Please take immediate action to sort this out. Thanks...\"\"\"))) define stream AttentionRequiredCancellationStream (systemTime string, zone string, lastNoOfCancellations long); @info(name='Ride-cancellation-identifier') from every e1=TaxiRideEventStream[status == 'Assigned'] - e2=TaxiRideEventStream[e1.passengerId == passengerId and status == 'Cancelled' and additionalNote == 'WT is High'] within 30 seconds select e2.passengerName, e1.passengerGrade, e1.pickUpZone, e1.expectedFare insert into TaxiRideCancelStream; @info(name='Offer-for-premium-users') from TaxiRideCancelStream[passengerGrade == 'Premium'] select passengerName, pickUpZone, math:floor(expectedFare) / 2.0 as offerAmount insert into InstantOfferAlertStream; @info(name='Frequently-ride-cancellation-identifier') from TaxiRideCancelStream#window.timeBatch(1 min) select pickUpZone, count() as totalCancellations, time:currentTimestamp() as systemTime group by pickUpZone having totalCancellations 3 insert into NeedMoreRidersStream; @info(name='Dump-needMoreRider-events') from NeedMoreRidersStream select systemTime, pickUpZone as zone insert into NeedMoreRidersTable; @info(name='Ride-cancellation-increasing-trend-identifer') partition with (pickUpZone of NeedMoreRidersStream) begin from e1 = NeedMoreRidersStream, e2 = NeedMoreRidersStream[totalCancellations e1.totalCancellations], e3 = NeedMoreRidersStream[totalCancellations e2.totalCancellations] select e3.systemTime, e1.pickUpZone as zone, e3.totalCancellations as lastNoOfCancellations insert into AttentionRequiredCancellationStream; end; Source view of the Siddhi app. Below is the flow diagram of the above Siddhi App.","title":"Implement Streaming Queries"},{"location":"docs/guides/patterns-and-trends/guide/#testing","text":"NOTE: In the above provided Siddhi app, there are some environmental variables (MYSQL_DB_URL, MYSQL_USERNAME, and MYSQL_PASSWORD) are used. These values are required to be set to try out the scenario end to end. MYSQL related environmental variables are required to store the events of stream NeedMoreRidersStream . Environmental variables EMAIL_PASSWORD, EMAIL_USERNAME, SENDER_EMAIL_ADDRESS and MANAGER_EMAIL_ADDRESS are used to send an email alert when there is an increasing trend of cancellation on specific area. MYSQL_DB_URL: MySQL database jdbc url to persist failed events. (eg: 'jdbc:mysql://localhost:3306/MyTaxi') MYSQL_USERNAME: Username of the user account to connect MySQL database. (eg: 'root') MYSQL_PASSWORD: Password of the user account to connect MySQL database. (eg: 'root') EMAIL_USERNAME: Username of the email account which used to send email alerts. (eg: 'siddhi.gke.user') EMAIL_PASSWORD: Password of the email account which used to send email alerts. (eg: 'siddhi123') SENDER_EMAIL_ADDRESS: Email address of the account used to send email alerts. (eg: 'siddhi.gke.user@gmail.com') MANAGER_EMAIL_ADDRESS: Destination Email address where escalation mails are sent. (eg: 'manager@mytaxi.com')","title":"Testing"},{"location":"docs/guides/patterns-and-trends/guide/#setup-mysql","text":"Download and Install MySQL database as per the guidelines (https://www.mysql.com/downloads/) Log in to the MySQL server and create a database called \u201cMyTaxi\u201d Download the MySQL client connector jar and add it to jars (if it is non OSGI) or bundles (if it is OSGI bundle) directory of Siddhi distribution","title":"Setup MySQL"},{"location":"docs/guides/patterns-and-trends/guide/#tryout","text":"There are multiple options available to test the developed Siddhi App. As mentioned in the previous step you could simply simulate some events directly into the stream and test your queries. But, if you are willing to test the end to end flow (from an input source to sink) then you can start the Siddhi app in the editor itself. In this guide, we are going to run the Siddhi App in the editor itself. Once the server is started, you will see below logs get printed in the editor console. As written in the above Siddhi application, taxi ride requests are accepted by the TCP endpoint of the Siddhi Stream Processor; those events are pushed to a stream called taxiRideEventStream . The first query is written to identify the pattern of rider cancellation after rider request within 30 seconds due to high waiting time. If such a pattern is identified then system will the user grade and grant some offers for subsequent rides for premium users. Parallelly, Stream Processor keep tracking the number of ride cancellations for each minute and if it found a situation of more than 3 ride cancellation then the system will identify that area/zone and send that details to next processing system to take necessary action. In the above query, such events are pushed to a database table. There is another query which continuously listens for the total number of cancellations for each minute and looking for increasing trend of ride cancellations and notifies accordingly. In this situation, if the Streaming system sends an email alert to the manager of the MyTaxi for his/her further consideration.","title":"Tryout"},{"location":"docs/guides/patterns-and-trends/guide/#invoking-the-siddhi-app","text":"To try out the above use case, you have to send a set of events in a certain order to match with the query conditions. There is a sample TCP publisher could publish events in such an order. Hence, you could use the sample publisher given in here . Then you can execute below command to run the TCP client. java -jar tcp-producer-1.0.0-jar-with-dependencies.jar It will take nearly 3 minutes to publish events which required to test all the flows in the given Siddhi app. When you are publishing events to Siddhi Stream processor, you could see the logs that get printed in Siddhi Stream processor side as well. There are related to instant offer alerts. Once, TCP publisher completes publishing events then you could check the email alert which is generated.","title":"Invoking the Siddhi App"},{"location":"docs/guides/patterns-and-trends/guide/#deployment","text":"Once you are done with the development, export the Siddhi app that you have developed with 'File' - 'Export File' option. You can deploy the Siddhi app using any of the methods listed below.","title":"Deployment"},{"location":"docs/guides/patterns-and-trends/guide/#deploy-on-vm-bare-metal","text":"","title":"Deploy on VM/ Bare Metal"},{"location":"docs/guides/patterns-and-trends/guide/#prerequisites_1","text":"First, please make sure that necessary prerequisites are met as given the Testing section . MySQL is required to try out the use case. Then, as given in the Setup MySQL section. Download the MySQL database and install it. Then create a database called \u201cMyTaxi\u201d in the MySQL database.","title":"Prerequisites"},{"location":"docs/guides/patterns-and-trends/guide/#siddhi-runtime-configuration","text":"Make sure to set the necessary environmental variables as given above. Note: In the above provided Siddhi app, there are some environmental variables (MYSQL_DB_URL, MYSQL_USERNAME, and MYSQL_PASSWORD) are used. These values are required to be set to try out the scenario end to end. MYSQL related environmental variables are required to store the events of stream NeedMoreRidersStream . Environmental variables EMAIL_PASSWORD, EMAIL_USERNAME, SENDER_EMAIL_ADDRESS and MANAGER_EMAIL_ADDRESS are used to send an email alert when there is an increasing trend of cancellation on specific area. Hence, make sure to set the environmental variables with the proper values in the system (make sure to follow necessary steps based on the underneath operating system). Download the latest Siddhi Runner distribution . Unzip the siddhi-runner-x.x.x.zip . You have to copy necessary Mysql client jar to Siddhi runner distribution to connect with MySQL database. Copy the MySQL client connector jar to jars (if it is non OSGI) or bundles (if it is OSGI bundle) directory of Siddhi runner. Please refer this for information. Start Siddhi app with the runner config by executing the following commands from the distribution directory. Linux/Mac : ./bin/runner.sh -Dapps= siddhi-file-path Windows : bin\\runner.bat -Dapps= siddhi-file-path Eg: If exported siddhi app in Siddhi home directory, ./bin/runner.sh -Dapps=Taxi-Rider-Requests-Processing-App.siddhi Once server is started, download the sample TCP event generator from here and execute below command. java -jar tcp-producer-1.0.0-jar-with-dependencies.jar tcp://localhost:9892/taxiRideRequests Above event publishes send binary events through TCP to the TCP endpoint defined in the Siddhi application. You can change the TCP endpoint url by passing them as java arguments. If not, sample client consider tcp://localhost:9892/taxiRideRequests as the TCP endpoint url. You can find the sample client source code in here In this situation, you can find the logs printed in the Siddhi runner console/log, events related to NeedMoreRidersStream are stored in the database table and escalation email is sent to the manager when there is an increasing trend found in the cancellations.","title":"Siddhi Runtime Configuration"},{"location":"docs/guides/patterns-and-trends/guide/#deploy-on-docker","text":"","title":"Deploy on Docker"},{"location":"docs/guides/patterns-and-trends/guide/#prerequisites_2","text":"MySQL is an external dependency for this use case. Hence, you could use the corresponding MySQL docker artifact to test the requirement. First, you can create a docker network for the deployment as shown below docker network create siddhi-tier --driver bridge Then, you can get the MySQL docker image from here and run it with below command. We are going to use mysql version 5.7.27. Start the MySQL docker images with below command, docker run --name mysql-server --network siddhi-tier -e MYSQL_ROOT_PASSWORD=root e1e1680ac726 e1e1680ac726 is the MySQL docker image id in this case Login to the MySQL docker instance and create a database called \u201cMyTaxi\u201d. Now, you have configured necessary prerequisites that required to run the use case.","title":"Prerequisites"},{"location":"docs/guides/patterns-and-trends/guide/#siddhi-docker-configuration","text":"Since, MySQL client jar is required for the Siddhi runner; you have to create the docker image accordingly. Below is the sample Docker file created FROM siddhiio/siddhi-runner-base-alpine:5.1.0-alpha MAINTAINER Siddhi IO Docker Maintainers \"siddhi-dev@googlegroups.com\" ARG HOST_BUNDLES_DIR=./files/bundles ARG HOST_JARS_DIR=./files/jars ARG JARS=${RUNTIME_SERVER_HOME}/jars ARG BUNDLES=${RUNTIME_SERVER_HOME}/bundles # copy bundles jars to the siddhi-runner distribution COPY --chown=siddhi_user:siddhi_io ${HOST_JARS_DIR}/ ${JARS} # expose ports EXPOSE 9090 9443 9712 9612 7711 7611 7070 7443 RUN bash ${RUNTIME_SERVER_HOME}/bin/install-jars.sh STOPSIGNAL SIGINT ENTRYPOINT [\"/home/siddhi_user/siddhi-runner/bin/runner.sh\", \"--\"] Here, you have to create a folder called jars to add necessary external client dependencies to the docker image. You can refer the official Siddhi documentation reference for this purpose. Once, Dockerfile is created you can create the docker image with below command. docker build -t siddhi_mysql . Then, you can run the Siddhi docker image that you created with necessary external dependencies to work with MySQL. docker run --network siddhi-tier -it -p 9892:9892 -v /Users/mohan/siddhi-apps/:/siddhi-apps -e MYSQL_DB_URL=jdbc:mysql://mysql-server:3306/MyTaxi -e MYSQL_USERNAME=root -e MYSQL_PASSWORD=root -e EMAIL_USERNAME=siddhi.gke.user -e EMAIL_PASSWORD=siddhi123 -e SENDER_EMAIL_ADDRESS=siddhi.gke.user@gmail.com -e MANAGER_EMAIL_ADDRESS=mohan@wso2.com siddhi_mysql:latest -Dapps=/siddhi-apps/Taxi-Rider-Requests-Processing-App.siddhi Note: In the above provided Siddhi app, there are some environmental variables (MYSQL_DB_URL, MYSQL_USERNAME, and MYSQL_PASSWORD) are used. These values are required to be set to try out the scenario end to end. MYSQL related environmental variables are required to store the events of stream NeedMoreRidersStream . Environmental variables EMAIL_PASSWORD, EMAIL_USERNAME, SENDER_EMAIL_ADDRESS and MANAGER_EMAIL_ADDRESS are used to send an email alert when there is an increasing trend of cancellation on specific area. You can use the sample TCP event publisher to simulate required events. Use the below command to publish events to TCP endpoint. java -jar tcp-producer-1.0.0-jar-with-dependencies.jar tcp://localhost:9892/taxiRideRequests Then, you could see below logs get printed in the Siddhi runner console/log, events related to NeedMoreRidersStream are stored in the database table and escalation email is sent to the manager when there is an increasing trend found in the cancellations.","title":"Siddhi Docker Configuration"},{"location":"docs/guides/patterns-and-trends/guide/#deploy-on-kubernetes","text":"It is advisable to create a namespace in Kubernetes to follow below steps. kubectl create ns siddhi-mysql-test There is a prerequisite that you should meet to tryout below SiddhiProcess; configuring MySQL database server within the above created namespace. You can use the official helm chart provided for MySQL. First, install the MySQL helm chart as shown below, helm install --name mysql-db --namespace=siddhi-mysql-test --set mysqlRootPassword=root,mysqlDatabase=MyTaxi stable/mysql Here, you can define the root password to connect to the MYSQL database and also define the database name. BTW, make sure to do helm init if it is not done yet. Then, you can set a port forwarding to the MySQL service which allows you to connect from the Host. kubectl port-forward svc/mysql-db 13306:3306 --namespace=siddhi-mysql-test Then, you can login to the MySQL server from your host machine as shown below. Then, you can install Siddhi Operator To install the Siddhi Kubernetes operator run the following commands. kubectl apply -f https://github.com/siddhi-io/siddhi-operator/releases/download/v0.2.0-alpha/00-prereqs.yaml --namespace=siddhi-mysql-test kubectl apply -f https://github.com/siddhi-io/siddhi-operator/releases/download/v0.2.0-alpha/01-siddhi-operator.yaml --namespace=siddhi-mysql-test You can verify the installation by making sure the following deployments are running in your Kubernetes cluster. Siddhi applications can be deployed on Kubernetes using the Siddhi operator. To deploy the above created Siddhi app, we have to create custom resource object yaml file (with the kind as SiddhiProcess) as given below apiVersion: siddhi.io/v1alpha2 kind: SiddhiProcess metadata: name: taxi-rider-requests-processing-app spec: apps: - script: | @App:name(\"Taxi-Rider-Requests-Processing-App\") @App:description(\"Siddhi application that processes Taxi Rider request events\") -- TCP source which accepts Taxi rider requests @source(type='tcp', context='taxiRideRequests', @map(type='binary')) define stream TaxiRideEventStream(id long, time string, passengerId string, passengerName string, pickUpAddress string, pickUpZone string, dropOutAddress string, routingDetails string, expectedFare double, status string, passengerGrade string, additionalNote string); -- For testing purposes, offer messages are logged in console. -- This could be further extended to send as sms to the premium users @sink(type='log', @map(type = 'text', @payload(\"\"\" Hi {{passengerName}} Unfortunately, you couldn't travel with us Today. We apologise for the high waiting time. As a token of apology please accept {{offerAmount}} USD off from your next ride. Truly, MyTaxi Team\"\"\"))) define stream InstantOfferAlertStream(passengerName string, pickUpZone string, offerAmount double); -- RDBMS event table which stores events related to the requirement of need more riders @store(type=\"rdbms\", jdbc.url=\"${MYSQL_DB_URL}\", username=\"${MYSQL_USERNAME}\", password=\"${MYSQL_PASSWORD}\" , jdbc.driver.name=\"com.mysql.jdbc.Driver\") define table NeedMoreRidersTable (systemTime string, zone string); -- Email sink which send email alerts to the manager of MyTaxi @sink(type = 'email', username = \"${EMAIL_USERNAME}\", address = \"${SENDER_EMAIL_ADDRESS}\", password = \"${EMAIL_PASSWORD}\", subject = \"[Need Immediate Attention] High Waiting Time\", to = \"${MANAGER_EMAIL_ADDRESS}\", host = \"smtp.gmail.com\", port = \"465\", ssl.enable = \"true\", auth = \"true\", @map(type = 'text', @payload(\"\"\" Hi, There is an increasing trend of ride cancellations in the {{zone}} area due to high waiting time. Increasing trend is detected at {{systemTime}}. Please take immediate action to sort this out. Thanks...\"\"\"))) define stream AttentionRequiredCancellationStream (systemTime string, zone string, lastNoOfCancellations long); @info(name='Ride-cancellation-identifier') from every e1=TaxiRideEventStream[status == 'Assigned'] - e2=TaxiRideEventStream[e1.passengerId == passengerId and status == 'Cancelled' and additionalNote == 'WT is High'] within 30 seconds select e2.passengerName, e1.passengerGrade, e1.pickUpZone, e1.expectedFare insert into TaxiRideCancelStream; @info(name='Offer-for-premium-users') from TaxiRideCancelStream[passengerGrade == 'Premium'] select passengerName, pickUpZone, math:floor(expectedFare) / 2.0 as offerAmount insert into InstantOfferAlertStream; @info(name='Frequently-ride-cancellation-identifier') from TaxiRideCancelStream#window.timeBatch(1 min) select pickUpZone, count() as totalCancellations, time:currentTimestamp() as systemTime group by pickUpZone having totalCancellations 3 insert into NeedMoreRidersStream; @info(name='Dump-needMoreRider-events') from NeedMoreRidersStream select systemTime, pickUpZone as zone insert into NeedMoreRidersTable; @info(name='Ride-cancellation-increasing-trend-identifer') partition with (pickUpZone of NeedMoreRidersStream) begin from e1 = NeedMoreRidersStream, e2 = NeedMoreRidersStream[totalCancellations e1.totalCancellations], e3 = NeedMoreRidersStream[totalCancellations e2.totalCancellations] select e3.systemTime, e1.pickUpZone as zone, e3.totalCancellations as lastNoOfCancellations insert into AttentionRequiredCancellationStream; end; persistentVolumeClaim: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi storageClassName: hostpath volumeMode: Filesystem container: env: - name: MYSQL_DB_URL value: \"jdbc:mysql://mysql-db:3306/MyTaxi\" - name: MYSQL_USERNAME value: \"root\" - name: MYSQL_PASSWORD value: \"root\" - name: EMAIL_PASSWORD value: \"siddhi123\" - name: EMAIL_USERNAME value: \"siddhi.gke.user\" - name: SENDER_EMAIL_ADDRESS value: \"siddhi.gke.user@gmail.com\" - name: MANAGER_EMAIL_ADDRESS value: \"mohan@wso2.com\" image: \"mohanvive/siddhi_mysql:latest\" Note: In the above provided Siddhi app, there are some environmental variables (MYSQL_DB_URL, MYSQL_USERNAME, and MYSQL_PASSWORD) are used. These values are required to be set to try out the scenario end to end. MYSQL related environmental variables are required to store the events of stream NeedMoreRidersStream . Environmental variables EMAIL_PASSWORD, EMAIL_USERNAME, SENDER_EMAIL_ADDRESS and MANAGER_EMAIL_ADDRESS are used to send an email alert when there is an increasing trend of cancellation on specific area. Hence, make sure to add proper values for the environmental variables in the above yaml file (check the \u2018env\u2019 section of the yaml file). Here, you can use the docker image that created in the section since you need a docker images with required extensions and client jars to test it in Kubernetes. Now, let\u2019s create the above resource in the Kubernetes cluster with below command. kubectl --namespace=siddhi-mysql-test create -f absolute-yaml-file-path /taxi-rider-requests-processing-app.yaml Once, siddhi app is successfully deployed. You can verify its health with below Kubernetes commands You can find the alert logs in the siddhi runner log file. To see the Siddhi runner log file, you can invoke below command. kubectl get pods Then, find the pod name of the siddhi app deployed. Then invoke below command, kubectl logs siddhi-app-pod-name -f Then, you can set a port forwarding to the Siddhi TCP endpoint which allows you to connect from the Host with below command. kubectl port-forward svc/taxi-rider-requests-processing-app-0 9892:9892 --namespace=siddhi-mysql-test Then execute below command to send TCP events to Siddhi Stream Processor. java -jar tcp-producer-1.0.0-jar-with-dependencies.jar tcp://0.0.0.0:9892/taxiRideRequests Then, you could see below logs get printed in the Siddhi runner console/log, events related to NeedMoreRidersStream are stored in the database table and escalation email is sent to the manager when there is an increasing trend found in the cancellations. Refer here to get more details about running Siddhi on Kubernetes.","title":"Deploy on Kubernetes"},{"location":"docs/guides/realtime-movie-recommendation/guide/","text":"Realtime predictions with pre-trained ML models In this guide, we are going to understand how we can use Siddhi\u2019s capability to perform real time predictions with a pre-trained machine learning models. Scenario - Predicting movie ratings Recommendation systems that model users and their interests are often used to improve various user services. Such systems are usually based on automatic prediction of user ratings of the items provided by the service. Within this guide, we focus on using machine learning capabilities integrated with Siddhi to perform Sentiment Analysis and generate movie recommendations. We'll use a pretrained tensorflow model to predict whether a movie review is positive or negative using BERT in Tensorflow and then a PMML model trained with the MovieLense dataset to generate recommendations for a positively reviewed movie. The following sections are available in this guide. What you'll build Prerequisites Implementation Testing Deployment Output What you'll build We are going to consider a real world use case of implementing a real time prediction requirement. This will help you to understand some Siddhi Stream Processing constructs and extensions such as filters, source, sinks, tensorflow, pmml extensions\u2026 Let\u2019s jump in to the use case directly. This guide demonstrates how we can build a recommendation system which recommends movies based on the user\u2019s review comments. User would write a review comment on a movie and this review comment would be the initial input to the system. From the system, we would perform sentiment analysis using a pre-trained tensorflow model to binary classify the review comment as positive or negative. If the user\u2019s comment is positive, we pass the UserId and the MovieId as inputs to a pre-trained PMML model which then generates movie recommendations. Prerequisites Below are the prerequisites that should be considered to implement the above use case. Mandatory Requirements Siddhi tooling VM/Local distribution One of the Siddhi runner distributions VM/Local Runtime Docker Image K8S Operator (commands are given in deployment section) Java 8 or higher Siddhi Gpl Execution PMML extension Siddhi Execution Tensorflow extension Pre-trained sentiment analysis tensorflow model Pre-trained movie recommendation PMML model Training Tensorflow model for Sentiment Analysis We use the IMDB dataset that contains the text of 50,000 movie reviews from the Internet Movie Database. These are split into 25,000 reviews for training and 25,000 reviews for testing. The training and testing sets are balanced, meaning they contain an equal number of positive and negative reviews. We use this data to train a Tensorflow model. You can find the Jupyter notebook used to train the tensorflow sentiment analysis model. Training PMML model for movie recommendation We use the MovieLens 1M Dataset that contain 1,000,209 anonymous ratings of approximately 3,900 movies for training the model. We use Scikit-Learn library to create the recommendation model and exports it using SkLearn2PMML to a PMML model. You can find the Jupyter notebook used to train the movie recommendation model. Requirements needed to deploy Siddhi in Docker/Kubernetes Docker Minikube or Google Kubernetes Engine(GKE) Cluster or Docker for Mac Implementation Let\u2019s assume the system sends a HTTP request upon user adding a review comment to a movie to endpoint /feedbackInputStream registered by Siddhi Runtime. This HTTP request would contain the UserId, MovieId and the ReviewComment. Siddhi Runtime performs sentiment analysis on the received ReviewComment using a pre-trained tensorflow model. If the review comment is positive, then the MovieId and UserId are passed to a recommendation PMML model and it recommends few movies for the passed combination. This recommendation is then sent back as an HTTP request back to the system. The use case depends on two extensions namely Siddhi-gpl-execution-pmml and Siddhi-execution-tensorflow extensions that are not bundled to the Siddhi distributions by default. Hence, these dependencies needs to be copied to the distribution before you start implementing the use case. Copy siddhi-gpl-execution-pmml and siddhi-execution-tensorflow extensions to SIDDHI_TOOLING_HOME/bundles directory. Refer Adding Extensions and Third Party Dependencies documentation for more details. Implement Streaming Queries Start the Siddhi tooling runtime and go to the editor UI in http://localhost:9390/editor Follow below steps to start the Siddhi tooling runtime. * Extract the downloaded zip and navigate to /bin. (TOOLING_HOME refers to the extracted folder) * Issue the following command in the command prompt (Windows) / terminal (Linux/Mac) For Windows: tooling.bat For Linux/Mac: ./tooling.sh Select File - New option, then you could either use the source view or design view to write/build the Siddhi Application. You can find the Siddhi Application bellow, that implements the requirements mentioned above. Let\u2019s write (develop) the Siddhi Application, as given below. @App:name('Realtime-Movie-Recommendation-App') @App:description('Performs realtime movie recommendations') @Source(type = 'http', receiver.url = 'http://0.0.0.0:8006/feedbackInputStream', basic.auth.enabled = 'false', @map(type = 'json')) define stream FeedbackInputStream (userId string, movieId string, reviewComment string); @Source(type = 'http', receiver.url = 'http://0.0.0.0:8006/recomendationRequestStream', basic.auth.enabled = 'false', @map(type = 'json')) define stream RecomendationRequestStream(userId string, movieId string); -- HTTP sink to publish movie recommendations. For testing purposes, there is a mock logger service provided @sink(type = 'http', publisher.url = \"http://${LOGGER_SERVICE_HOST}:8080/logger\", method = \"POST\", @map(type = 'json')) @sink(type = 'http', publisher.url = 'http://0.0.0.0:8008/movieRecommendations', @map(type = 'json')) @sink(type = 'log', @map(type = 'text')) define stream RecommendedMovieStream (userId string, movieList string); -- Perform sentiment analysis on the review comment --probabilities0 and probabilities1 stands for positive and negative probabilities respectlively. @info(name = 'query1') From FeedbackInputStream#tensorFlow:predict('${SENTIMENT_ANALYSIS_TF_SAVED_MODEL_PATH}', 'input_string_text', 'label', 'probabilities', str:concat(\"{'input_text':'\", reviewComment, \"'}\")) select userId, movieId, label, probabilities0, probabilities1 insert into SentimentPredictionStream; -- Filter positive review comment @info(name = 'query2') from SentimentPredictionStream[label == \"positive\"] select userId, movieId insert into RecomendationRequestStream; -- Recommends a movie list for the provided userId and movieId @info(name = 'query3') From RecomendationRequestStream#pmml:predict('${MOVIE_RECOMMENDATION_PMML_MODEL_PATH}', userId, movieId) select userId, movieList insert into RecommendedMovieStream; Source view of the Siddhi app. Below is the flow diagram of the above Siddhi App. Once the Siddhi app is created, you can use the Event Simulator option in the editor to simulate events to streams and perform developer testing. Testing NOTE: In the provided Siddhi app, there are some environmental variables (SENTIMENT_ANALYSIS_TF_SAVED_MODEL_PATH, and MOVIE_RECOMMENDATION_PMML_MODEL_PATH) used which are the absolute paths for the pre-trained models. Again, there is a mock service configured to receive the recommended movies, and its host is configured via LOGGER_SERVICE_HOST environment property. Hence, make sure to set the environmental variables with the proper values in the system SENTIMENT_ANALYSIS_TF_SAVED_MODEL_PATH: Absolute path to the pre-trained tensorflow saved model directory. MOVIE_RECOMMENDATION_PMML_MODEL_PATH: Absolute path to the pre-trained PMML file. LOGGER_SERVICE_HOST: IP address of the host where logger service is running. (eg: 'localhost') When you run the Siddhi app in the editor, you will see below logs getting printed in the editor console. You could simply simulate some events directly into the stream and test your Siddhi app in the editor itself. Then, you can also simulate some events through HTTP to test the application. The following sections explain how you can test the Siddhi app via HTTP using cURL. Run Mock Logger service In the provided Siddhi app, there is a HTTP sink configured to push output events to an HTTP endpoint. To verify that, please download the mock server jar and run that mock service by executing below command. java -jar logservice-1.0.0.jar Invoking the Siddhi App As mentioned in the previous steps, there is a service running in Siddhi side which is listening for events related to movie review comments. As per the Siddhi query that we wrote in the \u2018Implementation\u2019 section, respective service can be accessed via 'http://0.0.0.0:8006/feedbackInputStream'. As per the app, if the review comment is positive, movies similar to the reviewed movie will be returned as recommendations. curl -v -X POST -d \\ '{\"event\": {\"userId\": \"user105\", \"movieId\": \"avatar\", \"reviewComment\": \"Best movie experience of the last 30 years..\"}}' \\ \"http://localhost:8006/feedbackInputStream\" -H \"Content-Type:application/json\" If you invoke, above cURL request then Siddhi would identify it as a positive feedback and recommends similar movies. In this guide, for simplicity we are just logging the movie recommendations as below. INFO {io.siddhi.core.stream.output.sink.LogSink} - Realtime-Movie-Recommendation-App : RecommendedMovieStream : Event{timestamp=1568785937271, data=[user105, {After Earth, Star Trek Into Darkness, Oblivion, The Croods}], isExpired=false} Deployment Once you are done with the development, export the Siddhi app that you have developed with File - Export File option. You can deploy the Siddhi app using any of the methods listed below. NOTE: In the above provided Siddhi app, there are some environmental variables (SENTIMENT_ANALYSIS_TF_SAVED_MODEL_PATH and MOVIE_RECOMMENDATION_PMML_MODEL_PATH) which are mandatory to be set for Siddhi application to execute. Again, there is a mock service configured to publish the movie recommendations. Please configure LOGGER_SERVICE_HOST environment property to point the host where mock service is running. SENTIMENT_ANALYSIS_TF_SAVED_MODEL_PATH: Absolute path to the pre-trained tensorflow saved model directory. MOVIE_RECOMMENDATION_PMML_MODEL_PATH: Absolute path to the pre-trained PMML file. LOGGER_SERVICE_HOST: IP address of the host where logger service is running. (eg: 'localhost') Deploy on VM/ Bare Metal Download the latest Siddhi Runner distribution . Unzip the siddhi-runner-x.x.x.zip . Copy the downloaded siddhi-gpl-execution-pmml and siddhi-execution-tensorflow to SIDDHI_RUNNER_HOME/bundles directory. Start Siddhi app with the runner config by executing the following commands from the distribution directory. Linux/Mac : ./bin/runner.sh -Dapps= siddhi-file-path -DSENTIMENT_ANALYSIS_TF_SAVED_MODEL_PATH= sentiment-analysis-tf-model-path -DMOVIE_RECOMMENDATION_PMML_MODEL_PATH= movie-recommendation-pmml-model-path Windows : bin\\runner.bat -Dapps= siddhi-file-path -DSENTIMENT_ANALYSIS_TF_SAVED_MODEL_PATH= sentiment-analysis-tf-model-path -DMOVIE_RECOMMENDATION_PMML_MODEL_PATH= movie-recommendation-pmml-model-path Eg: If exported siddhi app in Siddhi home directory, ./bin/runner.sh -Dapps=Realtime-Movie-Recommendation-App.siddhi Download the mock logging service which is used to demonstrate the capability of Siddhi HTTP sink. Execute the below command to run the mock server. java -jar logservice-1.0.0.jar Invoke the service with below cURL request. curl -v -X POST -d \\ '{ event : { userId : user105 , movieId : avatar , reviewComment : Best movie experience of the last 30 years.. }}' \\ http://localhost:8006/feedbackInputStream -H Content-Type:application/json We can see the output log in the console as shown below. You could see there is an alert log printed as shown in the below image. At the same time, you could also see the events received to HTTP mock service endpoint (started in step #5) via its log as below. Deploy on Docker Create a folder locally on your host machine (eg: /home/siddhi-apps ) and copy the Siddhi app into it. Create a folder locally (eg: /home/tf_model ) and copy the pretrained Sentiment Analysis tensorflow model in to it. Create a folder locally (eg: /home/pmml_model ) and copy the pretrained Movie Recommendation PMML model in to it. Pull the latest Siddhi Runner image from [Siddhiio Docker Hub] (https://hub.docker.com/u/siddhiio). docker pull siddhiio/siddhi-runner-alpine:5.1.0-beta Start SiddhiApp by executing the following docker command. docker run -it -p 8006:8006 -v /home/siddhi-apps:/apps -v /home/tf_model:/tf_model -v /home/pmml_model:/pmml_model -e SENTIMENT_ANALYSIS_TF_SAVED_MODEL_PATH=/tf_model -e MOVIE_RECOMMENDATION_PMML_MODEL_PATH=/pmml_model/movie-recommendation.pmml siddhiio/siddhi-runner-alpine:5.1.0-beta -Dapps=/apps/Realtime-Movie-Recommendation-App.siddhi NOTE: In the above provided Siddhi app, there are some environmental variables (SENTIMENT_ANALYSIS_TF_SAVED_MODEL_PATH and MOVIE_RECOMMENDATION_PMML_MODEL_PATH) which are mandatory to be set for Siddhi application to execute. Again, there is a mock service configured to receive the recommended movies (instructions given below), and its host is configured via LOGGER_SERVICE_HOST environment property. Hence, make sure to add proper values for the environmental variables in the above command. Download the mock logging service which is used to demonstrate the capability of SIddhi HTTP sink. Execute below command to run the mock server. java -jar logservice-1.0.0.jar Send a POST request with the movie feedback to the endpoint. curl -v -X POST -d \\ '{\"event\": {\"userId\": \"user105\", \"movieId\": \"avatar\", \"reviewComment\": \"Best movie experience of the last 30 years..\"}}' \\ \"http://localhost:8006/feedbackInputStream\" -H \"Content-Type:application/json\" Since you have started the docker in interactive mode you can see the output in its console as below. (If it is not started in the interactive mode then you can run docker exec -it docker-container-id sh command, go into the container and check the log file in home/siddhi_user/siddhi-runner/wso2/runner/logs/carbon.log file) At the same time, you could also see the events received to HTTP mock service endpoint (started in step #4) via its log as below. Deploy on Kubernetes Install Siddhi Operator To install the Siddhi Kubernetes operator run the following commands. kubectl apply -f https://github.com/siddhi-io/siddhi-operator/releases/download/v0.2.0-alpha/00-prereqs.yaml kubectl apply -f https://github.com/siddhi-io/siddhi-operator/releases/download/v0.2.0-alpha/01-siddhi-operator.yaml You can verify the installation by making sure the following deployments are running in your Kubernetes cluster. Download the mock logging service which is used to demonstrate the capability of Siddhi HTTP sink. Execute the below command to run the mock server. java -jar logservice-1.0.0.jar Siddhi applications can be deployed on Kubernetes using the Siddhi operator. Before deploying the apps you have to define an Ingress , this is because there is an HTTP endpoint in the Siddhi app you have written and you will be sending events to that. We need a custom docker image bundling the pretrained models to the container. # use siddhi-runner-base FROM siddhiio/siddhi-runner-base-alpine:5.1.0-beta MAINTAINER Siddhi IO Docker Maintainers \"siddhi-dev@googlegroups.com\" ARG HOST_BUNDLES_DIR=./bundles ARG HOST_JARS_DIR=./jars ARG HOST_APPS_DIR=./siddhi-files ARG TRAINED_MODEL_DIR=./models ARG JARS=${RUNTIME_SERVER_HOME}/jars ARG BUNDLES=${RUNTIME_SERVER_HOME}/bundles ARG APPS=${RUNTIME_SERVER_HOME}/deployment/siddhi-files ARG CONFIG_FILE=./configurations.yaml ARG CONFIG_FILE_PATH=${HOME}/configurations.yaml # copy bundles jars to the siddhi-runner distribution COPY --chown=siddhi_user:siddhi_io ${HOST_APPS_DIR}/ ${APPS} COPY --chown=siddhi_user:siddhi_io ${HOST_BUNDLES_DIR}/ ${BUNDLES} COPY --chown=siddhi_user:siddhi_io ${CONFIG_FILE}/ ${USER_HOME} COPY --chown=siddhi_user:siddhi_io ${TRAINED_MODEL_DIR}/ ${USER_HOME} # expose ports EXPOSE 9090 9443 9712 9612 7711 7611 7070 7443 STOPSIGNAL SIGINT ENTRYPOINT [\"/home/siddhi_user/siddhi-runner/bin/runner.sh\"] To create the docker image, save the above content to a docker file and execute the below command. docker build absolute-docker-file-path -t siddhi-runner-mov-recommendation:1.0.0 To deploy the above created Siddhi app, you have to create a custom resource object YAML file (with the kind as SiddhiProcess) as following apiVersion: siddhi.io/v1alpha2 kind: SiddhiProcess metadata: name: movie-recommendation-app spec: apps: - script: | @App:name('Realtime-Movie-Recommendation-App') @App:description('Performs realtime movie recommendations') @Source(type = 'http', receiver.url = 'http://0.0.0.0:8006/feedbackInputStream', basic.auth.enabled = 'false', @map(type = 'json')) define stream FeedbackInputStream (userId string, movieId string, reviewComment string); @Source(type = 'http', receiver.url = 'http://0.0.0.0:8006/recomendationRequestStream', basic.auth.enabled = 'false', @map(type = 'json')) define stream RecomendationRequestStream(userId string, movieId string); -- HTTP sink to publish movie recommendations. For testing purposes, there is a mock logger service provided @sink(type = 'http', publisher.url = \"http://${LOGGER_SERVICE_HOST}:8080/logger\", method = \"POST\", @map(type = 'json')) @sink(type = 'log', @map(type = 'text')) @sink(type = 'http', publisher.url = 'http://0.0.0.0:8008/movieRecommendations', @map(type = 'json')) define stream RecommendedMovieStream (userId string, movieList string); -- Perform sentiment analysis on the review comment --probabilities0 and probabilities1 stands for positive and negative probabilities respectlively. @info(name = 'query1') From FeedbackInputStream#tensorFlow:predict('${SENTIMENT_ANALYSIS_TF_SAVED_MODEL_PATH}', 'input_string_text', 'label', 'probabilities', str:concat(\"{'input_text':'\", reviewComment, \"'}\")) select userId, movieId, label, probabilities0, probabilities1 insert into SentimentPredictionStream; -- Filter positive review comment @info(name = 'query2') from SentimentPredictionStream[label == \"positive\"] select userId, movieId insert into RecomendationRequestStream; -- Recommends a movie list for the provided userId and movieId @info(name = 'query3') From RecomendationRequestStream#pmml:predict('${MOVIE_RECOMMENDATION_PMML_MODEL_PATH}', userId, movieId) select userId, movieList insert into RecommendedMovieStream; container: env: - name: LOGGER_SERVICE_HOST value: \"10.100.1.88\" - name: SENTIMENT_ANALYSIS_TF_SAVED_MODEL_PATH value: /home/siddhi_user/trained_models/tf - name: MOVIE_RECOMMENDATION_PMML_MODEL_PATH value: /home/siddhi_user/trained_models/pmml/movie-recommendation.pmml image: \"siddhi-runner-mov-recommendation:1.0.0\" NOTE: In the above provided Siddhi app, there are some environmental variables (SENTIMENT_ANALYSIS_TF_SAVED_MODEL_PATH and MOVIE_RECOMMENDATION_PMML_MODEL_PATH) which are mandatory to be set for Siddhi application to execute. Again, there is a mock service configured to to receive the recommended movies (instructions given below), and its host is configured via LOGGER_SERVICE_HOST environment property. Hence, make sure to add proper values for the environmental variables in the above YAML file (check the env section of the YAML file). Now, let\u2019s create the above resource in the Kubernetes cluster with the following command. kubectl create -f absolute-yaml-file-path /Movie-Recommendation-App.yaml Once, Siddhi app is successfully deployed. You can verify its health using the following commands Then, add the host siddhi and related external IP (ADDRESS) to the /etc/hosts file in your machine. For Docker for Mac , external IP is 0.0.0.0 . For Minikube, you have to use Minikube IP as the external IP. Hence, run minikube ip command to get the IP of the Minikube cluster. You can find the recommendations logged in the Siddhi runner log file. To see the Siddhi runner log file, first, invoke below command to get the pods. kubectl get pods Then, find the pod name of the Siddhi app deployed, and invoke below command to view the logs. kubectl logs -f siddhi-app-pod-name Eg: as shown below image, Pass the movie review along with the userId and the movieId as a CURL request. curl -v -X POST -d \\ '{\"event\": {\"userId\": \"user105\", \"movieId\": \"avatar\", \"reviewComment\": \"Best movie experience of the last 30 years..\"}}' \\ \"http://siddhi/movie-recommendation-app-0/8006/feedbackInputStream\" -H \"Content-Type:application/json\" Then, we could see the movie recommendations as console logs (as given below). At the same time, you could see the events received to HTTP mock endpoint that started in the step [2]. Refer here to get more details about running Siddhi on Kubernetes.","title":"Realtime predictions with pre-trained ML models"},{"location":"docs/guides/realtime-movie-recommendation/guide/#realtime-predictions-with-pre-trained-ml-models","text":"In this guide, we are going to understand how we can use Siddhi\u2019s capability to perform real time predictions with a pre-trained machine learning models.","title":"Realtime predictions with pre-trained ML models"},{"location":"docs/guides/realtime-movie-recommendation/guide/#scenario-predicting-movie-ratings","text":"Recommendation systems that model users and their interests are often used to improve various user services. Such systems are usually based on automatic prediction of user ratings of the items provided by the service. Within this guide, we focus on using machine learning capabilities integrated with Siddhi to perform Sentiment Analysis and generate movie recommendations. We'll use a pretrained tensorflow model to predict whether a movie review is positive or negative using BERT in Tensorflow and then a PMML model trained with the MovieLense dataset to generate recommendations for a positively reviewed movie. The following sections are available in this guide. What you'll build Prerequisites Implementation Testing Deployment Output","title":"Scenario - Predicting movie ratings"},{"location":"docs/guides/realtime-movie-recommendation/guide/#what-youll-build","text":"We are going to consider a real world use case of implementing a real time prediction requirement. This will help you to understand some Siddhi Stream Processing constructs and extensions such as filters, source, sinks, tensorflow, pmml extensions\u2026 Let\u2019s jump in to the use case directly. This guide demonstrates how we can build a recommendation system which recommends movies based on the user\u2019s review comments. User would write a review comment on a movie and this review comment would be the initial input to the system. From the system, we would perform sentiment analysis using a pre-trained tensorflow model to binary classify the review comment as positive or negative. If the user\u2019s comment is positive, we pass the UserId and the MovieId as inputs to a pre-trained PMML model which then generates movie recommendations.","title":"What you'll build"},{"location":"docs/guides/realtime-movie-recommendation/guide/#prerequisites","text":"Below are the prerequisites that should be considered to implement the above use case.","title":"Prerequisites"},{"location":"docs/guides/realtime-movie-recommendation/guide/#mandatory-requirements","text":"Siddhi tooling VM/Local distribution One of the Siddhi runner distributions VM/Local Runtime Docker Image K8S Operator (commands are given in deployment section) Java 8 or higher Siddhi Gpl Execution PMML extension Siddhi Execution Tensorflow extension Pre-trained sentiment analysis tensorflow model Pre-trained movie recommendation PMML model","title":"Mandatory Requirements"},{"location":"docs/guides/realtime-movie-recommendation/guide/#training-tensorflow-model-for-sentiment-analysis","text":"We use the IMDB dataset that contains the text of 50,000 movie reviews from the Internet Movie Database. These are split into 25,000 reviews for training and 25,000 reviews for testing. The training and testing sets are balanced, meaning they contain an equal number of positive and negative reviews. We use this data to train a Tensorflow model. You can find the Jupyter notebook used to train the tensorflow sentiment analysis model.","title":"Training Tensorflow model for Sentiment Analysis"},{"location":"docs/guides/realtime-movie-recommendation/guide/#training-pmml-model-for-movie-recommendation","text":"We use the MovieLens 1M Dataset that contain 1,000,209 anonymous ratings of approximately 3,900 movies for training the model. We use Scikit-Learn library to create the recommendation model and exports it using SkLearn2PMML to a PMML model. You can find the Jupyter notebook used to train the movie recommendation model.","title":"Training PMML model for movie recommendation"},{"location":"docs/guides/realtime-movie-recommendation/guide/#requirements-needed-to-deploy-siddhi-in-dockerkubernetes","text":"Docker Minikube or Google Kubernetes Engine(GKE) Cluster or Docker for Mac","title":"Requirements needed to deploy Siddhi in Docker/Kubernetes"},{"location":"docs/guides/realtime-movie-recommendation/guide/#implementation","text":"Let\u2019s assume the system sends a HTTP request upon user adding a review comment to a movie to endpoint /feedbackInputStream registered by Siddhi Runtime. This HTTP request would contain the UserId, MovieId and the ReviewComment. Siddhi Runtime performs sentiment analysis on the received ReviewComment using a pre-trained tensorflow model. If the review comment is positive, then the MovieId and UserId are passed to a recommendation PMML model and it recommends few movies for the passed combination. This recommendation is then sent back as an HTTP request back to the system. The use case depends on two extensions namely Siddhi-gpl-execution-pmml and Siddhi-execution-tensorflow extensions that are not bundled to the Siddhi distributions by default. Hence, these dependencies needs to be copied to the distribution before you start implementing the use case. Copy siddhi-gpl-execution-pmml and siddhi-execution-tensorflow extensions to SIDDHI_TOOLING_HOME/bundles directory. Refer Adding Extensions and Third Party Dependencies documentation for more details.","title":"Implementation"},{"location":"docs/guides/realtime-movie-recommendation/guide/#implement-streaming-queries","text":"Start the Siddhi tooling runtime and go to the editor UI in http://localhost:9390/editor Follow below steps to start the Siddhi tooling runtime. * Extract the downloaded zip and navigate to /bin. (TOOLING_HOME refers to the extracted folder) * Issue the following command in the command prompt (Windows) / terminal (Linux/Mac) For Windows: tooling.bat For Linux/Mac: ./tooling.sh Select File - New option, then you could either use the source view or design view to write/build the Siddhi Application. You can find the Siddhi Application bellow, that implements the requirements mentioned above. Let\u2019s write (develop) the Siddhi Application, as given below. @App:name('Realtime-Movie-Recommendation-App') @App:description('Performs realtime movie recommendations') @Source(type = 'http', receiver.url = 'http://0.0.0.0:8006/feedbackInputStream', basic.auth.enabled = 'false', @map(type = 'json')) define stream FeedbackInputStream (userId string, movieId string, reviewComment string); @Source(type = 'http', receiver.url = 'http://0.0.0.0:8006/recomendationRequestStream', basic.auth.enabled = 'false', @map(type = 'json')) define stream RecomendationRequestStream(userId string, movieId string); -- HTTP sink to publish movie recommendations. For testing purposes, there is a mock logger service provided @sink(type = 'http', publisher.url = \"http://${LOGGER_SERVICE_HOST}:8080/logger\", method = \"POST\", @map(type = 'json')) @sink(type = 'http', publisher.url = 'http://0.0.0.0:8008/movieRecommendations', @map(type = 'json')) @sink(type = 'log', @map(type = 'text')) define stream RecommendedMovieStream (userId string, movieList string); -- Perform sentiment analysis on the review comment --probabilities0 and probabilities1 stands for positive and negative probabilities respectlively. @info(name = 'query1') From FeedbackInputStream#tensorFlow:predict('${SENTIMENT_ANALYSIS_TF_SAVED_MODEL_PATH}', 'input_string_text', 'label', 'probabilities', str:concat(\"{'input_text':'\", reviewComment, \"'}\")) select userId, movieId, label, probabilities0, probabilities1 insert into SentimentPredictionStream; -- Filter positive review comment @info(name = 'query2') from SentimentPredictionStream[label == \"positive\"] select userId, movieId insert into RecomendationRequestStream; -- Recommends a movie list for the provided userId and movieId @info(name = 'query3') From RecomendationRequestStream#pmml:predict('${MOVIE_RECOMMENDATION_PMML_MODEL_PATH}', userId, movieId) select userId, movieList insert into RecommendedMovieStream; Source view of the Siddhi app. Below is the flow diagram of the above Siddhi App. Once the Siddhi app is created, you can use the Event Simulator option in the editor to simulate events to streams and perform developer testing.","title":"Implement Streaming Queries"},{"location":"docs/guides/realtime-movie-recommendation/guide/#testing","text":"NOTE: In the provided Siddhi app, there are some environmental variables (SENTIMENT_ANALYSIS_TF_SAVED_MODEL_PATH, and MOVIE_RECOMMENDATION_PMML_MODEL_PATH) used which are the absolute paths for the pre-trained models. Again, there is a mock service configured to receive the recommended movies, and its host is configured via LOGGER_SERVICE_HOST environment property. Hence, make sure to set the environmental variables with the proper values in the system SENTIMENT_ANALYSIS_TF_SAVED_MODEL_PATH: Absolute path to the pre-trained tensorflow saved model directory. MOVIE_RECOMMENDATION_PMML_MODEL_PATH: Absolute path to the pre-trained PMML file. LOGGER_SERVICE_HOST: IP address of the host where logger service is running. (eg: 'localhost') When you run the Siddhi app in the editor, you will see below logs getting printed in the editor console. You could simply simulate some events directly into the stream and test your Siddhi app in the editor itself. Then, you can also simulate some events through HTTP to test the application. The following sections explain how you can test the Siddhi app via HTTP using cURL.","title":"Testing"},{"location":"docs/guides/realtime-movie-recommendation/guide/#run-mock-logger-service","text":"In the provided Siddhi app, there is a HTTP sink configured to push output events to an HTTP endpoint. To verify that, please download the mock server jar and run that mock service by executing below command. java -jar logservice-1.0.0.jar","title":"Run Mock Logger service"},{"location":"docs/guides/realtime-movie-recommendation/guide/#invoking-the-siddhi-app","text":"As mentioned in the previous steps, there is a service running in Siddhi side which is listening for events related to movie review comments. As per the Siddhi query that we wrote in the \u2018Implementation\u2019 section, respective service can be accessed via 'http://0.0.0.0:8006/feedbackInputStream'. As per the app, if the review comment is positive, movies similar to the reviewed movie will be returned as recommendations. curl -v -X POST -d \\ '{\"event\": {\"userId\": \"user105\", \"movieId\": \"avatar\", \"reviewComment\": \"Best movie experience of the last 30 years..\"}}' \\ \"http://localhost:8006/feedbackInputStream\" -H \"Content-Type:application/json\" If you invoke, above cURL request then Siddhi would identify it as a positive feedback and recommends similar movies. In this guide, for simplicity we are just logging the movie recommendations as below. INFO {io.siddhi.core.stream.output.sink.LogSink} - Realtime-Movie-Recommendation-App : RecommendedMovieStream : Event{timestamp=1568785937271, data=[user105, {After Earth, Star Trek Into Darkness, Oblivion, The Croods}], isExpired=false}","title":"Invoking the Siddhi App"},{"location":"docs/guides/realtime-movie-recommendation/guide/#deployment","text":"Once you are done with the development, export the Siddhi app that you have developed with File - Export File option. You can deploy the Siddhi app using any of the methods listed below. NOTE: In the above provided Siddhi app, there are some environmental variables (SENTIMENT_ANALYSIS_TF_SAVED_MODEL_PATH and MOVIE_RECOMMENDATION_PMML_MODEL_PATH) which are mandatory to be set for Siddhi application to execute. Again, there is a mock service configured to publish the movie recommendations. Please configure LOGGER_SERVICE_HOST environment property to point the host where mock service is running. SENTIMENT_ANALYSIS_TF_SAVED_MODEL_PATH: Absolute path to the pre-trained tensorflow saved model directory. MOVIE_RECOMMENDATION_PMML_MODEL_PATH: Absolute path to the pre-trained PMML file. LOGGER_SERVICE_HOST: IP address of the host where logger service is running. (eg: 'localhost')","title":"Deployment"},{"location":"docs/guides/realtime-movie-recommendation/guide/#deploy-on-vm-bare-metal","text":"Download the latest Siddhi Runner distribution . Unzip the siddhi-runner-x.x.x.zip . Copy the downloaded siddhi-gpl-execution-pmml and siddhi-execution-tensorflow to SIDDHI_RUNNER_HOME/bundles directory. Start Siddhi app with the runner config by executing the following commands from the distribution directory. Linux/Mac : ./bin/runner.sh -Dapps= siddhi-file-path -DSENTIMENT_ANALYSIS_TF_SAVED_MODEL_PATH= sentiment-analysis-tf-model-path -DMOVIE_RECOMMENDATION_PMML_MODEL_PATH= movie-recommendation-pmml-model-path Windows : bin\\runner.bat -Dapps= siddhi-file-path -DSENTIMENT_ANALYSIS_TF_SAVED_MODEL_PATH= sentiment-analysis-tf-model-path -DMOVIE_RECOMMENDATION_PMML_MODEL_PATH= movie-recommendation-pmml-model-path Eg: If exported siddhi app in Siddhi home directory, ./bin/runner.sh -Dapps=Realtime-Movie-Recommendation-App.siddhi Download the mock logging service which is used to demonstrate the capability of Siddhi HTTP sink. Execute the below command to run the mock server. java -jar logservice-1.0.0.jar Invoke the service with below cURL request. curl -v -X POST -d \\ '{ event : { userId : user105 , movieId : avatar , reviewComment : Best movie experience of the last 30 years.. }}' \\ http://localhost:8006/feedbackInputStream -H Content-Type:application/json We can see the output log in the console as shown below. You could see there is an alert log printed as shown in the below image. At the same time, you could also see the events received to HTTP mock service endpoint (started in step #5) via its log as below.","title":"Deploy on VM/ Bare Metal"},{"location":"docs/guides/realtime-movie-recommendation/guide/#deploy-on-docker","text":"Create a folder locally on your host machine (eg: /home/siddhi-apps ) and copy the Siddhi app into it. Create a folder locally (eg: /home/tf_model ) and copy the pretrained Sentiment Analysis tensorflow model in to it. Create a folder locally (eg: /home/pmml_model ) and copy the pretrained Movie Recommendation PMML model in to it. Pull the latest Siddhi Runner image from [Siddhiio Docker Hub] (https://hub.docker.com/u/siddhiio). docker pull siddhiio/siddhi-runner-alpine:5.1.0-beta Start SiddhiApp by executing the following docker command. docker run -it -p 8006:8006 -v /home/siddhi-apps:/apps -v /home/tf_model:/tf_model -v /home/pmml_model:/pmml_model -e SENTIMENT_ANALYSIS_TF_SAVED_MODEL_PATH=/tf_model -e MOVIE_RECOMMENDATION_PMML_MODEL_PATH=/pmml_model/movie-recommendation.pmml siddhiio/siddhi-runner-alpine:5.1.0-beta -Dapps=/apps/Realtime-Movie-Recommendation-App.siddhi NOTE: In the above provided Siddhi app, there are some environmental variables (SENTIMENT_ANALYSIS_TF_SAVED_MODEL_PATH and MOVIE_RECOMMENDATION_PMML_MODEL_PATH) which are mandatory to be set for Siddhi application to execute. Again, there is a mock service configured to receive the recommended movies (instructions given below), and its host is configured via LOGGER_SERVICE_HOST environment property. Hence, make sure to add proper values for the environmental variables in the above command. Download the mock logging service which is used to demonstrate the capability of SIddhi HTTP sink. Execute below command to run the mock server. java -jar logservice-1.0.0.jar Send a POST request with the movie feedback to the endpoint. curl -v -X POST -d \\ '{\"event\": {\"userId\": \"user105\", \"movieId\": \"avatar\", \"reviewComment\": \"Best movie experience of the last 30 years..\"}}' \\ \"http://localhost:8006/feedbackInputStream\" -H \"Content-Type:application/json\" Since you have started the docker in interactive mode you can see the output in its console as below. (If it is not started in the interactive mode then you can run docker exec -it docker-container-id sh command, go into the container and check the log file in home/siddhi_user/siddhi-runner/wso2/runner/logs/carbon.log file) At the same time, you could also see the events received to HTTP mock service endpoint (started in step #4) via its log as below.","title":"Deploy on Docker"},{"location":"docs/guides/realtime-movie-recommendation/guide/#deploy-on-kubernetes","text":"Install Siddhi Operator To install the Siddhi Kubernetes operator run the following commands. kubectl apply -f https://github.com/siddhi-io/siddhi-operator/releases/download/v0.2.0-alpha/00-prereqs.yaml kubectl apply -f https://github.com/siddhi-io/siddhi-operator/releases/download/v0.2.0-alpha/01-siddhi-operator.yaml You can verify the installation by making sure the following deployments are running in your Kubernetes cluster. Download the mock logging service which is used to demonstrate the capability of Siddhi HTTP sink. Execute the below command to run the mock server. java -jar logservice-1.0.0.jar Siddhi applications can be deployed on Kubernetes using the Siddhi operator. Before deploying the apps you have to define an Ingress , this is because there is an HTTP endpoint in the Siddhi app you have written and you will be sending events to that. We need a custom docker image bundling the pretrained models to the container. # use siddhi-runner-base FROM siddhiio/siddhi-runner-base-alpine:5.1.0-beta MAINTAINER Siddhi IO Docker Maintainers \"siddhi-dev@googlegroups.com\" ARG HOST_BUNDLES_DIR=./bundles ARG HOST_JARS_DIR=./jars ARG HOST_APPS_DIR=./siddhi-files ARG TRAINED_MODEL_DIR=./models ARG JARS=${RUNTIME_SERVER_HOME}/jars ARG BUNDLES=${RUNTIME_SERVER_HOME}/bundles ARG APPS=${RUNTIME_SERVER_HOME}/deployment/siddhi-files ARG CONFIG_FILE=./configurations.yaml ARG CONFIG_FILE_PATH=${HOME}/configurations.yaml # copy bundles jars to the siddhi-runner distribution COPY --chown=siddhi_user:siddhi_io ${HOST_APPS_DIR}/ ${APPS} COPY --chown=siddhi_user:siddhi_io ${HOST_BUNDLES_DIR}/ ${BUNDLES} COPY --chown=siddhi_user:siddhi_io ${CONFIG_FILE}/ ${USER_HOME} COPY --chown=siddhi_user:siddhi_io ${TRAINED_MODEL_DIR}/ ${USER_HOME} # expose ports EXPOSE 9090 9443 9712 9612 7711 7611 7070 7443 STOPSIGNAL SIGINT ENTRYPOINT [\"/home/siddhi_user/siddhi-runner/bin/runner.sh\"] To create the docker image, save the above content to a docker file and execute the below command. docker build absolute-docker-file-path -t siddhi-runner-mov-recommendation:1.0.0 To deploy the above created Siddhi app, you have to create a custom resource object YAML file (with the kind as SiddhiProcess) as following apiVersion: siddhi.io/v1alpha2 kind: SiddhiProcess metadata: name: movie-recommendation-app spec: apps: - script: | @App:name('Realtime-Movie-Recommendation-App') @App:description('Performs realtime movie recommendations') @Source(type = 'http', receiver.url = 'http://0.0.0.0:8006/feedbackInputStream', basic.auth.enabled = 'false', @map(type = 'json')) define stream FeedbackInputStream (userId string, movieId string, reviewComment string); @Source(type = 'http', receiver.url = 'http://0.0.0.0:8006/recomendationRequestStream', basic.auth.enabled = 'false', @map(type = 'json')) define stream RecomendationRequestStream(userId string, movieId string); -- HTTP sink to publish movie recommendations. For testing purposes, there is a mock logger service provided @sink(type = 'http', publisher.url = \"http://${LOGGER_SERVICE_HOST}:8080/logger\", method = \"POST\", @map(type = 'json')) @sink(type = 'log', @map(type = 'text')) @sink(type = 'http', publisher.url = 'http://0.0.0.0:8008/movieRecommendations', @map(type = 'json')) define stream RecommendedMovieStream (userId string, movieList string); -- Perform sentiment analysis on the review comment --probabilities0 and probabilities1 stands for positive and negative probabilities respectlively. @info(name = 'query1') From FeedbackInputStream#tensorFlow:predict('${SENTIMENT_ANALYSIS_TF_SAVED_MODEL_PATH}', 'input_string_text', 'label', 'probabilities', str:concat(\"{'input_text':'\", reviewComment, \"'}\")) select userId, movieId, label, probabilities0, probabilities1 insert into SentimentPredictionStream; -- Filter positive review comment @info(name = 'query2') from SentimentPredictionStream[label == \"positive\"] select userId, movieId insert into RecomendationRequestStream; -- Recommends a movie list for the provided userId and movieId @info(name = 'query3') From RecomendationRequestStream#pmml:predict('${MOVIE_RECOMMENDATION_PMML_MODEL_PATH}', userId, movieId) select userId, movieList insert into RecommendedMovieStream; container: env: - name: LOGGER_SERVICE_HOST value: \"10.100.1.88\" - name: SENTIMENT_ANALYSIS_TF_SAVED_MODEL_PATH value: /home/siddhi_user/trained_models/tf - name: MOVIE_RECOMMENDATION_PMML_MODEL_PATH value: /home/siddhi_user/trained_models/pmml/movie-recommendation.pmml image: \"siddhi-runner-mov-recommendation:1.0.0\" NOTE: In the above provided Siddhi app, there are some environmental variables (SENTIMENT_ANALYSIS_TF_SAVED_MODEL_PATH and MOVIE_RECOMMENDATION_PMML_MODEL_PATH) which are mandatory to be set for Siddhi application to execute. Again, there is a mock service configured to to receive the recommended movies (instructions given below), and its host is configured via LOGGER_SERVICE_HOST environment property. Hence, make sure to add proper values for the environmental variables in the above YAML file (check the env section of the YAML file). Now, let\u2019s create the above resource in the Kubernetes cluster with the following command. kubectl create -f absolute-yaml-file-path /Movie-Recommendation-App.yaml Once, Siddhi app is successfully deployed. You can verify its health using the following commands Then, add the host siddhi and related external IP (ADDRESS) to the /etc/hosts file in your machine. For Docker for Mac , external IP is 0.0.0.0 . For Minikube, you have to use Minikube IP as the external IP. Hence, run minikube ip command to get the IP of the Minikube cluster. You can find the recommendations logged in the Siddhi runner log file. To see the Siddhi runner log file, first, invoke below command to get the pods. kubectl get pods Then, find the pod name of the Siddhi app deployed, and invoke below command to view the logs. kubectl logs -f siddhi-app-pod-name Eg: as shown below image, Pass the movie review along with the userId and the movieId as a CURL request. curl -v -X POST -d \\ '{\"event\": {\"userId\": \"user105\", \"movieId\": \"avatar\", \"reviewComment\": \"Best movie experience of the last 30 years..\"}}' \\ \"http://siddhi/movie-recommendation-app-0/8006/feedbackInputStream\" -H \"Content-Type:application/json\" Then, we could see the movie recommendations as console logs (as given below). At the same time, you could see the events received to HTTP mock endpoint that started in the step [2]. Refer here to get more details about running Siddhi on Kubernetes.","title":"Deploy on Kubernetes"},{"location":"docs/quick-start/","text":"Siddhi 5.1 Quick Start Guide Siddhi is a cloud native Streaming and Complex Event Processing engine that understands Streaming SQL queries in order to capture events from diverse data sources, process them, detect complex conditions, and publish output to various endpoints in real time. Siddhi is used by many companies including Uber, eBay, PayPal (via Apache Eagle), here Uber processed more than 20 billion events per day using Siddhi for their fraud analytics use cases. Siddhi is also used in various analytics and integration platforms such as Apache Eagle as a policy enforcement engine, WSO2 API Manager as analytics and throttling engine, WSO2 Identity Server as an adaptive authorization engine. This quick start guide contains the following six sections: Domain of Siddhi Overview of Siddhi architecture Using Siddhi for the first time Writing first Siddhi Application Testing Siddhi Application A bit of Stream Processing 1. Domain of Siddhi Siddhi is an event driven system where all the data it consumes, processes and sends are modeled as events. Therefore, Siddhi can play a vital part in any event-driven architecture. As Siddhi works with events, first let's understand what an event is through an example. If we consider transactions carried out via an ATM as a data stream, one withdrawal from it can be considered as an event . This event contains data such as amount, time, account number, etc. Many such transactions form a stream. Siddhi provides following functionalities, Streaming Data Analytics Forrester defines Streaming Analytics as: Software that provides analytical operators to orchestrate data flow , calculate analytics , and detect patterns on event data from multiple, disparate live data sources to allow developers to build applications that sense, think, and act in real time . Complex Event Processing (CEP) Gartner\u2019s IT Glossary defines CEP as follows: \"CEP is a kind of computing in which incoming data about events is distilled into more useful, higher level \u201ccomplex\u201d event data that provides insight into what is happening.\" \" CEP is event-driven because the computation is triggered by the receipt of event data. CEP is used for highly demanding, continuous-intelligence applications that enhance situation awareness and support real-time decisions.\" Streaming Data Integration Streaming data integration is a way of integrating several systems by processing, correlating, and analyzing the data in memory, while continuously moving data in real-time from one system to another. Alerts Notifications The system to continuously monitor event streams, and send alerts and notifications, based on defined KPIs and other analytics. Adaptive Decision Making A way to dynamically making real-time decisions based on predefined rules, the current state of the connected systems, and machine learning techniques. Basically, Siddhi receives data event-by-event and processes them in real-time to produce meaningful information. Using the above Siddhi can be used to solve may use-cases as follows: Fraud Analytics Monitoring System Integration Anomaly Detection Sentiment Analysis Processing Customer Behavior .. etc 2. Overview of Siddhi architecture As indicated above, Siddhi can: Accept event inputs from many different types of sources. Process them to transform, enrich, and generate insights. Publish them to multiple types of sinks. To use Siddhi, you need to write the processing logic as a Siddhi Application in the Siddhi Streaming SQL language which is discussed in the section 4 . Here a Siddhi Application is a script file that contains business logic for a scenario. When the Siddhi application is started, it: Consumes data one-by-one as events. Pipe the events to queries through various streams for processing. Generates new events based on the processing done at the queries. Finally, Sends newly generated events through output to streams. 3. Using Siddhi for the first time In this section, we will be using the Siddhi tooling distribution\u200a\u2014\u200aa server version of Siddhi that has a sophisticated web based editor with a GUI (referred to as \u201cSiddhi Editor\u201d ) where you can write Siddhi Apps and simulate events to test your scenario. Step 1 \u200a\u2014\u200aInstall Oracle Java SE Development Kit (JDK) version 1.8. Step 2 \u200a\u2014\u200a Set the JAVA_HOME environment variable. Step 3 \u200a\u2014\u200aDownload the latest tooling distribution from here . Step 4 \u200a\u2014\u200aExtract the downloaded zip and navigate to TOOLING_HOME /bin . ( TOOLING_HOME refers to the extracted folder) Step 5 \u200a\u2014\u200aIssue the following command in the command prompt (Windows) / terminal (Linux/Mac) For Windows: tooling.bat For Linux/Mac: ./tooling.sh After successfully starting the Siddhi Editor, the terminal should look like as shown below: After starting the Siddhi Editor, access the Editor GUI by visiting the following link in your browser (Google Chrome is the Recommended). http://localhost:9390/editor This takes you to the Siddhi Editor landing page. 4. Writing first Siddhi Application Siddhi Streaming SQL is a rich, compact, easy-to-use SQL-like language. As the first Siddhi Application, let's learn how to find the total of values from the incoming events and output the current running total value for each event. Siddhi has lot of in-built functions and extensions available for complex analysis, and you can find more information about the Siddhi grammar and its functions from the Siddhi Query Guide . Let's consider sample scenario where we are loading cargo boxes into a ship . Here, we need to keep track of the total weight of the cargo added, and the weight of each loaded cargo box is considered an event . We can write a Siddhi Application for the above scenario using the following 4 parts . Part 1\u200a\u2014\u200aGiving our Siddhi application a suitable name. This allows us to uniquely identity a Siddhi Application. In this example, let's name our application as \u201cHelloWorldApp\u201d @App:name(\"HelloWorldApp\") Part 2\u200a\u2014\u200aDefining the input stream. The stream needs to have a name and a schema defining the data that each incoming event should contain. The event data attributes are expressed as name and type pairs. We can also attach a \"source\" to the created stream, so that we can consume events from outside and send them to the stream. ( Source is the Siddhi way to consume streams from external systems ). For this scenario we will use an http source to consume Cargo Events. When added the http source will spin up a HTTP endpoint and keep on listening for messages. To learn more about sources, refer source ) In this scenario: The name of the input stream\u200a\u2014\u200a \u201cCargoStream\u201d This contains only one data attribute: The name of the data in each event\u200a\u2014\u200a \u201cweight\u201d Type of the data \u201cweight\u201d \u200a\u2014\u200aint Type of source - HTTP HTTP endpoint address - http://0.0.0.0:8006/cargo Accepted input data format - JSON @source(type = 'http', receiver.url = \"http://0.0.0.0:8006/cargo\", @map(type = 'json')) define stream CargoStream (weight int); Part 3 - Defining the output stream. This has the same info as the input \u201cCargoStream\u201d stream\u200adefinition with an additional totalWeight attribute containing the total weight calculated so far. In addition we also need to add a log \"sink\" to log the OutputStream so that we can observe the output produced by the stream. ( Sink is the Siddhi way to publish streams to external systems ). This particular log type sink simply logs the stream events. To learn more about sinks, refer sink ) @sink(type='log', prefix='LOGGER') define stream OutputStream(weight int, totalWeight long); Part 4\u200a\u2014\u200aWriting the Siddhi query. As part of the query we need to specify the following: A name for the query\u200a\u2014\u200a \u201cHelloWorldQuery\u201d The input stream from which the query consumes events \u2014\u200a \u201cCargoStream\u201d How the output to be calculated - by calculating the sum of the *weight**s The data outputted to the output stream\u200a\u2014\u200a \u201cweight\u201d , \u201ctotalWeight\u201d The output stream to which the event should be outputted\u200a\u2014\u200a \u201cOutputStream\u201d @info(name='HelloWorldQuery') from CargoStream select weight, sum(weight) as totalWeight insert into OutputStream; This query will calculate the sum of weights from the start of the Siddhi application. For more complex use cases refer Siddhi Query Guide ) Final Siddhi application in the editor will look like following. You can copy the final Siddhi app from below. @App:name(\"HelloWorldApp\") @source(type = 'http', receiver.url = \"http://0.0.0.0:8006/cargo\", @map(type = 'json')) define stream CargoStream (weight int); @sink(type='log', prefix='LOGGER') define stream OutputStream(weight int, totalWeight long); @info(name='HelloWorldQuery') from CargoStream select weight, sum(weight) as totalWeight insert into OutputStream; 5. Testing Siddhi Application In this section first we will test the logical accuracy of Siddhi query using in-built functions of Siddhi Editor. In a later section we will invoke the HTTP endpoint and perform an end to end test. The Siddhi Editor has in-built support to simulate events. You can do it via the \u201cEvent Simulator\u201d panel at the left of the Siddhi Editor. Before running the event simulation, you should save your HelloWorldApp by browsing to File menu - and clicking Save . To simulate events, click Event Simulator and configure Single Simulation as shown below. Step 1\u200a\u2014\u200aConfigurations: Siddhi App Name\u200a\u2014\u200a \u201cHelloWorldApp\u201d Stream Name\u200a\u2014\u200a \u201cCargoStream\u201d Timestamp\u200a\u2014\u200a(Leave it blank) weight\u200a\u2014\u200a2 (or some integer) Step 2\u200a\u2014\u200aClick \u201cRun\u201d mode and then click \u201cStart and Send\u201d . This starts the Siddhi Application and send the event. If the Siddhi application is successfully started, the following message is printed in the Stream Processor Studio console: HelloWorldApp.siddhi Started Successfully! Step 3\u200a\u2014\u200aClick \u201cSend\u201d and observe the terminal . This will send a new event for each click. You can see a logs containing outputData=[2, 2] and outputData=[2, 4] , etc. You can change the value of the weight and send it to see how the sum of the weight is updated. Bravo! You have successfully completed building and testing your first Siddhi Application! 6. A bit of Stream Processing This section will improve our Siddhi app to demonstrates how to carry out temporal window processing with Siddhi. Up to this point, we are calculating the sum of weights from the start of the Siddhi app, and now let's improve it to consider only the last three events for the calculation. For this scenario, let's imagine that when we are loading cargo boxes into the ship and we need to keep track of the average weight of the last three loaded boxes so that we can balance the weight across the ship. For this purpose, let's try to find the average weight of last three boxes of each event. For window processing, we need to modify our query as follows: @info(name='HelloWorldQuery') from CargoStream#window.length(3) select weight, sum(weight) as totalWeight, avg(weight) as averageWeight insert into OutputStream; from CargoStream#window.length(3) - Specifies that we need to consider the last three events in a sliding manner. avg(weight) as averageWeight - Specifies calculating the average of events stored in the window and producing the results as \"averageWeight\" (Note: Similarly the sum also calculates the totalWeight based on the last three events). We also need to modify the \"OutputStream\" definition to accommodate the new \"averageWeight\" . define stream OutputStream(weight int, totalWeight long, averageWeight double); The updated Siddhi Application is given below: @App:name(\"HelloWorldApp\") @source(type = 'http', receiver.url = \"http://0.0.0.0:8006/cargo\",@map(type = 'json')) define stream CargoStream (weight int); @sink(type='log', prefix='LOGGER') define stream OutputStream(weight int, totalWeight long, averageWeight double); @info(name='HelloWorldQuery') from CargoStream#window.length(3) select weight, sum(weight) as totalWeight, avg(weight) as averageWeight insert into OutputStream; Now you can send events using the Event Simulator and observe the log to see the sum and average of the weights based on the last three cargo events. In the earlier scenario when the window is not used, the system only stored the running sum in its memory, and it did not store any events. But for length based window processing the system will retain the events that fall into the window to perform aggregation operations such as average, maximum, etc. In this case when the 4 th event arrives, the first event in the window is removed ensuring the memory usage does not grow beyond a specific limit. Note: some window types in Siddhi are even more optimized to perform the operations with minimal or no event retention. 7. Running Siddhi Application as a Docker microservice In this step we will run above developed Siddhi application as a microservice utilizing Docker. For other available options please refer here . Here we will use siddhi-runner docker distribution. Follow the below steps to obtain the docker. Install docker in your machine and start the daemon ( https://docs.docker.com/install/ ). Pull the latest siddhi-runner image by executing below command. docker pull siddhiio/siddhi-runner-alpine:latest * Navigate to Siddhi Editor and choose File - Export File for download above Siddhi application as a file. * Move downloaded Siddhi file( HelloWorldApp.siddhi ) to a desired location (e.g. /home/me/siddhi-apps ) * Execute below command to start the Siddhi Application as a microservice. docker run -it -p 8006:8006 -v /home/me/siddhi-apps:/apps siddhiio/siddhi-runner-alpine -Dapps=/apps/HelloWorldApp.siddhi Note: Make sure to update the /home/me/siddhi-apps with the folder path you have stored the HelloWorldApp.siddhi app. * Once container is started use below curl command to send events into \"CargoStream\" curl -X POST http://localhost:8006/cargo \\ --header \"Content-Type:application/json\" \\ -d '{\"event\":{\"weight\":2}}' * You will be able to observe outputs via logs as shown below. [2019-04-24 08:54:51,755] INFO {io.siddhi.core.stream.output.sink.LogSink} - LOGGER : Event{timestamp=1556096091751, data=[2, 2, 2.0], isExpired=false} [2019-04-24 08:56:25,307] INFO {io.siddhi.core.stream.output.sink.LogSink} - LOGGER : Event{timestamp=1556096185307, data=[2, 4, 2.0], isExpired=false} To learn more about the Siddhi functionality, see Siddhi Documentation . If you have questions please post them on Stackoverflow with \"Siddhi\" tag.","title":"Quick Start"},{"location":"docs/quick-start/#siddhi-51-quick-start-guide","text":"Siddhi is a cloud native Streaming and Complex Event Processing engine that understands Streaming SQL queries in order to capture events from diverse data sources, process them, detect complex conditions, and publish output to various endpoints in real time. Siddhi is used by many companies including Uber, eBay, PayPal (via Apache Eagle), here Uber processed more than 20 billion events per day using Siddhi for their fraud analytics use cases. Siddhi is also used in various analytics and integration platforms such as Apache Eagle as a policy enforcement engine, WSO2 API Manager as analytics and throttling engine, WSO2 Identity Server as an adaptive authorization engine. This quick start guide contains the following six sections: Domain of Siddhi Overview of Siddhi architecture Using Siddhi for the first time Writing first Siddhi Application Testing Siddhi Application A bit of Stream Processing","title":"Siddhi 5.1 Quick Start Guide"},{"location":"docs/quick-start/#1-domain-of-siddhi","text":"Siddhi is an event driven system where all the data it consumes, processes and sends are modeled as events. Therefore, Siddhi can play a vital part in any event-driven architecture. As Siddhi works with events, first let's understand what an event is through an example. If we consider transactions carried out via an ATM as a data stream, one withdrawal from it can be considered as an event . This event contains data such as amount, time, account number, etc. Many such transactions form a stream. Siddhi provides following functionalities, Streaming Data Analytics Forrester defines Streaming Analytics as: Software that provides analytical operators to orchestrate data flow , calculate analytics , and detect patterns on event data from multiple, disparate live data sources to allow developers to build applications that sense, think, and act in real time . Complex Event Processing (CEP) Gartner\u2019s IT Glossary defines CEP as follows: \"CEP is a kind of computing in which incoming data about events is distilled into more useful, higher level \u201ccomplex\u201d event data that provides insight into what is happening.\" \" CEP is event-driven because the computation is triggered by the receipt of event data. CEP is used for highly demanding, continuous-intelligence applications that enhance situation awareness and support real-time decisions.\" Streaming Data Integration Streaming data integration is a way of integrating several systems by processing, correlating, and analyzing the data in memory, while continuously moving data in real-time from one system to another. Alerts Notifications The system to continuously monitor event streams, and send alerts and notifications, based on defined KPIs and other analytics. Adaptive Decision Making A way to dynamically making real-time decisions based on predefined rules, the current state of the connected systems, and machine learning techniques. Basically, Siddhi receives data event-by-event and processes them in real-time to produce meaningful information. Using the above Siddhi can be used to solve may use-cases as follows: Fraud Analytics Monitoring System Integration Anomaly Detection Sentiment Analysis Processing Customer Behavior .. etc","title":"1. Domain of Siddhi"},{"location":"docs/quick-start/#2-overview-of-siddhi-architecture","text":"As indicated above, Siddhi can: Accept event inputs from many different types of sources. Process them to transform, enrich, and generate insights. Publish them to multiple types of sinks. To use Siddhi, you need to write the processing logic as a Siddhi Application in the Siddhi Streaming SQL language which is discussed in the section 4 . Here a Siddhi Application is a script file that contains business logic for a scenario. When the Siddhi application is started, it: Consumes data one-by-one as events. Pipe the events to queries through various streams for processing. Generates new events based on the processing done at the queries. Finally, Sends newly generated events through output to streams.","title":"2. Overview of Siddhi architecture"},{"location":"docs/quick-start/#3-using-siddhi-for-the-first-time","text":"In this section, we will be using the Siddhi tooling distribution\u200a\u2014\u200aa server version of Siddhi that has a sophisticated web based editor with a GUI (referred to as \u201cSiddhi Editor\u201d ) where you can write Siddhi Apps and simulate events to test your scenario. Step 1 \u200a\u2014\u200aInstall Oracle Java SE Development Kit (JDK) version 1.8. Step 2 \u200a\u2014\u200a Set the JAVA_HOME environment variable. Step 3 \u200a\u2014\u200aDownload the latest tooling distribution from here . Step 4 \u200a\u2014\u200aExtract the downloaded zip and navigate to TOOLING_HOME /bin . ( TOOLING_HOME refers to the extracted folder) Step 5 \u200a\u2014\u200aIssue the following command in the command prompt (Windows) / terminal (Linux/Mac) For Windows: tooling.bat For Linux/Mac: ./tooling.sh After successfully starting the Siddhi Editor, the terminal should look like as shown below: After starting the Siddhi Editor, access the Editor GUI by visiting the following link in your browser (Google Chrome is the Recommended). http://localhost:9390/editor This takes you to the Siddhi Editor landing page.","title":"3. Using Siddhi for the first time"},{"location":"docs/quick-start/#4-writing-first-siddhi-application","text":"Siddhi Streaming SQL is a rich, compact, easy-to-use SQL-like language. As the first Siddhi Application, let's learn how to find the total of values from the incoming events and output the current running total value for each event. Siddhi has lot of in-built functions and extensions available for complex analysis, and you can find more information about the Siddhi grammar and its functions from the Siddhi Query Guide . Let's consider sample scenario where we are loading cargo boxes into a ship . Here, we need to keep track of the total weight of the cargo added, and the weight of each loaded cargo box is considered an event . We can write a Siddhi Application for the above scenario using the following 4 parts . Part 1\u200a\u2014\u200aGiving our Siddhi application a suitable name. This allows us to uniquely identity a Siddhi Application. In this example, let's name our application as \u201cHelloWorldApp\u201d @App:name(\"HelloWorldApp\") Part 2\u200a\u2014\u200aDefining the input stream. The stream needs to have a name and a schema defining the data that each incoming event should contain. The event data attributes are expressed as name and type pairs. We can also attach a \"source\" to the created stream, so that we can consume events from outside and send them to the stream. ( Source is the Siddhi way to consume streams from external systems ). For this scenario we will use an http source to consume Cargo Events. When added the http source will spin up a HTTP endpoint and keep on listening for messages. To learn more about sources, refer source ) In this scenario: The name of the input stream\u200a\u2014\u200a \u201cCargoStream\u201d This contains only one data attribute: The name of the data in each event\u200a\u2014\u200a \u201cweight\u201d Type of the data \u201cweight\u201d \u200a\u2014\u200aint Type of source - HTTP HTTP endpoint address - http://0.0.0.0:8006/cargo Accepted input data format - JSON @source(type = 'http', receiver.url = \"http://0.0.0.0:8006/cargo\", @map(type = 'json')) define stream CargoStream (weight int); Part 3 - Defining the output stream. This has the same info as the input \u201cCargoStream\u201d stream\u200adefinition with an additional totalWeight attribute containing the total weight calculated so far. In addition we also need to add a log \"sink\" to log the OutputStream so that we can observe the output produced by the stream. ( Sink is the Siddhi way to publish streams to external systems ). This particular log type sink simply logs the stream events. To learn more about sinks, refer sink ) @sink(type='log', prefix='LOGGER') define stream OutputStream(weight int, totalWeight long); Part 4\u200a\u2014\u200aWriting the Siddhi query. As part of the query we need to specify the following: A name for the query\u200a\u2014\u200a \u201cHelloWorldQuery\u201d The input stream from which the query consumes events \u2014\u200a \u201cCargoStream\u201d How the output to be calculated - by calculating the sum of the *weight**s The data outputted to the output stream\u200a\u2014\u200a \u201cweight\u201d , \u201ctotalWeight\u201d The output stream to which the event should be outputted\u200a\u2014\u200a \u201cOutputStream\u201d @info(name='HelloWorldQuery') from CargoStream select weight, sum(weight) as totalWeight insert into OutputStream; This query will calculate the sum of weights from the start of the Siddhi application. For more complex use cases refer Siddhi Query Guide ) Final Siddhi application in the editor will look like following. You can copy the final Siddhi app from below. @App:name(\"HelloWorldApp\") @source(type = 'http', receiver.url = \"http://0.0.0.0:8006/cargo\", @map(type = 'json')) define stream CargoStream (weight int); @sink(type='log', prefix='LOGGER') define stream OutputStream(weight int, totalWeight long); @info(name='HelloWorldQuery') from CargoStream select weight, sum(weight) as totalWeight insert into OutputStream;","title":"4. Writing first Siddhi Application"},{"location":"docs/quick-start/#5-testing-siddhi-application","text":"In this section first we will test the logical accuracy of Siddhi query using in-built functions of Siddhi Editor. In a later section we will invoke the HTTP endpoint and perform an end to end test. The Siddhi Editor has in-built support to simulate events. You can do it via the \u201cEvent Simulator\u201d panel at the left of the Siddhi Editor. Before running the event simulation, you should save your HelloWorldApp by browsing to File menu - and clicking Save . To simulate events, click Event Simulator and configure Single Simulation as shown below. Step 1\u200a\u2014\u200aConfigurations: Siddhi App Name\u200a\u2014\u200a \u201cHelloWorldApp\u201d Stream Name\u200a\u2014\u200a \u201cCargoStream\u201d Timestamp\u200a\u2014\u200a(Leave it blank) weight\u200a\u2014\u200a2 (or some integer) Step 2\u200a\u2014\u200aClick \u201cRun\u201d mode and then click \u201cStart and Send\u201d . This starts the Siddhi Application and send the event. If the Siddhi application is successfully started, the following message is printed in the Stream Processor Studio console: HelloWorldApp.siddhi Started Successfully! Step 3\u200a\u2014\u200aClick \u201cSend\u201d and observe the terminal . This will send a new event for each click. You can see a logs containing outputData=[2, 2] and outputData=[2, 4] , etc. You can change the value of the weight and send it to see how the sum of the weight is updated. Bravo! You have successfully completed building and testing your first Siddhi Application!","title":"5. Testing Siddhi Application"},{"location":"docs/quick-start/#6-a-bit-of-stream-processing","text":"This section will improve our Siddhi app to demonstrates how to carry out temporal window processing with Siddhi. Up to this point, we are calculating the sum of weights from the start of the Siddhi app, and now let's improve it to consider only the last three events for the calculation. For this scenario, let's imagine that when we are loading cargo boxes into the ship and we need to keep track of the average weight of the last three loaded boxes so that we can balance the weight across the ship. For this purpose, let's try to find the average weight of last three boxes of each event. For window processing, we need to modify our query as follows: @info(name='HelloWorldQuery') from CargoStream#window.length(3) select weight, sum(weight) as totalWeight, avg(weight) as averageWeight insert into OutputStream; from CargoStream#window.length(3) - Specifies that we need to consider the last three events in a sliding manner. avg(weight) as averageWeight - Specifies calculating the average of events stored in the window and producing the results as \"averageWeight\" (Note: Similarly the sum also calculates the totalWeight based on the last three events). We also need to modify the \"OutputStream\" definition to accommodate the new \"averageWeight\" . define stream OutputStream(weight int, totalWeight long, averageWeight double); The updated Siddhi Application is given below: @App:name(\"HelloWorldApp\") @source(type = 'http', receiver.url = \"http://0.0.0.0:8006/cargo\",@map(type = 'json')) define stream CargoStream (weight int); @sink(type='log', prefix='LOGGER') define stream OutputStream(weight int, totalWeight long, averageWeight double); @info(name='HelloWorldQuery') from CargoStream#window.length(3) select weight, sum(weight) as totalWeight, avg(weight) as averageWeight insert into OutputStream; Now you can send events using the Event Simulator and observe the log to see the sum and average of the weights based on the last three cargo events. In the earlier scenario when the window is not used, the system only stored the running sum in its memory, and it did not store any events. But for length based window processing the system will retain the events that fall into the window to perform aggregation operations such as average, maximum, etc. In this case when the 4 th event arrives, the first event in the window is removed ensuring the memory usage does not grow beyond a specific limit. Note: some window types in Siddhi are even more optimized to perform the operations with minimal or no event retention.","title":"6. A bit of Stream Processing"},{"location":"docs/quick-start/#7-running-siddhi-application-as-a-docker-microservice","text":"In this step we will run above developed Siddhi application as a microservice utilizing Docker. For other available options please refer here . Here we will use siddhi-runner docker distribution. Follow the below steps to obtain the docker. Install docker in your machine and start the daemon ( https://docs.docker.com/install/ ). Pull the latest siddhi-runner image by executing below command. docker pull siddhiio/siddhi-runner-alpine:latest * Navigate to Siddhi Editor and choose File - Export File for download above Siddhi application as a file. * Move downloaded Siddhi file( HelloWorldApp.siddhi ) to a desired location (e.g. /home/me/siddhi-apps ) * Execute below command to start the Siddhi Application as a microservice. docker run -it -p 8006:8006 -v /home/me/siddhi-apps:/apps siddhiio/siddhi-runner-alpine -Dapps=/apps/HelloWorldApp.siddhi Note: Make sure to update the /home/me/siddhi-apps with the folder path you have stored the HelloWorldApp.siddhi app. * Once container is started use below curl command to send events into \"CargoStream\" curl -X POST http://localhost:8006/cargo \\ --header \"Content-Type:application/json\" \\ -d '{\"event\":{\"weight\":2}}' * You will be able to observe outputs via logs as shown below. [2019-04-24 08:54:51,755] INFO {io.siddhi.core.stream.output.sink.LogSink} - LOGGER : Event{timestamp=1556096091751, data=[2, 2, 2.0], isExpired=false} [2019-04-24 08:56:25,307] INFO {io.siddhi.core.stream.output.sink.LogSink} - LOGGER : Event{timestamp=1556096185307, data=[2, 4, 2.0], isExpired=false} To learn more about the Siddhi functionality, see Siddhi Documentation . If you have questions please post them on Stackoverflow with \"Siddhi\" tag.","title":"7. Running Siddhi Application as a Docker microservice"},{"location":"docs/rest-guides/default-ports/","text":"Configuring Default Ports This page describes the default ports that are used for the REST API. Default Ports Port Description Runner Tooling HTTP transport 9090 9390 HTTPS transport 9443 9743 Configuring Ports See Configuring port offset to offset the above ports uniformly. See Configuring Admin REST API to configure HTTP ad HTTPS ports separately","title":"Default Ports"},{"location":"docs/rest-guides/default-ports/#configuring-default-ports","text":"This page describes the default ports that are used for the REST API.","title":"Configuring Default Ports"},{"location":"docs/rest-guides/default-ports/#default-ports","text":"Port Description Runner Tooling HTTP transport 9090 9390 HTTPS transport 9443 9743 Configuring Ports See Configuring port offset to offset the above ports uniformly. See Configuring Admin REST API to configure HTTP ad HTTPS ports separately","title":"Default Ports"},{"location":"docs/rest-guides/health-check-api/","text":"Health Check APIs Only available in runner distribution Fetch Health status of the Siddhi Runner Fetch Health status of the Siddhi Runner Overview Description Fetches the health status of the Siddhi Runner. API Context /health HTTP Method GET Request/Response Format application/json Authentication Not Required Username N/A Password N/A Runtime Runner curl command syntax curl -k -X GET http://localhost:9090/health Sample curl command curl -k -X GET http://localhost:9090/health Sample output {\"status\":\"healthy\"} Response HTTP Status Code Possible codes are 200 and 404. For descriptions of the HTTP status codes, see HTTP Status Codes","title":"Health Check APIs"},{"location":"docs/rest-guides/health-check-api/#health-check-apis","text":"Only available in runner distribution Fetch Health status of the Siddhi Runner","title":"Health Check APIs"},{"location":"docs/rest-guides/health-check-api/#fetch-health-status-of-the-siddhi-runner","text":"","title":"Fetch Health status of the Siddhi Runner"},{"location":"docs/rest-guides/health-check-api/#overview","text":"Description Fetches the health status of the Siddhi Runner. API Context /health HTTP Method GET Request/Response Format application/json Authentication Not Required Username N/A Password N/A Runtime Runner","title":"Overview"},{"location":"docs/rest-guides/health-check-api/#curl-command-syntax","text":"curl -k -X GET http://localhost:9090/health","title":"curl command syntax"},{"location":"docs/rest-guides/health-check-api/#sample-curl-command","text":"curl -k -X GET http://localhost:9090/health","title":"Sample curl command"},{"location":"docs/rest-guides/health-check-api/#sample-output","text":"{\"status\":\"healthy\"}","title":"Sample output"},{"location":"docs/rest-guides/health-check-api/#response","text":"HTTP Status Code Possible codes are 200 and 404. For descriptions of the HTTP status codes, see HTTP Status Codes","title":"Response"},{"location":"docs/rest-guides/http-status-code/","text":"HTTP Status Codes When REST API requests are sent to carryout various actions, various HTTP status codes will be returned based on the state of the action (success or failure) and the HTTP method ( POST, GET, PUT, DELETE ) executed. The following are the definitions of the various HTTP status codes that are returned. Success HTTP status codes Error HTTP status codes Success HTTP status codes Code Code Summary Description 200 Ok HTTP request was successful. The output corresponding to the HTTP request will be returned. Generally used as a response to a successful GET and PUT REST API HTTP methods. 201 Created HTTP request was successfully processed and a new resource was created. Generally used as a response to a successful POST REST API HTTP method. 204 No content HTTP request was successfully processed. No content will be returned. Generally used as a response to a successful DELETE REST API HTTP method. 202 Accepted HTTP request was accepted for processing, but the processing has not been completed. This generally occurs when your successful in trying to undeploy an application. Error HTTP status codes Code Code Summary Description 404 Not found Requested resource not found. Generally used as a response for unsuccessful GET and PUT REST API HTTP methods. 409 Conflict Request could not be processed because of conflict in the request. This generally occurs when you are trying to add a resource that already exists. For example, when trying to add an auto-scaling policy that has an already existing ID. 500 Internal server error Server error occurred.","title":"HTTP Status Codes"},{"location":"docs/rest-guides/http-status-code/#http-status-codes","text":"When REST API requests are sent to carryout various actions, various HTTP status codes will be returned based on the state of the action (success or failure) and the HTTP method ( POST, GET, PUT, DELETE ) executed. The following are the definitions of the various HTTP status codes that are returned. Success HTTP status codes Error HTTP status codes","title":"HTTP Status Codes"},{"location":"docs/rest-guides/http-status-code/#success-http-status-codes","text":"Code Code Summary Description 200 Ok HTTP request was successful. The output corresponding to the HTTP request will be returned. Generally used as a response to a successful GET and PUT REST API HTTP methods. 201 Created HTTP request was successfully processed and a new resource was created. Generally used as a response to a successful POST REST API HTTP method. 204 No content HTTP request was successfully processed. No content will be returned. Generally used as a response to a successful DELETE REST API HTTP method. 202 Accepted HTTP request was accepted for processing, but the processing has not been completed. This generally occurs when your successful in trying to undeploy an application.","title":"Success HTTP status codes"},{"location":"docs/rest-guides/http-status-code/#error-http-status-codes","text":"Code Code Summary Description 404 Not found Requested resource not found. Generally used as a response for unsuccessful GET and PUT REST API HTTP methods. 409 Conflict Request could not be processed because of conflict in the request. This generally occurs when you are trying to add a resource that already exists. For example, when trying to add an auto-scaling policy that has an already existing ID. 500 Internal server error Server error occurred.","title":"Error HTTP status codes"},{"location":"docs/rest-guides/on-demand-query-api/","text":"On-Demand Query APIs Query records through ad-hoc queries Query records through ad-hoc queries Overview On-demand queries provide a way of performing add hock operations on Siddhi tables (stores), named-windows, and named-aggregations. Description Queries records in the Siddhi stores, named windows and named aggregations. API Context /query HTTP Method POST Request/Response Format application/json Authentication Basic Username admin Password admin Runtime Runner curl command syntax curl -X POST https://localhost:9443/query -H \"content-type: application/json\" -u \"admin:admin\" -d '{\"appName\" : \"AggregationTest\", \"query\" : \"from stockAggregation select *\" }' -k Sample curl command for runner distribution curl -X POST https://localhost:9443/query -H \"content-type: application/json\" -u \"admin:admin\" -d '{\"appName\" : \"ProductDetails\", \"query\" : \"from productTable select *\" }' -k Sample curl command for tooling distribution curl -X POST https://localhost:9743/query -H \"content-type: application/json\" -u \"admin:admin\" -d '{\"appName\" : \"ProductDetails\", \"query\" : \"from productTable select *\" }' -k Sample output { \"records\": [ [ \"ID234\", \"Chocolate\" ], [ \"ID235\", \"Ice Cream\" ] ] } Response HTTP Status Code Possible codes are 200 and 404. For descriptions of the HTTP status codes, see HTTP Status Codes","title":"On-Demand Query APIs"},{"location":"docs/rest-guides/on-demand-query-api/#on-demand-query-apis","text":"Query records through ad-hoc queries","title":"On-Demand Query APIs"},{"location":"docs/rest-guides/on-demand-query-api/#query-records-through-ad-hoc-queries","text":"","title":"Query records through ad-hoc queries"},{"location":"docs/rest-guides/on-demand-query-api/#overview","text":"On-demand queries provide a way of performing add hock operations on Siddhi tables (stores), named-windows, and named-aggregations. Description Queries records in the Siddhi stores, named windows and named aggregations. API Context /query HTTP Method POST Request/Response Format application/json Authentication Basic Username admin Password admin Runtime Runner","title":"Overview"},{"location":"docs/rest-guides/on-demand-query-api/#curl-command-syntax","text":"curl -X POST https://localhost:9443/query -H \"content-type: application/json\" -u \"admin:admin\" -d '{\"appName\" : \"AggregationTest\", \"query\" : \"from stockAggregation select *\" }' -k","title":"curl command syntax"},{"location":"docs/rest-guides/on-demand-query-api/#sample-curl-command-for-runner-distribution","text":"curl -X POST https://localhost:9443/query -H \"content-type: application/json\" -u \"admin:admin\" -d '{\"appName\" : \"ProductDetails\", \"query\" : \"from productTable select *\" }' -k","title":"Sample curl command for runner distribution"},{"location":"docs/rest-guides/on-demand-query-api/#sample-curl-command-for-tooling-distribution","text":"curl -X POST https://localhost:9743/query -H \"content-type: application/json\" -u \"admin:admin\" -d '{\"appName\" : \"ProductDetails\", \"query\" : \"from productTable select *\" }' -k","title":"Sample curl command for tooling distribution"},{"location":"docs/rest-guides/on-demand-query-api/#sample-output","text":"{ \"records\": [ [ \"ID234\", \"Chocolate\" ], [ \"ID235\", \"Ice Cream\" ] ] }","title":"Sample output"},{"location":"docs/rest-guides/on-demand-query-api/#response","text":"HTTP Status Code Possible codes are 200 and 404. For descriptions of the HTTP status codes, see HTTP Status Codes","title":"Response"},{"location":"docs/rest-guides/siddhi-app-api/","text":"Siddhi Application Management APIs Only available in runner distribution Creating a Siddhi Application Updating a Siddhi Application Deleting a Siddhi Application Listing all active Siddhi Applications Retrieving a specific Siddhi Application Fetching the status of a Siddhi Application Taking a snapshot of a Siddhi Application Restoring a Siddhi Application via a snapshot Returning real-time statistics of a runner Enabling/disabling runner statistics Returning general details of a runner Returning detailed statistics of all Siddhi Applications Enabling/disabling the statistics of a specific Siddhi Application Enabling/disabling the statistics of all Siddhi Applications Creating a Siddhi application Overview Description Creates a new Siddhi Application. API Context /siddhi-apps HTTP Method POST Request/Response format Request : text/plain Response : application/json Authentication Basic Username admin Password admin Runtime Runner curl command syntax curl -X POST \"https://localhost:9443/siddhi-apps\" -H \"accept: application/json\" -H \"Content-Type: text/plain\" --data-binary @TestSiddhiApp.siddhi -u admin:admin -k Sample curl command curl -X POST \"https://localhost:9443/siddhi-apps\" -H \"accept: application/json\" -H \"Content-Type: text/plain\" --data-binary @TestSiddhiApp.siddhi -u admin:admin -k Sample output The response for the sample curl command given above can be one of the following. If API request is valid and there is no existing Siddhi application with the given name, a response similar to the following is generated with response code 201. This response contains a location header with the path of the newly created file from product root home. { \"type\":\"success\", \"message\":\"Siddhi App saved succesfully and will be deployed in next deployment cycle\" } If the API request is valid, but a Siddhi application with the given name already exists, a response similar to the following is generated with response code 409. { \"type\": \"conflict\", \"message\": \"There is a Siddhi App already exists with same name\" } If the API request is invalid due to invalid content in the Siddhi queries you have included in the request body, a response similar to the following is generated is generated with response code 400. { \"code\": 800101, \"type\": \"validation error\", \"message\": \"You have an error in your SiddhiQL at line 8:8, missing INTO at 'BarStream'\" } If the API request is valid, but an exception occured during file processing or saving, the following response is generated with response code 500. { \"code\": 800102, \"type\": \"file processing error\", \"message\": error-message } Response HTTP Status Code Possible codes are 201, 409, 400, and 500. For descriptions of the HTTP status codes, see HTTP Status Codes Updating a Siddhi Application Overview Description Updates a Siddhi Application. API Context /siddhi-apps HTTP Method PUT Request/Response format Request : text/plain Response : application/json Authentication Basic Username admin Password admin Runtime Runner curl command syntax curl -X PUT \"http://localhost:9090/siddhi-apps\" -H \"accept: application/json\" -H \"Content-Type: text/plain\" --data-binary @ SIDDHI_APPLICATION_NAME .siddhi -u admin:admin -k Sample curl command curl -X PUT \"https://localhost:9443/siddhi-apps\" -H \"accept: application/json\" -H \"Content-Type: text/plain\" --data-binary @TestSiddhiApp.siddhi -u admin:admin -k Sample output If the API request is invalid due to invalid content in the Siddhi query, a response similar to the following is returned with response code 400. { \"code\": 800101, \"type\": \"validation error\", \"message\": \"You have an error in your SiddhiQL at line 8:8, missing INTO at 'BarStream'\" } If the API request is valid, but an exception occured when saving or processing files, a response similar to the following is returned with response code 500. { \"code\": 800102, \"type\": \"file processing error\", \"message\": error-message } Response HTTP Status Code Possible codes are 200, 201, 400, and 500. For descriptions of the HTTP status codes, see HTTP Status Codes Deleting a Siddhi application Overview Description Sends the name of a Siddhi application as a URL parameter. API Context /siddhi-apps/{appName} HTTP Method DELETE Request/Response format application/json Authentication Basic Username admin Password admin Runtime Runner Parameter Description Parameter Description {appName} The name of the Siddhi application to be deleted. curl command syntax curl -X DELETE \"http://localhost:9090/siddhi-apps/{app-name}\" -H \"accept: application/json\" -u admin:admin -k Sample curl command curl -X DELETE \"https://localhost:9443/siddhi-apps/TestSiddhiApp\" -H \"accept: application/json\" -u admin:admin -k Sample output The response for the sample curl command given above can be one of the following: If the API request is valid and a Siddhi application with the given name exists, a received with response code 200. If the API request is valid, but a Siddhi application with the given name is not deployed, the following response is received with response code 404. { \"type\": \"not found\", \"message\": \"There is no Siddhi App exist with provided name : TestExecutionPlan1\" } If the API request is valid, but an exception occured when deleting the given Siddhi application, the following response is received with response code 500. { \"code\": 800102, \"type\": \"file processing error\", \"message\": error-message } If the API request is valid, but there are restricted characters in the given Siddhi application name, the following response is received with response code 400. { \"code\": 800101, \"type\": \"validation error\", \"message\": \"File name contains restricted path elements . : ../../siddhiApp2'\" } Response HTTP Status Code Possible codes are 200, 404, 400, and 500. For descriptions of the HTTP status codes, see HTTP Status Codes Listing all active Siddhi applications Overview Description Lists all the currently active Siddhi applications. If the isActive=true parameter is set, all the active Siddhi Applications are listed. If not, all the inactive Siddhi applications are listed. API Context /siddhi-apps HTTP Method GET Request/Response format Request content type : any Response content type : application/json Authentication Basic Username admin Password admin Runtime Runner curl command syntax curl -X GET \"http://localhost:9090/siddhi-apps\" -H \"accept: application/json\" -u admin:admin -k Sample curl command curl -X GET \"https://localhost:9443/siddhi-apps?isActive=true\" -H \"accept: application/json\" -u admin:admin -k Sample output Possible responses are as follows: If the API request is valid and there are Siddhi applications deployed in your SP setup, a response similar to the following is returned with response code 200. [\"TestExecutionPlan3\", \"TestExecutionPlan4\"] If the API request is valid, there are Siddhi applications deployed in your SP setup, and a query parameter is defined in the request, a response similar to the following is returned with response code 200. This response only contains Siddhi applications that are active. If these conditions are met, but the isActive parameter is set to false , the response contains only inactive Siddhi applications. [\"TestExecutionPlan3\"] If the API request is valid, but there are no Siddhi applications deployed in your SP setup, the following response is returned. [] Response HTTP Status Code 200. For descriptions of the HTTP status codes, see HTTP Status Codes Retrieving a specific Siddhi application Overview Description Retrieves the given Siddhi application. API Context /siddhi-apps/{appName} HTTP Method GET Request/Response format application/json Authentication Basic Username admin Password admin Runtime Runner Parameter Description Parameter Description {appName} The name of the Siddhi application to be retrieved. curl command syntax curl -X GET \"http://localhost:9090/siddhi-apps/{app-name}\" -H \"accept: application/json\" -u admin:admin -k Sample curl command curl -X GET \"https://localhost:9443/siddhi-apps/SiddhiTestApp\" -H \"accept: application/json\" -u admin:admin -k Sample output The possible outputs are as follows: If the API request is valid and a Siddhi application of the given name exists, a response similar to the following is returned with response code 200. { \"content\": \"\\n@Plan:name('TestExecutionPlan')\\ndefine stream FooStream (symbol string, price float, volume long);\\n\\n@source(type='inMemory', topic='symbol', @map(type='passThrough'))Define stream BarStream (symbol string, price float, volume long);\\n\\nfrom FooStream\\nselect symbol, price, volume\\ninsert into BarStream;\" } If the API request is valid, but a Siddhi application of the given name is not deployed, a response similar to the following is returned with response code 404. { \"type\": \"not found\", \"message\": \"There is no Siddhi App exist with provided name : TestExecutionPlan1\" } Response HTTP Status Code Possible codes are 200 and 404. For descriptions of the HTTP status codes, see HTTP Status Codes Fetching the status of a Siddhi Application Overview Description This fetches the status of the specified Siddhi application API Context /siddhi-apps/{appName}/status HTTP Method GET Request/Response format application/json Authentication Basic Username admin Password admin Runtime Runner Parameter Description Parameter Description {appName} The name of the Siddhi application of which the status needs to be fetched. curl command syntax curl -X GET \"http://localhost:9090/siddhi-apps/{app-file-name}/status\" -H \"accept: application/json\" -u admin:admin -k Sample curl command curl -X GET \"https://localhost:9443/siddhi-apps/TestSiddhiApp/status\" -H \"accept: application/json\" -u admin:admin -k Sample output If the Siddhi application is active, the following is returned with response code 200. {\"status\":\"active\"} If the Siddhi application is inactive, the following is returned with response code 200. {\"status\":\"inactive\"} If the Siddhi application does not exist, but the REST API call is valid, the following is returned with the response code 404. { \"type\": \"not found\", \"message\": \"There is no Siddhi App exist with provided name : TestExecutionPlan1\" } Response HTTP Status Code Possible codes are 200 and 404. For descriptions of the HTTP status codes, see HTTP Status Codes Taking a snapshot of a Siddhi Application Overview Description This takes a snapshot of the specific Siddhi application. API Context /siddhi-apps/{appName}/backup HTTP Method POST Request/Response format application/json Authentication Basic Username admin Password admin Runtime Runner Parameter Description Parameter Description {appName} The name of the Siddhi application of which a snapshot needs to be taken. curl command syntax curl -X POST \"http://localhost:9090/siddhi-apps/{appName}/backup\" -H \"accept: application/json\" -u admin:admin -k Sample curl command curl -X POST \"https://localhost:9443/siddhi-apps/TestSiddhiApp/backup\" -H \"accept: application/json\" -u admin:admin -k Sample output The output can be one of the following: If the API request is valid and a Siddhi application exists with the given name, an output similar to the following (i.e., with the snapshot revision number) is returned with response code 201. {\"revision\": \"89489242494242\"} If the API request is valid, but no Siddhi application with the given name is deployed, an output similar to the following is returned with response code 404. { \"type\": \"not found\", \"message\": \"There is no Siddhi App exist with provided name : TestExecutionPlan1\" } If the API request is valid, but an exception has occured when backing up the state at Siddhi level, an output similar to the following is returned with response code 500. { \"code\": 800102, \"type\": \"file processing error\", \"message\": error-message } Response HTTP Status Code Possible codes are 201, 404, and 500. For descriptions of the HTTP status codes, see HTTP Status Codes Restoring a Siddhi Application via a snapshot !!! info \"In order to call this API, you need to have already taken a snapshot of the Siddhi application to be restored. For more information about the API via which the snapshot is taken, see Taking a snapshot of a Siddhi Application . Overview Description This restores a Siddhi application using a snapshot of the same that you have previously taken. API Context To restore without considering the version : /siddhi-apps/{appName}/restore To restore a specific version : /siddhi-apps/{appName}/restore?version= HTTP Method POST Request/Response format application/json Authentication Basic Username admin Password admin Runtime Runner Parameter Description Parameter Description {appName} The name of the Siddhi application that needs to be restored. curl command syntax curl -X POST \"http://localhost:9090/siddhi-apps/{appName}/restore\" -H \"accept: application/json\" -u admin:admin -k Sample curl command curl -X POST \"https://localhost:9443/siddhi-apps/TestSiddhiApp/restore?revision=1514981290838_TestSiddhiApp\" -H \"accept: application/json\" -u admin:admin -k Sample output The above sample curl command can generate either one of the following responses: If the API request is valid, a Siddhi application with the given name exists, and no revision information is passed as a query parameter, the following response is returned with response code 200. { \"type\": \"success\", \"message\": \"State restored to last revision for Siddhi App :TestExecutionPlan\" } If the API request is valid, a Siddhi application with the given name exists, and revision information is passed as a query parameter, the following response is returned with response code 200. In this scenario, the Siddhi snapshot is created in the file system. { \"type\": \"success\", \"message\": \"State restored to revision 1234563 for Siddhi App :TestExecutionPlan\" } If the API request is valid, but no Siddhi application is deployed with the given name, the following response is returned with response code 404. { \"type\": \"not found\", \"message\": \"There is no Siddhi App exist with provided name : TestExecutionPlan1\" } If the API request is valid, but an exception occured when restoring the state at Siddhi level, the following response is returned with response code 500. { \"code\": 800102, \"type\": \"file processing error\", \"message\": error-message } Response HTTP Status Code Possible codes are 200, 404, and 500. For descriptions of the HTTP status codes, see HTTP Status Codes Returning real-time statistics of a runner Overview Description Returns the real-time statistics of a runner. API Context /statistics HTTP Method GET Request/Response format application/json Authentication Basic Username admin Password admin Runtime Runner Parameter Description curl command syntax curl -X GET \"https://localhost:9443/statistics\" -H \"accept: application/json\" -u admin:admin -k Sample curl command curl -X GET \"https://localhost:9443/statistics\" -H \"accept: application/json\" -u admin:admin -k Sample output Response HTTP Status Code Possible codes are 200 and 404. For descriptions of the HTTP status codes, see HTTP Status Codes Enabling/disabling runner statistics Overview Description Enables/diables generating statistics for runner nodes. API Context /statistics HTTP Method PUT Request/Response format application/json Authentication Basic Username admin Password admin Runtime Runner Parameter Description curl command syntax curl -X PUT \"https://localhost:9443/statistics\" -H \"accept: application/json\" -H \"Content-Type: application/json\" -d \"{\u201cstatsEnable\u201d:\u201dtrue\u201d}\" -u admin:admin -k Sample curl command curl -X PUT \"https://localhost:9443/statistics\" -H \"accept: application/json\" -H \"Content-Type: application/json\" -d \"{\u201cstatsEnable\u201d:\u201dtrue\u201d}\" -u admin:admin -k Sample output Response HTTP Status Code Possible codes are 200 and 404. For descriptions of the HTTP status codes, see HTTP Status Codes Returning general details of a runner Overview Description Returns general details of a runner. API Context /system-details HTTP Method GET Request/Response format application/json Authentication Basic Username admin Password admin Runtime Runner Parameter Description curl command syntax curl -X GET \"https://localhost:9443/system-details\" -H \"accept: application/json\" -u admin:admin -k Sample curl command curl -X GET \"https://localhost:9443/system-details\" -H \"accept: application/json\" -u admin:admin -k Sample output Response HTTP Status Code Possible codes are 200 and 404. For descriptions of the HTTP status codes, see HTTP Status Codes Returning detailed statistics of all Siddhi applications Overview Description Returns the detailed statistics of all the Siddhi applications currently deployed in the SP setup. API Context /siddhi-apps/statistics HTTP Method GET Request/Response format application/json Authentication Basic Username admin Password admin Runtime Runner Parameter Description curl command syntax curl -X GET \"https://localhost:9443/siddhi-apps/statistics\" -H \"accept: application/json\" -u admin:admin -k Sample curl command curl -X GET \"https://localhost:9443/siddhi-apps/statistics\" -H \"accept: application/json\" -u admin:admin -k Sample output Response HTTP Status Code Possible codes are 200 and 404. For descriptions of the HTTP status codes, see HTTP Status Codes Enabling/disabling the statistics of a specific Siddhi application Overview Description Enables/disables statistics for a specified Siddhi application. API Context /siddhi-apps/{appName}/statistics HTTP Method PUT Request/Response format application/json Authentication Basic Username admin Password admin Runtime Runner Parameter Description Parameter Description appName The name of the Siddhi application for which the Siddhi applications need to be enabled/disabled. curl command syntax curl -X PUT \"https://localhost:9443/siddhi-apps/{appName}/statistics\" -H \"accept: application/json\" -H \"Content-Type: application/json\" -d \"{\u201cstatsEnable\u201d:\u201dtrue\u201d}\" -u admin:admin -k Sample curl command curl -X PUT \"https://localhost:9443/siddhi-apps/TestSiddhiApp/statistics\" -H \"accept: application/json\" -H \"Content-Type: application/json\" -d \"{\u201cstatsEnable\u201d:\u201dtrue\u201d}\" -u admin:admin -k Sample output Response HTTP Status Code Possible codes are 200 and 404. For descriptions of the HTTP status codes, see HTTP Status Codes Enabling/disabling the statistics of all Siddhi applications Overview Description Enables/disables statistics for all the Siddhi applications. API Context /siddhi-apps/statistics HTTP Method PUT Request/Response format application/json Authentication Basic Username admin Password admin Runtime Runner Parameter Description curl command syntax curl -X PUT \"https://localhost:9443/siddhi-apps/statistics\" -H \"accept: application/json\" -H \"Content-Type: application/json\" -d \"{\u201cstatsEnable\u201d:\u201dtrue\u201d}\" -u admin:admin -k Sample curl command curl -X PUT \"https://localhost:9443/siddhi-apps/statistics\" -H \"accept: application/json\" -H \"Content-Type: application/json\" -d \"{\u201cstatsEnable\u201d:\u201dtrue\u201d}\" -u admin:admin -k Sample output Response HTTP Status Code Possible codes are 200 and 404. For descriptions of the HTTP status codes, see HTTP Status Codes","title":"Siddhi App APIs"},{"location":"docs/rest-guides/siddhi-app-api/#siddhi-application-management-apis","text":"Only available in runner distribution Creating a Siddhi Application Updating a Siddhi Application Deleting a Siddhi Application Listing all active Siddhi Applications Retrieving a specific Siddhi Application Fetching the status of a Siddhi Application Taking a snapshot of a Siddhi Application Restoring a Siddhi Application via a snapshot Returning real-time statistics of a runner Enabling/disabling runner statistics Returning general details of a runner Returning detailed statistics of all Siddhi Applications Enabling/disabling the statistics of a specific Siddhi Application Enabling/disabling the statistics of all Siddhi Applications","title":"Siddhi Application Management APIs"},{"location":"docs/rest-guides/siddhi-app-api/#creating-a-siddhi-application","text":"","title":"Creating a Siddhi application"},{"location":"docs/rest-guides/siddhi-app-api/#overview","text":"Description Creates a new Siddhi Application. API Context /siddhi-apps HTTP Method POST Request/Response format Request : text/plain Response : application/json Authentication Basic Username admin Password admin Runtime Runner","title":"Overview"},{"location":"docs/rest-guides/siddhi-app-api/#curl-command-syntax","text":"curl -X POST \"https://localhost:9443/siddhi-apps\" -H \"accept: application/json\" -H \"Content-Type: text/plain\" --data-binary @TestSiddhiApp.siddhi -u admin:admin -k","title":"curl command syntax"},{"location":"docs/rest-guides/siddhi-app-api/#sample-curl-command","text":"curl -X POST \"https://localhost:9443/siddhi-apps\" -H \"accept: application/json\" -H \"Content-Type: text/plain\" --data-binary @TestSiddhiApp.siddhi -u admin:admin -k","title":"Sample curl command"},{"location":"docs/rest-guides/siddhi-app-api/#sample-output","text":"The response for the sample curl command given above can be one of the following. If API request is valid and there is no existing Siddhi application with the given name, a response similar to the following is generated with response code 201. This response contains a location header with the path of the newly created file from product root home. { \"type\":\"success\", \"message\":\"Siddhi App saved succesfully and will be deployed in next deployment cycle\" } If the API request is valid, but a Siddhi application with the given name already exists, a response similar to the following is generated with response code 409. { \"type\": \"conflict\", \"message\": \"There is a Siddhi App already exists with same name\" } If the API request is invalid due to invalid content in the Siddhi queries you have included in the request body, a response similar to the following is generated is generated with response code 400. { \"code\": 800101, \"type\": \"validation error\", \"message\": \"You have an error in your SiddhiQL at line 8:8, missing INTO at 'BarStream'\" } If the API request is valid, but an exception occured during file processing or saving, the following response is generated with response code 500. { \"code\": 800102, \"type\": \"file processing error\", \"message\": error-message }","title":"Sample output"},{"location":"docs/rest-guides/siddhi-app-api/#response","text":"HTTP Status Code Possible codes are 201, 409, 400, and 500. For descriptions of the HTTP status codes, see HTTP Status Codes","title":"Response"},{"location":"docs/rest-guides/siddhi-app-api/#updating-a-siddhi-application","text":"","title":"Updating a Siddhi Application"},{"location":"docs/rest-guides/siddhi-app-api/#overview_1","text":"Description Updates a Siddhi Application. API Context /siddhi-apps HTTP Method PUT Request/Response format Request : text/plain Response : application/json Authentication Basic Username admin Password admin Runtime Runner","title":"Overview"},{"location":"docs/rest-guides/siddhi-app-api/#curl-command-syntax_1","text":"curl -X PUT \"http://localhost:9090/siddhi-apps\" -H \"accept: application/json\" -H \"Content-Type: text/plain\" --data-binary @ SIDDHI_APPLICATION_NAME .siddhi -u admin:admin -k","title":"curl command syntax"},{"location":"docs/rest-guides/siddhi-app-api/#sample-curl-command_1","text":"curl -X PUT \"https://localhost:9443/siddhi-apps\" -H \"accept: application/json\" -H \"Content-Type: text/plain\" --data-binary @TestSiddhiApp.siddhi -u admin:admin -k","title":"Sample curl command"},{"location":"docs/rest-guides/siddhi-app-api/#sample-output_1","text":"If the API request is invalid due to invalid content in the Siddhi query, a response similar to the following is returned with response code 400. { \"code\": 800101, \"type\": \"validation error\", \"message\": \"You have an error in your SiddhiQL at line 8:8, missing INTO at 'BarStream'\" } If the API request is valid, but an exception occured when saving or processing files, a response similar to the following is returned with response code 500. { \"code\": 800102, \"type\": \"file processing error\", \"message\": error-message }","title":"Sample output"},{"location":"docs/rest-guides/siddhi-app-api/#response_1","text":"HTTP Status Code Possible codes are 200, 201, 400, and 500. For descriptions of the HTTP status codes, see HTTP Status Codes","title":"Response"},{"location":"docs/rest-guides/siddhi-app-api/#deleting-a-siddhi-application","text":"","title":"Deleting a Siddhi application"},{"location":"docs/rest-guides/siddhi-app-api/#overview_2","text":"Description Sends the name of a Siddhi application as a URL parameter. API Context /siddhi-apps/{appName} HTTP Method DELETE Request/Response format application/json Authentication Basic Username admin Password admin Runtime Runner","title":"Overview"},{"location":"docs/rest-guides/siddhi-app-api/#parameter-description","text":"Parameter Description {appName} The name of the Siddhi application to be deleted.","title":"Parameter Description"},{"location":"docs/rest-guides/siddhi-app-api/#curl-command-syntax_2","text":"curl -X DELETE \"http://localhost:9090/siddhi-apps/{app-name}\" -H \"accept: application/json\" -u admin:admin -k","title":"curl command syntax"},{"location":"docs/rest-guides/siddhi-app-api/#sample-curl-command_2","text":"curl -X DELETE \"https://localhost:9443/siddhi-apps/TestSiddhiApp\" -H \"accept: application/json\" -u admin:admin -k","title":"Sample curl command"},{"location":"docs/rest-guides/siddhi-app-api/#sample-output_2","text":"The response for the sample curl command given above can be one of the following: If the API request is valid and a Siddhi application with the given name exists, a received with response code 200. If the API request is valid, but a Siddhi application with the given name is not deployed, the following response is received with response code 404. { \"type\": \"not found\", \"message\": \"There is no Siddhi App exist with provided name : TestExecutionPlan1\" } If the API request is valid, but an exception occured when deleting the given Siddhi application, the following response is received with response code 500. { \"code\": 800102, \"type\": \"file processing error\", \"message\": error-message } If the API request is valid, but there are restricted characters in the given Siddhi application name, the following response is received with response code 400. { \"code\": 800101, \"type\": \"validation error\", \"message\": \"File name contains restricted path elements . : ../../siddhiApp2'\" }","title":"Sample output"},{"location":"docs/rest-guides/siddhi-app-api/#response_2","text":"HTTP Status Code Possible codes are 200, 404, 400, and 500. For descriptions of the HTTP status codes, see HTTP Status Codes","title":"Response"},{"location":"docs/rest-guides/siddhi-app-api/#listing-all-active-siddhi-applications","text":"","title":"Listing all active Siddhi applications"},{"location":"docs/rest-guides/siddhi-app-api/#overview_3","text":"Description Lists all the currently active Siddhi applications. If the isActive=true parameter is set, all the active Siddhi Applications are listed. If not, all the inactive Siddhi applications are listed. API Context /siddhi-apps HTTP Method GET Request/Response format Request content type : any Response content type : application/json Authentication Basic Username admin Password admin Runtime Runner","title":"Overview"},{"location":"docs/rest-guides/siddhi-app-api/#curl-command-syntax_3","text":"curl -X GET \"http://localhost:9090/siddhi-apps\" -H \"accept: application/json\" -u admin:admin -k","title":"curl command syntax"},{"location":"docs/rest-guides/siddhi-app-api/#sample-curl-command_3","text":"curl -X GET \"https://localhost:9443/siddhi-apps?isActive=true\" -H \"accept: application/json\" -u admin:admin -k","title":"Sample curl command"},{"location":"docs/rest-guides/siddhi-app-api/#sample-output_3","text":"Possible responses are as follows: If the API request is valid and there are Siddhi applications deployed in your SP setup, a response similar to the following is returned with response code 200. [\"TestExecutionPlan3\", \"TestExecutionPlan4\"] If the API request is valid, there are Siddhi applications deployed in your SP setup, and a query parameter is defined in the request, a response similar to the following is returned with response code 200. This response only contains Siddhi applications that are active. If these conditions are met, but the isActive parameter is set to false , the response contains only inactive Siddhi applications. [\"TestExecutionPlan3\"] If the API request is valid, but there are no Siddhi applications deployed in your SP setup, the following response is returned. []","title":"Sample output"},{"location":"docs/rest-guides/siddhi-app-api/#response_3","text":"HTTP Status Code 200. For descriptions of the HTTP status codes, see HTTP Status Codes","title":"Response"},{"location":"docs/rest-guides/siddhi-app-api/#retrieving-a-specific-siddhi-application","text":"","title":"Retrieving a specific Siddhi application"},{"location":"docs/rest-guides/siddhi-app-api/#overview_4","text":"Description Retrieves the given Siddhi application. API Context /siddhi-apps/{appName} HTTP Method GET Request/Response format application/json Authentication Basic Username admin Password admin Runtime Runner","title":"Overview"},{"location":"docs/rest-guides/siddhi-app-api/#parameter-description_1","text":"Parameter Description {appName} The name of the Siddhi application to be retrieved.","title":"Parameter Description"},{"location":"docs/rest-guides/siddhi-app-api/#curl-command-syntax_4","text":"curl -X GET \"http://localhost:9090/siddhi-apps/{app-name}\" -H \"accept: application/json\" -u admin:admin -k","title":"curl command syntax"},{"location":"docs/rest-guides/siddhi-app-api/#sample-curl-command_4","text":"curl -X GET \"https://localhost:9443/siddhi-apps/SiddhiTestApp\" -H \"accept: application/json\" -u admin:admin -k","title":"Sample curl command"},{"location":"docs/rest-guides/siddhi-app-api/#sample-output_4","text":"The possible outputs are as follows: If the API request is valid and a Siddhi application of the given name exists, a response similar to the following is returned with response code 200. { \"content\": \"\\n@Plan:name('TestExecutionPlan')\\ndefine stream FooStream (symbol string, price float, volume long);\\n\\n@source(type='inMemory', topic='symbol', @map(type='passThrough'))Define stream BarStream (symbol string, price float, volume long);\\n\\nfrom FooStream\\nselect symbol, price, volume\\ninsert into BarStream;\" } If the API request is valid, but a Siddhi application of the given name is not deployed, a response similar to the following is returned with response code 404. { \"type\": \"not found\", \"message\": \"There is no Siddhi App exist with provided name : TestExecutionPlan1\" }","title":"Sample output"},{"location":"docs/rest-guides/siddhi-app-api/#response_4","text":"HTTP Status Code Possible codes are 200 and 404. For descriptions of the HTTP status codes, see HTTP Status Codes","title":"Response"},{"location":"docs/rest-guides/siddhi-app-api/#fetching-the-status-of-a-siddhi-application","text":"","title":"Fetching the status of a Siddhi Application"},{"location":"docs/rest-guides/siddhi-app-api/#overview_5","text":"Description This fetches the status of the specified Siddhi application API Context /siddhi-apps/{appName}/status HTTP Method GET Request/Response format application/json Authentication Basic Username admin Password admin Runtime Runner","title":"Overview"},{"location":"docs/rest-guides/siddhi-app-api/#parameter-description_2","text":"Parameter Description {appName} The name of the Siddhi application of which the status needs to be fetched.","title":"Parameter Description"},{"location":"docs/rest-guides/siddhi-app-api/#curl-command-syntax_5","text":"curl -X GET \"http://localhost:9090/siddhi-apps/{app-file-name}/status\" -H \"accept: application/json\" -u admin:admin -k","title":"curl command syntax"},{"location":"docs/rest-guides/siddhi-app-api/#sample-curl-command_5","text":"curl -X GET \"https://localhost:9443/siddhi-apps/TestSiddhiApp/status\" -H \"accept: application/json\" -u admin:admin -k","title":"Sample curl command"},{"location":"docs/rest-guides/siddhi-app-api/#sample-output_5","text":"If the Siddhi application is active, the following is returned with response code 200. {\"status\":\"active\"} If the Siddhi application is inactive, the following is returned with response code 200. {\"status\":\"inactive\"} If the Siddhi application does not exist, but the REST API call is valid, the following is returned with the response code 404. { \"type\": \"not found\", \"message\": \"There is no Siddhi App exist with provided name : TestExecutionPlan1\" }","title":"Sample output"},{"location":"docs/rest-guides/siddhi-app-api/#response_5","text":"HTTP Status Code Possible codes are 200 and 404. For descriptions of the HTTP status codes, see HTTP Status Codes","title":"Response"},{"location":"docs/rest-guides/siddhi-app-api/#taking-a-snapshot-of-a-siddhi-application","text":"","title":"Taking a snapshot of a Siddhi Application"},{"location":"docs/rest-guides/siddhi-app-api/#overview_6","text":"Description This takes a snapshot of the specific Siddhi application. API Context /siddhi-apps/{appName}/backup HTTP Method POST Request/Response format application/json Authentication Basic Username admin Password admin Runtime Runner","title":"Overview"},{"location":"docs/rest-guides/siddhi-app-api/#parameter-description_3","text":"Parameter Description {appName} The name of the Siddhi application of which a snapshot needs to be taken.","title":"Parameter Description"},{"location":"docs/rest-guides/siddhi-app-api/#curl-command-syntax_6","text":"curl -X POST \"http://localhost:9090/siddhi-apps/{appName}/backup\" -H \"accept: application/json\" -u admin:admin -k","title":"curl command syntax"},{"location":"docs/rest-guides/siddhi-app-api/#sample-curl-command_6","text":"curl -X POST \"https://localhost:9443/siddhi-apps/TestSiddhiApp/backup\" -H \"accept: application/json\" -u admin:admin -k","title":"Sample curl command"},{"location":"docs/rest-guides/siddhi-app-api/#sample-output_6","text":"The output can be one of the following: If the API request is valid and a Siddhi application exists with the given name, an output similar to the following (i.e., with the snapshot revision number) is returned with response code 201. {\"revision\": \"89489242494242\"} If the API request is valid, but no Siddhi application with the given name is deployed, an output similar to the following is returned with response code 404. { \"type\": \"not found\", \"message\": \"There is no Siddhi App exist with provided name : TestExecutionPlan1\" } If the API request is valid, but an exception has occured when backing up the state at Siddhi level, an output similar to the following is returned with response code 500. { \"code\": 800102, \"type\": \"file processing error\", \"message\": error-message }","title":"Sample output"},{"location":"docs/rest-guides/siddhi-app-api/#response_6","text":"HTTP Status Code Possible codes are 201, 404, and 500. For descriptions of the HTTP status codes, see HTTP Status Codes","title":"Response"},{"location":"docs/rest-guides/siddhi-app-api/#restoring-a-siddhi-application-via-a-snapshot","text":"!!! info \"In order to call this API, you need to have already taken a snapshot of the Siddhi application to be restored. For more information about the API via which the snapshot is taken, see Taking a snapshot of a Siddhi Application .","title":"Restoring a\u00a0Siddhi Application via a snapshot"},{"location":"docs/rest-guides/siddhi-app-api/#overview_7","text":"Description This restores a Siddhi application using a snapshot of the same that you have previously taken. API Context To restore without considering the version : /siddhi-apps/{appName}/restore To restore a specific version : /siddhi-apps/{appName}/restore?version= HTTP Method POST Request/Response format application/json Authentication Basic Username admin Password admin Runtime Runner","title":"Overview"},{"location":"docs/rest-guides/siddhi-app-api/#parameter-description_4","text":"Parameter Description {appName} The name of the Siddhi application that needs to be restored.","title":"Parameter Description"},{"location":"docs/rest-guides/siddhi-app-api/#curl-command-syntax_7","text":"curl -X POST \"http://localhost:9090/siddhi-apps/{appName}/restore\" -H \"accept: application/json\" -u admin:admin -k","title":"curl command syntax"},{"location":"docs/rest-guides/siddhi-app-api/#sample-curl-command_7","text":"curl -X POST \"https://localhost:9443/siddhi-apps/TestSiddhiApp/restore?revision=1514981290838_TestSiddhiApp\" -H \"accept: application/json\" -u admin:admin -k","title":"Sample curl command"},{"location":"docs/rest-guides/siddhi-app-api/#sample-output_7","text":"The above sample curl command can generate either one of the following responses: If the API request is valid, a Siddhi application with the given name exists, and no revision information is passed as a query parameter, the following response is returned with response code 200. { \"type\": \"success\", \"message\": \"State restored to last revision for Siddhi App :TestExecutionPlan\" } If the API request is valid, a Siddhi application with the given name exists, and revision information is passed as a query parameter, the following response is returned with response code 200. In this scenario, the Siddhi snapshot is created in the file system. { \"type\": \"success\", \"message\": \"State restored to revision 1234563 for Siddhi App :TestExecutionPlan\" } If the API request is valid, but no Siddhi application is deployed with the given name, the following response is returned with response code 404. { \"type\": \"not found\", \"message\": \"There is no Siddhi App exist with provided name : TestExecutionPlan1\" } If the API request is valid, but an exception occured when restoring the state at Siddhi level, the following response is returned with response code 500. { \"code\": 800102, \"type\": \"file processing error\", \"message\": error-message }","title":"Sample output"},{"location":"docs/rest-guides/siddhi-app-api/#response_7","text":"HTTP Status Code Possible codes are 200, 404, and 500. For descriptions of the HTTP status codes, see HTTP Status Codes","title":"Response"},{"location":"docs/rest-guides/siddhi-app-api/#returning-real-time-statistics-of-a-runner","text":"","title":"Returning real-time statistics of a runner"},{"location":"docs/rest-guides/siddhi-app-api/#overview_8","text":"Description Returns the real-time statistics of a runner. API Context /statistics HTTP Method GET Request/Response format application/json Authentication Basic Username admin Password admin Runtime Runner","title":"Overview"},{"location":"docs/rest-guides/siddhi-app-api/#parameter-description_5","text":"","title":"Parameter Description"},{"location":"docs/rest-guides/siddhi-app-api/#curl-command-syntax_8","text":"curl -X GET \"https://localhost:9443/statistics\" -H \"accept: application/json\" -u admin:admin -k","title":"curl command syntax"},{"location":"docs/rest-guides/siddhi-app-api/#sample-curl-command_8","text":"curl -X GET \"https://localhost:9443/statistics\" -H \"accept: application/json\" -u admin:admin -k","title":"Sample curl command"},{"location":"docs/rest-guides/siddhi-app-api/#sample-output_8","text":"","title":"Sample output"},{"location":"docs/rest-guides/siddhi-app-api/#response_8","text":"HTTP Status Code Possible codes are 200 and 404. For descriptions of the HTTP status codes, see HTTP Status Codes","title":"Response"},{"location":"docs/rest-guides/siddhi-app-api/#enablingdisabling-runner-statistics","text":"","title":"Enabling/disabling runner statistics"},{"location":"docs/rest-guides/siddhi-app-api/#overview_9","text":"Description Enables/diables generating statistics for runner nodes. API Context /statistics HTTP Method PUT Request/Response format application/json Authentication Basic Username admin Password admin Runtime Runner","title":"Overview"},{"location":"docs/rest-guides/siddhi-app-api/#parameter-description_6","text":"","title":"Parameter Description"},{"location":"docs/rest-guides/siddhi-app-api/#curl-command-syntax_9","text":"curl -X PUT \"https://localhost:9443/statistics\" -H \"accept: application/json\" -H \"Content-Type: application/json\" -d \"{\u201cstatsEnable\u201d:\u201dtrue\u201d}\" -u admin:admin -k","title":"curl command syntax"},{"location":"docs/rest-guides/siddhi-app-api/#sample-curl-command_9","text":"curl -X PUT \"https://localhost:9443/statistics\" -H \"accept: application/json\" -H \"Content-Type: application/json\" -d \"{\u201cstatsEnable\u201d:\u201dtrue\u201d}\" -u admin:admin -k","title":"Sample curl command"},{"location":"docs/rest-guides/siddhi-app-api/#sample-output_9","text":"","title":"Sample output"},{"location":"docs/rest-guides/siddhi-app-api/#response_9","text":"HTTP Status Code Possible codes are 200 and 404. For descriptions of the HTTP status codes, see HTTP Status Codes","title":"Response"},{"location":"docs/rest-guides/siddhi-app-api/#returning-general-details-of-a-runner","text":"","title":"Returning general details of a runner"},{"location":"docs/rest-guides/siddhi-app-api/#overview_10","text":"Description Returns general details of a runner. API Context /system-details HTTP Method GET Request/Response format application/json Authentication Basic Username admin Password admin Runtime Runner","title":"Overview"},{"location":"docs/rest-guides/siddhi-app-api/#parameter-description_7","text":"","title":"Parameter Description"},{"location":"docs/rest-guides/siddhi-app-api/#curl-command-syntax_10","text":"curl -X GET \"https://localhost:9443/system-details\" -H \"accept: application/json\" -u admin:admin -k","title":"curl command syntax"},{"location":"docs/rest-guides/siddhi-app-api/#sample-curl-command_10","text":"curl -X GET \"https://localhost:9443/system-details\" -H \"accept: application/json\" -u admin:admin -k","title":"Sample curl command"},{"location":"docs/rest-guides/siddhi-app-api/#sample-output_10","text":"","title":"Sample output"},{"location":"docs/rest-guides/siddhi-app-api/#response_10","text":"HTTP Status Code Possible codes are 200 and 404. For descriptions of the HTTP status codes, see HTTP Status Codes","title":"Response"},{"location":"docs/rest-guides/siddhi-app-api/#returning-detailed-statistics-of-all-siddhi-applications","text":"","title":"Returning detailed statistics of all Siddhi applications"},{"location":"docs/rest-guides/siddhi-app-api/#overview_11","text":"Description Returns the detailed statistics of all the Siddhi applications currently deployed in the SP setup. API Context /siddhi-apps/statistics HTTP Method GET Request/Response format application/json Authentication Basic Username admin Password admin Runtime Runner","title":"Overview"},{"location":"docs/rest-guides/siddhi-app-api/#parameter-description_8","text":"","title":"Parameter Description"},{"location":"docs/rest-guides/siddhi-app-api/#curl-command-syntax_11","text":"curl -X GET \"https://localhost:9443/siddhi-apps/statistics\" -H \"accept: application/json\" -u admin:admin -k","title":"curl command syntax"},{"location":"docs/rest-guides/siddhi-app-api/#sample-curl-command_11","text":"curl -X GET \"https://localhost:9443/siddhi-apps/statistics\" -H \"accept: application/json\" -u admin:admin -k","title":"Sample curl command"},{"location":"docs/rest-guides/siddhi-app-api/#sample-output_11","text":"","title":"Sample output"},{"location":"docs/rest-guides/siddhi-app-api/#response_11","text":"HTTP Status Code Possible codes are 200 and 404. For descriptions of the HTTP status codes, see HTTP Status Codes","title":"Response"},{"location":"docs/rest-guides/siddhi-app-api/#enablingdisabling-the-statistics-of-a-specific-siddhi-application","text":"","title":"Enabling/disabling the statistics of a specific Siddhi application"},{"location":"docs/rest-guides/siddhi-app-api/#overview_12","text":"Description Enables/disables statistics for a specified Siddhi application. API Context /siddhi-apps/{appName}/statistics HTTP Method PUT Request/Response format application/json Authentication Basic Username admin Password admin Runtime Runner","title":"Overview"},{"location":"docs/rest-guides/siddhi-app-api/#parameter-description_9","text":"Parameter Description appName The name of the Siddhi application for which the Siddhi applications need to be enabled/disabled.","title":"Parameter Description"},{"location":"docs/rest-guides/siddhi-app-api/#curl-command-syntax_12","text":"curl -X PUT \"https://localhost:9443/siddhi-apps/{appName}/statistics\" -H \"accept: application/json\" -H \"Content-Type: application/json\" -d \"{\u201cstatsEnable\u201d:\u201dtrue\u201d}\" -u admin:admin -k","title":"curl command syntax"},{"location":"docs/rest-guides/siddhi-app-api/#sample-curl-command_12","text":"curl -X PUT \"https://localhost:9443/siddhi-apps/TestSiddhiApp/statistics\" -H \"accept: application/json\" -H \"Content-Type: application/json\" -d \"{\u201cstatsEnable\u201d:\u201dtrue\u201d}\" -u admin:admin -k","title":"Sample curl command"},{"location":"docs/rest-guides/siddhi-app-api/#sample-output_12","text":"","title":"Sample output"},{"location":"docs/rest-guides/siddhi-app-api/#response_12","text":"HTTP Status Code Possible codes are 200 and 404. For descriptions of the HTTP status codes, see HTTP Status Codes","title":"Response"},{"location":"docs/rest-guides/siddhi-app-api/#enablingdisabling-the-statistics-of-all-siddhi-applications","text":"","title":"Enabling/disabling the statistics of all Siddhi applications"},{"location":"docs/rest-guides/siddhi-app-api/#overview_13","text":"Description Enables/disables statistics for all the Siddhi applications. API Context /siddhi-apps/statistics HTTP Method PUT Request/Response format application/json Authentication Basic Username admin Password admin Runtime Runner","title":"Overview"},{"location":"docs/rest-guides/siddhi-app-api/#parameter-description_10","text":"","title":"Parameter Description"},{"location":"docs/rest-guides/siddhi-app-api/#curl-command-syntax_13","text":"curl -X PUT \"https://localhost:9443/siddhi-apps/statistics\" -H \"accept: application/json\" -H \"Content-Type: application/json\" -d \"{\u201cstatsEnable\u201d:\u201dtrue\u201d}\" -u admin:admin -k","title":"curl command syntax"},{"location":"docs/rest-guides/siddhi-app-api/#sample-curl-command_13","text":"curl -X PUT \"https://localhost:9443/siddhi-apps/statistics\" -H \"accept: application/json\" -H \"Content-Type: application/json\" -d \"{\u201cstatsEnable\u201d:\u201dtrue\u201d}\" -u admin:admin -k","title":"Sample curl command"},{"location":"docs/rest-guides/siddhi-app-api/#sample-output_13","text":"","title":"Sample output"},{"location":"docs/rest-guides/siddhi-app-api/#response_13","text":"HTTP Status Code Possible codes are 200 and 404. For descriptions of the HTTP status codes, see HTTP Status Codes","title":"Response"},{"location":"docs/rest-guides/store-api/","text":"Store APIs (Deprecated) Query records in Siddhi store Query records in Siddhi store Overview Replaced with on-demand queries. Please check here for more details. Description Queries records in the Siddhi store. For more information, see Managing Stored Data via REST API . API Context /stores/query HTTP Method POST Request/Response Format application/json Authentication Basic Username admin Password admin Runtime Runner curl command syntax curl -X POST https://localhost:9443/stores/query -H \"content-type: application/json\" -u \"admin:admin\" -d '{\"appName\" : \"AggregationTest\", \"query\" : \"from stockAggregation select *\" }' -k Sample curl command for runner distribution curl -X POST https://localhost:9443/query -H \"content-type: application/json\" -u \"admin:admin\" -d '{\"appName\" : \"ProductDetails\", \"query\" : \"from productTable select *\" }' -k Sample curl command for tooling distribution curl -X POST https://localhost:9743/query -H \"content-type: application/json\" -u \"admin:admin\" -d '{\"appName\" : \"ProductDetails\", \"query\" : \"from productTable select *\" }' -k Sample output { \"records\": [ [ \"ID234\", \"Chocolate\" ], [ \"ID235\", \"Ice Cream\" ] ] } Response HTTP Status Code Possible codes are 200 and 404. For descriptions of the HTTP status codes, see HTTP Status Codes","title":"Store APIs (Deprecated)"},{"location":"docs/rest-guides/store-api/#store-apis-deprecated","text":"Query records in Siddhi store","title":"Store APIs (Deprecated)"},{"location":"docs/rest-guides/store-api/#query-records-in-siddhi-store","text":"","title":"Query records in Siddhi store"},{"location":"docs/rest-guides/store-api/#overview","text":"Replaced with on-demand queries. Please check here for more details. Description Queries records in the Siddhi store. For more information, see Managing Stored Data via REST API . API Context /stores/query HTTP Method POST Request/Response Format application/json Authentication Basic Username admin Password admin Runtime Runner","title":"Overview"},{"location":"docs/rest-guides/store-api/#curl-command-syntax","text":"curl -X POST https://localhost:9443/stores/query -H \"content-type: application/json\" -u \"admin:admin\" -d '{\"appName\" : \"AggregationTest\", \"query\" : \"from stockAggregation select *\" }' -k","title":"curl command syntax"},{"location":"docs/rest-guides/store-api/#sample-curl-command-for-runner-distribution","text":"curl -X POST https://localhost:9443/query -H \"content-type: application/json\" -u \"admin:admin\" -d '{\"appName\" : \"ProductDetails\", \"query\" : \"from productTable select *\" }' -k","title":"Sample curl command for runner distribution"},{"location":"docs/rest-guides/store-api/#sample-curl-command-for-tooling-distribution","text":"curl -X POST https://localhost:9743/query -H \"content-type: application/json\" -u \"admin:admin\" -d '{\"appName\" : \"ProductDetails\", \"query\" : \"from productTable select *\" }' -k","title":"Sample curl command for tooling distribution"},{"location":"docs/rest-guides/store-api/#sample-output","text":"{ \"records\": [ [ \"ID234\", \"Chocolate\" ], [ \"ID235\", \"Ice Cream\" ] ] }","title":"Sample output"},{"location":"docs/rest-guides/store-api/#response","text":"HTTP Status Code Possible codes are 200 and 404. For descriptions of the HTTP status codes, see HTTP Status Codes","title":"Response"}]}