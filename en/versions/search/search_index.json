{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Current version (Stable) Download Documentation Release Notes This is the current stable version of the project. Pre-release versions These are the latest changes that have yet to be released. Work In Progress Download Documentation Siddhi Source Code Distribution Source Code Past Versions","title":"Home"},{"location":"#current-version-stable","text":"Download Documentation Release Notes This is the current stable version of the project.","title":"Current version (Stable)"},{"location":"#pre-release-versions","text":"These are the latest changes that have yet to be released. Work In Progress Download Documentation Siddhi Source Code Distribution Source Code","title":"Pre-release versions"},{"location":"#past-versions","text":"","title":"Past Versions"},{"location":"license/","text":"Copyright (c) 2017 WSO2 Inc. ( http://www.wso2.org ) All Rights Reserved. WSO2 Inc. licenses this file to you under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. ------------------------------------------------------------------------- Apache License Version 2 . 0 , January 2004 http : // www . apache . org / licenses / TERMS AND CONDITIONS FOR USE , REPRODUCTION , AND DISTRIBUTION 1 . Definitions . License shall mean the terms and conditions for use , reproduction , and distribution as defined by Sections 1 through 9 of this document . Licensor shall mean the copyright owner or entity authorized by the copyright owner that is granting the License . Legal Entity shall mean the union of the acting entity and all other entities that control , are controlled by , or are under common control with that entity . For the purposes of this definition , control means ( i ) the power , direct or indirect , to cause the direction or management of such entity , whether by contract or otherwise , or ( ii ) ownership of fifty percent ( 50 % ) or more of the outstanding shares , or ( iii ) beneficial ownership of such entity . You ( or Your ) shall mean an individual or Legal Entity exercising permissions granted by this License . Source form shall mean the preferred form for making modifications , including but not limited to software source code , documentation source , and configuration files . Object form shall mean any form resulting from mechanical transformation or translation of a Source form , including but not limited to compiled object code , generated documentation , and conversions to other media types . Work shall mean the work of authorship , whether in Source or Object form , made available under the License , as indicated by a copyright notice that is included in or attached to the work ( an example is provided in the Appendix below ) . Derivative Works shall mean any work , whether in Source or Object form , that is based on ( or derived from ) the Work and for which the editorial revisions , annotations , elaborations , or other modifications represent , as a whole , an original work of authorship . For the purposes of this License , Derivative Works shall not include works that remain separable from , or merely link ( or bind by name ) to the interfaces of , the Work and Derivative Works thereof . Contribution shall mean any work of authorship , including the original version of the Work and any modifications or additions to that Work or Derivative Works thereof , that is intentionally submitted to Licensor for inclusion in the Work by the copyright owner or by an individual or Legal Entity authorized to submit on behalf of the copyright owner . For the purposes of this definition , submitted means any form of electronic , verbal , or written communication sent to the Licensor or its representatives , including but not limited to communication on electronic mailing lists , source code control systems , and issue tracking systems that are managed by , or on behalf of , the Licensor for the purpose of discussing and improving the Work , but excluding communication that is conspicuously marked or otherwise designated in writing by the copyright owner as Not a Contribution. Contributor shall mean Licensor and any individual or Legal Entity on behalf of whom a Contribution has been received by Licensor and subsequently incorporated within the Work . 2 . Grant of Copyright License . Subject to the terms and conditions of this License , each Contributor hereby grants to You a perpetual , worldwide , non - exclusive , no - charge , royalty - free , irrevocable copyright license to reproduce , prepare Derivative Works of , publicly display , publicly perform , sublicense , and distribute the Work and such Derivative Works in Source or Object form . 3 . Grant of Patent License . Subject to the terms and conditions of this License , each Contributor hereby grants to You a perpetual , worldwide , non - exclusive , no - charge , royalty - free , irrevocable ( except as stated in this section ) patent license to make , have made , use , offer to sell , sell , import , and otherwise transfer the Work , where such license applies only to those patent claims licensable by such Contributor that are necessarily infringed by their Contribution ( s ) alone or by combination of their Contribution ( s ) with the Work to which such Contribution ( s ) was submitted . If You institute patent litigation against any entity ( including a cross - claim or counterclaim in a lawsuit ) alleging that the Work or a Contribution incorporated within the Work constitutes direct or contributory patent infringement , then any patent licenses granted to You under this License for that Work shall terminate as of the date such litigation is filed . 4 . Redistribution . You may reproduce and distribute copies of the Work or Derivative Works thereof in any medium , with or without modifications , and in Source or Object form , provided that You meet the following conditions : ( a ) You must give any other recipients of the Work or Derivative Works a copy of this License ; and ( b ) You must cause any modified files to carry prominent notices stating that You changed the files ; and ( c ) You must retain , in the Source form of any Derivative Works that You distribute , all copyright , patent , trademark , and attribution notices from the Source form of the Work , excluding those notices that do not pertain to any part of the Derivative Works ; and ( d ) If the Work includes a NOTICE text file as part of its distribution , then any Derivative Works that You distribute must include a readable copy of the attribution notices contained within such NOTICE file , excluding those notices that do not pertain to any part of the Derivative Works , in at least one of the following places : within a NOTICE text file distributed as part of the Derivative Works ; within the Source form or documentation , if provided along with the Derivative Works ; or, within a display generated by the Derivative Works , if and wherever such third - party notices normally appear . The contents of the NOTICE file are for informational purposes only and do not modify the License . You may add Your own attribution notices within Derivative Works that You distribute , alongside or as an addendum to the NOTICE text from the Work , provided that such additional attribution notices cannot be construed as modifying the License . You may add Your own copyright statement to Your modifications and may provide additional or different license terms and conditions for use , reproduction , or distribution of Your modifications , or for any such Derivative Works as a whole , provided Your use , reproduction , and distribution of the Work otherwise complies with the conditions stated in this License . 5 . Submission of Contributions . Unless You explicitly state otherwise , any Contribution intentionally submitted for inclusion in the Work by You to the Licensor shall be under the terms and conditions of this License , without any additional terms or conditions . Notwithstanding the above , nothing herein shall supersede or modify the terms of any separate license agreement you may have executed with Licensor regarding such Contributions . 6 . Trademarks . This License does not grant permission to use the trade names , trademarks , service marks , or product names of the Licensor , except as required for reasonable and customary use in describing the origin of the Work and reproducing the content of the NOTICE file . 7 . Disclaimer of Warranty . Unless required by applicable law or agreed to in writing , Licensor provides the Work ( and each Contributor provides its Contributions ) on an AS IS BASIS , WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND , either express or implied , including , without limitation , any warranties or conditions of TITLE , NON - INFRINGEMENT , MERCHANTABILITY , or FITNESS FOR A PARTICULAR PURPOSE . You are solely responsible for determining the appropriateness of using or redistributing the Work and assume any risks associated with Your exercise of permissions under this License . 8 . Limitation of Liability . In no event and under no legal theory , whether in tort ( including negligence ) , contract , or otherwise , unless required by applicable law ( such as deliberate and grossly negligent acts ) or agreed to in writing , shall any Contributor be liable to You for damages , including any direct , indirect , special , incidental , or consequential damages of any character arising as a result of this License or out of the use or inability to use the Work ( including but not limited to damages for loss of goodwill , work stoppage , computer failure or malfunction , or any and all other commercial damages or losses ) , even if such Contributor has been advised of the possibility of such damages . 9 . Accepting Warranty or Additional Liability . While redistributing the Work or Derivative Works thereof , You may choose to offer , and charge a fee for , acceptance of support , warranty , indemnity , or other liability obligations and / or rights consistent with this License . However , in accepting such obligations , You may act only on Your own behalf and on Your sole responsibility , not on behalf of any other Contributor , and only if You agree to indemnify , defend , and hold each Contributor harmless for any liability incurred by , or claims asserted against , such Contributor by reason of your accepting any such warranty or additional liability . END OF TERMS AND CONDITIONS","title":"License"},{"location":"docs/examples/basic-types/","text":"var base_url = \"\"; Basic Types Provides introduction to basic Siddhi attribute types which are int , long , float , double , string , and object , and some key functions such as convert() , instanceOf...() , and cast() . In Siddhi, other types such as list, map, etc, should be passed as object into streams. For more information on other types refer other examples under Values and Types section. For information values , and other useful functions , refer the Siddhi query guide . define stream PatientRegistrationInputStream ( seqNo long , name string , age int , height float , weight double , photo object , isEmployee bool , wardNo object ); define stream PatientRegistrationStream ( seqNo long , name string , age int , height double , weight double , photo object , isPhotoString bool , isEmployee bool , wardNo int ); @ info ( name = Type - processor ) from PatientRegistrationInputStream select seqNo , name , age , convert ( height , double ) as height , weight , photo , instanceOfString ( photo ) as isPhotoString , isEmployee , cast ( wardNo , int ) as wardNo insert into PatientRegistrationStream ; define stream PatientRegistrationInputStream ( seqNo long, name string, age int, height float, weight double, photo object, isEmployee bool, wardNo object); Defines PatientRegistrationInputStream having information in all primitive types. define stream PatientRegistrationStream ( seqNo long, name string, age int, height double, weight double, photo object, isPhotoString bool, isEmployee bool, wardNo int); Defines the resulting PatientRegistrationStream after processing. @info(name = Type-processor ) from PatientRegistrationInputStream select seqNo, name, age, convert(height, double ) as height, convert() used to convert float type to double . weight, photo, instanceOfString(photo) as isPhotoString, instanceOfString() checks if the photo is an instance of string . isEmployee, cast(wardNo, int ) as wardNo cast() cast the value of wardNo to int . insert into PatientRegistrationStream; Input Below event is sent to PatientRegistrationInputStream , [ 1200098 , 'Peter Johnson' , 34 , 194.3f , 69.6 , #Fjoiu59%3hkjnknk$#nFT , true , 34 ] Here, assume that the content of the photo ( #Fjoiu59%3hkjnknk$#nFT ) is binary. Output After processing, the event arriving at PatientRegistrationStream will be as follows: [ 1200098 , 'Peter Johnson' , 34 , 194.3 , 69.6 , #Fjoiu59%3hkjnknk$#nFT , false , true , 34 ]","title":"Basic types"},{"location":"docs/examples/batch-length/","text":"var base_url = \"\"; Batch (Tumbling) Event Count Provides examples on aggregating events based on event count in a batch (tumbling) manner. To aggregate events in a sliding manner, based on time, or by session, refer other the examples in Data Summarization section. For more information on windows refer the Siddhi query guide . define stream TemperatureStream ( sensorId string , temperature double ); @ info ( name = Overall - analysis ) from TemperatureStream # window . lengthBatch ( 4 ) select avg ( temperature ) as avgTemperature , max ( temperature ) as maxTemperature , count () as numberOfEvents insert into OverallTemperatureStream ; @ info ( name = SensorId - analysis ) from TemperatureStream # window . lengthBatch ( 5 ) select sensorId , avg ( temperature ) as avgTemperature , min ( temperature ) as maxTemperature group by sensorId having avgTemperature = 20.0 insert into SensorIdTemperatureStream ; define stream TemperatureStream (sensorId string, temperature double); @info(name = Overall-analysis ) from TemperatureStream#window.lengthBatch(4) Aggregate every 4 events in a batch (tumbling) manner. select avg(temperature) as avgTemperature, max(temperature) as maxTemperature, count() as numberOfEvents insert into OverallTemperatureStream; Calculate average, maximum, and count for temperature attribute. @info(name = SensorId-analysis ) from TemperatureStream#window.lengthBatch(5) Aggregate every 5 events in a batch (tumbling) manner. select sensorId, avg(temperature) as avgTemperature, min(temperature) as maxTemperature group by sensorId Calculate average, and minimum for temperature , by grouping events by sensorId . having avgTemperature = 20.0 Output events only when avgTemperature is greater than or equal to 20.0 . insert into SensorIdTemperatureStream; Aggregation Behavior When events are sent to TemperatureStream stream, following events will get emitted at OverallTemperatureStream stream via Overall-analysis query, and SensorIdTemperatureStream stream via SensorId-analysis query. .md-typeset table:not([class]) th:nth-child(2) { min-width: 123px; } Input to TemperatureStream Output at OverallTemperatureStream Output at SensorIdTemperatureStream [ '1001' , 19.0 ] - - [ '1002' , 26.0 ] - - [ '1002' , 24.0 ] - - [ '1001' , 20.0 ] [ 22.5 , 26.0 , 4 ] - [ '1001' , 21.0 ] - [ '1002' , 25.5 , 24.0 ], [ '1001' , 20.0 , 19.0 ] [ '1002' , 22.0 ] - - [ '1001' , 21.0 ] - - [ '1002' , 22.0 ] [ 21.5 , 22.0 , 4 ] -","title":"Batch length"},{"location":"docs/examples/batch-time/","text":"var base_url = \"\"; Batch (Tumbling) Time Provides examples on aggregating events over time in a batch (tumbling) manner. To aggregate events in a sliding manner, based on events, or by session, refer other the examples in Data Summarization section. For more information on windows refer the Siddhi query guide . define stream TemperatureStream ( sensorId string , temperature double ); @ info ( name = Overall - analysis ) from TemperatureStream # window . timeBatch ( 1 min ) select avg ( temperature ) as avgTemperature , max ( temperature ) as maxTemperature , count () as numberOfEvents insert into OverallTemperatureStream ; @ info ( name = SensorId - analysis ) from TemperatureStream # window . timeBatch ( 30 sec , 0 ) select sensorId , avg ( temperature ) as avgTemperature , min ( temperature ) as maxTemperature group by sensorId having avgTemperature 20.0 insert into SensorIdTemperatureStream ; define stream TemperatureStream (sensorId string, temperature double); @info(name = Overall-analysis ) from TemperatureStream#window.timeBatch(1 min) Aggregate events every 1 minute , from the arrival of the first event. select avg(temperature) as avgTemperature, max(temperature) as maxTemperature, count() as numberOfEvents Calculate average, maximum, and count for temperature attribute. insert into OverallTemperatureStream; @info(name = SensorId-analysis ) from TemperatureStream#window.timeBatch(30 sec, 0) Aggregate events every 30 seconds from epoch timestamp 0 . select sensorId, avg(temperature) as avgTemperature, min(temperature) as maxTemperature group by sensorId Calculate average, and minimum for temperature , by grouping events by sensorId . having avgTemperature 20.0 Output events only when avgTemperature is greater than 20.0 . insert into SensorIdTemperatureStream; Aggregation Behavior When events are sent to TemperatureStream stream, following events will get emitted at OverallTemperatureStream stream via Overall-analysis query, and SensorIdTemperatureStream stream via SensorId-analysis query. .md-typeset table:not([class]) th:nth-child(2) { min-width: 123px; } Time Input to TemperatureStream Output at OverallTemperatureStream Output at SensorIdTemperatureStream 9:00:10 [ '1001' , 21.0 ] - - 9:00:20 [ '1002' , 25.0 ] - - 9:00:30 - - [ '1001' , 21.0 , 21.0 ],[ '1002' , 25.0 , 25.0 ] 9:00:35 [ '1002' , 26.0 ] - - 9:00:40 [ '1002' , 27.0 ] - - 9:00:55 [ '1001' , 19.0 ] - - 9:00:00 - - [ '1002' , 26.5 , 26.0 ] 9:01:10 - [ 23.6 , 27.0 , 5 ] - 9:01:20 [ '1001' , 21.0 ] - - 9:01:30 - - [ '1001' , 21.0 , 21.0 ] 9:02:10 - [ 21.0 , 21.0 , 1 ] -","title":"Batch time"},{"location":"docs/examples/counting-pattern/","text":"var base_url = \"\"; Counting Pattern Counting patterns allow to match multiple events that may have been received for the same matching condition. The number of events matched per condition can be limited via condition postfixes. Refer the Siddhi query guide for more information. define stream TemperatureStream ( sensorID long , roomNo int , temp double ); define stream RegulatorStream ( deviceID long , roomNo int , tempSet double , isOn bool ); @ sink ( type = log ) define stream TemperatureDiffStream ( roomNo int , tempDiff double ); from every ( e1 = RegulatorStream ) - e2 = TemperatureStream [ e1 . roomNo == roomNo ] 1 : - e3 = RegulatorStream [ e1 . roomNo == roomNo ] select e1 . roomNo , e2 [ 0 ]. temp - e2 [ last ]. temp as tempDiff insert into TemperatureDiffStream ; define stream TemperatureStream (sensorID long, roomNo int, temp double); Defines TemperatureStream having information on room temperature such as sensorID , roomNo and temp . define stream RegulatorStream (deviceID long, roomNo int, tempSet double, isOn bool); Defines RegulatorStream which contains the events from regulator with attributes deviceID , roomNo , tempSet , and isOn . @sink(type = log ) define stream TemperatureDiffStream(roomNo int, tempDiff double); Defines TemperatureDiffStream which contains the events related to temperature difference. from every( e1 = RegulatorStream) - e2 = TemperatureStream[e1.roomNo == roomNo] 1: - e3 = RegulatorStream[e1.roomNo == roomNo] Calculates the temperature difference between two regulator events. Here, when at least one TemperatureStream event needs to arrive between two RegulatorStream events. select e1.roomNo, e2[0].temp - e2[last].temp as tempDiff Finds the temperature difference between the first and last temperature event. insert into TemperatureDiffStream; This application calculates the temperature difference between two regulator events. Here, when at least one TemperatureStream event occurs between two RegulatorStream events the pattern is valid and logs can be seen. Input First, below event is sent to RegulatorStream , [ 21 , 2 , 25 , true ] Below events are sent to TemperatureStream , [ 21 , 2 , 29 ] [ 21 , 2 , 26 ] Finally, below event is sent again to RegulatorStream , [ 21 , 2 , 30 , true ] Output After processing the above input events, the event arriving at TemperatureDiffStream will be as follows: [ 2 , 3.0 ]","title":"Counting pattern"},{"location":"docs/examples/deduplicate/","text":"var base_url = \"\"; Remove Duplicate Events Provides examples of removing duplicate events that arrive within a given time duration. define stream TemperatureStream ( sensorId string , seqNo string , temperature double ); @ info ( name = Deduplicate - sensorId ) from TemperatureStream #unique:deduplicate( sensorId , 1 min ) select * insert into UniqueSensorStream ; @ info ( name = Deduplicate - sensorId - and - seqNo ) from TemperatureStream #unique:deduplicate( str : concat ( sensorId , - , seqNo ), 1 min ) select * insert into UniqueSensorSeqNoStream ; define stream TemperatureStream (sensorId string, seqNo string, temperature double); @info(name = Deduplicate-sensorId ) from TemperatureStream#unique:deduplicate(sensorId, 1 min) Remove duplicate events arriving within 1 minute time gap, based on unique sensorId . select * insert into UniqueSensorStream; @info(name = Deduplicate-sensorId-and-seqNo ) from TemperatureStream#unique:deduplicate( str:concat(sensorId, - ,seqNo), 1 min) Remove duplicate events arriving within 1 minute time gap, based on unique sensorId and seqNo combination. select * insert into UniqueSensorSeqNoStream; Behavior When events are sent to TemperatureStream stream, following events will get emitted after deduplication on UniqueSensorStream stream via Deduplicate-sensorId query, and UniqueSensorSeqNoStream stream via Deduplicate-sensorId-and-seqNo query. .md-typeset table:not([class]) th:nth-child(2) { min-width: 155px; } Time Input to TemperatureStream Output at UniqueSensorStream Output at UniqueSensorSeqNoStream 9:00:00 [ 'AD11' , '200' , 18.0 ] [ 'AD11' , '200' , 18.0 ] [ 'AD11' , '200' , 18.0 ] 9:00:10 [ 'AD11' , '201' , 23.0 ] - [ 'AD11' , '201' , 23.0 ] 9:00:20 [ 'FR45' , '500' , 22.0 ] [ 'FR45' , '500' , 22.0 ] [ 'FR45' , '500' , 22.0 ] 9:00:40 [ 'AD11' , '200' , 18.0 ] - - 9:00:50 [ 'AD11' , '202' , 28.0 ] - [ 'AD11' , '202' , 28.0 ] 9:01:05 [ 'FR45' , '501' , 22.0 ] - [ 'FR45' , '501' , 22.0 ] 9:01:10 [ 'AD11' , '203' , 30.0 ] [ 'AD11' , '203' , 30.0 ] [ 'AD11' , '203' , 30.0 ] 9:02:20 [ 'AD11' , '202' , 28.0 ] [ 'AD11' , '202' , 28.0 ] [ 'AD11' , '202' , 28.0 ] 9:03:10 [ 'AD11' , '204' , 30.0 ] - [ 'AD11' , '204' , 30.0 ]","title":"Deduplicate"},{"location":"docs/examples/default/","text":"var base_url = \"\"; Default This application demonstrates how to use default function to process attributes with null values Please see Values in Query guide for more information on format of the various data types. define stream PatientRegistrationInputStream ( seqNo long , name string , age int , height float , weight double , photo object , isEmployee bool , wardNo object ); @ info ( name = SimpleIfElseQuery ) from PatientRegistrationInputStream select default ( name , invalid ) as name , default ( seqNo , 0 l ) as seqNo , default ( weight , 0 d ) as weight , default ( age , 0 ) as age , default ( height , 0 f ) as height insert into PreprocessedPatientRegistrationInputStream ; define stream PatientRegistrationInputStream ( seqNo long, name string, age int, height float, weight double, photo object, isEmployee bool, wardNo object); Defines PatientRegistrationInputStream having information in all primitive types. @info(name = SimpleIfElseQuery ) from PatientRegistrationInputStream select default(name, invalid ) as name, Default value of invalid to be used if name is null default(seqNo, 0l) as seqNo, Default value of 0l to be used if seqNo is null default(weight, 0d) as weight, Default value of 0d to be used if weight is null default(age, 0) as age, Default value of 0 to be used if age is null default(height, 0f) as height Default value of 0f to be used if height is null insert into PreprocessedPatientRegistrationInputStream; Input An event of all null attributes is sent to PatientRegistrationInputStream , Output After processing, the event arriving at PreprocessedPatientRegistrationInputStream will be as follows, [ 'invalid' , 0 0.0 , 0 , 0.0 ] with types, [ string , long , double , int , float ]","title":"Default"},{"location":"docs/examples/error-handling-with-logs/","text":"var base_url = \"\"; Logging Errors in Siddhi can be handled at the Streams and in the Sinks. This example explains how errors are handled at Sink level. There can be cases where external systems becoming unavailable or coursing errors when the events are published to them. By default sinks log and drop the events causing event losses, and this can be handled gracefully by configuring on.error parameter of the @sink annotation. Refer the Siddhi query guide for more information. define stream GlucoseReadingStream ( locationRoom string , locationBed string , timeStamp string , sensorID long , patientFirstName string , patientLastName string , sensorValue double ); @ sink ( type = http , on . error = log , publisher . url = http://localhost:8080/logger , method = POST , @ map ( type = json )) define stream AbnormalGlucoseReadingStream ( timeStampInLong long , locationRoom string , locationBed string , sensorID long , patientFullName string , sensorReadingValue double ); @ info ( name = abnormal - reading - identifier ) from GlucoseReadingStream [ sensorValue 220 ] select math : parseLong ( timeStamp ) as timeStampInLong , locationRoom , locationBed , sensorID , str : concat ( patientFirstName , , patientLastName ) as patientFullName , sensorValue as sensorReadingValue insert into AbnormalGlucoseReadingStream ; define stream GlucoseReadingStream (locationRoom string, locationBed string, timeStamp string, sensorID long, patientFirstName string, patientLastName string, sensorValue double); Defines GlucoseReadingStream stream which contains events related to Glucose readings. @sink(type = http , on.error= log , publisher.url = http://localhost:8080/logger , method = POST , @map(type = json )) If HTTP endpoint which defined in sink annotation is unavailable then it logs the event with the error and drops the event. Errors can be gracefully handled by configuring on.error parameter. define stream AbnormalGlucoseReadingStream (timeStampInLong long, locationRoom string, locationBed string, sensorID long, patientFullName string, sensorReadingValue double); @info(name= abnormal-reading-identifier ) from GlucoseReadingStream[sensorValue 220] select math:parseLong(timeStamp) as timeStampInLong, locationRoom, locationBed, sensorID, Identifies the abnormal Glucose reading if sensorValue 220 str:concat(patientFirstName, , patientLastName) as patientFullName, Concatenate string attributes patientFirstName and patientLastName sensorValue as sensorReadingValue insert into AbnormalGlucoseReadingStream; Above is a simple example to publish abnormal Glucose reading events to an unavailable HTTP endpoint and error is handled by logging the events to the logs. Input Below event is sent to GlucoseReadingStream stream, [ 'Get-1024' , 'Level2' , '1576829362' , 10348 , 'Alex' , 'John' , 250 ] Output After processing, the following log gets printed in the console: ERROR {io.siddhi.core.stream.output.sink.Sink} - Error on 'ErrorHandling'. Dropping event at Sink 'http' at 'AbnormalGlucoseReadingStream' as its still trying to reconnect!, events dropped '{ event :{ timeStampInLong :1576829362, locationRoom : 1024 , locationBed : Level2 , sensorID :10348, patientFullName : Alex John , sensorReadingValue :250.0}}'","title":"Error handling with logs"},{"location":"docs/examples/error-handling-with-wait-retry/","text":"var base_url = \"\"; Wait & Retry This example explains how errors are handled at Sink level by wait and retry mode. In this mode, publishing threads wait in back-off and re-trying mode, and only send the events when the connection is re-established. During this time the threads will not consume any new messages causing the systems to introduce back pressure on the systems that publish to it. Refer the Siddhi query guide for more information. define stream GlucoseReadingStream ( locationRoom string , locationBed string , timeStamp string , sensorID long , patientFirstName string , patientLastName string , sensorValue double ); @ sink ( type = http , on . error = wait , publisher . url = http://localhost:8080/logger , method = POST , @ map ( type = json )) define stream AbnormalGlucoseReadingStream ( timeStampInLong long , locationRoom string , locationBed string , sensorID long , patientFullName string , sensorReadingValue double ); @ info ( name = abnormal - reading - identifier ) from GlucoseReadingStream [ sensorValue 220 ] select math : parseLong ( timeStamp ) as timeStampInLong , locationRoom , locationBed , sensorID , str : concat ( patientFirstName , , patientLastName ) as patientFullName , sensorValue as sensorReadingValue insert into AbnormalGlucoseReadingStream ; define stream GlucoseReadingStream (locationRoom string, locationBed string, timeStamp string, sensorID long, patientFirstName string, patientLastName string, sensorValue double); Defines GlucoseReadingStream stream which contains events related to Glucose readings. @sink(type = http , on.error= wait , publisher.url = http://localhost:8080/logger , method = POST , @map(type = json )) If HTTP endpoint is unavailable then threads who bring events via AbnormalGlucoseReadingStream wait in back-off and re-trying mode. Errors can be gracefully handled by configuring on.error parameter. define stream AbnormalGlucoseReadingStream (timeStampInLong long, locationRoom string, locationBed string, sensorID long, patientFullName string, sensorReadingValue double); @info(name= abnormal-reading-identifier ) from GlucoseReadingStream[sensorValue 220] select math:parseLong(timeStamp) as timeStampInLong, locationRoom, locationBed, sensorID, Identifies the abnormal Glucose reading if sensorValue 220 str:concat(patientFirstName, , patientLastName) as patientFullName, Concatenate string attributes patientFirstName and patientLastName sensorValue as sensorReadingValue insert into AbnormalGlucoseReadingStream; Above is a simple example to publish abnormal Glucose reading events to an unavailable HTTP endpoint and error is handled by wait and retry mode. prerequisites Download the mock logger service from here . Input Output Below event is sent to GlucoseReadingStream stream, [ 'Get-1024' , 'Level2' , '1576829362' , 10348 , 'Alex' , 'John' , 250 ] You could see ConnectException is get printed since logger service is unavailable. Then, execute the below command to start the mock logger service. java -jar logservice-1.0.0.jar Now, you could see the event sent in step #1 is get logged in the logger service console as given below. LoggerService:42 - {event={timeStampInLong=1.576829362E9, locationRoom=Get-1024, locationBed=Level2, sensorID=10348.0, patientFullName=Alex John, sensorReadingValue=250.0}}","title":"Error handling with wait retry"},{"location":"docs/examples/grpc-service-integration/","text":"var base_url = \"\"; gRPC Service Integration This application demonstrates how to achieve gRPC service integration (request-response) while processing events in the realtime. There could be use cases which need to integrate with an external gRPC service to make some decision when processing events. The below example demonstrates such a requirement. define stream TicketBookingStream ( name string , phoneNo string , movie string , ticketClass string , qty int , bookingTime long ); @ sink ( type = grpc - call , publisher . url = grpc : //localhost:5003/org.wso2.grpc.EventService/process , sink . id = ticket - price , @ map ( type = json )) define stream TicketPriceFinderStream ( name string , phoneNo string , movie string , ticketClass string , qty int , bookingTime long ); @ source ( type = grpc - call - response , receiver . url = grpc : //localhost:9763/org.wso2.grpc.EventService/process , sink . id = ticket - price , @ map ( type = json , @ attributes ( customerName = trp : name , phoneNo = trp : phoneNo , movie = trp : movie , qty = trp : qty , bookingTime = trp : bookingTime , ticketPrice = price ))) define stream TicketPriceResponseStream ( customerName string , phoneNo string , movie string , qty int , ticketPrice double , bookingTime long ); @ sink ( type = log ) define stream TotalTicketPaymentStream ( customerName string , phoneNo string , movie string , totalAmount double , bookingTime long ); @ info ( name = filter - basic - ticket - bookings ) from TicketBookingStream [ ticketClass == BASIC ] select name as customerName , phoneNo , movie , qty * 20.0 as totalAmount , bookingTime insert into TotalTicketPaymentStream ; @ info ( name = filter - non - basic - tickets ) from TicketBookingStream [ ticketClass != BASIC ] select * insert into TicketPriceFinderStream ; @ info ( name = total - price - calculator ) from TicketPriceResponseStream select customerName , phoneNo , movie , ( qty * ticketPrice ) as totalAmount , bookingTime insert into TotalTicketPaymentStream ; define stream TicketBookingStream (name string, phoneNo string, movie string, ticketClass string, qty int, bookingTime long); Defines TicketBookingStream which is the input stream that contains the ticket booking events. @sink(type= grpc-call , publisher.url = grpc://localhost:5003/org.wso2.grpc.EventService/process , sink.id= ticket-price , @map(type= json )) The grpc-call sink is used for scenarios where we send a request out and expect a response back. In default mode this will use EventService process method. grpc-call-response source is used to receive the responses. A unique sink.id is used to correlate between the sink and its corresponding source. define stream TicketPriceFinderStream (name string, phoneNo string, movie string, ticketClass string, qty int, bookingTime long); Defines TicketPriceFinderStream to forward events to the gRPC endpoint. @source(type= grpc-call-response , receiver.url = grpc://localhost:9763/org.wso2.grpc.EventService/process , sink.id= ticket-price , This grpc-call-response source receives responses received from gRPC server for requests sent from a grpc-call sink. The source will receive responses for sink with the same sink.id . @map(type= json , @attributes(customerName= trp:name , phoneNo= trp:phoneNo , movie= trp:movie , qty= trp:qty , bookingTime= trp:bookingTime , ticketPrice= price ))) Attributes of the event that initiated the call, and the response headers and properties via transport properties in the format trp: attribute name and trp: header/property respectively. define stream TicketPriceResponseStream (customerName string, phoneNo string, movie string, qty int, ticketPrice double, bookingTime long); Defines TicketPriceResponseStream which contains the response events for the ticket price requests. @sink(type= log ) define stream TotalTicketPaymentStream (customerName string, phoneNo string, movie string, totalAmount double, bookingTime long); Defines TotalTicketPaymentStream which contains the events with the total payment amount for tickets. @info(name = filter-basic-ticket-bookings ) from TicketBookingStream[ticketClass == BASIC ] select name as customerName, phoneNo, movie, qty * 20.0 as totalAmount, bookingTime insert into TotalTicketPaymentStream; Filter the ticket bookings of class Basic and apply the default ticket price as 20 USD. @info(name = filter-non-basic-tickets ) from TicketBookingStream[ticketClass != BASIC ] select * insert into TicketPriceFinderStream; Filter the ticket bookings other than the class Basic and route them to stream called TicketPriceFinderStream . @info(name = total-price-calculator ) from TicketPriceResponseStream select customerName, phoneNo, movie, (qty * ticketPrice) as totalAmount, bookingTime Calculate the total ticket payment amount based on the price amount received from the gRPC service. insert into TotalTicketPaymentStream; Input and Output Let s assume there is an external gRPC service that responds with the ticket price based on the gRPC request. When an event with values [ Mohan , +181234579212 , Iron Man , Gold , 4, 0130] is sent to TicketBookingStream stream then a gRPC request is sent to the loan gRPC service to find the ticket price if ticket class is not basic . Then, gRPC server responds with the ticket price as shown below. { price : 25 } There is a grpc-call-response source configured to consume the response from the gRPC server and those responses will be mapped to the stream called TicketPriceResponseStream . Then Siddhi calculates the total ticket payment amount accordingly and pushes it to a stream called TotalTicketPaymentStream . In this example, those events are logged to the console. Sample console log is give below, TotalPurchaseCalculator : TotalTicketPaymentStream : Event{timestamp=1575449841536, data=[Mohan, +181234579212, Iron Man, 100.0, 0130], isExpired=false}","title":"Grpc service integration"},{"location":"docs/examples/http-service-integration/","text":"var base_url = \"\"; HTTP Service Integration This application demonstrates how to achieve HTTP service integration (request-response) while processing events in the realtime. There could be use cases which need to integrate with an external service to make some decision when processing events. The below example demonstrates such a requirement. @ sink ( type = http - call , publisher . url = http : //localhost:8005/validate-loan , method = POST , sink . id = loan - validation , @ map ( type = json )) define stream LoanValidationStream ( clientId long , name string , amount double ); @ source ( type = http - call - response , sink . id = loan - validation , http . status . code = 2 \\ d + , @ map ( type = json , @ attributes ( customerName = trp : name , clientId = trp : clientId , loanAmount = trp : amount , interestRate = validation - response . rate , totalYears = validation - response . years - approved ))) define stream SuccessLoanRequestStream ( clientId long , customerName string , loanAmount double , interestRate double , totalYears int ); @ source ( type = http - call - response , sink . id = loan - validation , http . status . code = 400 , @ map ( type = json , @ attributes ( customerName = trp : name , clientId = trp : clientId , failureReason = validation - response . reason ))) define stream FailureLoanRequestStream ( clientId long , customerName string , failureReason string ); define stream LoanRequestStream ( clientId long , name string , amount double ); @ sink ( type = log ) define stream LoanResponseStream ( clientId long , customerName string , message string ); @ info ( name = attribute - projection ) from LoanRequestStream select clientId , name , amount insert into LoanValidationStream ; @ info ( name = successful - message - generator ) from SuccessLoanRequestStream select clientId , customerName , Loan Request is accepted for processing as message insert into LoanResponseStream ; @ info ( name = failure - message - generator ) from FailureLoanRequestStream select clientId , customerName , str : concat ( Loan Request is rejected due to , failureReason ) as message insert into LoanResponseStream ; @sink(type= http-call , publisher.url= http://localhost:8005/validate-loan , method= POST , sink.id= loan-validation , @map(type= json )) http-call sink publishes messages to endpoints via HTTP or HTTPS protocols. and consume responses through its corresponding http-call-response source define stream LoanValidationStream (clientId long, name string, amount double); Defines LoanValidationStream to forward events to the loan validation purposes. @source(type= http-call-response , sink.id= loan-validation , The http-call-response source receives the responses for the calls made by its corresponding http-call sink. sink.id used to map the corresponding http-call sink. http.status.code= 2\\d+ , To handle messages with different HTTP status codes having different formats, multiple http-call-response sources are allowed to associate with a single http-call sink. In this case, it only handles HTTP responses which come with 2xx response codes. @map(type= json , @attributes(customerName= trp:name , clientId= trp:clientId , loanAmount= trp:amount , interestRate= validation-response.rate , totalYears= validation-response.years-approved ))) Attributes of the event that initiated the call, and the response headers and properties via transport properties in the format trp: attribute name and trp: header/property respectively. define stream SuccessLoanRequestStream(clientId long, customerName string, loanAmount double, interestRate double, totalYears int); Defines SuccessLoanRequestStream to process loan requests which are validated successfully. @source(type= http-call-response , sink.id= loan-validation , http.status.code= 400 , @map(type= json , @attributes(customerName= trp:name , clientId= trp:clientId , failureReason= validation-response.reason ))) The http-call-response source which handles the responses which come with 400 HTTP response code. define stream FailureLoanRequestStream(clientId long, customerName string, failureReason string); Defines FailureLoanRequestStream to process loan requests which are rejected. define stream LoanRequestStream (clientId long, name string, amount double); Defines LoanRequestStream which is the input stream that contains the initial loan request events. @sink(type= log ) define stream LoanResponseStream(clientId long, customerName string, message string); Defines LoanResponseStream which contains the events with the loan request responses. @info(name = attribute-projection ) from LoanRequestStream select clientId, name, amount insert into LoanValidationStream; Project attributes clientId , name and amount to perform the http request to validate-loan service. @info(name = successful-message-generator ) from SuccessLoanRequestStream select clientId, customerName, Loan Request is accepted for processing as message insert into LoanResponseStream; Process the successful loan requests and generate a response message. @info(name = failure-message-generator ) from FailureLoanRequestStream select clientId, customerName, str:concat( Loan Request is rejected due to , failureReason) as message insert into LoanResponseStream; Process the rejected loan requests and generate a response message. Input and Output Let s assume there is an external HTTP service that performs the loan request validation. When an event with values [002345, David Warner , 200000] is sent to LoanRequestStream stream then the respective event is sent to the loan validation service. If the loan request is valid then Siddhi gets a response as shown below from the loan validation service. In this situation, the response code would be 200 . { validation-response : { isValid : true, rate : 12.5, years-approved : 5 } } If the loan request is rejected then the loan validation service sent a response with response code 400 with below payload. { validation-response : { isValid : false, reason : Not a registered user } } Then Siddhi logs the response message accordingly. If the loan request is accepted then log message would be something similar as below. LoanRequestProcessor : LoanResponseStream : Event{timestamp=1575293895348, data=[002345, David Warner, Loan Request is accepted for processing], isExpired=false","title":"Http service integration"},{"location":"docs/examples/if-then-else/","text":"var base_url = \"\"; If-Then-Else This application demonstrates how to enrich events based on a simple if-then-else conditions. define stream TemperatureStream ( sensorId string , temperature double ); @ info ( name = SimpleIfElseQuery ) from TemperatureStream select sensorId , ifThenElse ( temperature - 2 , Valid , InValid ) as isValid insert into ValidTemperatureStream ; @ info ( name = ComplexIfElseQuery ) from TemperatureStream select sensorId , ifThenElse ( temperature - 2 , ifThenElse ( temperature 40 , High , Normal ), InValid ) as tempStatus insert into ProcessedTemperatureStream ; define stream TemperatureStream (sensorId string, temperature double); Defines TemperatureStream stream to process events having sensorId and temperature (F). @info(name = SimpleIfElseQuery ) from TemperatureStream select sensorId, ifThenElse(temperature -2, Valid , InValid ) as isValid if temperature -2, isValid will be true else false insert into ValidTemperatureStream; @info(name = ComplexIfElseQuery ) from TemperatureStream select sensorId, ifThenElse(temperature -2, ifThenElse(temperature 40, High , Normal ), InValid ) as tempStatus If the temperature 40 the status is set to High , between -2 and 40 as Normal less than -2 as InValid insert into ProcessedTemperatureStream; Events at each stream When an event with values [ 'sensor1' , 35.4 ] is sent to TemperatureStream stream it will get converted and travel through the streams as below. ValidTemperatureStream : [ 'sensor1' , 'Valid' ] ProcessedTemperatureStream : [ 'sensor1' , 'Normal' ]","title":"If then else"},{"location":"docs/examples/list/","text":"var base_url = \"\"; List Provides examples on basic list functions provided via siddhi-execution-list extension. For information of performing scatter and gather using list:tokenize() , and list:collect() refer the examples in Data Pipelining section. For information on all list functions , refer the Siddhi APIs . define stream ProductComboStream ( product1 string , product2 string , product3 string ); @ info ( name = Create - list ) from ProductComboStream select list : create ( product1 , product2 , product3 ) as productList insert into NewListStream ; @ info ( name = Check - list ) from NewListStream select list : isList ( productList ) as isList , list : contains ( productList , Cake ) as isCakePresent , list : isEmpty ( productList ) as isEmpty , list : get ( productList , 1 ) as valueAt1 , list : size ( productList ) as size insert into ListAnalysisStream ; @ info ( name = Clone - and - update ) from NewListStream select list : remove ( list : add ( list : clone ( productList ), Toffee ), Cake ) as productList insert into UpdatedListStream ; define stream ProductComboStream ( product1 string, product2 string, product3 string); Defines ProductComboStream having string type attributes product1 , product2 , and product3 . @info(name = Create-list ) from ProductComboStream select list:create(product1, product2, product3) as productList Create a list with values of product1 , product2 , and product3 . insert into NewListStream; @info(name = Check-list ) from NewListStream select list:isList(productList) as isList, Check if productList is a List. list:contains(productList, Cake ) as isCakePresent, Check if productList contains 'Cake' . list:isEmpty(productList) as isEmpty, Check if productList is empty. list:get(productList, 1) as valueAt1, Get the value at index 1 from productList . list:size(productList) as size Get size of productList . insert into ListAnalysisStream; @info(name = Clone-and-update ) from NewListStream select list:remove( list:add(list:clone(productList), Toffee ), Cake ) as productList Clone productList , add Toffee to the end of the list, and remove Cake from the list. insert into UpdatedListStream; Input Below event is sent to ProductComboStream , [ 'Ice Cream' , 'Chocolate' , 'Cake' ] Output After processing, the following events will be arriving at each stream: NewListStream: [ [Ice Cream, Chocolate, Cake] ] ListAnalysisStream: [ true , true , false , Chocolate , 3 ] UpdatedListStream: [ [Ice Cream, Chocolate, Toffee] ]","title":"List"},{"location":"docs/examples/logical-pattern/","text":"var base_url = \"\"; Logical Pattern Logical patterns match events that arrive in temporal order and correlate them with logical relationships such as and , or and not . Refer the Siddhi query guide for more information. define stream RegulatorStateChangeStream ( deviceID long , roomNo int , tempSet double , action string ); define stream RoomKeyStream ( deviceID long , roomNo int , action string ); @ sink ( type = log ) define stream RegulatorActionStream ( roomNo int , action string ); from every e1 = RegulatorStateChangeStream [ action == on ] - e2 = RoomKeyStream [ e1 . roomNo == roomNo and action == removed ] or e3 = RegulatorStateChangeStream [ e1 . roomNo == roomNo and action == off ] select e1 . roomNo , ifThenElse ( e2 is null , none , stop ) as action having action != none insert into RegulatorActionStream ; define stream RegulatorStateChangeStream(deviceID long, roomNo int, tempSet double, action string); Defines RegulatorStateChangeStream having information of regulator state change such as deviceID , roomNo , tempSet and action . define stream RoomKeyStream(deviceID long, roomNo int, action string); Defines RoomKeyStream which contains the events related to room key usage. @sink(type= log ) define stream RegulatorActionStream(roomNo int, action string); Defines RegulatorActionStream which contains the events related to regulator state changes. from every e1=RegulatorStateChangeStream[ action == on ] - e2=RoomKeyStream [ e1.roomNo == roomNo and action == removed ] or e3=RegulatorStateChangeStream [ e1.roomNo == roomNo and action == off ] Sends a stop action on RegulatorActionStream stream, if a removed action is triggered in the RoomKeyStream stream before the regulator state changing to off which is notified RegulatorStateChangeStream stream select e1.roomNo, ifThenElse( e2 is null, none , stop ) as action having action != none Checks whether pattern triggered due to removal of room key. insert into RegulatorActionStream; This application sends a stop action on the regulator if a removed action is triggered in the RoomKeyStream stream. Input First, below event is sent to RegulatorStateChangeStream , [ 10 , 5 , 30 , on ] Then, send below events are sent to RoomKeyStream , [ 10 , 5 , removed ] Output After processing the above input events, the event arriving at RegulatorActionStream will be as follows: [ 5 , stop ]","title":"Logical pattern"},{"location":"docs/examples/logical-sequence/","text":"var base_url = \"\"; Logical Sequence The sequence can repetitively match event sequences and use logical event ordering (using and, or, and not). Refer the Siddhi query guide for more information. define stream TempSensorStream ( deviceID long , isActive bool ); define stream HumidSensorStream ( deviceID long , isActive bool ); define stream RegulatorStream ( deviceID long , isOn bool ); @ sink ( type = log ) define stream StateNotificationStream ( deviceID long , tempSensorActive bool , humidSensorActive bool ); from every e1 = RegulatorStream [ isOn == true ], e2 = TempSensorStream and e3 = HumidSensorStream select e1 . deviceID , e2 . isActive as tempSensorActive , e3 . isActive as humidSensorActive insert into StateNotificationStream ; define stream TempSensorStream(deviceID long, isActive bool); Defines TempSensorStream having information of temperature sensor device. define stream HumidSensorStream(deviceID long, isActive bool); Defines HumidSensorStream having information of humidity sensor device. define stream RegulatorStream(deviceID long, isOn bool); Defines RegulatorStream which contains the events from regulator with attributes deviceID and isOn . @sink(type= log ) define stream StateNotificationStream (deviceID long, tempSensorActive bool, humidSensorActive bool); Defines StateNotificationStream which tells the state of temperature and humidity sensors. from every e1=RegulatorStream[isOn == true], e2=TempSensorStream and e3=HumidSensorStream select e1.deviceID, e2.isActive as tempSensorActive, e3.isActive as humidSensorActive insert into StateNotificationStream; Identifies a regulator activation event immediately followed by both temperature sensor and humidity sensor activation events in either order. This application can be used identify a regulator activation event immediately followed by both temperature sensor and humidity sensor activation events in either order. Input First, below event is sent to RegulatorStream , [ 2134 , true ] Then, below event is sent to HumidSensorStream , [ 124 , true ] Then, below event is sent to TempSensorStream , [ 242 , false ] Output After processing the above input events, the event arriving at StateNotificationStream will be as follows: [ 2134 , false , true ]","title":"Logical sequence"},{"location":"docs/examples/map/","text":"var base_url = \"\"; Map Provides examples on basic map functions provided via siddhi-execution-map extension. For information of performing scatter and gather using map:tokenize() , and map:collect() refer the examples in Data Pipelining section. For information on all map functions , refer the Siddhi APIs . define stream CoupleDealInfoStream ( item1 string , price1 double , item2 string , price2 double ); @ info ( name = Create - map ) from CoupleDealInfoStream select map : create ( item1 , price1 , item2 , price2 ) as itemPriceMap insert into NewMapStream ; @ info ( name = Check - map ) from NewMapStream select map : isMap ( itemPriceMap ) as isMap , map : containsKey ( itemPriceMap , Cookie ) as isCookiePresent , map : containsValue ( itemPriceMap , 24.0 ) as isThereItemWithPrice24 , map : isEmpty ( itemPriceMap ) as isEmpty , map : keys ( itemPriceMap ) as keys , map : size ( itemPriceMap ) as size insert into MapAnalysisStream ; @ info ( name = Clone - and - update ) from NewMapStream select map : replace ( map : put ( map : clone ( itemPriceMap ), Gift , 1.0 ), Cake , 12.0 ) as itemPriceMap insert into ItemInsertedMapStream ; define stream CoupleDealInfoStream ( item1 string, price1 double, item2 string, price2 double); Defines CoupleDealInfoStream having attributes item1 , price1 , item2 , and price2 with string and double types. @info(name = Create-map ) from CoupleDealInfoStream select map:create(item1, price1, item2, price2) as itemPriceMap Create a map with values of item1 and item2 as keys, and price1 and price2 as values. insert into NewMapStream; @info(name = Check-map ) from NewMapStream select map:isMap(itemPriceMap) as isMap, Check if itemPriceMap is a Map. map:containsKey(itemPriceMap, Cookie ) as isCookiePresent, Check if itemPriceMap contains a key 'Cookie' . map:containsValue(itemPriceMap, 24.0) as isThereItemWithPrice24, Check if itemPriceMap contains a value 24.0 . map:isEmpty(itemPriceMap) as isEmpty, Check if itemPriceMap is empty. map:keys(itemPriceMap) as keys, Get all keys of itemPriceMap as a List. map:size(itemPriceMap) as size Get size of itemPriceMap . insert into MapAnalysisStream; @info(name = Clone-and-update ) from NewMapStream select map:replace( map:put(map:clone(itemPriceMap), Gift , 1.0), Cake , 12.0) as itemPriceMap Clone itemPriceMap , put Gift key with value 1.0 to it, and replace Cake key with value 12.0 . insert into ItemInsertedMapStream; Input Below event is sent to CoupleDealInfoStream , [ 'Chocolate' , 18.0 , 'Ice Cream' , 24.0 ] Output After processing, the following events will be arriving at each stream: NewMapStream: [ {Ice Cream=24.0, Chocolate =18.0} ] MapAnalysisStream: [ true , false , true , false , [Ice Cream, Chocolate] , 2 ] ItemInsertedMapStream: [ {Ice Cream=24.0, Gift=1.0, Chocolate =18.0} ]","title":"Map"},{"location":"docs/examples/math-logical-operations/","text":"var base_url = \"\"; Math & Logical Operation Provides examples on performing math or logical operations on events. To see all complex math operations that can be performed please see Siddhi Execution Math Docs define stream TemperatureStream ( sensorId string , temperature double ); @ infor ( name = celciusTemperature ) from TemperatureStream select sensorId , ( temperature * 9 / 5 ) + 32 as temperature insert into FahrenheitTemperatureStream ; @ info ( name = Overall - analysis ) from FahrenheitTemperatureStream select sensorId , math : floor ( temperature ) as approximateTemp insert all events into OverallTemperatureStream ; @ info ( name = RangeFilter ) from OverallTemperatureStream [ approximateTemp - 2 and approximateTemp 40 ] select * insert into NormalTemperatureStream ; define stream TemperatureStream (sensorId string, temperature double); @infor(name = celciusTemperature ) from TemperatureStream select sensorId, (temperature * 9 / 5) + 32 as temperature Converts Celsius value into Fahrenheit. insert into FahrenheitTemperatureStream; @info(name = Overall-analysis ) from FahrenheitTemperatureStream select sensorId, math:floor(temperature) as approximateTemp Calculate approximated temperature to the first digit insert all events into OverallTemperatureStream; @info(name = RangeFilter ) from OverallTemperatureStream [ approximateTemp -2 and approximateTemp 40] Filter out events where -2 approximateTemp 40 select * insert into NormalTemperatureStream; Input Below event is sent to TemperatureStream , [ 'SensorId' , -17 ] Output After processing, the following events will be arriving at each stream: FahrenheitTemperatureStream: [ 'SensorId' , 1.4 ] OverallTemperatureStream: [ 'SensorId' , 1.0 ] NormalTemperatureStream: [ 'SensorId' , 1.0 ]","title":"Math logical operations"},{"location":"docs/examples/named-window/","text":"var base_url = \"\"; Named Window Provides examples on defining a named window, and summarizing data based on that. This example uses time window as the named window, but any window can be defined and used as a name window. For more information on named windows refer the Siddhi query guide . define stream TemperatureStream ( sensorId string , temperature double ); define window OneMinTimeWindow ( sensorId string , temperature double ) time ( 1 min ) ; @ info ( name = Insert - to - window ) from TemperatureStream insert into OneMinTimeWindow ; @ info ( name = Min - max - analysis ) from OneMinTimeWindow select min ( temperature ) as minTemperature , max ( temperature ) as maxTemperature insert into MinMaxTemperatureOver1MinStream ; @ info ( name = Per - sensor - analysis ) from OneMinTimeWindow select sensorId , avg ( temperature ) as avgTemperature group by sensorId insert into AvgTemperaturePerSensorStream ; define stream TemperatureStream (sensorId string, temperature double); define window OneMinTimeWindow (sensorId string, temperature double) time(1 min) ; Define a named window with name OneMinTimeWindow to retain events over 1 minute in a sliding manner. @info(name = Insert-to-window ) from TemperatureStream insert into OneMinTimeWindow; Insert events in to the named time window. @info(name = Min-max-analysis ) from OneMinTimeWindow select min(temperature) as minTemperature, max(temperature) as maxTemperature Calculate minimum and maximum of temperature on events in OneMinTimeWindow window. insert into MinMaxTemperatureOver1MinStream; @info(name = Per-sensor-analysis ) from OneMinTimeWindow select sensorId, avg(temperature) as avgTemperature group by sensorId Calculate average of temperature , by grouping events by sensorId , on the OneMinTimeWindow window. insert into AvgTemperaturePerSensorStream; Aggregation Behavior When events are sent to TemperatureStream stream, following events will get emitted at MinMaxTemperatureOver1MinStream stream via Min-max-analysis query, and AvgTemperaturePerSensorStream stream via Per-sensor-analysis query. .md-typeset table:not([class]) th:nth-child(2) { min-width: 125px; } Time Input to TemperatureStream Output at MinMaxTemperatureOver1MinStream Output at AvgTemperaturePerSensorStream 9:00:10 [ '1001' , 21.0 ] [ 21.0 , 21.0 ] [ '1001' , 21.0 ] 9:00:20 [ '1002' , 25.0 ] [ 21.0 , 25.0 ] [ '1002' , 25.0 ] 9:00:35 [ '1002' , 26.0 ] [ 21.0 , 26.0 ] [ '1002' , 25.5 ] 9:00:40 [ '1002' , 27.0 ] [ 21.0 , 27.0 ] [ '1002' , 26.0 ] 9:00:55 [ '1001' , 19.0 ] [ 19.0 , 27.0 ] [ '1001' , 20.0 ] 9:01:30 [ '1002' , 22.0 ] [ 19.0 , 27.0 ] [ '1002' , 25.0 ] 9:02:10 [ '1001' , 18.0 ] [ 18.0 , 22.0 ] [ '1001' , 18.0 ]","title":"Named window"},{"location":"docs/examples/non-occurrence-pattern/","text":"var base_url = \"\"; Non Occurrence Pattern Non occurrence patterns identifies the absence of events when detecting a pattern. Siddhi detects non-occurrence of events using the not keyword, and its effective non-occurrence checking period is bounded either by fulfillment of a condition associated by and or via an expiry time using time period . Refer the Siddhi query guide for more information. define stream RegulatorStateChangeStream ( deviceID long , roomNo int , tempSet double , action string ); define stream TemperatureStream ( roomNo int , temp double ); @ sink ( type = log ) define stream RoomTemperatureAlertStream ( roomNo int ); from e1 = RegulatorStateChangeStream [ action == on ] - not TemperatureStream [ e1 . roomNo == roomNo and temp = e1 . tempSet ] for 30 sec select e1 . roomNo as roomNo insert into RoomTemperatureAlertStream ; define stream RegulatorStateChangeStream(deviceID long, roomNo int, tempSet double, action string); Defines RegulatorStateChangeStream having information of regulator state change such as deviceID , roomNo , tempSet and action . define stream TemperatureStream (roomNo int, temp double); Defines TemperatureStream having information of room temperature such as roomNo and temp . @sink(type= log ) define stream RoomTemperatureAlertStream(roomNo int); Defines RoomTemperatureAlertStream which contains the temperature alerts. from e1=RegulatorStateChangeStream[action == on ] - not TemperatureStream[e1.roomNo == roomNo and temp = e1.tempSet] for 30 sec Alerts if no temperature event having a temperature less than what is set in regulator arrives within 5 minutes after switching on the regulator. select e1.roomNo as roomNo insert into RoomTemperatureAlertStream; This application sends a notification alert if the room temperature is not reduced to the expected level after the regulator is started. Input First, below event is sent to RegulatorStateChangeStream , [ 10 , 5 , 30 , on ] Output After processing the above input event, there will be an alert event arriving at RoomTemperatureAlertStream after the 30 seconds (from the first event): [ 5 ]","title":"Non occurrence pattern"},{"location":"docs/examples/null/","text":"var base_url = \"\"; Null Provides examples on using nulls in Siddhi Apps. For more information refer Siddhi query guide . define stream ProductInputStream ( item string , price double ); define Table ProductInfoTable ( item string , discount double ); @ info ( name = Check - for - null ) from ProductInputStream [ not ( item is null )] select item , price is null as isPriceNull insert into ProductValidationStream ; @ info ( name = Outer - join - with - table ) from ProductInputStream as s left outer join ProductInfoTable as t on s . item == t . item select s . item , s . price , t . discount , math : power ( t . discount , 2 ) is null as isFunctionReturnsNull , t is null as isTNull , s is null as isSNull , t . discount is null as isTDiscountNull , s . item is null as isSItemNull insert into DiscountValidationStream ; define stream ProductInputStream (item string, price double); define Table ProductInfoTable (item string, discount double); Empty ProductInfoTable with attributes item and discount . @info(name = Check-for-null ) from ProductInputStream [not(item is null)] Filter events with item not having null value. select item, price is null as isPriceNull Checks if price contains null value. insert into ProductValidationStream; @info(name = Outer-join-with-table ) from ProductInputStream as s left outer join ProductInfoTable as t on s.item == t.item select s.item, s.price, t.discount, math:power(t.discount, 2) is null as isFunctionReturnsNull, Check if math:power() returns null . t is null as isTNull, s is null as isSNull, Check if streams t and s are null . t.discount is null as isTDiscountNull, s.item is null as isSItemNull Check if streams attributes t.discount and s.item are null . insert into DiscountValidationStream; Input Below event is sent to ProductInputStream , [ 'Cake' , 12.0 ] Output After processing, the following events will be arriving at each stream: ProductValidationStream: [ Cake , false ] DiscountValidationStream: [ Cake , 12.0 , null , true , true , false , true , false ]","title":"Null"},{"location":"docs/examples/partition-by-value/","text":"var base_url = \"\"; Partition Events by Value Provides example on partitioning events by attribute values. For more information on partitioning events based on value ranges, refer other examples under data pipelining section. For more information on partition refer the Siddhi query guide . define stream LoginStream ( userID string , loginSuccessful bool ); @ purge ( enable = true , interval = 10 sec , idle . period = 1 hour ) partition with ( userID of LoginStream ) begin @ info ( name = Aggregation - query ) from LoginStream # window . length ( 3 ) select userID , loginSuccessful , count () as attempts group by loginSuccessful insert into # LoginAttempts ; @ info ( name = Alert - query ) from # LoginAttempts [ loginSuccessful == false and attempts == 3 ] select userID , 3 consecutive login failures! as message insert into UserSuspensionStream ; end ; define stream LoginStream ( userID string, loginSuccessful bool); @purge(enable= true , interval= 10 sec , idle.period= 1 hour ) Optional purging configuration, to remove partition instances that haven t received events for 1 hour by checking every 10 sec . partition with ( userID of LoginStream ) Partitions the events based on userID . begin @info(name= Aggregation-query ) from LoginStream#window.length(3) select userID, loginSuccessful, count() as attempts group by loginSuccessful Calculates success and failure login attempts from last 3 events of each userID . insert into #LoginAttempts; Inserts results to #LoginAttempts inner stream that is only accessible within the partition instance. @info(name= Alert-query ) from #LoginAttempts[loginSuccessful==false and attempts==3] select userID, 3 consecutive login failures! as message insert into UserSuspensionStream; Consumes events from the inner stream, and suspends userID s that have 3 consecutive login failures. end; Partition Behavior When events are sent to LoginStream stream, following events will be generated at #LoginAttempts inner stream via Aggregation-query query, and UserSuspensionStream stream via Alert-query query. .md-typeset table:not([class]) th:nth-child(2) { min-width: 150px; } Input to TemperatureStream At #LoginAttempts Output at UserSuspensionStream [ '1001' , false ] [ '1001' , false , 1 ] - [ '1002' , true ] [ '1002' , true , 1 ] - [ '1002' , false ] [ '1002' , false , 1 ] - [ '1002' , false ] [ '1002' , false , 2 ] - [ '1001' , false ] [ '1001' , false , 2 ] - [ '1001' , true ] [ '1001' , true , 1 ] - [ '1001' , false ] [ '1001' , false , 2 ] - [ '1002' , false ] [ '1002' , false , 2 ] [ '1002' , '3 consecutive login failures!' ]","title":"Partition by value"},{"location":"docs/examples/regex-matching/","text":"var base_url = \"\"; Regex Matching This demonstrates event cleansing using regex expressions. Regex operations can be performed using Siddhi Execution Regex extension. define stream SweetProductionStream ( name string , amount int ); @ info ( name = ProcessSweetProductionStream ) from SweetProductionStream select name , regex : matches ( chocolate (. * ) , name ) as isAChocolateProduct , regex : group ( . * \\ s (. * ) , name , 1 ) as sweetType insert into ChocolateProductStream ; define stream SweetProductionStream (name string, amount int); Defines SweetProductionStream having information of name and amount @info(name= ProcessSweetProductionStream ) from SweetProductionStream select name, regex:matches( chocolate(.*) , name) as isAChocolateProduct, Matches if name begins with the word chocolate regex:group( .*\\s(.*) , name, 1) as sweetType Captures the sweetType of the sweet following the flavour in name insert into ChocolateProductStream; Input Below event is sent to SweetProductionStream , [ 'chocolate cake' , 34 ] Output After processing, the event arriving at ChocolateProductStream will be as follows: [ 'chocolate cake' , true , 'cake' ]","title":"Regex matching"},{"location":"docs/examples/scatter-and-gather-json/","text":"var base_url = \"\"; Scatter and Gather (JSON) Provides example on performing scatter and gather on JSON values. For more information on performing scatter and gather on string, refer other examples under data pipelining section. define stream PurchaseStream ( order string , store string ); @ info ( name = Scatter - query ) from PurchaseStream #json:tokenize( order , $ . order . items ) select json : getString ( order , $ . order . id ) as orderId , jsonElement as item , store insert into TokenizedItemStream ; @ info ( name = Transform - query ) from TokenizedItemStream select orderId , ifThenElse ( json : getString ( item , name ) == cake , json : toString ( json : setElement ( item , price , json : getDouble ( item , price ) - 5 ) ), item ) as item , store insert into DiscountedItemStream ; @ info ( name = Gather - query ) from DiscountedItemStream # window . batch () select orderId , json : group ( item ) as items , store insert into GroupedItemStream ; @ info ( name = Format - query ) from GroupedItemStream select str : fillTemplate ( { discountedOrder : { id : {{ 1 }} , store : {{ 3 }} , items :{{2}} } } , orderId , items , store ) as discountedOrder insert into DiscountedOrderStream ; define stream PurchaseStream (order string, store string); @info(name = Scatter-query ) from PurchaseStream#json:tokenize(order, $.order.items ) select json:getString(order, $.order.id ) as orderId, jsonElement as item, store Scatter elements under $.order.items in to separate events. insert into TokenizedItemStream; @info(name = Transform-query ) from TokenizedItemStream select orderId, ifThenElse(json:getString(item, name ) == cake , json:toString( json:setElement(item, price , json:getDouble(item, price ) - 5 ) ), item) as item, store Provide $5 discount to cakes. insert into DiscountedItemStream; @info(name = Gather-query ) from DiscountedItemStream#window.batch() Collect events traveling as a batch via batch() window. select orderId, json:group(item) as items, store Combine item from all events in a batch as a single JSON Array. insert into GroupedItemStream; @info(name = Format-query ) from GroupedItemStream select str:fillTemplate( { discountedOrder : { id : {{1}} , store : {{3}} , items :{{2}} } } , orderId, items, store) as discountedOrder Format the final JSON by combining orderId , items , and store . insert into DiscountedOrderStream; Input Below event is sent to PurchaseStream , [{ order :{ id : 501 , items :[{ name : cake , price :25.0}, { name : cookie , price :15.0}, { name : bun , price :20.0} ] } }, 'CA'] Output After processing, following events will be arriving at TokenizedItemStream : [ '501' , '{ name : cake , price :25.0}' , 'CA' ], [ '501' , '{ name : cookie , price :15.0}' , 'CA' ], [ '501' , '{ name : bun , price :20.0}' , 'CA' ] The events arriving at DiscountedItemStream will be as follows: [ '501' , '{ name : cake , price :20.0}' , 'CA' ], [ '501' , '{ name : cookie , price :15.0}' , 'CA' ], [ '501' , '{ name : bun , price :20.0}' , 'CA' ] The event arriving at GroupedItemStream will be as follows: [ '501' , '[{ price :20.0, name : cake },{ price :15.0, name : cookie },{ price :20.0, name : bun }]' , 'CA' ] The event arriving at DiscountedOrderStream will be as follows: [ '{ discountedOrder : { id : 501 , store : CA , items :[{ price :20.0, name : cake }, { price :15.0, name : cookie }, { price :20.0, name : bun }] } }' ]","title":"Scatter and gather json"},{"location":"docs/examples/scatter-and-gather-string/","text":"var base_url = \"\"; Scatter and Gather (String) Provides example on performing scatter and gather on string values. For more information on performing scatter and gather on json, refer other examples under data pipelining section. define stream PurchaseStream ( userId string , items string , store string ); @ info ( name = Scatter - query ) from PurchaseStream #str:tokenize( items , , , true ) select userId , token as item , store insert into TokenizedItemStream ; @ info ( name = Transform - query ) from TokenizedItemStream select userId , str : concat ( store , - , item ) as itemKey insert into TransformedItemStream ; @ info ( name = Gather - query ) from TransformedItemStream # window . batch () select userId , str : groupConcat ( itemKey , , ) as itemKeys insert into GroupedPurchaseItemStream ; define stream PurchaseStream (userId string, items string, store string); @info(name = Scatter-query ) from PurchaseStream#str:tokenize(items, , , true) select userId, token as item, store Scatter value of items in to separate events by , . insert into TokenizedItemStream; @info(name = Transform-query ) from TokenizedItemStream select userId, str:concat(store, - , item) as itemKey Concat tokenized item with store . insert into TransformedItemStream; @info(name = Gather-query ) from TransformedItemStream#window.batch() Collect events traveling as a batch via batch() window. select userId, str:groupConcat(itemKey, , ) as itemKeys Concat all events in a batch separating them by , . insert into GroupedPurchaseItemStream; Input Below event containing a JSON string is sent to PurchaseStream , [ '501' , 'cake,cookie,bun,cookie' , 'CA' ] Output After processing, the events arriving at TokenizedItemStream will be as follows: [ '501' , 'cake' , 'CA' ], [ '501' , 'cookie' , 'CA' ], [ '501' , 'bun' , 'CA' ] The events arriving at TransformedItemStream will be as follows: [ '501' , 'CA-cake' ], [ '501' , 'CA-cookie' ], [ '501' , 'CA-bun' ] The event arriving at GroupedPurchaseItemStream will be as follows: [ '501' , 'CA-cake,CA-cookie,CA-bun' ]","title":"Scatter and gather string"},{"location":"docs/examples/sequence-with-count/","text":"var base_url = \"\"; Sequence with Count Sequence query does expect the matching events to occur immediately after each other, and it can successfully correlate the events who do not have other events in between. Here, sequence can count event occurrences. Refer the Siddhi query guide for more information. define stream TemperatureStream ( roomNo int , temp double ); @ sink ( type = log ) define stream PeekTemperatureStream ( roomNo int , initialTemp double , peekTemp double , firstDropTemp double ); partition with ( roomNo of TemperatureStream ) begin @ info ( name = temperature - trend - analyzer ) from every e1 = TemperatureStream , e2 = TemperatureStream [ ifThenElse ( e2 [ last ]. temp is null , e1 . temp = temp , e2 [ last ]. temp = temp )] + , e3 = TemperatureStream [ e2 [ last ]. temp temp ] select e1 . roomNo , e1 . temp as initialTemp , e2 [ last ]. temp as peekTemp , e3 . temp as firstDropTemp insert into PeekTemperatureStream ; end ; define stream TemperatureStream(roomNo int, temp double); Defines TemperatureStream having information on room temperatures such as roomNo and temp . @sink(type= log ) define stream PeekTemperatureStream(roomNo int, initialTemp double, peekTemp double, firstDropTemp double); Defines PeekTemperatureStream which contains events related to peak temperature trends. partition with (roomNo of TemperatureStream) begin Partition the TemperatureStream events by roomNo @info(name = temperature-trend-analyzer ) from every e1=TemperatureStream, e2=TemperatureStream[ifThenElse(e2[last].temp is null, e1.temp = temp, e2[last].temp = temp)]+, e3=TemperatureStream[e2[last].temp temp] Identifies the trend of the temperature in a room select e1.roomNo, e1.temp as initialTemp, e2[last].temp as peekTemp, e3.temp as firstDropTemp insert into PeekTemperatureStream ; Projects the lowest, highest and the first drop in the temperature trend end; This application identifies temperature peeks by monitoring continuous increases in temp attribute and alerts upon the first drop. Input Below events are sent to TemperatureStream , [ 20 , 29 ] [ 10 , 28 ] [ 20 , 30 ] [ 20 , 32 ] [ 20 , 35 ] [ 20 , 33 ] Output After processing the above input events, the event arriving at PeekTemperatureStream will be as follows: [ 20 , 29.0 , 35.0 , 33.0 ]","title":"Sequence with count"},{"location":"docs/examples/session/","text":"var base_url = \"\"; Session Provides examples on aggregating events over continuous activity sessions in a sliding manner. To aggregate events in batches, or based on events, refer other the examples in Data Summarization section. For more information on windows refer the Siddhi query guide . define stream PurchaseStream ( userId string , item string , price double ); @ info ( name = Session - analysis ) from PurchaseStream # window . session ( 1 min , userId ) select userId , count () as totalItems , sum ( price ) as totalPrice group by userId insert into UserIdPurchaseStream ; @ info ( name = Session - analysis - with - late - event - arrivals ) from PurchaseStream # window . session ( 1 min , userId , 20 sec ) select userId , count () as totalItems , sum ( price ) as totalPrice group by userId insert into OutOfOrderUserIdPurchaseStream ; define stream PurchaseStream (userId string, item string, price double); @info(name = Session-analysis ) from PurchaseStream#window.session(1 min, userId) Aggregate events over a userId based session window with 1 minute session gap. select userId, count() as totalItems, sum(price) as totalPrice group by userId Calculate count and sum of price per userId during the session. insert into UserIdPurchaseStream; Output when events are added to the session. @info(name = Session-analysis-with-late-event-arrivals ) from PurchaseStream#window.session(1 min, userId, 20 sec) Aggregate events over a userId based session window with 1 minute session gap, and 20 seconds of allowed latency to capture late event arrivals. select userId, count() as totalItems, sum(price) as totalPrice group by userId Calculate count and sum of price per userId during the session. insert into OutOfOrderUserIdPurchaseStream; Output when events are added to the session. Aggregation Behavior When events are sent to PurchaseStream stream, following events will get emitted at UserIdPurchaseStream stream via Session-analysis query, and OutOfOrderUserIdPurchaseStream stream via Session-analysis-with-late-event-arrivals query. .md-typeset table:not([class]) th:nth-child(3) { min-width: 120px; } .md-typeset table:not([class]) th { min-width: 0; } Time Event Timestamp Input to PurchaseStream Output at UserIdPurchaseStream Output at OutOfOrderUserIdPurchaseStream 9:00:00 9:00:00 [ '1001' , 'cake' , 18.0 ] [ '1001' , 1 , 18.0 ] [ '1001' , 1 , 18.0 ] 9:00:20 9:00:20 [ '1002' , 'croissant' , 23.0 ] [ '1002' , 1 , 23.0 ] [ '1002' , 1 , 23.0 ] 9:00:40 9:00:40 [ '1002' , 'cake' , 22.0 ] [ '1002' , 2 , 45.0 ] [ '1002' , 2 , 45.0 ] 9:01:05 9:00:50 [ '1001' , 'pie' , 22.0 ] No events, as event arrived late, and did not fall into a session. [ '1001' , 2 , 40.0 ] 9:01:10 9:01:10 [ '1001' , 'cake' , 10.0 ] [ '1001' , 1 , 10.0 ] [ '1001' , 3 , 50.0 ] 9:01:50 9:01:50 [ '1002' , 'cake' , 20.0 ] [ '1002' , 1 , 20.0 ] [ '1002' , 1 , 23.0 ] 9:02:40 9:02:40 [ '1001' , 'croissant' , 23.0 ] [ '1001' , 1 , 23.0 ] [ '1001' , 1 , 23.0 ]","title":"Session"},{"location":"docs/examples/siddhiapp/","text":"var base_url = \"\"; Siddhi Application Provides introduction to the concept of Siddhi Application . Siddhi App provides an isolated execution environment for processing the execution logic. It can be deployed and processed independently of other SiddhiApps in the system. Siddhi Apps can use inMemory sources and sinks to communicate between each other. @app:name( Temperature - Processor ) @app:description( App for processing temperature data . ) @ source ( type = inMemory , topic = SensorDetail ) define stream TemperatureStream ( sensorId string , temperature double ); @ sink ( type = inMemory , topic = Temperature ) define stream TemperatureOnlyStream ( temperature double ); @ info ( name = Simple - selection ) from TemperatureStream select temperature insert into TemperatureOnlyStream ; @app:name( Temperature-Processor ) Name of the Siddhi Application @app:description( App for processing temperature data. ) Optional description for Siddhi Application @source(type= inMemory , topic= SensorDetail ) define stream TemperatureStream ( sensorId string, temperature double); InMemory source to consume events from other Siddhi Apps. @sink(type= inMemory , topic= Temperature ) define stream TemperatureOnlyStream (temperature double); InMemory sink to publish events from other Siddhi Apps. @info(name = Simple-selection ) from TemperatureStream select temperature insert into TemperatureOnlyStream; Input When an event [ 'aq-14' , 35.4 ] is pushed via the SensorDetail topic of the inMemory transport from another Siddhi App, the event will be consumed and mapped to the TemperatureStream stream. Output After processing, the event [ 35.4 ] arriving at TemperatureOnlyStream will be emitted via Temperature topic of the inMemory transport to other subscribed Siddhi Apps.","title":"Siddhiapp"},{"location":"docs/examples/simple-pattern/","text":"var base_url = \"\"; Simple Pattern The pattern is a state machine implementation that detects event occurrences from events arrived via one or more event streams over time. This application demonstrates a simple pattern use case of detecting high-temperature event occurrence of a continuous event stream. define stream TemperatureStream ( roomNo int , temp double ); @ sink ( type = log ) define Stream HighTempAlertStream ( roomNo int , initialTemp double , finalTemp double ); @ info ( name = temperature - increase - identifier ) from every ( e1 = TemperatureStream ) - e2 = TemperatureStream [ e1 . roomNo == roomNo and ( e1 . temp + 5 ) = temp ] within 10 min select e1 . roomNo , e1 . temp as initialTemp , e2 . temp as finalTemp insert into HighTempAlertStream ; define stream TemperatureStream(roomNo int, temp double); Defines TemperatureStream having information of room temperature such as roomNo and temp . @sink(type = log ) define Stream HighTempAlertStream(roomNo int, initialTemp double, finalTemp double); Defines HighTempAlertStream which contains the alerts for high temperature. @info(name= temperature-increase-identifier ) from every( e1 = TemperatureStream ) - e2 = TemperatureStream[ e1.roomNo == roomNo and (e1.temp + 5) = temp ] within 10 min Identify if the temperature of a room increases by 5 degrees within 10 min. select e1.roomNo, e1.temp as initialTemp, e2.temp as finalTemp insert into HighTempAlertStream; This application sends an alert if the temperature of a room increases by 5 degrees within 10 min. Input Below events are sent to TemperatureStream within 10 minutes, [ 2 , 35 ] [ 2 , 37 ] [ 2 , 40 ] Output After processing the above input events, the event arriving at HighTempAlertStream will be as follows: [ 2 , 35.0 , 40.0 ]","title":"Simple pattern"},{"location":"docs/examples/simple-sequence/","text":"var base_url = \"\"; Simple Sequence Sequence is a state machine implementation that detects consecutive event occurrences from events arrived via one or more event streams over time. Here all matching events need to arrive consecutively, and there should not be any non-matching events in between the matching sequence of events. Refer the Siddhi query guide for more information. define stream StockRateStream ( symbol string , price float , volume int ); @ sink ( type = log ) define stream PeakStockRateStream ( symbol string , rateAtPeak float ); partition with ( symbol of StockRateStream ) begin from every e1 = StockRateStream , e2 = StockRateStream [ e1 . price price ], e3 = StockRateStream [ e2 . price price ] within 10 min select e1 . symbol , e2 . price as rateAtPeak insert into PeakStockRateStream ; end ; define stream StockRateStream (symbol string, price float, volume int); Defines StockRateStream having information on stock rate such as symbol , price and volume . @sink(type= log ) define stream PeakStockRateStream (symbol string, rateAtPeak float); Defines PeakStockRateStream which contains the peak stock rate. partition with (symbol of StockRateStream) begin Partition the StockRateStream events by symbol from every e1=StockRateStream, e2=StockRateStream[e1.price price], e3=StockRateStream[e2.price price] Identifies the peak stock price (top rate of the stock price trend) within 10 min select e1.symbol, e2.price as rateAtPeak insert into PeakStockRateStream ; end; This application can be used to detect trends from a stock trades stream; in the above example, peak stock rate identified. Input Below events are sent to StockRateStream within 10 minutes, [ mint-leaves , 35 , 20 ] [ mint-leaves , 40 , 15 ] [ mint-leaves , 38 , 20 ] Output After processing the above input events, the event arriving at PeakStockRateStream will be as follows: [ mint-leaves , 40 ]","title":"Simple sequence"},{"location":"docs/examples/sliding-length/","text":"var base_url = \"\"; Sliding Event Count Provides examples on aggregating events based on event count in a sliding manner. To aggregate events in batches, based on time, or by session, refer other the examples in Data Summarization section. For more information on windows refer the Siddhi query guide . define stream TemperatureStream ( sensorId string , temperature double ); @ info ( name = Overall - analysis ) from TemperatureStream # window . length ( 4 ) select avg ( temperature ) as avgTemperature , max ( temperature ) as maxTemperature , count () as numberOfEvents insert into OverallTemperatureStream ; @ info ( name = SensorId - analysis ) from TemperatureStream # window . length ( 5 ) select sensorId , avg ( temperature ) as avgTemperature , min ( temperature ) as maxTemperature group by sensorId having avgTemperature = 20.0 insert into SensorIdTemperatureStream ; define stream TemperatureStream (sensorId string, temperature double); @info(name = Overall-analysis ) from TemperatureStream#window.length(4) Aggregate last 4 events in a sliding manner. select avg(temperature) as avgTemperature, max(temperature) as maxTemperature, count() as numberOfEvents insert into OverallTemperatureStream; Calculate average, maximum, and count for temperature attribute. @info(name = SensorId-analysis ) from TemperatureStream#window.length(5) Aggregate last 5 events in a sliding manner. select sensorId, avg(temperature) as avgTemperature, min(temperature) as maxTemperature group by sensorId Calculate average, and minimum for temperature , by grouping events by sensorId . having avgTemperature = 20.0 Output events only when avgTemperature is greater than or equal to 20.0 . insert into SensorIdTemperatureStream; Aggregation Behavior When events are sent to TemperatureStream stream, following events will get emitted at OverallTemperatureStream stream via Overall-analysis query, and SensorIdTemperatureStream stream via SensorId-analysis query. .md-typeset table:not([class]) th:nth-child(2) { min-width: 123px; } Input to TemperatureStream Output at OverallTemperatureStream Output at SensorIdTemperatureStream [ '1001' , 19.0 ] [ 19.0 , 19.0 , 1 ] No events, as having condition not satisfied for '1001' . [ '1002' , 26.0 ] [ 22.5 , 26.0 , 2 ] [ '1002' , 26.0 , 26.0 ] [ '1002' , 24.0 ] [ 23.0 , 26.0 , 3 ] [ '1002' , 25.5 , 24.0 ] [ '1001' , 20.0 ] [ 22.5 , 26.0 , 4 ] No events, as having condition not satisfied for '1001' . [ '1001' , 21.0 ] [ 22.75 , 26.0 , 4 ] [ '1001' , 20.0 , 19.0 ] [ '1001' , 22.0 ] [ 21.75 , 24.0 , 4 ] [ '1001' , 21.0 , 20.0 ]","title":"Sliding length"},{"location":"docs/examples/sliding-time/","text":"var base_url = \"\"; Sliding Time Provides examples on aggregating events over time in a sliding manner. To aggregate events in batches, based on events, or by session, refer other the examples in Data Summarization section. For more information on windows refer the Siddhi query guide . define stream TemperatureStream ( sensorId string , temperature double ); @ info ( name = Overall - analysis ) from TemperatureStream # window . time ( 1 min ) select avg ( temperature ) as avgTemperature , max ( temperature ) as maxTemperature , count () as numberOfEvents insert all events into OverallTemperatureStream ; @ info ( name = SensorId - analysis ) from TemperatureStream # window . time ( 30 sec ) select sensorId , avg ( temperature ) as avgTemperature , min ( temperature ) as maxTemperature group by sensorId having avgTemperature 20.0 insert into SensorIdTemperatureStream ; define stream TemperatureStream (sensorId string, temperature double); @info(name = Overall-analysis ) from TemperatureStream#window.time(1 min) Aggregate events over 1 minute sliding window select avg(temperature) as avgTemperature, max(temperature) as maxTemperature, count() as numberOfEvents Calculate average, maximum, and count for temperature attribute. insert all events into OverallTemperatureStream; Output when events are added, and removed (expired) from window.time() . @info(name = SensorId-analysis ) from TemperatureStream#window.time(30 sec) Aggregate events over 30 seconds sliding window select sensorId, avg(temperature) as avgTemperature, min(temperature) as maxTemperature group by sensorId Calculate average, and minimum for temperature , by grouping events by sensorId . having avgTemperature 20.0 Output events only when avgTemperature is greater than 20.0 . insert into SensorIdTemperatureStream; Output only when events are added to window.time() . Aggregation Behavior When events are sent to TemperatureStream stream, following events will get emitted at OverallTemperatureStream stream via Overall-analysis query, and SensorIdTemperatureStream stream via SensorId-analysis query. .md-typeset table:not([class]) th:nth-child(2) { min-width: 123px; } Time Input to TemperatureStream Output at OverallTemperatureStream Output at SensorIdTemperatureStream 9:00:00 [ '1001' , 18.0 ] [ 18.0 , 18.0 , 1 ] No events, as having condition not satisfied. 9:00:10 [ '1002' , 23.0 ] [ 20.5 , 23.0 , 2 ] [ '1002' , 23.0 , 23.0 ] 9:00:20 [ '1002' , 22.0 ] [ 21.0 , 23.0 , 3 ] [ '1002' , 22.5 , 22.0 ] 9:00:40 - - No events, as expired events are not emitted. 9:00:50 - - No events, as expired events are not emitted. 9:00:00 - [ 22.5 , 23.0 , 2 ] - 9:01:10 [ '1001' , 17.0 ] [ 19.5 , 22.0 , 2 ] - 9:01:20 - [ 17.0 , 17.0 , 1 ] - 9:02:10 - [ null , null , 0 ] -","title":"Sliding time"},{"location":"docs/examples/source-and-sink/","text":"var base_url = \"\"; Source and Sink Provides introduction to sources and sink that are used to consume and publish events to external systems. There are multiple source and sink types, but this example only explains http source, log sink, and kafka sink. For more info refer the Siddhi query guide . @ source ( type = http , receiver . url = http : //0.0.0.0:8006/temp , @ map ( type = json )) define stream TemperatureStream ( sensorId string , temperature double ); @ sink ( type = log ) @ sink ( type = kafka , topic = temperature , bootstrap . servers = localhost : 9092 , @ map ( type = json , @payload( { temp : {{ temperature }} } ))) define stream TemperatureOnlyStream ( temperature double ); @ info ( name = Simple - selection ) from TemperatureStream select temperature insert into TemperatureOnlyStream ; @source(type= http , receiver.url= http://0.0.0.0:8006/temp , @map(type= json )) HTTP source to consume JSON messages with default mapping via url http://0.0.0.0:8006/temp . define stream TemperatureStream ( sensorId string, temperature double); Defines TemperatureStream stream having sensorId and temperature attributes of types string and double . @sink(type= log ) Log sink to log Siddhi events arriving via TemperatureOnlyStream stream. @sink(type= kafka , topic= temperature , bootstrap.servers= localhost:9092 , @map(type= json , @payload( { temp : {{temperature}} } ))) Kafka sink to map events arriving via TemperatureOnlyStream stream as custom JSON events, and publish to temperature topic. define stream TemperatureOnlyStream (temperature double); Defines TemperatureOnlyStream stream having temperature attribute of type double . @info(name = Simple-selection ) from TemperatureStream select temperature insert into TemperatureOnlyStream; Input When a JSON message in the following default message format is sent to url http://0.0.0.0:8006/temp with content type application/json . It will automatically get mapped to an event in the TemperatureStream stream. { event :{ sensorId : aq-14 , temperature :35.4 } } To process custom input messages, please refer the examples related to Input Data Mapping. Output After processing, the event arriving at TemperatureOnlyStream will be emitted via log and kafka sinks. As log sink uses passThrough mapper by default, it directly logs the Siddhi Events to the console as following; Event{timestamp=1574515771712, data=[35.4], isExpired=false} The kafka sink maps the event to a custom JSON message as below and publishes it to the temperature topic. { temp : 35.4 } To output messages using other message formats, pleases refer the examples related to Output Data Mapping.","title":"Source and sink"},{"location":"docs/examples/stream-and-query/","text":"var base_url = \"\"; Stream and Query Provides introduction to streams , queries , and how queries can be chained to one another. There are multiple type of queries such as window query, join query, pattern query, etc. But this example only explains how pass-through and selection queries work. For more info refer the Siddhi query guide . define stream InputTemperatureStream ( sensorId string , temperature double ); @ info ( name = Pass - through ) from InputTemperatureStream select * insert into TemperatureAndSensorStream ; @ info ( name = Simple - selection ) from TemperatureAndSensorStream select temperature insert into TemperatureOnlyStream ; define stream InputTemperatureStream ( sensorId string, temperature double); Defines InputTemperatureStream stream to pass events having sensorId and temperature attributes of types string and double . @info(name = Pass-through ) Optional @info annotation to name the query. from InputTemperatureStream select * insert into TemperatureAndSensorStream; Query to consume events from InputTemperatureStream , produce new events by selecting all the attributes from the incoming events, and outputs them to TemperatureStream . @info(name = Simple-selection ) from TemperatureAndSensorStream Consumes events from TemperatureAndSensorStream . The schema of the stream is inferred from the previous query, hence no need to be defined. select temperature insert into TemperatureOnlyStream; Selects only the temperature attribute from events, and outputs to TemperatureOnlyStream . Events at each stream When an event with values [ 'aq-14' , 35.4 ] is sent to InputTemperatureStream stream it will get converted and travel through the streams as below. InputTemperatureStream : [ 'aq-14' , 35.4 ] TemperatureAndSensorStream : [ 'aq-14' , 35.4 ] TemperatureOnlyStream : [ 35.4 ]","title":"Stream and query"},{"location":"docs/examples/stream-join/","text":"var base_url = \"\"; Stream Join Provides examples on joining two stream based on a condition. For more information on other join operations refer the Siddhi query guide . define stream TemperatureStream ( roomNo string , temperature double ); define stream HumidityStream ( roomNo string , humidity double ); @ info ( name = Equi - join ) from TemperatureStream #window.unique:time( roomNo , 1 min ) as t join HumidityStream #window.unique:time( roomNo , 1 min ) as h on t . roomNo == h . roomNo select t . roomNo , t . temperature , h . humidity insert into TemperatureHumidityStream ; @ info ( name = Join - on - temperature ) from TemperatureStream as t left outer join HumidityStream # window . time ( 1 min ) as h on t . roomNo == h . roomNo select t . roomNo , t . temperature , h . humidity insert into EnrichedTemperatureStream ; define stream TemperatureStream (roomNo string, temperature double); define stream HumidityStream (roomNo string, humidity double); @info(name = Equi-join ) from TemperatureStream#window.unique:time(roomNo, 1 min) as t join HumidityStream#window.unique:time(roomNo, 1 min) as h on t.roomNo == h.roomNo Join latest temperature and humidity events arriving within 1 minute for each roomNo . select t.roomNo, t.temperature, h.humidity insert into TemperatureHumidityStream; @info(name = Join-on-temperature ) from TemperatureStream as t Join when events arrive in TemperatureStream . left outer join HumidityStream#window.time(1 min) as h on t.roomNo == h.roomNo When events get matched in time() window, all matched events are emitted, else null is emitted. select t.roomNo, t.temperature, h.humidity insert into EnrichedTemperatureStream; Join Behavior When events are sent to TemperatureStream stream and HumidityStream stream, following events will get emitted at TemperatureHumidityStream stream via Equi-join query, and EnrichedTemperatureStream stream via Join-on-temperature query. .md-typeset table:not([class]) th:nth-child(3) { min-width: 123px; } .md-typeset table:not([class]) th:nth-child(2) { min-width: 123px; } .md-typeset table:not([class]) th { min-width: 0; } Time Input to TemperatureStream Input to HumidityStream Output at TemperatureHumidityStream Output at EnrichedTemperatureStream 9:00:00 [ '1001' , 18.0 ] - - [ '1001' , 18.0 , null ] 9:00:10 - [ '1002' , 72.0 ] - - 9:00:15 - [ '1002' , 73.0 ] - - 9:00:30 [ '1002' , 22.0 ] - [ '1002' , 22.0 , 73.0 ] [ '1002' , 22.0 , 72.0 ], [ '1002' , 22.0 , 73.0 ] 9:00:50 - [ '1001' , 60.0 ] [ '1001' , 18.0 , 60.0 ] - 9:01:10 - [ '1001' , 62.0 ] - - 9:01:20 [ '1001' , 17.0 ] - [ '1001' , 17.0 , 62.0 ] [ '1001' , 17.0 , 60.0 ], [ '1001' , 17.0 , 62.0 ] 9:02:10 [ '1002' , 23.5 ] - - [ '1002' , 23.5 , null ]","title":"Stream join"},{"location":"docs/examples/table-and-store/","text":"var base_url = \"\"; Table and Store Provides introduction to in-memory tables and database backed stores that can be used to store events. For information on various types of stores, primary keys, indexes, and caching, refer examples related to Event Store Integration and Siddhi query guide . define stream TemperatureStream ( sensorId string , temperature double ); define table TemperatureLogTable ( sensorId string , roomNo string , temperature double ); @ store ( type = rdbms , jdbc . url = jdbc:mysql://localhost:3306/sid , username = root , password = root , jdbc . driver . name = com.mysql.jdbc.Driver ) define table SensorIdInfoTable ( sensorId string , roomNo string ); @ info ( name = Join - query ) from TemperatureStream as t join SensorIdInfoTable as s on t . sensorId == s . sensorId select t . sensorId as sensorId , s . roomNo as roomNo t . temperature as temperature insert into TemperatureLogTable ; define stream TemperatureStream ( sensorId string, temperature double); Defines TemperatureStream stream having sensorId and temperature attributes of types string and double . define table TemperatureLogTable ( sensorId string, roomNo string, temperature double); Defines in-memory TemperatureLogTable having sensorId , roomNo , and temperature attributes of types string , string , and double . @store(type= rdbms , jdbc.url= jdbc:mysql://localhost:3306/sid , username= root , password= root , jdbc.driver.name= com.mysql.jdbc.Driver ) Store annotation to back SensorIdInfoTable by a MySQL RDBMS with sid DB and SensorIdInfoTable table. define table SensorIdInfoTable ( sensorId string, roomNo string); Defines SensorIdInfoTable table. @info(name = Join-query ) from TemperatureStream as t join SensorIdInfoTable as s on t.sensorId == s.sensorId TemperatureStream with alias t joins with SensorIdInfoTable with alias s based on sensorId . select t.sensorId as sensorId, s.roomNo as roomNo t.temperature as temperature insert into TemperatureLogTable; Selects sensorId , roomNo , and temperature attributes from stream and table, and adds events to TemperatureLogTable . Event at table and store When SensorIdInfoTable table contains a recode [ 'aq-14' , '789' ], and when an event with values [ 'aq-14' , 35.4 ] is sent to TemperatureStream stream. The event will get converted and added to the TemperatureLogTable table as below. [ 'aq-14' , '789' , 35.4 ] Retrieving values from tables and stores The stored values can be retrieved by join tables and stores with the streams as in the Join-query depicted in the example, or using on-demand queries. The data in TemperatureDetailsTable can be retrieved via on-demand queries as below, using the On Demand Query REST API or by calling query() method of SiddhiAppRuntime . from TemperatureDetailsTable select *","title":"Table and store"},{"location":"docs/examples/time-rate-limit/","text":"var base_url = \"\"; Rate Limit Based on Time Output rate-limiting limits the number of events emitted by the queries based on a specified criterion such as time, and the number of events. This example provides some basic understanding of how rate limiting can be done based on time. Refer the Siddhi query guide for more information. define stream APIRequestStream ( apiName string , version string , tier string , user string , userEmail string ); define stream UserNotificationStream ( user string , apiName string , version string , tier string , userEmail string , throttledCount long ); @ info ( name = api - throttler ) from APIRequestStream # window . timeBatch ( 1 min , 0 , true ) select apiName , version , user , tier , userEmail , count () as totalRequestCount group by apiName , version , user having totalRequestCount == 3 or totalRequestCount == 0 insert all events into ThrottledStream ; @ info ( name = throttle - flag - generator ) from ThrottledStream select apiName , version , user , tier , userEmail , ifThenElse ( totalRequestCount == 0 , false , true ) as isThrottled insert into ThrottleOutputStream ; @ info ( name = notification - generator ) from ThrottleOutputStream [ isThrottled ] # window . time ( 1 hour ) select user , apiName , version , tier , userEmail , count () as throttledCount group by user , apiName , version , tier having throttledCount 2 output first every 15 min insert into UserNotificationStream ; define stream APIRequestStream (apiName string, version string, tier string, user string, userEmail string); Defines APIRequestStream stream which contains the events regarding the API request. define stream UserNotificationStream (user string, apiName string, version string, tier string, userEmail string, throttledCount long); Defines UserNotificationStream stream which contains the notification events. @info(name= api-throttler ) from APIRequestStream#window.timeBatch(1 min, 0, true) select apiName, version, user, tier, userEmail, count() as totalRequestCount group by apiName, version, user having totalRequestCount == 3 or totalRequestCount == 0 insert all events into ThrottledStream; This query generates events when API is throttled and released from throttling. It generates a throttling event if an API is called more than 3 times in a minute. @info(name= throttle-flag-generator ) from ThrottledStream select apiName, version, user, tier, userEmail, ifThenElse(totalRequestCount == 0, false, true) as isThrottled Create isThrottled flag based on the API request count insert into ThrottleOutputStream; @info(name= notification-generator ) from ThrottleOutputStream[isThrottled]#window.time(1 hour) select user, apiName, version, tier, userEmail, count() as throttledCount group by user, apiName, version, tier This query helps to generate notifications to the user based on the API usage. User is notified to upgrade the subscription tier if he is frequently throttled. having throttledCount 2 Find the users who are throttled more than 2 times in an hour output first every 15 min Notify the first occurrence when throttledCount 2 for every 15 minutes. insert into UserNotificationStream; API throttling use case is considered in this example to explain the time-based rate limiting. API request events arrive through APIRequestStream stream. If there are more than 3 API requests sent by a user within 1 minute then the subsequence requests are getting throttled. And, if a user gets throttled more than 2 times within 1 hour then there is a notification event generated to notify the user to consider upgrading the subscription tier. In this case, the notification events are get rate limited based on time. For example, within 15 minutes there will be only one notification for an API, tier and user combination. Input Sent below three events to APIRequestStream stream for three minutes (three events for every minute), [ 'Get-Weather' , '1.0.0' , 'Gold' , 'George' , 'george@gmail.com' ] [ 'Get-Weather' , '1.0.0' , 'Gold' , 'George' , 'george@gmail.com' ] [ 'Get-Weather' , '1.0.0' , 'Gold' , 'George' , 'george@gmail.com' ] Output After processing, the event arriving at UserNotificationStream will be as follows: [ 'Get-Weather' , '1.0.0' , 'Gold' , 'george@gmail.com' , 3 ]","title":"Time rate limit"},{"location":"docs/examples/transform-json/","text":"var base_url = \"\"; Transform JSON Provides examples on transforming JSON object within Siddhi. For all functions available to transform JSON see Siddhi Execution JSON . define stream InputStream ( jsonString string ); from InputStream select json : toObject ( jsonString ) as jsonObj insert into PersonalDetails ; from PersonalDetails select jsonObj , json : getString ( jsonObj , $ . name ) as name , json : isExists ( jsonObj , $ . salary ) as isSalaryAvailable , json : toString ( jsonObj ) as jsonString insert into OutputStream ; from OutputStream [ isSalaryAvailable == false ] select json : setElement ( jsonObj , $ , 0 f , salary ) as jsonObj insert into PreprocessedStream ; define stream InputStream(jsonString string); from InputStream select json:toObject(jsonString) as jsonObj Transforms JSON string to JSON object which can then be manipulated insert into PersonalDetails; from PersonalDetails select jsonObj, json:getString(jsonObj, $.name ) as name, Get the name element(string) form the JSON json:isExists(jsonObj, $.salary ) as isSalaryAvailable, Validate if salary element is available json:toString(jsonObj) as jsonString Stringify the JSON object insert into OutputStream; from OutputStream[isSalaryAvailable == false] select json:setElement(jsonObj, $ , 0f, salary ) as jsonObj Set salary element to 0 is not available insert into PreprocessedStream; Input Below event is sent to InputStream , [ { name : siddhi.user , address : { country : Sri Lanka , }, contact : +9xxxxxxxx } ] Output After processing, the following events will be arriving: OutputStream: [ { address :{ country : Sri Lanka }, contact : +9xxxxxxxx , name : siddhi.user } , siddhi.user , false , {\\ name\\ : \\ siddhi.user\\ , \\ address\\ : { \\ country\\ : \\ Sri Lanka\\ , }, \\ contact\\ : \\ +9xxxxxxxx\\ } ] PreprocessedStream: [ { name : siddhi.user , salary : 0.0 address : { country : Sri Lanka , }, contact : +9xxxxxxxx } ]","title":"Transform json"},{"location":"docs/examples/type-based-filtering/","text":"var base_url = \"\"; Type based Filtering This application demonstrates filter out events based on data type of the attribute define stream SweetProductionStream ( name string , amount int ); @ info ( name = ProcessSweetProductionStream ) from SweetProductionStream select instanceOfInteger ( amount ) as isAIntInstance , name , amount insert into ProcessedSweetProductionStream ; define stream SweetProductionStream (name string, amount int); Defines SweetProductionStream having information of name and amount @info(name= ProcessSweetProductionStream ) from SweetProductionStream select instanceOfInteger(amount) as isAIntInstance, true if amount is of int type name, amount insert into ProcessedSweetProductionStream; Input Below event is sent to SweetProductionStream , [ 'chocolate cake' , 'invalid' ] Output After processing, the event arriving at ProcessedSweetProductionStream will be as follows: [ false , 'chocolate cake' , 'invalid' ]","title":"Type based filtering"},{"location":"docs/examples/value-based-filtering/","text":"var base_url = \"\"; Value based Filtering This application demonstrates filter out events based on simple conditions such as number value, range or null type. define stream TemperatureStream ( sensorId string , temperature double ); @ info ( name = EqualsFilter ) from TemperatureStream [ sensorId == A1234 ] select * insert into SenorA1234TemperatureStream ; @ info ( name = RangeFilter ) from TemperatureStream [ temperature - 2 and temperature 40 ] select * insert into NormalTemperatureStream ; @ info ( name = NullFilter ) from TemperatureStream [ sensorId is null ] select * insert into InValidTemperatureStream ; define stream TemperatureStream ( sensorId string, temperature double); Defines TemperatureStream stream to process events having sensorId and temperature (F). @info(name = EqualsFilter ) from TemperatureStream[ sensorId == A1234 ] Filter out events with sensorId equalling A1234 select * insert into SenorA1234TemperatureStream; @info(name = RangeFilter ) from TemperatureStream[ temperature -2 and temperature 40] Filter out events where -2 temperature 40 select * insert into NormalTemperatureStream; @info(name = NullFilter ) from TemperatureStream[ sensorId is null ] Filter out events with SensorId being null select * insert into InValidTemperatureStream; Input Below events are sent to TemperatureStream , [ 'A1234' , 39] [ 'sensor1' , 35] [ null , 43] Output After processing, the following events will be arriving at each stream: SenorA1234TemperatureStream: [ 'A1234' , 39] only NormalTemperatureStream: [ 'sensor1' , 35] only InValidTemperatureStream: [ null , 43] only","title":"Value based filtering"}]}